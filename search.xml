<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【置顶】Learning Linux目录]]></title>
    <url>%2Flinux%2F20171111-learning-linux-toc%2F</url>
    <content type="text"><![CDATA[这里存放的我的Linux笔记，供自己随时搜索和使用，同时共享给需要的人。系统采用的是CentOS7.3（主要使用，2017.9.18日之后更换为CentOS7.4)和CentOS6.9（少量使用）。每篇文章都可以评论，使用GitHub、微信、QQ、微博等均可登录评论。 ✔：代表已完成。 ✘：代表未完成。 近期调整目录结构，如果跳转链接有问题，请留言给我进行调整。 前言： ✔文章所使用的markdown语法 ✔本文章集合所遵循的开源协议以及开源协议介绍(GPL,LGPL,BSD,MIT,Apache,CC) ✔工欲善其事必先利其器 —— Xmanager Enterprise 5 和 RealVNC 5/6介绍 ✔学会科学上网 —— 用搬瓦工和Shadowsocks搭梯子 第一章 Linux 基础Linux简介 ✔Linux发行版时间线2017年版本 Linux安装 ✔CentOS 6 安装步骤 ✔CentOS 7 安装步骤 Linux基础命令 ✔01-命令基本格式 ✔02-查看命令帮助 ✔03-命令信息相关命令 ✔04-别名的使用方法 ✔05-关机重启命令 ✔06-日期时间相关命令 ✔07-回显命令 ✔08-换行（LF）和回车（CR）详解 ✔09-延伸小技巧screen、script、scriptreplay ✔10-基础命令练习题 Linux文件和目录管理 ✔01-Linux系统层级结构标准 ✔02-历史命令用法 ✔03-bash快捷键 ✔04-Linux文件类型和文件相关命令 ✔05-inode、软硬链接 IO重定向和管道 ✔01-IO重定向 ✔02-管道 ✔03-IO重定向和管道练习题 用户用户组管理 ✔01-什么是用户和组 ✔02-用户和组文件 ✔03-用户和用户组的管理命令 ✔04-用户和组练习题 ✔05-实践：Linux用户、组和密码相关文件被破坏如何恢复系统 权限和ACL访问控制 ✔01-权限 ✔02-特殊权限 ✔03-访问控制列表（ACL） ✔04-权限和ACL练习题 文本处理和正则表达式 ✔01-文本处理工具(cut、sort、tr、grep等) ✔02-正则表达式 ✔03-文本处理练习题 vim用法 ✔01-vim简明教程(附快速记忆方法) rpm、yum、编译安装程序 ✔01-程序包管理工具rpm ✔02-程序包管理工具yum ✔03-打包压缩命令tar、zip、split ✔04-编译安装make Shell基础 ✔01-全局变量和局部变量 ✔02-位置变量和退出码 ✘03-运算 ✔04-条件测试 ✔05-bash配置相关 第一章测试题 ✔第一阶段测试题 第二章 Linux磁盘、网络、进程管理磁盘管理 ✔磁盘分区管理 ✔磁盘阵列（RAID） ✔HPE x86服务器硬RAID做法 ✔逻辑卷管理（LVM） 网络管理 ✔网络模型 ✔CentOS6网络命令 ✔CentOS6网络配置 ✔CentOS7网络配置 ✔网卡绑定bonding 进程管理 ✔进程 ✔kill命令 ✔计划任务at、cron shell进阶 ✔流程 ✔函数 ✔数组和关联数组 ✔字符串处理 ✔sed ▬awk 推荐看《AWK程序设计语言》的中文开源翻译 20170704——练习题 ✔Shell流程练习题 ✔Shell函数练习题 第三章 LFS编译安装Linux启动流程 ✔CentOS5、CentOS6启动流程 ✘CentOS7启动流程 待更新 ✔破坏实验–破坏5的initrd、6和7的initramfs文件，然后恢复 LFS编译安装 ✔Linux From Scratch 8 之一–构建前准备主机系统 ✔Linux From Scratch 8 之二–创建LFS的分区，下载源码包 未完待续 第四章 Linux安全管理SELinux 待更新 OpenSSL ✔CA认证和证书 SSH ✔SSH客户端命令 ✔SSH转发 ✔SSH服务端 Sudo ✔Sudo TCP_Wrapper ✔TCP_Wrapper应用级防火墙 PAM ✔PAM安全认证模块 第五章 Linux自动化安装 ✘Kickstart详解 ✔自制Kickstart光盘 ✔DHCP服务 ✔TFTP服务和PXE功能 ✔Cobbler——无人值守安装多种版本多种配置操作系统 ✔Cobbler——添加网络同步仓库（Reposync用法) 第六章 Linux基础服务日志管理服务✘ DNS服务（bind的使用） ✔01-什么是DNS ✔02-DNS如何工作 ✔03-DNS记录类型 ✔04-bind配置文件 ✔05-bind辅助工具 ✔06-bind主从复制 ✔07-DNS实战配置指南（转载） HTTP服务 ✘HTTP服务简介 ✘httpd2.2 ✘httpd2.4 ✘LAMP搭建 ✔LAMP 一键安装脚本 FTP服务 ✘ NFS服务 ✘ Samba服务 ✘ 防火墙服务 ✔防火墙Netfilter/iptables 第七章 负载均衡和高可用集群服务Vagrant前期实验，用Vmware和Virtualbox手动安装使用就够了，但是后面会用到很多机器做实验，故推荐使用Vagrant。 ✔Vagrant–快速搭建实验环境利器自制的 Vagrant box —— longdream/centos7 LVS ✔LVS原理 ✔LVS/NAT实验 ✔LVS/DR实验 Nginx ✔01-Nginx简介 ✔02-IO模型 ✔03-Nginx的主配置文件 ✔04-Nginx的默认http配置 ✔05-ngx_http_core_module详解 ✔06-Nginx的HTTP相关的杂项模块 ✔07-ngx_http_proxy_module详解 ✔08-ngx_http_fastcgi_module详解 ✔09-ngx_http_upstream_module详解 ✔10-ngx_stream*_module详解 HAProxy ✔01-HAProxy简介 ✔02-HAProxy简单配置和基本用法 ✔03-HAProxy 具体配置参数 ✔04-HAProxy统计页面 未完待续 Tomcat ✔01-Tomcat基础知识 ✔02-Tomcat设置管理页面 ✘Tomcat基本配置 ✘Tomcat高级配置 Keepalived ✔01-Keepalived介绍 ✔02-Keepalived配置 ✔03-Keepalived各种模式配置 ✔04-单网络双活模式的Keepalived+LVS配置 ✔05-双网络双活模式的Keepalived+Nginx配置 第八章 缓存、消息队列Memcached Memcached简介Memcached配置详解✘Tomcat搭配Memcached Varnish ✘Varnish原理 ✘Varnish配置 RabbitMQ**✘ 第九章 数据库SQL基础 ✔01-数据库和SQL简介 ✔02-MariaDB安装 ✔03-SQL语句简单示例 ✔04-SELECT语句 ✔05-INSERT、DELETE、UPDATE语句 MySQL基础 ✘数据管理系统基础 ✘MySQL存储引擎 ✘MySQL事务和隔离级别 ✘MYSQL账号和权限管理 ✘MySQL索引 ✘MySQL日志 ✔ MySQL备份恢复（mysqldump、xtrabackup） ✘MySQL扩展机制概述 ✘MySQL复制实践 ✘MySQL复制及读写分离 ✘MySQL主节点高可用 OracleMongoDBRedis第十章 自动化部署Git、GitHub、GitLab ✔Git和GitHub笔记） JekinsAnsible ✘Ansible简介✘Ansible Roles Puppet ✘ 第十一章 监控Zabbix ✔01-Zabbix Server安装指南） ✔02-Zabbix Agent 安装指南和 Zabbix Server 设置自动发现 ✔03-Zabbix Server设置主机监控 ✔04-Zabbix Web 中文字体显示问题 ✔05-Zabbix trigger（触发器）设置 ✔06-Zabbix Actions(动作）设置 ✘Zabbix SNMP 监控设置 ✘Zabbix JMX 监控设置 ✘Zabbix IPMI 监控设置 ✘Zabbix Proxy 设置 ✘Zabbix自定义监控脚本 Open-FalconElastic Stack第十二章 虚拟化KVMOpenStack✘KVM ✔OpenStack简介 ✔01-OpenStack(Pike)安装实战——环境准备 LXCDockerKubernetes]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>TOC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【置顶】Learning Python3 目录]]></title>
    <url>%2Fpython%2F20171111-learning-python3-toc%2F</url>
    <content type="text"><![CDATA[**此文章集合是一个笔记，记录了我在学习Python3中学到的东西。 所有文章代码如无特殊说明，Python 版本均用 3.6.2。 ✔：表示已更新完毕 第一部分 Python基础第1章 环境搭建 ✔1.1 macOS下Python多版本控制软件的安装：pyenv、pyenv-virtualenv ✔1.2 CentOS下Python多版本控制软件的安装：pyenv、pyenv-virtualenv ✔1.3 开发环境配置 第2章 基础语法 ✔2.1 变量与命名规则 ✔2.2 运算符 ✔2.3 程序控制结构 第3章 内置数据结构 ✔3.1 数字（Numbers） ✔3.2 列表（Lists） ✔3.3 元组（Tuples） ✔3.4 字符串（Strings） ✔3.5 切片（Slice） ✔3.6 解包（Unpacking） ✔3.7 格式化（Formatting） ✔3.8 集合（Sets） ✔3.9 字典（Dictionarys） ✔3.10 bytes和bytearray ✔ 3.11 解析式（Comprehensions) ✔ 3.12 简单了解 可迭代对象(Iterables)、迭代器(Iterators)、生成器(Generators) ✔ 3.13 详解 可迭代对象(Iterables)、迭代器(Iterators)、生成器(Generators) 第4章 函数 ✔ 4.1 函数的基本定义与函数调用 ✔ 4.2 函数的执行过程 ✔ 4.3 函数的参数 ✔ 4.4 函数注解（类型示意） ✔ 4.5 递归函数 ✔ 4.6 生成器函数 ✔ 4.7 高阶函数、闭包、偏函数、柯里化、匿名函数 ✔ 4.8 装饰器 第二部分 Python高级第5章 面向对象和类 ✔ 5.1 类（Class）的定义 ✔ 5.2 OOP的三大特征：封装、继承、多态 ✔ 5.3 多继承与MRO算法 ✔5.4 魔术方法/特殊方法(Magic Methods) 第6章 异常处理 ✔6.1 异常（Exception） 第7章 模块化 ✔7.1 模块化（Modularization） 第8章 数据结构 ✔8.1 单链表（Singly Linked List） ✔8.2 栈（stack） ✔8.3 stack应用–表达式解析 ✔8.4 stack应用-规则解析 ✔8.5 队列（queue） ✔8.6 哈希（hash） ✔8.7 树（tree） ✔8.8 堆（heap） ——— 华丽分割线 2017.10.16 ———- 第三章 Web开发第9章 实现一个简单的Web框架]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>TOC</tag>
        <tag>Python3</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈目前技术框架下，期货公司可用的新技术]]></title>
    <url>%2Fessay%2F20171101-01-futures-new-tech%2F</url>
    <content type="text"><![CDATA[不知道下面场景是不是大部分期货公司的情况： 服务器崩了，要装一台新机器，用的kickstart光盘或u盘……亦或是，更原始的人肉下一步下一步？ 什么，你应用所依赖的rpm包是上传到Linux里，一个个安装的？不是吧…… 哦，用的yum的安装方式，但是竟然是用的mount挂载光盘或iso的方式，没有内网yum repo么？ 监控，竟然还在用老掉牙的Hostmonitor…… 系统出现问题了，要一条条的去查日志，对于日志，有没有一个更好的一个方法，做汇总、过滤、搜索？如何对日志做一个更好的监控？ 虚拟化，用的VMware ESXi或者好一点用VMware vCenter, 但是，你有没有被VMWare收取“保护费”呢？ 以上的一些问题，在互联网时代，已经有很多比较好的技术可以解决了。 本文下面从互联网的技术角度，来尝试解决上述的一些问题，如有浅薄之处和纰漏之处，请广大期货圈技术朋友留言点出。 注：以下技术并不是完全掌握，个人只能算刚入门的毛小子，尤其是Elastic Stack和OpenStack……但是，我会努力朝这个方向去学习和研究。 目录： Cobbler —— Linux自动化安装、内网yum repo Ansible —— 自动化部署 GitLab —— 版本控制 Zabbix —— 监控 Elastic Stack —— 日志监控 OpenStack —— 企业私有云解决方案 Cobbler —— Linux安装、内网yum仓库 Cobbler实现了各种版本Linux的自动分区、系统安装、包安装，还可以作为内网的yum源使用。 Cobbler，底层基于DHCP、PXE、TFTP、HTTP、Kickstart等技术的融合，来提供一个对外自动安装操作系统的服务。 当和Cobbler同网段的服务器启动，首先出现的界面是这样的： 上面那个图是我做实验的时候的菜单图，当然也可以根据不同操作系统和不同交易系统的不同应用来做不同的ks配置。 菜单和系统完全可以自定义，读秒时间也可以自定义。每一条菜单对应的就是一个kickstart的.ks文件（指向对应的镜像），如果有不同的系统、分区、安装包，都可以写为一个不同配置的ks文件。举个菜单的例子，可以根据不同的操作系统、交易系统、组件，来定制化安装界面，例如这样： 1234567891011(local)OL6.8-CTP-dbRHEL6.8-CTP-tradeRHEL6.8-CTP-risk...OL6.8-Femas_2.0-dbCentOS6.8-Femas_2.0-front...RHEL5.10-Sunguard_v6-sybaseRHEL5.10-Sunguard_v6-app... 注：此处RHEL指的是Red Hat Enterprise Linux, OL指的是Oracle Linux 另外我们可以把Cobbler作为一个本地yum仓库，可以直接从网上每天同步yum源，而内网服务器，可以把yum的baseurl指向cobbler，举个CentOS7 的yum repo文件的例子，192.168.100.222就是cobbler所在的服务器地址： cat /etc/yum.repos.d/centos7-all-in-one.repo 1234567891011121314151617181920212223242526272829303132333435[centos7.4-base]name=centos7.4-basebaseurl=http://192.168.100.222/cobbler/repo_mirror/centos7.4-baseenabled=1priority=99gpgcheck=0[centos7.4-updates]name=centos7.4-updatesbaseurl=http://192.168.100.222/cobbler/repo_mirror/centos7.4-updatesenabled=1priority=99gpgcheck=0[centos7.4-extras]name=centos7.4-extrasbaseurl=http://192.168.100.222/cobbler/repo_mirror/centos7.4-updatesenabled=1priority=99gpgcheck=0[centos7-zabbix-3.4]name=centos7-zabbix-3.4baseurl=http://192.168.100.222/cobbler/repo_mirror/centos7-zabbix-3.4enabled=1priority=99gpgcheck=0[centos7-gitlab-ce]name=centos7-zabbix-cebaseurl=http://192.168.100.222/cobbler/repo_mirror/centos7-gitlab-ceenabled=1priority=99gpgcheck=0# 省略其他yum repo。。。。 tips: 无论是ks文件，还是repo文件，都可以用git来做版本控制，可以随时更改和添加，关于git，后面再说。 部署好Cobbler服务之后，每当我们添加一台新的服务器，需要安装操作系统的时候，只要做好raid，开机，选择对应的操作系统菜单，就可以一键自动安装了。 Ansible —— 自动化部署 Ansible是一个基于Python研发的自动化部署工具，实现了实现了批量系统配置、批量程序部署、批量运行命令等功能。利用Python的jinja2模板引擎，可以定义配置的jinja2模板，可以实现同一组类型应用根据主机的参数不同来进行变量替换。 在Ansible中，简单的一些批量操作可以使用Ad-Hoc，例如批量升级某个补丁包；还可以使用Playbook功能，实现更复杂的安装和配置，例如交易系统内的一个复杂组件的配置。 Playbook使用yaml语法，非常便于书写。 Ansible最大的一个好处是agentless，无需安装客户端，直接通过ssh来连接，并操作相应的服务器。对于大部分期货公司来说，几百台服务器，用Ansible就足够了。当然，Ansible也支持客户端模式，只是并没有无客户端模式那么好用。 我们可以把交易系统各种组件定义成一个个roles，然后书写Playbook去实现各种组件的自动配置和部署功能。 当然，应用和配置，我们也需要做到版本控制，当升级组件时候有问题了，可以更好回滚。 tips: 如果是上千台服务器的话，这时候Ansible就有点捉襟见肘了，这时候SaltStack更好用一些，不过期货公司暂时没有这么大量的服务器需要批量操作，所以SaltStack暂且不说。 Zabbix —— 监控 Zabbix是目前企业中，使用量最高的监控软件了。（不要再用HostMonitor了……真的） Zabbix可以像Hostmonitor一样设定各种监控，并可以设定触发器阈值，可以自定义各种图表： 并且还可以选择一些重要监控，放到Dashboard里进行大屏展示，并且还可以实现slide show(幻灯片切换），有好几套界面风格可供选择（个人很喜欢上图中的Dark主体）。 当然，还有更炫酷的大屏展示，那就是……Grafana（炫酷到没有朋友）： 还可以可以自定义邮件报警、短信报警、微信报警等功能： GitLab —— 版本控制 Git工具，在互联网公司所用甚广，例如著名的”程序员大型同性社交网站“——GayHub，哈哈，玩笑一下，是代码托管网站GitHub，在国内，也有类似的网站，如Coding.net，都是比较流行的远程Git仓库。 当然，很多期货公司可能没有开发，用不上这个东西。但其实不然，运维也可以很好的使用。 而在企业内部的话，为了方便建立私有Git仓库，一般会使用GitLab，其实在程序员中用的比较多，而对于运维人员来说，也是值得去研究的，与其他工具结合使用，可以做到很好的版本控制。 如图，我们可以建立起各个交易系统的Git仓库，在应用和配置方面做到更好的一个版本控制功能，在系统升级和回退的时候，有一个更快速和高效的方法。 在多组件升级的时候，Ansible结合GitLab,可以做到一个快速升级、快速回退。在期货公司，在有了夜盘之后，很多时候都是在下午收盘后，晚上开盘前做升级操作，时间特别紧张，如果是能做到一个快速回退，那么升级的风险度可以降到很低的程度。 当然，升级和回退不是简单的加减法，这就需要运维人员编写Playbook的时候，需要考虑到方方面面的问题。 另外GitLab可以更好的来管理技术文档。系统的操作，还有自己的一些Idea，都可以在GitLab上写出来，GitLab同GitHub一样，支持markdown语法的写作。 甚至用来管理技术合同都可以。把合同的扫描件，做一个分类归档，放在GitLab里，可以按照日期或tags进行查找，方便快捷（当然这个要设为私有仓库啦）。 Elastic Stack在期货公司组件出现问题的时候，我们需要一个个手工的去排查日志，导致定位问题非常慢。有没有一个日志汇总的工具，可以做聚合、索引、搜索呢？使之查询更快？ 有的，Elastic Stack。 Elastic Stack的框架： beats：⽤于轻量级⽇志采集，⽀持⽂件采集，系统数据采集，特定中间件数据采集等logstash：⽤于⽇志结构化，标签化，⽀持DSL⽅式将数据进⾏结构化elasticsearch：⽤于提供⽇志相关的索引，使得⽇志能够有效的检索kibana：⽤于提供⽇志检索，特定metric展示的⾯板，⽅便使⽤的UIx-pack：⽤于监控与预警相关的组件，可以集成到es中，kibana有特定的⾯板⽤于展示UI 对于期货公司的日志量来说，基本都不用做分布式，一台足够承担，如果日后需要扩展，可以直接横向分布式扩展，非常方便。 稍微说一下虚拟化很多公司会用VMware ESXi，偶尔有公司会去用VMware vCenter 私有云，但随着国家开始正版化的推进，出于成本考虑，更多的公司开始使用开源软件。如KVM，甚至是OpenStack私有云方案（少数研发能力较强的）。当我们还在感叹OpenStack的部署和二次开发很难的时候，互联网已经转向了Docker和Kubernetes，各种应用秒级部署…… 总结：在一个证券和期货群里，很多做了10来年的老运维说道，现在技术发展越来越快了，再往后，自己不走管理层，可能就是落后淘汰的那一批了……时代总是在变化的，真心希望我们不要成为被DevOps或AIOps淘汰的那一批人。 DevOps发展起来后，第一批失业的，可能就是公司里的初级甚至中级运维……当大部分底层工作，都自动化了，初中级运维的危机也就来了，而那时候，只能选择去学习新的知识，新的技术，要么只能被淘汰。 —— 传统运维之殇 个人一点小分享，仅供茶余饭后谈资。个人也是刚学互联网的一些技术。 另外，本人最近在找工作，有合适的公司可以介绍给我，万分感谢。]]></content>
      <categories>
        <category>essay</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[01-OpenStack(Pike)安装实战——环境准备]]></title>
    <url>%2Flinux%2F20170930-01-openstack-installation-prepare%2F</url>
    <content type="text"><![CDATA[实验准备：（1）准备一台内存比较大的机器。 因为实验需要内存比较大，找了一台型号为 HP 380 Gen9 物理服务器，做为OpenStack测试机器（拥有两颗6核12线程CPU，256G内存，说实话，做实验奢侈了……） （2）物理机是Windows，切在远程机房，不容易修改操作系统，再加上很多人熟悉VMWare WorkStation，故采用Vware Workstation Pro 14做为虚拟机软件用来做OpenStack的实验。 （3）网络编辑器设置两个网络。 NAT模式（VMnet8）设置成192.168.10.0/24网络 仅主机模式（VMnet1）设置成192.168.20/24网络 （4）创建一个CentOS7.4的模板系统。 内存16GB（不够可调小，最低2、3G) CPU分6核（和物理CPU相同）并开启虚拟化 Intel VT/EPT 或 AMD-V/RVI(V) 硬盘分256GB（随便分） 光盘挂载CentOS 7.4 的ISO镜像 4个网卡（两个NAT，两个仅主机，后续做bond） 移除打印机，其余默认 开启虚拟机，选择Install CentOS 7,按Tab键，在quiet后面加个空格后，传递两个内核参数net.ifnames=0 biosdevname=0，然后回车启动安装，这样网卡名就是eth0这种的，而不是ens之类的。（习惯eth了）： 时区选择上海，安装选择最小化安装，分区如下分区： 创建完虚拟机后，copy下面脚本到虚拟机，（注意不要运行）。 init.sh 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103# 关闭和停止下次启动SELinux、firewalld、NetworkManagersetenforce 0sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/configsystemctl stop firewalldsystemctl disable firewalldsystemctl stop NetworkManagersystemctl disable NetworkManager# 镜像源更换源为阿里云的源，添加阿里云的epel源，安装一些必要的包。mv /etc/yum.repos.d/CentOS-Base.repo&#123;,.bak&#125;curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repocurl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo# 安装一些必须的工具yum install -y chrony vim tree wget bash-completion net-tools tcpdump ab expect lrzsz lsof screen telnet# 配置时间同步服务chrony（如果机房内有时间服务器，要配置/etc/chrony.confsystemctl start chronydchstemctl enable chronyd# 安装OpenStack Pike 源yum install -y centos-release-openstack-pikeyum install -y https://rdoproject.org/repos/rdo-release.rpmyum install -y python-openstackclient openstack-selinux# 俩俩绑定网卡。eth0,eth1绑定在bond0上；eth2、eth3绑定在bond1上。cat &gt; /etc/sysconfig/network-scripts/ifcfg-bond0 &lt;&lt;EOFBOOTPROTO=staticNAME=bond0DEVICE=bond0ONBOOT=yesBONDING_MASTER=yesBONDING_OPTS="mode=1 miimon=100"IPADDR=192.168.10.21PREFIX=24GATEWAY=192.168.10.2DNS1=223.5.5.5DNS2=223.6.6.6EOFcat &gt; /etc/sysconfig/network-scripts/ifcfg-bond1 &lt;&lt;EOFBOOTPROTO=staticNAME=bond1DEVICE=bond1ONBOOT=yesBONDING_MASTER=yesBONDING_OPTS="mode=1 miimon=100"IPADDR=192.168.20.21PREFIX=24EOFcat &gt; /etc/sysconfig/network-scripts/ifcfg-eth0 &lt;&lt;EOFBOOTPROTO=staticNAME=eth0DEVICE=eth0ONBOOT=yesNM_CONTROLLED=noMASTER=bond0USERCTL=noSLAVE=yesEOFcat &gt; /etc/sysconfig/network-scripts/ifcfg-eth1 &lt;&lt;EOFBOOTPROTO=staticNAME=eth1DEVICE=eth1ONBOOT=yesNM_CONTROLLED=noMASTER=bond0USERCTL=noSLAVE=yesEOFcat &gt; /etc/sysconfig/network-scripts/ifcfg-eth2 &lt;&lt;EOFBOOTPROTO=staticNAME=eth2DEVICE=eth2ONBOOT=yesNM_CONTROLLED=noMASTER=bond1USERCTL=noSLAVE=yesEOFcat &gt; /etc/sysconfig/network-scripts/ifcfg-eth3 &lt;&lt;EOFBOOTPROTO=staticNAME=eth3DEVICE=eth3ONBOOT=yesNM_CONTROLLED=noMASTER=bond1USERCTL=noSLAVE=yesEOFsystemctl restart network tips：可以更改两个bond的ip地址为传入的参数，这样可移植性更高，只要把模板复制到其他机器，运行脚本时候指定ip参数，即刻立即设定网络。 关闭虚拟机，拍摄快照，根据快照创建多个5个以上虚拟机。然后分别修改脚本里面的ip值，创建5个不同ip的服务器。 Node1： Bond0：192.168.10.21，Bond1：192.168.20.21Node2： Bond0：192.168.10.22，Bond1：192.168.20.22Node3： Bond0：192.168.10.23，Bond1：192.168.20.23Node4： Bond0：192.168.10.24，Bond1：192.168.20.24Node5： Bond0：192.168.10.25，Bond1：192.168.20.25 如果要添加更多的节点，就顺序添加Node，更改ip即刻。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Openstack</tag>
        <tag>Pike</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenStack简介]]></title>
    <url>%2Flinux%2F20170930-00-openstack-introduction%2F</url>
    <content type="text"><![CDATA[OpenStack项目是用于所有类型云的开放源码云计算平台，旨在实现简单，大规模扩展，功能丰富。来自世界各地的开发商和云计算技术专家创建了OpenStack项目。 OpenStack 通过一系列相互关联的服务提供了基础设施即服务（IaaS）解决方案。每个服务都提供一个便于集成的 应用程序编程接口（API）。根据您的需要，您可以安装部分或全部服务。 OpenStack服务下表描述构成OpenStack体系结构的OpenStack服务： Service Project name Description Dashboard Horizon 提供基于Web的自助服务门户与基础OpenStack服务进行交互，例如启动实例，分配IP地址和配置访问控制。 Compute service Nova 管理OpenStack环境中计算实例的生命周期。责任包括产生，调度和虚拟机的退役。 Networking service Neutron 为其他OpenStack服务启用网络连接即服务，如OpenStack Compute。为用户提供一个API来定义网络及其附件。具有支持许多流行网络供应商和技术的可插拔架构。 Object Storage service Swift 通过RESTful，基于HTTP的API存储和检索任意非结构化数据对象。它的数据复制和横向扩展架构具有高度的容错能力。它的实现不像具有可安装目录的文件服务器。在这种情况下，它将对象和文件写入多个驱动器，确保数据跨服务器集群复制。 Block Storage service Cinder 为运行的实例提供持久性块存储。其可插拔驱动程序架构有助于块存储设备的创建和管理。 Identity service Keystone 为其他OpenStack服务提供认证和授权服务。提供所有OpenStack服务的端点目录。 Image service Glance 存储和检索虚拟机磁盘映像。OpenStack Compute在实例配置期间利用这一点。 Telemetry service Ceilometer 监控和计量OpenStack云计算，基准测试，可扩展性和统计目的。 Orchestration service Heat 通过使用本地HOT模板格式或AWS CloudFormation模板格式，通过OpenStack本机REST API和兼容CloudFormation的Query API来协调多个复合云应用程序。 Database service Trove 为关系数据库和非关系数据库引擎提供可扩展和可靠的云数据库即服务功能。 Data Processing service Sahara 通过指定参数，如Hadoop版本，集群拓扑和节点硬件细节，提供在OpenStack中配置和扩展Hadoop集群的功能。 OpenStack架构概念架构 下图显示了OpenStack各服务之间的关系： 逻辑架构 要设计，部署和配置OpenStack，管理员必须了解逻辑架构。 如上面概念架构所示，OpenStack由几个独立的部分组成，命名为OpenStack服务。所有服务通过通用身份服务（Identity service）进行身份验证。个人服务通过公共API互相交互，除非需要特权管理员命令。 在内部，OpenStack服务由几个进程组成。所有服务至少有一个API进程，它监听API请求，对它们进行预处理，并将它们传递到服务的其他部分。身份服务除外，实际工作由不同的进程完成。 对于一个服务的进程之间的通信，使用AMQP消息代理。服务的状态存储在数据库中。在部署和配置OpenStack云时，您可以选择几种消息代理和数据库解决方案，例如RabbitMQ，MySQL，MariaDB和SQLite。 用户可以通过Horizo​​n Dashboard实现的基于Web的用户界面，通过命令行客户机访问OpenStack，并通过浏览器插件或curl等工具发出API请求。对于应用程序， 可以使用多个SDK。最终，所有这些访问方法都会向各种OpenStack服务发出REST API调用。 下图显示了OpenStack云中最常见但并非唯一的架构：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>OpenStack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06-Zabbix Actions(动作）设置]]></title>
    <url>%2Flinux%2F20170927-06-zabbix-actions-config%2F</url>
    <content type="text"><![CDATA[当我们触发器触发了问题，要通知运维人员去解决问题，动作（Actions)就是用来通知运维人员的设置。 动作分为几个定义块： （1）触发动作的条件：（这里的条件是：1 不在维护状态；2 触发了触发器 “网络进站包数&gt;10000”) （2）触发了动作后，相应的操作：（通过MyEmail(zabbix@yulongjun.com)发送给用户Admin（ops@yulongjun.com)。（此段邮箱设置，在第一节Zabbix Server安装指南上有） 测试一下效果： 1hping 192.168.0.10 右上角有报警提示，并且有报警音： 仪表盘上也有报警信息，还可以看到报警邮件已经发送： 去邮箱查看，确实有报警邮件：（因为阀值设置的比较大，hping一会达到阀值，一会儿不到阈值，所以会看到问题和解决两种邮件。）]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-Zabbix trigger（触发器）设置]]></title>
    <url>%2Flinux%2F20170927-05-zabbix-triggers-config%2F</url>
    <content type="text"><![CDATA[设置一个监控项–进站包数，当进站包数&gt;50触发器报警。 先设置一个进站包数的监控项（item)： 针对进站包数设置触发器（tragger），&gt;5000为警告，&gt;10000为严重： 设置图形（graph） 记得勾选查看触发器，我们预览一下，可以看到两条虚线，就是触发器的阈值线，黄色的是上面定义的告警线，红色的是严重线。 测试一下效果： 1hping --flood 192.168.0.10 我们可以看到有问题告警： 停掉hping，问题告警变绿，变成已解决：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-Zabbix Web 中文字体显示问题]]></title>
    <url>%2Flinux%2F20170927-04-zabbix-chinese-display%2F</url>
    <content type="text"><![CDATA[Zabbix中文字体问题，我们可以通过拷贝中文字体文件到zabbix的目录来实现。 上传一个中文字体文件到/usr/share/zabbix/fonts下，这里我拷贝的是windows下的微软雅黑msyh.ttf。 tips：只能支持ttf格式的字体文件，不能支持ttc的，本来copy了macOS下的一个苹方简体（PingFang.ttc)，结果不支持ttc类型的字体文件，最后换成微软雅黑（msyh.ttf)字体 原来配置文件里面配置的是graphfont.ttf，我们grep一下配置文件/usr/share/zabbix/include/defines.inc.php发现有两条记录： 1cat /usr/share/zabbix/include/defines.inc.php |grep graphfont 把这两个替换了就好： 1sed -i &apos;s/graphfont/msyh/g&apos; /usr/share/zabbix/include/defines.inc.php OK，替换完毕，可以grep验证一下： 刷新一下页面，即可看到中文正常显示：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-Zabbix Server设置主机监控]]></title>
    <url>%2Flinux%2F20170927-03-zabbix-host-monitor-config%2F</url>
    <content type="text"><![CDATA[设置主机的监控项(1) 设置CPU用户使用率，键值为sys.cpu.util[all,sys,avg1] 由于用的百分号作为单位，所以要在进程里改为100倍： (2)设置CPU用户使用率，同上，只是把键值变为sys.cpu.util[all,user,avg1] 可以看到创建的两个监控项： 创建监控图形（grah)创建如macOS上的iStat Menus的效果： 设置： 最终效果： tips：图中中文出现乱码，是由于字体库的原因，具体解决方法可以查看下一节。http://www.yulongjun.com/linux/20170927-04-zabbix-chinese-display/]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-Zabbix Agent 安装指南和 Zabbix Server 设置自动发现]]></title>
    <url>%2Flinux%2F20170927-02-zabbix-agent-installation%2F</url>
    <content type="text"><![CDATA[Zabbix Agent分为两种模式，被动模式（Passive)和主动模式（ 我们实验在node1.yulongjun.com 和node2.yulongjun.com上分别配置Zabbix Agent。 yum 安装 Zabbix Agent123rpm -ivh http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-1.el7.centos.noarch.rpmyum install -y zabbix-agent 如果不能联网，可以去官网下载rpm包安装。repo地址：http://repo.zabbix.com/zabbix/可以找到当前你所用的版本的agent。写这篇文章的时候，Zabbix的最新版本是3.4.2：CentOS7的相关包下载地址：http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-agent-3.4.2-1.el7.x86_64.rpm如果后续有更新，可以自行查找。 Agent可以设置被动检查模式或者主动检查模式： node1上，vim /etc/zabbix/zabbix_agentd.conf： 123456789101112131415161718192021##### Passive checks related 被动模式检查相关# Zabbix Server地址,如果想被Zabbix Proxy发现，可以添加zabbix-proxy的地址。Server=zabbix.yulongjun.com#Server=zabbix.yulongnjun.com, zabbix-proxy.yulongjun.com# 监听端口，默认10050ListenPort=10050# 限定Server可以监控的IP地址，可以用逗号隔开多个。默认0.0.0.0，即监控所有ip地址。# ListenIP=0.0.0.0# 代理进程并发数，如果监控的选项比较多，可以改大StartAgents=3##### Active checks related 主动模式检查相关## Zabbix Server地址ServerActive=zabbix.yulongjun.com#ServerActive=zabbix.yulongjun.com,zabbix-proxy.yulongjun.com## 本机主机名，也是在Zabbix Server上显示的名字Hostname=node1.yulongjun.com node2的配置，更改相应的设置为node2.yulongjun.com和192.168.0.20 启动服务，下次开启自启动： 12systemctl start zabbix-agent.servicesystemctl enable zabbix-agent.service 配置服务器的自动发现功能配置–&gt;自动发现，我们看到默认有一个默认本地网段（Local Network)的规则，我们直接用就可以了，，点状态那里打开为启用： 在dashboard上就可以看到有发现两台主机： 点开上图Local network可以看到主机： 发现的主机，我们要添加到主机信息里，需要设置每一台主机的监控项。 设置主机组，添加主机node1和node2分别安装Nginx，我们就对Nginx进行监控： 123yum install nginxsystemctl start nginxsystemctl enable nginx 在配置–&gt;主机选项卡，点右上角创建主机：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-Zabbix Server安装指南]]></title>
    <url>%2Flinux%2F20170927-01-zabbix-server-installation%2F</url>
    <content type="text"><![CDATA[安装 Zabbix 的 yum 仓库在所有机器上安装zabbix的yum仓库文件： 1rpm -ivh http://repo.zabbix.com/zabbix/3.4/rhel/7/x86_64/zabbix-release-3.4-3.el7.centos.noarch.rpm 安装配置MariaDB数据库并启动数据库可以装在和 Zabbix Server 同一台机器上（zabbix.yulongjun.com)，也可以装在不同的机器上，这里直接装在同一台机器上了： 123yum install -y mariadb-serversystemctl start mariadbsystemctl enable mariadb 在 Zabbix Server 上创建数据库zabbix、用户zabbix、运行建库脚本schema.sql、schema.sql、images.sql、data.sql（如果是Zabbix Proxy，就不用运行后两个） 123456789# shell下运行：shell&gt; mysql -uroot# mysql命令行里运行：mysql&gt; create database zabbix character set utf8 collate utf8_bin;mysql&gt; grant all privileges on zabbix.* to zabbix@localhost identified by 'zabbix';mysql&gt; quit;# shell下运行：shell&gt; cd /usr/share/doc/zabbix-server-mysql-3.4.2shell&gt; zcat create.sql.gz | mysql -uroot zabbix # zcat出来的脚本写入zabbix库 安装配置Zabbix Server并启动Server后端在zabbix.yulongjun.com节点上,安装Zabbix Server相关： 1yum install zabbix-server-mysql zabbix-web-mysql zabbix-get Zabbix 的配置主要分为下面几段： 12345grep "^#####" /etc/zabbix/zabbix_server.conf############ GENERAL PARAMETERS ############################# ADVANCED PARAMETERS ####################### LOADABLE MODULES ############## TLS-RELATED PARAMETERS ####### 通用参数、高级参数、加载的模块、TLS加密通信相关配置 12cp /etc/zabbix/zabbix_server.conf&#123;,.bak&#125;vim /etc/zabbix/zabbix_server.conf 主要更改通用参数（GENERAL PARAMETERS）： 下面是通用参数的说明： 12345678910111213141516171819############ GENERAL PARAMETERS #################ListenPort=10051 # trapper监听端口，一般不变SourceIP=192.168.0.222 # 对外服务ip，这里要设置一下，要不会开放给所有ipLogType=file # 日志格式，默认为file，可设置为system（syslog）、file（需要定义LogFile参数）、console（标准输出）LogFile=/var/log/zabbix/zabbix_server.log # 上面定义为LogFile，这里就要定义路径了。LogFileSize=50 # 日志到多大滚动，0表示不滚动，一般需要设置下，最大为1024（MB)PidFile=/var/run/zabbix/zabbix_server.pid # pid对应的文件位置和名字DBHost=localhost # 数据库地址DBName=zabbix # 数据库名字DBUser=zabbix # 数据库用户DBPassword=zabbix # 数据库密码SocketDir=/var/run/zabbix # Zabbix的IPC socket目录DebugLevel=3 # debug 级别，默认为3，一般不动，需要详细日志是可设置为5DBHost=localhost # 不变，因为zabbix用户就是授权到地址localhost了DBName=zabbix # 不变，当时创建的数据库名就是zabbixDBUser=zabbix # 不变，当时创建的用户就是zabbixDBPassword=zabbix # 这条原来没设置，设置为上面定义的密码DBSocket=/var/lib/mysql/mysql.sock # DBSocket文件路径默认为`/tmp/mysql.sock`，所以这条需要设置数据库的sock文件所在位置；或者 `ln -sv /var/lib/mysql/mysql.sock /tmp/`，这样不用改配置也能用DBPort=3306 # 数据库端口 其实主要改的就这几项，其余的如果不一样再修改： 123SourceIP=192.168.0.222DBPassword=zabbixDBSocket=/var/lib/mysql/mysql.sock 启动zabbix-server并设置下次开机自动开启 1systemctl start zabbix-server 编辑Zabbix前端PHP配置，并启动Zabbix的Web服务只要把httpd配置文件/etc/httpd/conf.d/zabbix.conf中的php_value date.timezone启用并设置为当前时区： 1php_value date.timezone Asia/Shanghai 或者是，把/etc/php.ini里的date.timezone =启用，并设置为当前时区： 1date.timezone = Asia/Shanghai 上述两种方法均可。 启动Apache Web服务： 12systemctl start httpdsystemctl enable httpd 设置Zabbix Server在浏览器输入zabbix.yulongjun.com/zabbix即可登录Web页面，然后开始进一步的设置： 点击Next step 配置，进入检查阶段，全部OK可以进入下一步配置： 输入密码，其他的如果有自己更改过的可以自行更改： 输入主机名或者ip地址、端口、还有名字 安装摘要，点下一步开始安装： 安装成功： 可以登录了，默认用户名admin, 密码zabbix： 进入页面： 可以更换页面风格和语言： Administration –&gt; General –&gt; 选择Dark主题–&gt;Update 可以更改密码，语言（支持中文哦）： 看一下中文界面： 设置media类型（报警媒介类型）由于某些原因，无法使用自带的一些媒介，所以使用自定义的邮箱设置 自带的Media，国内无法使用： 点击右上角创建媒体类型创建自定义的媒介，这里的媒介指的是出现报警后，用什么媒介来报警，这里设置的一个zabbix@yulongjun.com来负责发送报警邮件。 在Admin用户的设置里设置报警媒介，即出现报警后发送给谁，这里设置的ops@yulongjun.com。如果出现报警，则可以设置通过MyEmail(zabbix@yulongjun.com),向用户Admin设置的邮箱ops@yulongjun.com发送报警。 这里设置里仅设置了警告以上级别： 如果触发了triggers（触发器）的阈值，如果设定了相应的发送报警信息的Actions（行动），则会通过定义的规则来发送信息。 还可以设置页面的报警音（右上角人头–&gt;正在发送消息） 其他设置dashboard页面可以定义每个仪表盘的刷新时间： 监控模板网站在https://share.zabbix.com/，提供了各种各样的监控模板，可以自行搜索，套用。 下一节根据具体Agent来设置监控]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二十五章：Iaas平台建设]]></title>
    <url>%2Funder-the-ops%2F20170923-25-buid-iaas-platform%2F</url>
    <content type="text"><![CDATA[在本部分，我们探讨当传统IT建设和云计算的浪潮相遇的时候，作为互联网企业究竟应该如何抉择，是顺势而为投入云计算的怀抱，还是坚持基础设施自建，精耕细作。其实这两者并不矛盾，在我们的实践过程中，一方面，在私有云建设方面持续投入，不断提高技术、人才储备，实现资源的最优利用以及自动化程度的持续提升；另一方面，从“轻资产”的思路出发，在公有云服务逐渐普及、单位成本持续下降的情况下，按照业务的特点类型，将适合的业务迁移至公有云，提升业务稳定性，减少成本支出。 在展开详细内容之前，我们先来一起看看传统的IT建设方式是什么样子的。我们用一个应用场景来说明：在企业内部，或多或少地需要一些基础设施资源，包括计算资源、存储资源和网络资源等。传统的做法是组建一个系统团队，经过服务器选型、服务器采购、数据中心选择、服务器上架、网络建设等一系列过程，才能完成资源的交付。整个过程比较长，而且需要较多的人力。对于一个初创公司或者中小型企业来说，基础设施资源建设是个必需但不太划算的投入，本来公司规模小、人不多，公司内部的人员应该更加专注于自己的业务，基础设施资源建设不应该耗费公司较多的人力和物力。 和传统IT建设方式不同，IaaS（Infrastructure as a Service）作为一种新型的基础设施资源的获取方式产生了，即把服务器的基础资源作为一种服务形式，通过互联网提供给大家租赁获取。假设企业打算架设自己的服务，原来需要买一堆服务器、网络带宽等，现在完全可以通过IaaS服务商租赁虚拟机，而且这些虚拟机所需要的存储资源和网络资源是可以随时按需调配的，这样可以节省大量的人力、物力。一般的IaaS厂商会比单个企业在服务器规模上更大，在资源管理方面也会更加专业，因此，无论是服务质量还是投入成本都比企业自建更加优化。 一般来说，IaaS服务可以分为公共的IaaS和私有的IaaS。公共的IaaS又称公有云，规模大，服务比较全面，通过租赁的方式提供给公众使用，像AWS、金山云、阿里云和腾讯云等都是比较著名的IaaS服务提供商。私有的IaaS又称私有云，在公司内部组建，把服务器资源做成服务，提供给公司内部各个部门使用，极大地提高了资源分配的灵活性，缩短了资源交付周期，相比公有云来说安全可控性更高。IaaS服务既能有效地提高资源利用率，又能减少服务提供成本。由于私有云在企业内部的建设更有价值一些，后面我们将重点讨论私有云的建设过程，以及在使用过程中遇到的问题和解决方法。 IaaS平台选择前面介绍了什么是IaaS，这里我们讨论一下IaaS的形式之一——私有云的建设过程。最近几年，私有云建设受到了国内外很多公司的关注和参与，阿里、腾讯、百度等都纷纷建立起自己的私有云平台。建立的方式主要有两种：一种是以开源的虚拟机计算平台为基础，开发自己的云计算管理分配平台；另一种是完全采用开源云技术方案，如OpenStack等来设计自己的私有云平台。这两种方式各有优缺点，第一种方式采用自主开发，可控性好，对代码和功能能够完全掌控，问题追查也比较方便，但是开发周期长，搭建速度慢，很难在短时间内实现一套完整的私有云平台；第二种方式采用开源云技术方案，能在短时间内搭建起一套完整的私有云平台，但是需要完全熟悉和掌握你所使用的那套开源软件，有一定的学习成本，随着开源软件越来越复杂，对平台的掌控有时也会力不从心。具体采用哪种方式，主要看公司的规模和对私有云人力的投入。大公司由于人力充足，对私有云定制的需求较多，对平台的掌控性要求较高，所以完全可以自己开发。而小公司由于仅想使用私有云来方便自己的资源管理和分配，也不需要对私有云进行太多的定制，所以选择开源的私有云方案是一个很不错的选择。后续内容主要探讨开源私有云方案的选择和架设过程。 OpenStackOpenStack是一个非常流行的开源云计算平台，以Apache许可证授权，提供计算资源、存储资源和网络资源的分配与管理，是AWS服务的一个开源解决方案。最早是由美国国家航空航天局和RackSpace联合发起的，目前由OpenStack基金会管理和维护，已经有AT&amp;T、Dell、Intel、RedHat等200多个公司参与该项目。 OpenStack发展更新速度很快，从发起到现在，一直维持着半年一个版本的发布周期，在产品的规划周期里会定时举行峰会以确定开发需求和方向。操作系统对OpenStack的支持也非常紧密，目前Ubuntu和RedHat对OpenStack都有良好的支持。OpenStack是由Python编写的，不同的功能被划分成不同的模块，模块与模块之间采用松散耦合的方式，使用队列进行异步通信。OpenStack功能非常完整，社区非常活跃，支持参与的厂商较多，这些都是它的优点。但是缺点也是非常明显的，如学习成本较高和稳定性欠佳等。 OpenNebulaOpenNebula起源于2005年的一个研究性项目，2008年3月发布第一版，目前已经发展成一个流行的开源云软件，得到了广大用户的参与支持。OpenNebula的目标是将一群实体集群转换为弹性的虚拟基础设备，且可动态调适服务器工作负载的改变，OpenNebula作为在服务器设备和应用之间新的虚拟层，使得服务器的使用效率能够极大地优化。目前OpenNebula可支持Xen和KVM，更重要的是还支持EC2管理。OpenNebula可以构建私有云、公有云，同时还提供Deltacloud适配器与Amazon EC2相配合来管理混合云。OpenNebula由于起源比较早，同时由于项目更加偏向于学术研究，因此OpenNebula有很多创新功能，其中有很多已经应用到生产实践中了。鉴于其项目性质的特点，其稳定性方面稍差一些，企业和社区的支持力度也较小。 CloudStackCloudStack始于Cloud.com，其目标是使服务供应商和企业创建运营能力类似于亚马逊公司的公有云、私有云。2010年，Cloud.com提供了基于GPLv3的社区版本，供用户免费下载，并随后发布了两个支持版本。思杰（Citrix）公司在2011年7月收购了Cloud.com。思杰公司是OpenStack社区早期成员之一，但在2012年决定离开OpenStack社区。据媒体报道，做出这一决定是因为思杰公司认为，最初由Cloud.com提供的代码相比OpenStack更稳定，可为用户提供更多的功能。 CloudStack是一个开源的具有高可用性及扩展性的云计算平台，支持管理大部分主流的Hypervisor，如KVM、VMware、Oracle VM、Xen等。同时CloudStack是一个开源云计算解决方案，可以加速高伸缩性的公共云和私有云（IaaS）的部署、管理、配置。使用CloudStack作为基础，数据中心运营商可以快速、轻松地提供云存储服务和弹性云计算服务。CloudStack采用Java编写，在设计思想上更加趋向于简单稳定。但在使用者和社区支持度上，都比OpenStack差了不少。 IaaS开源软件选择上面介绍了三种IaaS平台，它们各有自己的优缺点，如图1所示是三者近年来的Google趋势图，其中蓝线表示OpenStack，红线表示CloudStack，黄线表示OpenNebula。从趋势上看，OpenNebula起源最早，但是关注度一直不高；CloudStack从2012年开始，已经超过了OpenNebula，位居第二；OpenStack从诞生之日开始，就非常火爆，目前稳居第一。 三种IaaS平台我们已经介绍完了，该是做决定的时候了。对于开源软件的选择，我们认为应该遵循以下几个原则。 功能完备性：所需功能必须完整，这是选择的前提。 系统稳定性：由于服务对系统的稳定性要求较高，任何不稳定的软件将会给后续维护带来非常大的困难。 使用广泛性：相信大家的眼光，和别人相同的选择至少不吃亏。 社区支持度：具备良好的社区支持度，即使出现问题，也很容易通过社区的力量解决。 图1 IaaS软件对比趋势图 从以上几点来看，OpenStack是这几个候选者中综合指标最好的一个，因此，采用OpenStack搭建IaaS平台是一个较好的选择。后续将以OpenStack为基础，详细介绍IaaS平台的搭建过程。 IaaS平台建设前面我们已经介绍过选择使用OpenStack建设IaaS平台。在使用OpenStack建设IaaS平台时，首先需要明确我们的需求。我们为公司建设IaaS平台，主要是满足公司内部的服务器虚拟资源的使用，作为私有云平台。通过对公司的私有云平台的需求进行收集，总结起来包括如下几点。 提供弹性的、高效的计算资源池，缩短机器的交付周期。 提高稳定的、高可用性的服务，支持公司线上和线下业务。 在网络连接上与现有业务无缝衔接，在使用体验上与物理机无明显差异。 虚拟机生命周期有效管理，虚拟机资产有效管理。 以上这4点需求，是大多数企业建设云平台的首要需求。第一点需求是云平台自有的特点，无须特别关注；第二点是提供高可用性的服务，鉴于OpenStack目前的稳定性，提供一个企业级高可用性的服务还需要做很多事情；第三点主要体现在网络方面，要求私有云网络和现有的业务网络高效互通，此外，在性能方面不能和物理机有太大差异；第四点主要是和现有业务系统和管理系统的对接，这和每个企业的资产管理系统相关，这里不多叙述了。下面我们就针对以上需求，了解整个私有云环境的建设过程。 架构设计在私有云建设过程中，OpenStack的架构设计是非常重要的一部分。由于OpenStack的组件相当多，搭建起来会有不同的架构。考虑到之前收集的需求，高可用性是比较重要的需求，因此需要设计成一个高可用性的架构。由OpenStack提供的私有云服务，主要分为计算、存储、网络、管理4种资源，对每一种资源我们都要评估其高可用性。 计算资源主要由KVM、Xen等虚拟机提供。虚拟机技术已经在业界使用了很多年，属于比较成熟的方案，稳定性也已经不是问题。 存储资源分为两种：本地磁盘和共享存储。本地磁盘的稳定性由硬件的稳定性保证，一般来说，本地磁盘如RAID等已经能够满足大部分应用的稳定性需求。若应用需要更高的稳定性，可以考虑使用共享存储。OpenStack的共享存储主要分为GlusterFS和Ceph两种，都能保证数据的高可用性。GlusterFS和Ceph我们都测试过，但是由于一些原因，最后投向了Ceph的怀抱。 网络资源是OpenStack私有云最重要的地方，决定了整个私有云的成败，必须选择一种高效稳定的模型。在OpenStack中提供了Flat/FlatDHCP、GRE、VLAN等几种网络模型。通过对比，VLAN模型是最适合企业级需求的，具体原因会在后面详述。 鉴于目前OpenStack的稳定性还不够高，我们无法保证在大规模应用中，不会因为OpenStack的问题导致整个云服务宕机。那么我们必须在架构设计上保证，OpenStack的任何服务挂掉都不会影响云服务的稳定性。综合以上考虑，我们的私有云架构如图2所示。 上面的架构是从公司的需求出发，结合公司业务定制而成的。 首先，OpenStack控制节点故障，不会影响虚拟机的正常使用。因为虚拟机只是通过OpenvSwitch上网，通过物理路由器直接路由出去，跟控制节点没有关系，只是无法对虚拟机进行管理操作。如若是短时间宕机，无须切换备机，等待主节点启动即可。倘若主控制节点长时间故障，也可以启用备控制节点。 图2 私有云架构图 其次，计算节点不存在单点，而且计算节点之间也不相互依赖，单个计算节点宕机只会影响本节点的虚拟机。虚拟机可以使用本地磁盘和共享云存储相结合的方式，本地磁盘可以提高虚拟机磁盘的性能，减少网络带宽的消耗，但是有一定的故障率（取决于系统硬件的故障率），适合做操作系统等非业务数据。倘若是业务数据，为了提高数据的可用率，也可以通过存储网络获取共享存储资源。使用共享存储的虚拟机，在宿主机宕机之时，也可以通过实时迁移或者重新挂载等方式，快速恢复虚拟机的使用。 最后，虚拟机上网通过OpenvSwitch直接和硬件交换机等相连，不需要通过L3 Agent等连接网络，规避了OpenStack L3 Agent不稳定问题，既可以无缝和现有业务访问衔接，也可以提高虚拟机网络的效率和稳定性，使虚拟机网络和其他物理机网络的稳定性在同一个级别上。 网络设计基于OpenStack的私有云建设，最复杂的当属网络部分。在前面，我们提到了虚拟机网络中OpenvSwitch和交换机直连，下面重点分析一下为什么这样设计。在OpenStack中，支持Flat/FlatDHCP、GRE、VLAN等几种模型，我们先分析一下这必种网络模型。 （1）Flat/FlatDHCP Flat和FlatDHCP基本没有太多差异，只是有无DHCP功能的区别，它们都属于一种扁平的网络模型，所有的虚拟机网络在一个大二层的环境当中，不利于租户网络之间的隔离；由于共享段广播的存在，不利于在大规模网络中使用。 （2）GRE 一种网络隧道的方式，通过OpenvSwitch GRE模块传输，在IP报文上添加GRE头，重新封装而成。不需要交换机特殊配置，就可以实现租户网络之间的隔离，但是GRE的性能不是太好。 （3）VLAN 通过OpenvSwitch在二层报文上打上相应的VLAN Tag，从而实现租户网络隔离，性能比较好。但由于VLAN ID最多4096个，所以网段只能限制在4096个，而且还需要交换机做相应配置。 Flat/FlatDHCP不能实现租户网络之间的隔离，同时由于二层中IP地址数量的限制，不太适用于中等规模以上的私有云建设。VLAN和GRE最大的区别在于性能差别，因此我们做了一个VLAN和GRE性能的对比测试，在不同宿主机上启动两个虚拟机，宿主机采用万兆网卡相连，CPU是Intel E5-2640，测试结果如表1所示。 表1 测试结果 网络模型 吞吐量 发送端CPU消耗（单核） 接收端CPU消耗（单核） VLAN 9.5Gbps 78% 65% GRE 2.5Gbps 76% 98% 从上面的测试结果可以看出，GRE的性能不太好，同时会消耗大量CPU，无法达到万兆网卡极限；而VLAN的性能比较好，可以轻松地达到网卡极限。在企业中，性能可能是需要极致追求的东西，因此VLAN对于多数企业的私有云来说都是一种优选模型。 L3（Neutron L3 Agent）也是需要考虑的部分，到目前为止，OpenStack还没有提供高可靠性的L3解决方案。虽然可以使用多网络节点的部署模式，但是它仅是一个负载均衡方案，并非一个高可用性方案，直到最近的Juno版本提供了基于Keepalived的HA解决方案，但也并非完美的解决方案。我们从以下三点来说明L3目前存在的问题。 第一，从我们的运营经验来看，OpenStack的L3 Agent本身稳定性不够，流量稍大就有可能达到L3的性能瓶颈，甚至有可能服务失效。 第二，多网络节点能解决负载均衡问题，但无法解决L3稳定性问题和节点失效问题。 第三，Juno版基于Keepalived的方案，依赖于VIP和VRRP协议，从理论上是可行的，但是从我们的经验来看，在现实的网络环境中，VRRP也不是完全可信的，经常出现主备来回切换的问题。由于以上问题无法解决，因此我们采用去L3的结构，如图3所示是最终的网络方案图。 图3 私有云网络结构图 在网络结构图中，与原生OpenStack的最大不同就是停用了L3 Agent，由物理交换设备或者路由器承担L3的功能，在稳定性和性能上较OpenStack软件的L3 Agent都有很大提升，同时也不需要关注单点问题。如图3所示，控制节点提供Neutron-Server、DHCP等功能，通过eth0管理计算节点OVS-Agent，控制OVS-Agent的VLAN ID的映射规则生成。虚拟机连接br-int，在br-int上已经做了VLAN ID映射规则，VLAN ID转换完成以后，出br-eth2，通过eth2的Trunk模式连接物理交换设备，和其他网络相连。不需要浮动IP，就可以实现和其他网络的互通。 存储选型存储选型是OpenStack私有云建设中比较重要的一部分，好的存储选型能够极大地提高系统的稳定性和数据的可用性。一般来说，可以分为本地存储和网络存储两种方式。 本地存储就是用本地磁盘作为虚拟机的存储磁盘，操作简单，无须任何配置，是OpenStack的默认存储方式。在性能方面，由于直接读取磁盘，没有经过网络等设备中转，因此性能较好，比较适合作为系统盘使用。但是，由于本地存储的数据可靠性依赖于本地硬件，所以可靠性并不高，因此需要用户自己做一些冗余性备份。 网络存储是采用一些分布式文件系统作为存储，通过网络的访问方式，提供给OpenStack私有云使用。分布式存储一般是采用多副本的冗余技术，当一个副本丢失的时候，系统会自动检查到，同时将副本复制到其他机器上，以保证副本的数量不小于设置的副本数量。因此，在数据可用性方面，分布式存储要远远好于本地存储。在网络条件较好和存储集群规模较大的情况下，分布式存储一般能够提供较高的存储性能。 但是，分布式存储由于冗余较大，建设成本也会较高。在一些典型的云计算环境中，一般将本地存储作为系统盘来提高本地磁盘的利用率，减少网络带宽的使用，而数据盘则采用分布式存储来提供数据的高可用性。比如AWS就是这样做的，当系统宕机以后，数据盘就可以轻松地挂载在其他机器上，几乎不影响数据的使用。 1．块存储 在OpenStack支持的存储中，比较常用的是Ceph和GlusterFS。GlusterFS是一个开源项目，后来被RedHat收购，但是仍然提供免费版本，是一个PB级别的分布式文件系统。GlusterFS的主要特点如下。 （1）高可用性 GlusterFS可以对文件进行自动复制，如镜像或多次复制，从而确保数据总是可以访问，甚至在硬件故障的情况下也能正常访问。自我修复功能能够把数据恢复到正确的状态，而且修复是以增量的方式在后台执行，几乎不会产生性能负载。 （2）无元数据 GlusterFS采用弹性Hash算法来进行数据的分布和查找，摒弃了传统的元数据节点结构，数据采用镜像复制结构，消除了单点故障和性能瓶颈，真正实现了并行化数据访问。 （3）访问方式 Gluster存储服务支持NFS、CIFS、HTTP、FTP以及Gluster原生协议，完全与POSIX标准兼容。现有的应用程序不需要做任何修改或使用专用API，就可以对Gluster中的数据进行访问，非常方便。 在GlusterFS中，分为分布卷（Distributed Volume）、复制卷（Replicated Volume）、分片卷（Striped Volume）三种卷类型。分布卷通常用于扩展存储能力，不支持数据的冗余，除非底层的Brick使用RAID等外部的冗余措施。复制卷为GlusterFS提供高可用性，卷创建时可以指定副本的数量，当某个卷发生故障时能够自动同步和修复。分片卷将单个文件分成小块（块大小支持配置默认为128KB），然后将小块存储在不同的Brick上，以提升文件的访问性能。 在分布式存储中，Ceph是一支后起之秀，是其创始人Sage Weil在加州大学攻读博士期间的研究课题。随着云计算兴起以后，OpenStack社区把Ceph作为其最核心的开源存储方案之一，推动了Ceph的快速发展。Ceph之所以成为OpenStack的官方存储组件，这和其优秀的特性和良好的性能是分不开的。总地来说，Ceph是为优秀的性能、可靠性和可扩展性而设计的，是一个统一的、分布式存储系统。在Ceph中，包括OSD、MON、MDS三个组件，OSD负责数据的存储和管理，MON负责数据的一致性维护，MDS负责CephFS元数据的管理。其主要特性如下。 （1）高可用性 Ceph采用多副本技术进行数据的冗余，从而防止了一份数据丢失导致整个数据失效的问题，没有任何单点。Ceph MON组件实时监控和管理着数据的冗余和一致性，以及所有故障的检测和自动恢复。恢复不需要人工介入，在恢复期间，可以保持正常的数据访问。 （2）分布式元数据 在Ceph中，元数据存储是可选的，只有当使用CephFS时，才会引入元数据节点。Ceph的数据访问采用CRUSH算法，这和GlusterFS类似，数据的访问通过算法就可以定位地址，真正做到无单点和性能瓶颈。而CephFS中的元数据，采用分布式存储，数据定位也是采用CRUSH算法。 （3）多访问方式 Ceph提供了块设备存储（Ceph RBD）、文件系统存储（CephFS）和对象存储三种方式，为用户提供更多的访问方式。块设备存储允许用户像使用物理裸磁盘一样使用Ceph块设备，可提供良好的性能和不错的特性，比如设备快照等。CephFS提供了一种方便的挂载方式，用户采用Ceph客户端就可以很轻松地将其挂载，访问较方便，但是会引入Ceph元数据节点。Ceph对象存储和Swift一样，提供了海量对象存储空间，同时拥有较好的性能。 在设计理念和产品特性上，Ceph和GlusterFS具有很大的相似性。但是作为OpenStack的存储组件，我们到底该如何选择？其实作为云存储组件，GlusterFS和Ceph都有很多公司在使用。我们主要考虑使用Ceph，原因如下。 OpenStack社区主推Ceph，作为社区的主推组件，从未来的趋势来看，会更加有前途和生命力。 Ceph提供多种存储形式，尤其是对象存储，是GlusterFS不支持的，而对象存储是云计算的一个核心组件。 Ceph在OpenStack使用中更加稳定（我们的观点），在作为虚拟机的磁盘使用过程中，Ceph几乎没有出现过问题，而GlusterFS偶尔会出现网络超时引起文件系统置为只读的情况。 2．对象存储 在OpenStack中，Swift是对象存储的核心组件，Swift是一个高可用的、完全对称的、无单点的、无限扩展的对象存储软件。最初是由RackSpace公司开发的高可用性分布式对象存储服务，2010年加入OpenStack开源社区作为其最初的核心子项目之一。Swift包括以下核心服务。 （1）Proxy Server 负责处理每个客户端的请求，将其均匀分散地分发到后端Storage Server上。Proxy提供了RESTful API，并且符合标准的HTTP协议规范，这使得开发者可以快捷地构建定制的Client与Swift交互。 （2）Storage Server 提供磁盘设备上的存储服务。在Swift中有三类存储服务器：Account、Container和Object。其中Container服务器负责处理Object的列表，Container服务器并不知道对象存放位置，只知道指定Container里存的是哪些Object。这些Object信息以SQlite数据库文件的形式存储。Container服务器也做一些跟踪统计，例如Object的总数、Container的使用情况。 （3）Consistency Server 查找并解决由数据损坏和硬件故障引起的错误。它们会在后台持续地扫描磁盘来检测对象、Container和账号的完整性，如果发现数据损坏，就会以副本拷贝的方式来修复数据。 从Swift的服务特点可以看出，Swift是一个分布式的、无单点的、能够自动容错的存储系统。因此，Swift的特点可以总结如下。 （1）高可用性 Swift由多副本技术进行冗余，能够进行自动故障恢复和容错，提供了极高的可用性。强一致性的容错方式和完全对称的结构，使得任何一个节点故障都不会影响整个系统的可用性。 （2）可扩展性 Swift的容量通过增加机器可线性提升。同时因为Swift是完全对称的结构，扩容只需简单地新增机器，系统就会自动完成数据迁移等工作，使各存储节点重新达到平衡状态，完全不需要人工干预。 （3）无单点故障 Swift的元数据存储是完全均匀随机分布的，并且与对象文件存储一样，元数据也会存储多份。在整个Swift集群中，也没有一个角色是单点的，良好的结构和设计保证了无单点故障存在。 典型的Swift架构如图4所示，每个节点采用完全相同的软件结构，以及完全对称的结构，机器的管理和维护非常方便，同时也不存在任何单点。 图4 Swift架构图 尽管Swift是OpenStack最稳定的组件之一，也是OpenStack社区推荐的对象存储组件，但我们在Swift使用过程中也遇到了一些问题。在使用过程中，作为存储Swift没有丢失过任何数据，作为OpenStack的Glance后端也完全能够胜任，但是如果作为其他目的使用，比如图片存储等服务，只要流量稍微一上来，Swift就必然遇到性能瓶颈，反应速度变慢，最后导致服务完全中断。因此，Swift对象存储不建议使用在流量较大的业务上。 通过前面的介绍我们知道，Ceph和GlusterFS大体的思想是差不多的，都是没有元数据节点的分布式存储，采用算法进行数据寻址，都是非常优秀的分布式文件系统。我们在使用GlusterFS作为虚拟机镜像时，经常会出现文件系统被写保护情况，非常影响虚拟机的使用。后续来我们换成了Ceph，在使用过程中，并没有出现明显的问题，因此推荐使用Ceph作为块存储。同时，Ceph的社区非常活跃，更新速度也非常快，相信在不久的将来，Ceph一定会变得更加稳定和成熟。另外，在对象存储方面，虽然OpenStack官方主推Swift，但是从笔者的惨痛经验来看，Swift的性能问题已经非常影响使用，因此Ceph的对象存储也是一种不错的选择。 服务器选型私有云的架构和网络模型已经确定了，接下来我们将对硬件配置进行描述。控制节点需要运行MySQL服务和RabbitMQ服务，在稳定性上要求较高。同时，Glance服务也是运行在控制节点上的，由于Glance需要存储大量的镜像文件，因此也需要较大的存储空间。总地来说，控制节点需要稳定的大存储服务器。计算节点兼计算和本地存储两部分功能，计算性能和存储性能要求服务器性能比较均衡，网络节点由硬件路由设备担任，不需要网络节点。存储节点只需能够多挂载硬盘，使用普通存储性服务器即可。下面给出我们使用的硬件配置，如表2所示，供大家参考。 表2 硬件配置 节点类型 机型 系统版本 数量 CPU 内存 系统盘 数据盘 网卡 控制节点 DELL R620 CentOS 6.4 ≥2 Xeon E5-2620 16GB×4 600GB SAS×2，10k转，RAID 1 600GB SATA×6，RAID5 1Gb×2 计算节点 DELL R720 CentOS6.4 ≥2 Xeon E5-2640x2 16GB×24 600GB SAS×2，RAID 1 2TB SATA×6，RAID 5 1Gb×2, 10Gb×2 存储节点 DELL R720XD CentOS6.4 ≥3 Xeon E5-2620 16GB×4 600GB SAS×2，RAID 1 2TB SATA×12 10Gb×2 安装部署由于OpenStack的组件较多，安装部署一直是OpenStack一个被诟病的问题。限于篇幅的原因，这里我们不再探讨每个组件该如何安装，只是简单地说明各种安装方式需要注意的问题。我们尝试过的安装方式有三种：Fuel安装、RDO安装和手动安装。以下是这三种安装方式的特点。 （1）Fuel安装 OpenStack的一键部署工具，由Mirantis公司提供，提供硬件自动发现机制。通过简单的Web界面操作，就可以将OpenStack连同操作系统一起部署完成。这是最简单的一种部署方式。 （2）RDO安装 RDO安装是使用PackStack安装部署工具，将OpenStack的RPM包安装部署并且完成配置的一种部署方式。整个过程只需配置一个PackStack的配置文件，部署较为方便。 （3）手动部署 使用Yum源，按照OpenStack官方部署文档，一步一步安装配置，安装比较麻烦，但是能够增进对OpenStack各组件的了解。 Fuel安装无疑是最简单的部署方式，作为初学者使用一个快速搭建的工具是一个不错的选择。但是Fuel作为一个整体会连操作系统一起安装，企业一般会有自己的长期使用的操作系统版本，同时会在此版本下做大量适合自己业务的定制和优化，因此，Fuel一般不适合已经成熟的企业用来部署私有云，而且若对OpenStack做了定制化修改，采用Fuel也不是一个太好的选择。 RDO部署相对Fuel来说复杂度略高，需要指定PackStack的配置文件，后续的过程也是自动化安装。有几个问题也是要注意的：第一，由于网络的问题，PackStack在安装的过程中有可能失败，需要反复重试。第二，部署完成以后，尽量不要手动修改集群配置，因为增加节点时，PackStack会再次检查配置，将修改的配置覆盖，可能造成不可预知的错误。第三，由于依赖Yum源，若集群扩容机器，前后时间相隔太长，Yum源上软件包可能已经更新，可能造成前后安装的小版本不一致，这可能导致软件版本不兼容问题。 手动安装是最复杂的一种方式，但也是最可控的一种方式。对操作系统和软件的依赖最小，只要在一个系统上正确完成一次安装，其他节点就可以大批量部署，效率极高。由于依赖Yum源，因此和RDO安装有相同的问题。为了解决Yum源更新问题，我们可以在第一次部署完成以后，就将Yum源同步到自己的机器上并且做成自己的Yum源，以后安装使用自己的源即可，速度快而且不易出错。因此，手动安装OpenStack是我们比较推荐的一种方式。 性能优化虚拟化技术已经是一项非常成熟的技术，但是在使用虚拟机时，很多人还是持半信半疑的态度，质疑虚拟机的性能问题。当然，使用虚拟机，服务器的性能会有所降低，到底会降低多少，我们用测试数据说话。在测试之前，我们需要对虚拟机——KVM主机进行一系列调优，以便虚拟机的性能达到较好的发挥。 1．CPU调优 CPU绑定是提升虚拟机性能比较常用的方法，通过绑定VCPU，能够减少CPU Cache的失效，从而提高虚拟机的性能。但是目前OpenStack虚拟机VCPU绑定不能自动化，必须手动使用virsh对虚拟机CPU绑定。另外，为宿主机预留CPU资源，是一个非常不错的选择。预留一定数量的CPU资源，也能提高整体的计算性能，通过配置nova.conf的参数就可以做到，如下所示： vcpu_pin_set = 4-$ 为宿主机预留了4个核，保证了宿主机对虚拟机的正常调度。另外，通过配置NUMA属性，能够显著提升虚拟机性能，因为绑定在同一个CPU上的两个超线程核心性能低于不同CPU上的两个超线程核心，不过这个特性只有J版本才支持。 2．内存调优 宿主机上的多个虚拟机会存在大量相似的内存页，如果将这些相似的内存页进行合并，就能节省大量的内存，KVM中的KSM就是这种技术，开启后会节省虚拟机的内存使用，但是同时会带来一定的CPU消耗。考虑到宿主机一般内存比较宽裕，而CPU增加成本较高，建议关闭内存合并，如下所示： echo 0 &gt; /sys/kernel/mm/ksm/pages_sharedecho 0 &gt; /sys/kernel/mm/ksm/pages_sharing 另外，采用Huge Page开启也能提升一定的性能，不过目前Nova不支持，只能通过修改源码，需要在Libvirt生成的配置中加入以下内容： 123&lt;memoryBacking&gt; &lt;hugepages/&gt;&lt;/memoryBacking&gt; 3．IO优化 在磁盘方面，通过改变磁盘的调度策略提高磁盘的效率，主要是将磁盘的调度方式改为Deadline，如下所示： echo dealine &gt; /sys/block//queue/scheduler 另外，使用SR_IOV网卡虚拟化技术可以显著提高网卡性能，不过配置起来有点复杂，感兴趣的读者可以按照https://wiki.openstack.org/wiki/SR-IOV-Passthrough-For-Networking自行配置。 性能测试基本优化工作已经完成，下面对虚拟机性能做一个对比测试，测试对比机型配置如表3所示。 表3 测试对比机型配置 机器类型 机器型号 CPU 核数 内存 磁盘 网卡 物理机 SuperMicro 3U8 E5-1230, 3.2G 8 32GB 500GB SATA 1Gbps 虚拟机 KVM E5-2640, 2.5G 8 32GB 500GB QCOW2（物理机2TB SATA） 1Gbps 为了避免虚拟机之间的竞争，影响测试结果，在物理主机上只运行了一个虚拟机。若有多个虚拟机，虚拟机性能必然下降，此测试的主要目的是测试出KVM Hypervisor带来的性能损耗，因此单个虚拟机就可以说明问题。首先，通过UnixBench测试系统的整体性能，如图5所示。 图5 UnixBench性能测试结果 从测试结果可以看出，虚拟机大约相当于物理机5/6的性能。而且从参数上看，物理机的CPU主频更高。因此，KVM虚拟机能够达到物理机接近90%的性能。 接下来，我们使用iozone对磁盘的性能进行测试，如图6所示。 图6 磁盘性能测试结果 从磁盘的性能测试结果来看，Reader和Re-reader的性能下降较多，下降20%左右；而Write和Re-Write性能下降较小，下降10%左右。 最后，我们对虚拟机的网络性能进行测试，分别采用小包和大包进行测试，如图7所示。 对于小包的测试，我们采用一个字节的文件，通过Web服务下载，最后吞吐量8Kbps左右，大约是物理主机的1/4，主要是受限于虚拟机网卡的PPS。对于大包的测试，使用一个320KB的文件，通过Web服务进行下载，虚拟机和物理机基本差不多，都能将网卡跑到800bps以上。 图7 网络性能测试结果 总地来说，虚拟机的性能和物理机已经非常接近，CPU的性能达到物理机的90%，磁盘性能达到物理机的80%以上，网络性能在小包处理上较差。但是在现实应用中，存储小包的场景较少，虚拟机已经足够应付一些常规业务场景。若一定要追求较好的网络性能，则可以使用网卡虚拟机技术（SR-IOV），相信能够带来非常显著的提升。 IaaS平台运营心得版本升级版本升级是开源软件都会遇到的一个问题，OpenStack也同样遇到此问题。由于OpenStack组件很多，各组件之间相互关联，不同版本之间的组件可能存在兼容性问题，不同版本的数据表也存在差异性，因此，OpenStack的版本升级成本较高，需要慎重。OpenStack版本升级一般有以下两种原因。 Bug修复。对于此问题，首先应考虑能否直接采取Bug修复补丁，这种方法成本较小，最后才考虑版本升级。 添加新功能。此种需求，首先应考虑新功能是否必需，有没有其他的代替方法，最后再考虑升级。 建议的升级方法如下： 在新服务器上搭建新版本控制节点，然后将线上控制节点数据库导入到新控制节点上，检查数据库兼容问题，两个控制节点数据库配置密码最好保持一致，使用OpenStack-status查看各组件状态，如果状态正常，说明控件节点升级完成。计算节点升级可能会升级Libvirt，有可能会导致虚拟机重启，因此应先升级一台做测试。为了安全起见，必须先通知虚拟机使用者，告知服务维护期间可能发生重启。准备工作都做好了以后，升级安装Nova和Neturon等，将控制节点指向新控制节点，其他节点都按照此操作，OpenStack的升级完成。 宿主机维护虚拟机都运行在宿主机上，OpenStack的宿主机维护也是必须面临的一个问题。宿主机的维护一般分为可预知的维护和不可预知的故障。 可预知的维护比较好处理，一般采用虚拟机迁移方式，OpenStack支持实时迁移和带磁盘迁移。如果使用了Ceph等共享存储的话，实时迁移是非常不错的选择，实时迁移的时间与内存大小及内存修改速度有关。一般说来，虚拟机实时迁移宕机时间会在1秒以下。OpenStack带磁盘迁移需要的时间会比较长，而且需要停机。由于虚拟机的磁盘一般较大，要想将宿主机上的虚拟机全部迁出，在短时间之内是无法做到的，因此可以挑选一些重要性较高的虚拟机做带磁盘迁移。 对于不可预知的故障，宿主机已经停机，处理就比较麻烦，如果虚拟机使用的是共享存储，只需要修改Nova数据库中宿主机字段，然后硬重启虚拟机，虚拟机就可以恢复。如果使用的是本地磁盘，虚拟机就无法很快启动了，必须等宿主机故障恢复以后，虚拟机才能启动。如果是磁盘问题，虚拟机磁盘可能就无法找回了。因此，虚拟机最好要做备份。虚拟机备份也是一个比较棘手的问题，可以从两个方面进行处理，一是由于虚拟机磁盘大，备份成本较高，不停机备份磁盘可能出现数据不一致性问题，直接备份虚拟机磁盘的操作成本也较高，这种方法适合一些重要而且能够短时间停机的虚拟机使用；二是采用应用服务冗余机制，在业务层面保证有多个虚拟机提供同一服务。 RabbitMQ使用和监控在OpenStack中，大量使用消息队列作为各组件交互的媒介，因此，消息组件的稳定性至关重要。在运营过程中，我们经常发现Nova或者Neutron连接RabbitMQ假死，导致虚拟机无法分配，或者虚拟机无法上网等情况。官方推荐使用RabbitMQ作为消息组件，尽量对RabbitMQ做高可用性配置，但是即使做了高可用性配置以后，还是会出现RabbitMQ连接超时的问题，因此需要对RabbitMQ进行监控。可以用Nagios或者Collectd等监控工具，监测到队列大于某个长度就可以告警，如果能写程序模拟队列的生成消费行为则更可靠。另外，可以对Nova-Compute和Neutron-OpenvSwitch-Agent的日志进行监控，一旦出现连接超时而且没有恢复就可以断定组件已经假死，需要重启才能恢复。为了减少消息队列的压力，尽量不要使用Ceilometer组件，若Ceilometer是必需组件，Ceilometer尽量使用单独的RabbitMQ队列，以减少对核心队列的压力。 虚拟机网络故障在OpenStack中，经常会遇到虚拟机无法联网的情况，定位网络故障也是OpenStack运维必选课，主要追查流程如下。 （1）在虚拟机控制台中使用ifconfig检查IP配置，然后ping网关。如若不通，执行步骤2。 （2）通过Dashboard检查Security Group，查看出口和入口的ICMP协议是否开启，若在开启情况下网络仍然不通，执行步骤3。 （3）在虚拟机宿主机上查看OpenFlow流。 若看到VlanID的转换规则，则说明VLAN转换没有问题；否则，需要重启Neutron-Server和RabbitMQ，再次查看VlanID转换规则是否已经生成。若还不通，执行步骤4。 （4）在出网的网卡（如em3）上抓包。 查看是否有包出去，如果看到包，则说明OpenStack配置没有问题，需要检查交换机是否配置Trunk模式。 总结在本章中，我们介绍了IaaS建设的意义，探讨了两种搭建私有IaaS平台的方式：自己开发和采用开源方案。考虑到建设成本，对比了不同的开源IaaS建设方案后，我们选择基于OpenStack来搭建IaaS平台。在架构设计中，重点讨论了网络架构设计，因为网络架构设计是OpenStack搭建过程中最复杂的部分，也是决定成败的关键。鉴于对稳定性和性能方面的考虑，采用虚拟机通过Trunk直联交换机的方式，停用了Neutron-L3-Agent的功能，由物理设备直接担任路由转发的功能；在存储方面，通过经验选择了Ceph，因为它使用起来更加稳定，并且在不断的完善过程中。对象存储Swift的稳定性没有问题，但是性能问题非常明显，期待Swift的改进。在安装部署上，推荐使用手动安装方式，这是一种最可控的安装方式，但是需要对OpenStack的安装方法深入了解，增加了学习成本，同时建议将Yum源同步到本地，防止OpenStack源升级后产生不可预知的错误。接着，我们对虚拟机的性能进行了测试，以便消除大家对虚拟机性能的担忧。最后，介绍了一些OpenStack运营心得，比如版本升级、宿主机维护等常见问题，供大家参考。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Iaas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二十三章：互联网企业级监控系统实践]]></title>
    <url>%2Funder-the-ops%2F20170923-23-monitor-system-of-internet-enterprise-level%2F</url>
    <content type="text"><![CDATA[在上一章中，我们介绍和对比了业界一些杰出的开源监控解决方案。早期，我们一直在用Zabbix，不过随着业务的快速发展，以及互联网公司特有的一些需求，现有的开源监控系统在性能、扩展性和用户的使用效率方面，已经无法支撑了。 因此，从公司的一些需求出发，从各位运维人员、研发人员的使用经验和反馈出发，结合业界一些大型互联网公司监控系统的设计理念，研发了Open-Falcon，目标是做最开放、最好用的互联网企业级监控产品（Open-Falcon的详细内容，可以参考http://open-falcon.org）。 监控系统主要有4个功能：数据采集、告警、图表展示和历史数据挖掘。Open-Falcon也是着重从这4个方面来设计和开发的。Open-Falcon具有以下一些特点。 强大灵活的数据采集：通过配套的Falcon-agent，可以自动采集400多个单机指标，也可以通过用户自定义的插件来增加更多的采集项。用户可以自己实现采集程序获取相关的指标，然后主动推送给监控系统，由监控系统负责后续的存储、展示和告警功能。比如编写脚本通过SNMP方式获取网络设备的相关运行指标，并把采集结果推送给监控系统。 良好的水平扩展能力：监控系统能通过水平扩展来支撑业务的快速发展。 高效率的告警策略管理：高效的用户配置界面，支持策略模板，支持模板继承和覆盖，支持多种告警方式和回调动作。 人性化的告警设置：支持最大告警次数、告警级别设置、告警恢复通知、告警暂停、不同时段不同阈值，支持维护周期设置，支持告警合并。 高效的历史数据查询：采用RRDtool的数据归档策略，秒级返回上百个指标一年的历史数据。 人性化的Dashboard：多维度的数据展示，用户自定义Dashboard等功能。 高可用：整个系统无核心单点，易运维，易部署。 Open-Falcon架构Open-Falcon架构示意图如图1所示。 图1 Open-Falcon架构示意图 整个监控系统，由Agent、Web-portal、Heartbeat-server、Dashboard、Transfer、Judge、Graph 等几个核心组件构成。 Agent做运维，不怕出问题，怕的是出了问题，抓不到现场两眼抹黑。所以，依靠强大的监控系统，收集尽可能多的指标，意义重大。但哪些指标才是有意义的呢？本着从实践中来的思想，各位工程师在长期摸爬滚打中总结出来的经验最有价值。在运维工程师的长期工作实践中，我们总结了在系统运维过程中经常会参考的一些指标，主要包括以下几个类别。 CPU Load 内存 磁盘 IO 网络相关 内核参数 netstat/ss命令统计输出 端口采集 核心服务的进程存活信息采集 关键业务进程资源消耗 NTP Offset采集 DNS解析采集 Agent是一个Daemon程序，只要安装了Falcon-agent的Linux服务器，便会自发现地采集单机的各种数据指标，共计400多个，基本覆盖了上面提到的各个类别。这些采集好的指标，Agent会以一个固定的周期主动上报到服务端，并不需要用户在服务端做任何配置（这和Zabbix有很大的不同）。这样做的好处就是用户维护起来方便，指标覆盖率高，在服务器上架初始化的时候，就会自动安装Agent。有了这些详细的指标，对于运维人员和研发人员来讲，事后追查问题不再是难题。当然，这样做也会给服务端带来一些压力。 另外，Agent提供了一个Proxy-gateway功能，用户可以方便地通过HTTP的方式，POST自定义的数据到本机的Proxy-gateway，Proxy-gateway会将这些数据转发到服务端，便于用户推送数据，并提高整个监控系统的稳定性。 Agent支持的这些采集项，都是大量运维人员在长期的工作中总结出来的，基本能覆盖常见的一些监控需求。但仍然会存在一些特殊的业务需求，或者不适合在Agent里面内置的采集项，对于这些需求，如何更好地进行支持呢？我们设计开发了插件机制。 插件是符合一定规范的，由用户开发的数据采集脚本或者可执行程序，Agent会负责周期性地调度这些插件，并将插件运行的输出推送到监控系统。插件的工作流程如图2所示。 图2 插件的工作流程图 插件是作为一个Git项目（Falcon-plugin），通过Git来管理的。用户编写插件，完成测试之后，提交到Falcon-plugin的dev分支，并发起一个pull-request，管理员审核确认通过后，会合并到master分支。所有的Agent都会定期通过git pull来检查获取更新，这样用户提交的插件就更新到所有的服务器了。通过Git管理插件，好处就是版本管理和协作，所有的用户都可以自由贡献插件，通过pull-request合并到master分支。 插件更新到服务器之后，Agent并不会对其进行调度，因为某些插件只是解决特定的问题，并不适合在所有的服务器上运行，或者无法正常运行在所有的服务器上。从安全的角度和产品设计的角度考虑，用户需要在Web-portal上进行配置，哪些机器允许执行哪些插件。完成这步操作后，相关的插件才会被相关服务器的Agent自动调度。 数据模型在讲述其他组件之前，有必要先描述一下Open-Falcon的数据模型（Data Model）。 数据模型是否强大、是否灵活，对于监控系统用户的“使用效率”至关重要。比如以Zabbix为例，上报的数据格式为hostname（或者IP地址）、metric、timestamp、value这4个维度，那么用户添加告警策略、管理告警策略的时候，就只能以这几个维度来进行。 举一个最常见的场景：hostA的磁盘空间，小于5%，就告警。在一般的服务器上，都会有两个主要的分区，即root分区和home分区，在Zabbix中，就得加两条规则；如果是Hadoop服务所在的机器，一般还会有十几块数据盘，还得再加十多条规则，这样就会很烦琐，不利于自动化（当然，Zabbix可以通过配置一些自动发现策略来搞定，不过仍然比较麻烦且不直观）。 Open-Falcon采用类似于OpenTSDB的数据模型，由以下7个字段构成。 metric：最核心的字段，代表这个数据采集项具体度量的是什么东西，比如是内存的使用量（memory_used），还是某个接口调用的次数（QPS）。 endpoint：代表metric的主体，比如metric是内存使用量（memory_uesd），那么endpoint就表示该metric属于哪台机器。 tags：这是一组用逗号分隔的键值对，用来对metric进行进一步的描述，比如service=falcon,location=beijing。 timestamp：UNIX时间戳，表示产生该数据的时间点。 value：整型或者浮点型，代表该metric在指定时间点的值。 step：整型，表示该数据采集项的汇报周期，这对于后续的配置监控策略很重要，必须明确指定。 counterType：只能是COUNTER或者GAUGE二选一，前者表示该采集项为计数器类型，后者表示其为原值。对于计数器类型，在告警判定以及图表展示前，会被先计算为速率。 举两个例子： 123456789101112131415161718&#123; metric: df.bytes.free.percent, endpoint: hostA,tags: mount=/home,value: 5, timestamp: UNIX时间戳,counterType: GAUGE, step: 60&#125;&#123;metric: df.bytes.free.percent, endpoint: hostA, tags: mount=/root, value: 15, timestamp: UNIX时间戳,counterType: GAUGE, step: 60 &#125; metric为df.bytes.free.percent，表示这个指标的具体含义为服务器的磁盘可用百分比；endpoint描述这个metric所在的主体是hostA；tags是对这个metric的一组补充描述，比如mount=/home这一组tag，表示这个metric描述的是home分区的磁盘可用百分比；同样mount=/root，表示这个metric描述的是root分区的磁盘可用百分比。 使用tags的好处是可以简化告警策略的配置。比如上面提到的这个最简单的场景，hostA的磁盘可用百分比小于5%就触发告警。对于root和home两个分区，在Open-Falcon中，可以用一条规则来描述，即 ： endpoint=hostA &amp;&amp; metric=df.bytes.free.percent &lt; 5% 而不再需要针对两个分区，写两条规则： endpoint=hostA &amp;&amp; metric=df.bytes.free.percent &amp;&amp; mount=/home &lt; 5%endpoint=hostA &amp;&amp; metric=df.bytes.free.percent &amp;&amp; mount=/root &lt; 5% 另外，在Dashboard中，可以借助于tags，把tags作为筛选过滤条件，更方便用户查看自己关心的指标。 数据采集数据采集，是监控系统一个最基本的功能。在Open-Falcon中，Agent采集到的数据，会先发送给Transfer组件。Transfer接收到客户端发送的数据，做一些数据规整、检查之后转发到多个后端系统去处理。在转发到每个后端业务系统的时候，Transfer会根据一致性哈希算法进行数据分片，来达到后端业务系统的水平扩展。Transfer自身是无状态的，挂掉一台或者多台不会有任何影响。 Transfer目前支持的业务后端有三种：Judge、Graph和OpenTSDB。Judge是我们开发的高性能告警判定组件，Graph是我们开发的高性能数据存储、归档、查询组件，OpenTSDB是开源的时间序列数据存储服务。每个业务后端都可以通过Transfer的配置文件来开启。 Transfer的数据来源一般有4种。 Falcon-agent主动采集的基础监控数据。 Falcon-agent执行用户自定义的插件返回的数据。 client-library：线上的业务系统，都嵌入使用了统一的基础库，对于业务系统中的每个业务接口，都会主动计算其CPS、Lantency等指标并上报。 用户产生的一些自定义的指标，由用户自行上报。 这4种数据都会先发送给本机的Proxy-gateway，再由Proxy-gateway转发给Transfer。 一个推送数据给Proxy-gateway的例子如下： 12345678910111213141516171819#!-*- coding:utf8 -*-import requestsimport timeimport jsonts = int(time.time())payload = [ &#123; ＂endpoint＂: ＂test-endpoint＂, ＂metric＂: ＂test-metric＂, ＂timestamp＂: ts, ＂step＂: 60, ＂value＂: 1, ＂counterType＂: ＂GAUGE＂, ＂tags＂: ＂location=beijing,service=falcon＂, &#125;,]r=requests.post(＂http://127.0.0.1:1988/v1/push＂,data=json.dumps(payload)) 业务性能数据采集基础库设计思路除了一些基础采集项，线上业务各接口的各项性能数据也是研发人员和运维人员重点关注的内容。我们抽象总结出了两类最通用的指标，供大家参考。 每秒调用次数：CPS。 接口调用延时：Lantency-75th、Lantency-95th、Lantency-99th。 即针对线上业务的每个接口，都会由基础库自动计算这两类指标，并由基础库周期性地推送给本地的Proxy-gateway。这样不用业务研发人员投入过多的精力，就可以自动化、标准化地采集业务性能指标数据。 有了相关的性能数据后，一方面，我们可以针对某些核心接口调用配置监控策略，在接口调用耗时增大到一定程度的时候，或者接口调用次数发生突升突降的时候，发送告警及时通知相关人员；另一方面，借助于性能数据，相关的研发人员和运维人员能够更从容地规划整个系统的容量，提高系统的稳定性以及资源利用率。 注：线上业务性能数据采集，一个开源的实现可以参考 http://metrics.dropwizard.io。 HTTP服务数据采集思路目前HTTP服务仍然在我们的所有业务中占据重要地位，所以如何自动化地监控、评估Web服务的可用性指标和性能数据是非常重要的。我们的线上业务大量使用Nginx作为Web Server。 在第一阶段，通过分析Nginx的访问日志得到访问次数，通过分析日志中的status code得到正常返回和服务器出错的数量。这个方案可以解决一部分问题，不过存在一些不足的地方，比如： 在线上服务器上分析日志，会对服务器造成一定的压力，拖慢正常业务的响应时间。 分析的时效性较差，一般都是5分钟的粒度和延迟。 日志的覆盖率有限，有些性能数据很难在日志中体现，比如upstream到某个后端花费的时间等。 在第二阶段，我们尝试在Nginx中内嵌Lua脚本来自动计算和获取每个Nginx Location对应的相关性能指标，包括： qps，该Location每秒被访问的次数。 request_time，请求的平均响应时间。 upstream_time_to_xxx，upstream到某个后端的平均响应时间，用来衡量Nginx到每个后端应用服务器花费的时间。 bytes_sent，每秒返回的字节数，用来衡量网络带宽的利用率。 status_code.200，每秒钟，HTTP的状态码等于200的次数。 status_code.4xx，每秒钟，HTTP的状态码大于等于400、小于500的次数。 status_code.5xx，HTTP的状态码大于等于500的次数。 availability : 1-(status_code.4xx + status_code.5xx) / qps，用来衡量服务的可用性。 采用该方案后，HTTP服务的各项指标的覆盖率得到大大提高，数据分析的时效性也提高到1分钟粒度，同时相对于第一阶段日志分析的方案，该方案的运维成本得到大大降低，对服务器资源的消耗也降低到可以忽略不计的水平。 告警告警判定，是由Judge组件来完成的。用户在Web-portal上配置相关的报警策略，存储在MySQL中。Heartbeat-server 会定期加载MySQL中的内容。Judge也会定期和Heartbeat-server保持沟通，来获取相关的报警策略。 Heartbeat-server不仅仅是单纯地加载MySQL中的内容。另一个重要功能是根据用户配置的模板继承关系、模板项覆盖关系、报警动作覆盖关系、模板和机器组的绑定关系，计算出最终关联到每个endpoint的告警策略，下发给Judge组件来进行告警判定。 Transfer转发到Judge的每条数据，都会触发相关策略的判定，来决定是否满足报警条件，如果条件满足，则会将告警信息写入告警队列。Alarm会从告警队列中获取数据，再以邮件、短信、米聊等形式通知相关用户，也可以执行用户预先配置好的回调动作。 用户可以很灵活地来配置告警判定策略，比如连续N次都满足条件，连续N次的最大值满足条件，不同的时间段设置不同的阈值，如果处于维护周期内则忽略告警等。同时也支持最大告警次数设置、告警级别设置，支持告警恢复通知、一段时间内突升突降判定等功能。 与告警组件相关的几个重要概念如下。 告警策略：某个metric触发告警需要满足的条件，称之为一个告警策略。比如cpu.idle，连续3次的值都小于10%，则触发告警。这就是一个典型的告警策略。 模板：一组告警策略的集合，称之为模板。模板和某个服务器分组绑定之后，该服务器分组下面的所有endpoint都会自动应用该模板中所有的策略。 服务器分组：一组endpoint的集合，称之为一个服务器分组。一个endpoint加入某个服务器分组中，那么就会自动应用该分组所绑定的策略模板；同理，一个endpoint从某个服务器分组中去掉，这个endpoint便不再拥有该分组所绑定的策略模板。通过这种管理方式，使得服务扩容和缩减时监控可以自动化地变更。服务器分组是绑定策略模板的唯一单位。 告警动作：满足告警策略需要后续执行的动作，称之为告警动作。目前支持的告警动作包括发送短信、发送邮件、发送米聊、执行指定的回调动作。回调动作这种机制，便于用户执行一些自动化的预案。 为了提高运维人员和研发人员配置、维护监控策略的效率，Open-Falcon有两个很重要的功能：一是根据tag来设置告警策略；二是引入模板来组织告警策略。 根据tag来设置告警策略，我们在前面讲述Open-Falcon数据模型的时候，有过简单介绍。这里再举几个场景来说明一下。 场景一：部署了Falcon这个服务的所有服务器，SDA盘的磁盘IO使用率，超过80%就告警。 service=falcon &amp;&amp; metric=disk.io.util &amp;&amp; device=sda &gt; 80% 场景二：部署了Falcon这个服务的所有服务器，任何一块磁盘IO使用率，超过80%就告警。 service=falcon &amp;&amp; metric=disk.io.util &gt; 80% 在上面两个场景中，我们用到了service和device这两个tag来作为配置告警的条件，仅仅通过一条配置，就能满足用户的需求。 模板是用户管理一组策略的唯一单元，一个模板可以和多个机器分组进行绑定。模板的作用在于用户修改了模板中的某个策略之后，与该模板绑定的所有机器分组中的endpoint都会即时生效该策略变动。 模板之间可以存在继承关系，子模板会自动拥有父模板中的所有策略。另外，在子模板中，可以覆盖父模板中的某些策略。比如我们可以创建一些标准的模板，其他用户只需要继承该模板，做一些简单的增、删、改就可以使用了。模板的继承和覆盖特性，可以有效地帮助运维人员减少维护告警策略的工作量。 对于模板的继承和覆盖特性，我们可以通过以下两个场景来阐述。 在图3所描述的场景一中，我们需要监控FalconHostGroup这个服务器分组下的所有服务器，其cpu.idle都不能低于10%；否则发送告警给falcon-dev用户组。我们通过给FalconHostGroup分组绑定Template-A策略模板，即可达到目的。 图3 场景一——模板继承和覆盖 这时候，用户的需求场景发生了变化，即如图4所示的新场景中所描述的，用户希望在场景一的基础上，监控portal这个服务器分组下的所有服务器，其cpu_idle不能低于50%；否则发送告警给portal-dev用户组。为了解决用户的新需求，通常的做法是，去除FalconHostGroup和Template-A的绑定关系，然后给下面的5个子分组分别绑定不同的策略模板。这样的操作方式，对用户来讲是非常烦琐的，特别是在子分组很多的情况下。 在Open-Falcon中，我们可以利用模板的继承和覆盖特性，方便地满足这个新需求。我们保持FalconHostGroup分组和Template-A的绑定关系不变，只需要给portal这个有特殊需求的分组单独绑定一个策略模板Template-B即可，其中Template-B继承自Template-A，且对Template-A中的cpu.idle策略项进行了覆盖。这样就可以满足用户的新需求了。 图4 新场景——模板继承和覆盖 告警分级同样是告警，有的告警需要立即处理；否则就会造成很大的影响，比如严重影响用户体验，无法正常使用该业务的核心功能等；有的告警则时效性要求较低，比如多台Nginx中的一台宕机了，暂时不影响服务，稍后处理即可。告警分级，让运维人员做到对重点故障重点关注。 考虑到这样的场景，监控系统在设计的时候，允许用户设置告警级别，对于高级别的告警，会优先在第一时间通过多种通道下发给相关用户；对于低级别的告警，则只会通过邮件进行发送。 告警级别，是按照对服务、对用户的影响范围来确定的。下面是相关定义，如表24-1所示。 表24-1 告警级别定义 级别 概述 对外影响 影响范围 处理要求 P0 业务核心功能出现不可用情况 业务整体或部分核心功能不可用 所有用户或部分地域用户 立即向上通报，立即处理 P1 业务非核心功能出现问题或业务处理效率、时效性下降 非核心功能不可用，数据延迟/业务响应变慢 所有用户或部分地域用户 立即向上通报，立即处理 P2 内部问题，对业务功能无影响，如部分服务器宕机、服务实例crash等 无 无 及时处理，无须向上通报 P3 内部预警类问题，对业务功能无影响，如磁盘空间、CPU使用等 无 无 24小时内完成处理，无须向上通报 告警合并不知道各位读者在运维过程中，有没有遇到过一些告警短信轰炸的场景，比如机房之间专线故障导致连通性下降，或者内网DNS出现问题导致各种解析失败，短时间内产生大批量的告警。这些瞬时的大批量告警给运维和研发人员造成了很大的干扰，造成短信网关、邮件网关资源的浪费，同时也会淹没一些重要的告警。因此，我们尝试通过告警合并来解决这种告警轰炸问题。 在告警未合并之前，只要产生了告警，就会立即发送。如果要做合并，那么就必然需要有一个等待时间，比如产生告警后等1分钟，然后看看这1分钟里哪些告警可以合并为一条，最后再统一发送给目标用户。告警合并需要有一定的规则，按照metric合并是一个很自然的方案，即很多endpoint的同一个metric告警，可以很自然地合并为一条，在减少告警数量的同时，尽可能少地丢失信息量。用户收到合并后的消息后，如果想进一步查看详情，可以点击附加在告警消息中的详情链接进入。 目前我们仅按照相同metric这一个维度来进行合并。更进一步，可以结合更多的纬度信息，比如相同机柜、同一接入交换机、同一类服务等，做到更智能、更准确的信息合并和问题定位。 实施告警合并，会对告警时效性存在影响，这和高级别的告警要求立即发送的原则存在矛盾，因此最终的方案是只对低级别的告警实施合并。 数据存储方案对于监控系统来讲，历史数据的存储、高效率查询和快速展示是很重要且困难的问题。这主要表现在如下三个方面。 数据量大：目前我们的监控系统每个周期大概有2500万次数据采集项上报（上报周期为1分钟和5分钟两种，各占50%），一天24小时里，从来不会有低峰，不管是白天还是黑夜，每个周期总会有那么多的数据要更新。 写操作多：一般的业务系统通常都是读多写少，可以方便地使用各种缓存技术。再者，各类数据库对于查询操作的处理效率远远高于写操作。而监控系统恰恰相反，写操作远远高于读。每个周期几千万次的更新操作，对于常用数据库（MySQL、PostgreSQL、MongoDB）都不是最合适和擅长的。 高效率查询：我们说监控系统读操作少，是相对写入来讲的。监控系统本身对于读的要求很高，用户经常会查询上百个metric，在过去一天、一周、一月、一年的数据。如何在秒级返回给用户并在前端展现，这是一个不小的挑战。 Open-Falcon在这块投入了较大的精力。我们把数据按照用途分成两类，一类是用于图表展示的；一类是用户做数据挖掘的。 对于图表展示的数据来讲，查询要快是关键，同时尽可能不丢失信息量。对于用户要查询100个metric在过去一年里的数据，数据量是巨大的，很难能在1秒之内返回。另外，就算返回了，前端也无法渲染这么多的数据，还得再采样，这就造成很多无谓的消耗。 我们参考RRDtool的理念，在数据每次存入的时候会自动进行采样、归档。归档策略是，历史数据保存5年。同时为了不丢失信息量，数据归档的时候，会按照平均值采样、最大值采样、最小值采样存三份。用户在查询某个metric在过去一年的历史数据时，Graph会选择最合适的采样频率，返回采样后的数据，提高了数据查询速度。 以1分钟的上报周期为例，下面是我们采用的数据归档采样策略（RRA的概念请参考RRDtool的文档）。 12345678910111213141516171819202122// 1分钟一个点存12小时RRA(＂AVERAGE＂, 0.5, 1, 720)// 5分钟一个点存2天RRA(＂AVERAGE＂, 0.5, 5, 576)RRA(＂MAX＂, 0.5, 5, 576)RRA(＂MIN＂, 0.5, 5, 576)// 20分钟一个点存7天RRA(＂AVERAGE＂, 0.5, 20, 504)RRA(＂MAX＂, 0.5, 20, 504)RRA(＂MIN＂, 0.5, 20, 504)// 3小时一个点存3个月RRA(＂AVERAGE＂, 0.5, 180, 766)RRA(＂MAX＂, 0.5, 180, 766)RRA(＂MIN＂, 0.5, 180, 766)// 1天一个点存1年RRA(＂AVERAGE＂, 0.5, 720, 730)RRA(＂MAX＂, 0.5, 720, 730)RRA(＂MIN＂, 0.5, 720, 730) 为了更进一步地提高写入性能，Graph充分使用内存，将待更新的数据缓存一定的时间（30分钟）后，再刷新到磁盘中，这样对磁盘IO的压力减少了一个数量级。同时为了避免刷新磁盘产生压力峰值，我们把刷新磁盘的操作打散到了每一秒钟。当然，使用缓存技术后，会产生两个副作用，一是当程序异常崩溃或者服务器断电的时候，会导致内存中尚未刷新到磁盘的数据丢失；二是用户查询数据的时候，就需要把内存中的数据和磁盘上的数据做合并，在一定程度上增加了代码的复杂度。 前面在讲述Transfer的时候，我们提到Transfer根据一致性哈希算法，将数据打散分发到后端不同的Graph实例上来达到水平扩展的能力。那么这里就存在一个问题，如果后端的Graph需要扩容实例数目的时候，一致性哈希算法得到的结果就会跟着发生变化，导致一部分历史数据丢失。为了解决这个问题，我们正在给Transfer增加一个migrating功能，当后端Graph需要扩容实例数目的时候，Transfer会根据扩容前后的实例信息，对每个数据采集项进行两次一致性哈希计算，根据计算结果来决定是否要进行数据迁移。 对于原始数据，Transfer会推送一份到HBase，也可以直接使用OpenTSDB，Transfer支持往OpenTSDB中写入数据。 数据查询到这里，数据已经成功地存储在Graph里了。如何快速地读出来呢？读过去1小时的、过去1天的、过去一月的、过去一年的，都需要在秒级时间内返回。 这些都是靠Graph和Query组件来实现的，Transfer会将数据往Graph组件中转发一份，Graph收到数据以后，会以RRDtool的数据归档方式存储，同时提供查询RPC接口。 Query面向终端用户，收到查询请求后，根据一致性哈希算法，会去相应的Graph里面查询不同metric的数据，汇总后统一返回给用户。 Dashboard在Dashboard首页，用户可以根据关键字来搜索endpoint，也可以根据上报的tags来搜索相关联的endpoint，如图5所示。 图5 搜索 用户可以自定义多个metric，添加到某个screen中，这样每天早上只需要打开screen看一眼，服务的运行情况便尽在掌握中了，如图6所示。 图6 掌握服务的运行情况 当然，也可以查看清晰大图，在横坐标上放大或缩小区间范围，快速筛选或反选监控项目，如图7所示。总之，用户的“使用效率”是第一要务。 图7 查看清晰大图 Web-portal一个高效的用户配置交互界面，对于提升用户的“使用效率”有很大的作用。 如图8所示是服务器分组管理页面，某个endpoint加入到某个服务器分组中，就会自动拥有该分组所绑定的所有策略列表。此处可以和服务树结合，服务器进出服务树节点，相关的模板会自动关联或者解除。这样服务上、下线都不需要手动来变更监控，大大提高了效率，降低了遗漏和误报警。 图8 服务器分组管理页面 如图9所示是一个最简单的模板的例子。模板支持继承和策略覆盖，模板和服务器分组绑定后，该服务器分组下的endpoint会自动应用该模板的所有策略。告警接收人这些配置信息，是作为模板的一个属性存在的。给定一个模板绑定了服务器分组之后，当该模板中的任何一个策略满足告警条件时，就会发送告警给模板的告警接收人。 图9 模板示例 总结监控系统是运维基础设施中最重要的组成部分。本章，我们分享了从监控系统的选型、对比、优化到自研的整个过程。Open-Falcon项目可以在我们的github主页https://github.com/open-falcon上找到，期待各位读者一起交流和改进。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Open-Falcon</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二十四章：分布式调用跟踪系统]]></title>
    <url>%2Funder-the-ops%2F20170923-24-distributed-systems-tracing%20-infrastructure%2F</url>
    <content type="text"><![CDATA[分布式调用跟踪系统，是在分布式系统日趋复杂、规模越来越大的背景下，监控系统在功能上的一种延伸。Google在2010年4月发布了一篇题为Dapper, a Large-Scale Distributed Systems Tracing Infrastructure的论文，介绍了Google在该方面的工作。至此，分布式调用跟踪系统的概念，开始为业界所熟知。此后，Twitter也于2012年开源了相关的分布式跟踪系统Zipkin。 举一个Google Dapper论文中的例子，来说明分布式调用跟踪系统是如何工作的。如图1所示，描述用户发起一个请求X，穿过一个简单的分布式系统的调用路径。该简单系统由5个部分组成，包括：前端（A）、两个中间层（B和C）和两个后端（D和E）。当用户发起一个请求X时，首先到达前端A，然后发送两个RPC到服务器B和C。B会马上做出反应，但是C需要与后端D、E交互之后再返回给A，由A来响应最初的请求。对于这样一个请求在多个模块中穿梭的过程，通过Dapper可以记录该请求在每个模块中的接收和发送动作，以及相关统计信息，从而描绘出请求的路径拓扑以及时序图。从该图中我们可以看到，Dapper的跟踪架构极其类似于一种描述调用关系的树形结构，简称为跟踪树。 图1 分布式调用跟踪系统工作示意图 在Dapper跟踪树结构中，树节点是整个架构的基本单元，每个节点称之为一个span。节点之间的连线表示span和它的父span之间的关系。一个调用链上的所有span，具有相同的跟踪ID（traceId），每个span记录着相关的标注信息和事件。这样通过traceId，就可以把与之相关的所有span组织起来，例如图2所示的调用时序图。 图2 Dapper跟踪树调用时序图 分布式调用跟踪系统，本质是通过在业务上、下游代码中埋点，来记录和分析同一次请求在整个调用链上的相关事件，从而帮助研发和运维人员分析系统瓶颈，快速定位异常和优化调用链路。跟踪系统适用于以下业务场景。 调用链跟踪：记录和分析同一次请求在调用链上各个节点的各种时间开销，帮助用户排查和跟踪请求。 调用链路径分析：分析应用的关键路径是什么，帮助运维和研发人员进行性能评估，合理规划容量。 调用链拓扑分析：帮助研发和运维人员理清服务调用的来源和去向，理清服务的相关调用依赖关系。 数据可视化：从可视化的角度来呈现各个业务、各个请求的调用关系、拓扑路径、时间开销、依赖程度、调用时序等。 一个健壮的、稳定的分布式调用跟踪系统，需要满足以下三个原则。 低消耗：跟踪系统对在线服务的影响应该做到足够小，对一些高度优化过的服务，即使一点点损耗也会很容易察觉到。 对业务透明：应用方不需要做任何改动或者只需做少量修改。对于业务的研发人员来说，在理想状态下，是不需要知道有跟踪系统这回事的。如果植入一个跟踪系统，需要依赖应用的开发者高度配合，那么这个跟踪系统推广起来肯定会有很多阻力。 可扩展性：能够适应主流的服务架构模型，能够应对服务规模的极速扩张，在性能和可运维方面做到可控。 对于分布式调用跟踪系统有兴趣的读者，可以进一步去学习Google的Dapper论文，以及参考Twitter的开源方案Zipkin。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Distributed systems Tracing Infranstructure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二十一章：Hadoop运维]]></title>
    <url>%2Funder-the-ops%2F20170923-21-ops-of-hadoop%2F</url>
    <content type="text"><![CDATA[“大数据”是近几年来IT行业的热词，大数据在各个行业的应用逐渐变得广泛。当听到这个名词的时候，第一想法就是从数据量级上来定义，如淘宝、百度、腾讯、360、小米等公司，服务器上存储着大量的用户数据和业务数据，这就是大数据。然而，这样理解是片面的，我们对大数据的理解是，在复杂多样的海量数据中通过计算分析，快速获取有用信息以产生价值。大数据可以说是计算机和互联网结合的产物，计算机实现了数据数字化，互联网实现了数据网络化，两者结合才赋予了大数据生命力。 我们的Hadoop集群经历了从几台规模到现在的数千台规模的发展历程，并且随着公司的业务迅速发展，以及全球业务量和用户量的飞速增长，不久的将来服务器规模将达到上万台甚至数万台。这也带来了新的挑战：服务器数量每年以5～10倍的速度增长，业务需求越来越复杂多样，机房每年以好几倍的速度新增，这些对Hadoop集群的运维管理是一个巨大的挑战。 我们在2014年上线了3种新的计算引擎：Storm（实时计算），秒级延迟；Spark（迭代计算），比MapReduce快10～170倍，代码量减少了70%～90%；Impala（数据仓库），比MapReduce/Hive快10～70倍。日志流式处理我们使用Scribe、Storm、Kafka，以满足对日志的归类和计算处理需求。多样化的集群类型对运维工作也是个挑战。 目前我们的Hadoop集群在部署和监控方面已经有了比较成熟的自研系统，能够灵活地支持同机多实例；抽象出Service/Job/Task的概念，直观的配置文件描述；灵活便捷的包管理，对开发团队更为友好；直观的WebUI Dashboard，方便的Command Line Tool既支持集群级别的管理，也支持指定Job/Task级别的管理；支持一键部署，方便运维工程师使用；支持除了Hadoop生态系统外的其他服务，可扩展性强。 本章会重点介绍Hadoop安全认证系统Kerberos及其自动化管理平台和多机房高可用方案；磁盘故障监控和处理自动化方案；软件模拟CPU、内存、磁盘、网络等硬件故障的Failover模拟系统；使用Puppet实现多机房高可用的服务器环境管理方案等运维经验。 Hadoop安全认证系统随着Hadoop集群规模的复杂度和业务的依赖性逐渐增强，以及考虑到用户数据隐私，Hadoop面临着一个重大的问题是安全，我们通过Kerberos认证实现了Hadoop集群的安全保障。 没有做Kerberos认证的Hadoop集群，只要有Client端就能够连接上。而且通过一个有root权限的内网机器，创建对应的Linux用户，就能够得到Hadoop集群上对应的权限，这样相当危险。而实行Kerberos认证后，任意机器的任意用户都必须先在Kerberos的KDC中有记录，才允许和集群中的模块进行通信。 Kerberos基本概念Kerberos是一个基于共享密钥对称加密的安全网络认证系统，避免将密码在网上传输，而是将密码作为对称加密的密钥，其设计目标是通过密钥系统为C/S应用程序提供强大的认证服务。 Princal（安全个体）：被认证的个体，有一个名字和口令。 KDC（Key Distribution Center）：一个网络服务，提供Ticket和临时会话密钥。 Ticket：一个记录，用户用它向服务器证明自己的身份，包括客户标识、会话密钥、时间戳。 AS（Authentication Server）：认证服务器 TGT（Ticket Granting Ticket）：票据授权票据，票据的票据。 TGS（Ticket Granting Server）：票据授权服务器。 SS（Service Server）：Hadoop中的服务组件，namenode、datanode、nodemanger。 使用Kerberos 时，一个客户端需要经过三个步骤来获取服务。 1. 认证 客户端向认证服务器发送一条报文，并获取一个含时间戳的TGT。 2. 授权 客户端使用TGT向TGS请求一个服务Ticket。 3. 服务请求 客户端向服务器出示服务Ticket，以证实自己的合法性。 Kerberos需要KDC来进行认证。KDC 只有一个Master机器，可以带多个Slave机器。Slave机器仅进行普通认证。在Master机器上做的修改需要自动同步到Slave机器。另外，KDC需要一个Admin来进行日常的管理操作。这个Admin可以通过远程或者本地方式登录。 Hadoop集群为什么使用Kerberos？ Hadoop集群使用Kerberos认证机制后，使得集群中的节点是可信任的。Kerberos可以将认证的密钥在集群部署时事先放到可靠的节点上。集群运行时，集群内的节点使用密钥得到认证，认证通过后的节点才能提供服务。企图冒充的节点由于没有事先得到的密钥信息，无法与集群内部的节点通信。这样就防止了恶意地使用或篡改Hadoop集群的问题，确保了Hadoop集群的可靠性、安全性。 Kerberos运维历程2012年年底，我们正式上线Hadoop集群服务。由于当时公司机房规模较小且单机房，所以考虑Kerberos规划的时候，只要能实现单机房HA，便可以满足当时需求，于是经过调研采用Keepalived通过漂移VIP的方式实现高可用方案。Kerberos主机器通过kdb5_util dump出数据库文件放在KDC Domain目录下，将KDC所在的Domain目录作为Rsync的同步目录，Kerberos备机器启动一个Rsync客户端将自己相应的KDC Domian目录和Kerberos主机器的Domain目录保持同步，Kerberos备机器通过kdb5_util load的方式将同步来的数据库文件导入KDC中。Hadoop所有请求通过请求内网域名，解析到Keepalived绑定的VIP的方式来使用KDC，如图22-1所示。 图22-1 单机房HA方案示意图 从2013年年初到2014年年底，由于公司业务飞速发展且机房快速扩张，短短一年多的时间，海内外上线了近10个机房，各个机房间通过专线互联。由于Hadoop集群都是内网服务，不推荐跨机房访问和部署，所以我们在每个机房都部署了ZooKeeper集群、HDFS集群、YARN集群、HBase集群等，且每个机房的所有集群使用的是同一套Kerberos Principal。然而这样的架构会遇到一个问题，Kerberos服务器所在机房和其他机房专线中断故障时，运维工程师根本没有办法做故障处理，只能等待专线恢复，Hadoop集群服务才能恢复，如果机房间专线故障时间较长，就必须紧急在相应机房部署Kerberos，同步KDC数据，这极大地影响了集群的稳定性和可用性。为了避免这类问题，我们为每个机房部署了一套Kerberos服务，实现了简单的Kerberos多机房HA架构，如图22-2所示。 图22-2 多机房HA架构示意图 每个机房都使用Keepalived漂移VIP的方式上线了一套Kerberos HA架构。使用Rsync同步目录的方式，将A机房的Kerberos主机器的KDC Domain目录作为Rsync Server，其他机房和本机房的Kerberos备机器的所有Kerberos KDC都通过A机房的主机器进行Rsync同步。并且使用了内网DNS分区域解析的方式，同机房的Hadoop集群使用本机房的Kerberos主机器服务。 2015年年初经过调研和分析Kerberos多机房架构，发现目前的多机房架构有很多的弊端。假设在Kerberos主机器宕机不可恢复的情况下，所有的同步都得重新连接；公司业务增长飞速，Hadoop用户也随着增多，基本上每天都有Kerberos Principal操作的需求，手动操作也越来越频繁，同时也伴随着更多的误操作。这时，我们意识到了Kerberos Principal申请和权限管理自动化的重要性，所以决定着手开发Kerberos管理系统，重新讨论了Kerberos多机房自动化管理方案。 目前最新的Kerberos多机房架构引入了MySQL主从HA方案、LVS LB方案；海外AWS服务引入了ELB LB方案；Principal申请流程自动化；Principal和LADP结合权限管理；HTTPS+CAS安全登录方案以及使用TCP协议。Kerberos多机房新架构示意图如图22-3所示。 图22-3 Kerberos多机房新架构示意图 针对新版Kerbose的设计实现分如下8个部分进行介绍： 1．新增用户操作管理 数百上千个Kerberos Principal管理是一个大问题，过去两年基本上所有的Principal都是运维工程师来手动操作的，如增、删、改、查。为了将运维工程师从苦海中解放出来，我们上线了Kerberos Principal申请和处理流程自动化。在新版Kerberos管理系统中对Kerberos账户的申请和调整进行了规范化，用户首次登录Kerberos管理系统，个人Principal会自动创建，服务Principal通过Web提交Kerberos Principal操作需求发起申请流程，Kerberos Admin收到申请邮件，开始审核，如果审核不通过，Admin打回给提交者重新修改申请；审核通过后，Web Server根据Kerberos Principal申请类型操作KDC，生成相应的Kerberos Principal所需密码或keytab文件。也支持用户删除自己有管理权限的Kerberos Principal，修改Kerberos Principal密码以及导出新的keytab文件，查询自己有权限的Kerberos Principal信息。 2．服务负载均衡优化 我们放弃了Hadoop集群Service请求Kerberos Keepalived VIP的方式，经过调研使用同机房走同机房LVS（或海外机房走ELB）的方式分机房进行内网域名解析。当LVS（或ELB）后端一台Kerbose服务器宕机的，LVS会将所有的流量切到另一台Kerbose主机器上，保证同机房Hadoop集群Service服务及Kerbose用户访问KDC的高可用性。目前国内外近10个机房上线使用。 3．数据同步优化 数据同步分两块进行介绍：①核心机房KDC双主的数据同步，使用MySQL双主的同步方式，将生成的记录写入MySQL数据库中。数据库采用双主的同步方式，同步到另外一个主库，这台机器的Web Server检测MySQL中有增量，根据增量情况，操作KDC创建相应的账户，以实现核心双主KDC数据库同步。②跨机房KDC数据使用Rsync工具进行增量同步。以国内A核心机房作为主机房，Rsync Server使用了Keepalived VIP的方式，当Kerberos主机宕机后，VIP漂移到另外一台主机器上，Rsync Client会以VIP所在的KDC主机器为Rsync Server进行数据同步，以保证KDC数据同步的高可用性。 4．监控优化 我们分为三类进行全方位监控：使用进程管理工具对Kerberos KDC、Web Server和MySQL Server进行进程存活监控，进程异常退出时，进程异常退出时都会被主动拉起；模拟Kerberos用户的真实场景使用KDC，创建了测试Kerberos Principal，客户端在不同机房请求KDC，对请求的返回值进行监控，对端口和KDC实例进行监控。如上三类监控能全方位地监控到KDC服务是否正常提供。 5．新增权限管理 通过公司个人LDAP 账户和Kerberos Principal的结合，将Kerberos Principal和公司邮箱账号进行绑定。比如LDAP用户USER1申请了自己服务的Kerberos Principal:s_ service_name，USER1便是s_service_name的管理员，USER1在KDC管理系统中可以赋予USER2对s_service_name的读权限，USER2便可通过Web页面获取s_service_name的密码和keytab文件。当USER1离职时，USER1可以将s_service_name的管理员权限赋予USER2，USER2后续便是s_service_name的管理员。友好的权限管理，对后期运维管理也是一个很大的改善。 6．新增安全登录 在新版Kerberos管理系统中，Web Server通过HTTPS进行域名访问。如果通过HTTP访问，Nginx层会自动将其rewrite至HTTPS，且接入到CAS安全认证登录系统，保证了Kerbose Web Server的访问，也方便记录用户的登录使用轨迹。 通过对Kerberos服务架构和功能的优化，对运维自动化和Hadoop集群安全都有可靠性保障。 7．记录日志 用户对Kerberos账户的一切操作都会进行日志记录，日志中包括用户查看账户密码，重置账户密码，导出账户keytab，增、删、改、查账户拥有人对账户的权限等，这样我们就能方便的记载用户的访问轨迹。此外，我们对MySQL向KDC同步数据的Replicate进程也进行了日志记录，来获取关于账户的创建、修改等方面的信息。 8．统计管理 新版Kerberos管理系统为其他业务系统提供了API接口，通过此接口可以方便的获取账号的拥有人等信息，这样为其他业务系统提供了强有力的支持。 通过对Kerberos服务架构和功能的优化，使账号的高效管理，运维的自动化，Hadoop服务的高可用性以及Hadoop集群安全性都得到了有效的提升。 Machine Failover SystemHadoop集群在上线前需要进行大量的模拟测试，测试完善后才能评估在外界各种故障影响及影响时间长短的情况下，Hadoop集群恢复的能力，同时也可以帮助开发人员不断地提升Hadoop集群的健壮性。在进行Machine Failover测试中，我们用软件模拟出CPU、内存、磁盘与网络等常见的故障与恢复，在这里我们对所用到的工具与技术进行总结。 在一台机器（HTTP Server）上启动Machine Failover System Server，该机器对所有要压测的host都有无密码授权，通过Curl或者Web发起HTTP请求，通过不同的模拟请求，改变不同的模拟参数，实现故障模拟。 用户通过以下两种方式访问Machine Failover System Server。 1. 通过访问浏览器http://failover.xxx.srv/$type?hostname=$hostname&amp;token=$token 2. 通过命令行请求curl ＂http://failover.xxx.srv/$type?hostname=$hostname&amp;token=$token＂ 设计token的用意在于做权限隔离，不同业务的用户根据自己的token只能操作自己授权的服务器，不影响其他用户的机器。 下面介绍模拟故障类型。 （1） RestartHost 有两种重启需求，暴力重启：通过调用机器远程管理卡提供的API强制断电重启机器，从而达到模拟暴力重启操作系统的效果；优雅重启：如果有优雅重启的需求也可以直接发reboot命令。根据不同的模拟需求选择相应的重启类型。 （2） CpuLimit 要限制单个进程的CPU使用率，可以使用开源工具CpuIimit，我们需要在测试服务器上安装CpuIimit，然后通过pid或者进程名对进程进行限制，如cpulimit -p 9001 -l 50，即可限制pid为9001的进程的CPU使用率不能超过50%。如果想模拟机器CPU紧张的情景，也可以使用OpenSSI工具，通过使用openssl speed来耗尽服务器的CPU。这样做的好处是真实地模拟了测试场景，弊端就是会影响其他进程的执行。 （3） MemoryLimit 模拟内存受限的情景，可以使用C语言（(unsigned char ) realloc(ptr, i+1) meg * sizeof(char))）函数实现的内存消耗程序，使被测进程的可用内存减少。使用kill -s SIGINT $pid，可以随意停止对内存限制的模拟。要实现这个内存限制，需要在服务器上手动编译这个C程序，生成对应的可执行程序。 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;unistd.h&gt;#include &lt;signal.h&gt; unsigned char *ptr = NULL;void sigint_handler(int);int main(int argc, char **argv)&#123; unsigned long meg = 1024 * 1024; unsigned int toalloc = 32*1024; // 32GB（可控制） unsigned int sleeptime = 60*60; //1小时（可控制） unsigned long i = 0; unsigned int j = 0; signal(SIGINT, sigint_handler); for (j = 0; j &lt; toalloc; j++) &#123; printf(＂Trying to allocate %u MB of RAM...＂, j + 1); ptr = (unsigned char *) realloc(ptr, (j+1) * meg * sizeof(char)); if (ptr == NULL) &#123; printf(＂failed\n＂); free(ptr); return 1; &#125; ptr[0] = 0; for (i = j * meg; i &lt; (j + 1) * meg; i++) &#123; ptr[i] = ptr[i - 1] + 1; &#125; printf(＂success\r＂); &#125; sleep(sleeptime); free(ptr); printf(＂\n＂); return 0;&#125;void sigint_handler(int status)&#123; printf(＂\nCaught SIGINT\n＂); free(ptr); exit(1);&#125; （4）DiskFail 模拟磁盘故障的方式有很多，使用echo offline &gt; /sys/block/$device/device/state，可以达到拔掉磁盘的目的，使得磁盘不可用；磁盘恢复使用echo running &gt; /sys/block/$device/ device/state。通过使用fiu-ctrl可以很好地控制进程I/O访问的失败率。继续了解发现，libfiu的另一个优势在于不用注入代码，只是被测程序需要使用fiu-run来运行，小型程序可以很方便地控制各种I/O读或写的成功率，但对于大型应用所有进程的启动入口都需要进行相应的修改。对于数据库系统，通过删除数据库持久化文件也能使大量磁盘请求失败。但对于磁盘故障不能读/写的情况还是有所区别的。对于RAID磁盘阵列有相应的工具mdadm，但是对于HDFS这种不开启RAID的文件系统就必须用SCSI fault injection test tool或者SystemTap这类工具了。也有一个小型的开源项目，如fsdisk提供了简便的接口去注入错误。另一种简单的方法是通过命令dd if=/dev/zero of=/dev/sdb/sdb1 bs=1024 count=102400破坏分区的信息，破坏前进行备份，然后通过dd即可恢复。唯一的缺点是恢复后需要重新mount，才能接受正常的读/写请求。本系统是通过对磁盘挂载点目录的权限进行控制，使得该目录没有rwx权限，相当于磁盘完全坏掉了。 （5）DiskFull 磁盘满表明没有可用空间了，最直接而且最真实的方法是创建大文件占满磁盘，我们可以使用命令dd if=/dev/zero of=/$path/tst.img bs=1G count=20K来创建超大文件把磁盘占满，恢复也很简单，只要把指定目录下的这个文件删除就可以了。当然，这样我们需要知道生成文件所用的时间，才能观察被测程序在磁盘满的情况下的表现。通过创建大量小文件可以使iNode节点耗尽，导致无法创建新文件，也可以达到磁盘资源不足的故障目的。 （6）DiskSlow 模拟磁盘访问速度慢，目前我们考虑使用fio这个压测工具，通过对磁盘进行大量的访问，必然导致其他进程的磁盘访问速度下降，kill掉这个进程相当于将磁盘恢复为正常状态。当然，这种方式很难控制磁盘速度减慢的程度。 （7）NetworkBandWidthLimit 模拟服务器网络带宽限制最好的工具是tc（traffic control），而且是由Linux内核提供的，在服务器中输入命令tc qdisc add dev eth0 root tbf rate 5800kbit latency 50ms burst 1540即可限制带宽为5800kbit，如果需要恢复则执行命令tc qdisc del dev eth0 root tbf rate 5800kbit latency 50ms burst 1540。这样我们在本地或者远程都可以控制测试服务器的带宽，进而观察被测程序在带宽受限时的表现。 （8）NetworkDelay 模拟网络延迟和网络故障都是通过tc来实现的，我们使用命令tc qdisc add dev eth0 root netem delay 300ms就可以对所有请求都增加300毫秒的延时，然后使用命令tc qdisc del dev eth0 root netem delay 300ms即可恢复过来。注意，设置网络延迟后，包括SSH操作都会感觉到明显的延迟，在测试时必须保证能够远程恢复过来。 （9）NetworkPackageCorrupt 模拟包损坏，一些开源项目也是使用tc，以这种形式来注入错误，通过tc qdisc add dev eth0 root netem corrupt 5%命令破坏并使用tc qdisc del dev eth0 root netem corrupt 5%命令来恢复。 （10）NetworkPackageLost 模拟丢包，使用sudo tc qdisc add dev eth0 root netem loss 5%命令可设置服务器5%的丢包率，执行sudo tc qdisc del dev eth0 root netem loss 5%命令恢复。设置网络丢包的命令一旦执行后，所有请求都有可能丢失，包括我们发出的恢复命令，所以在模拟这种故障时，恢复命令需要多次重试以保证服务器真的恢复过来了。 （11）NetworkUnavailable 如果希望网络不可用，则可以使用空路由ip route add blackhole 10.0.0.0/8，使Route都不可用，这样要恢复就很困难了。所以我们期望的网络不可用应该是针对特定进程或者端口的不可用，于是可以考虑使用iptable这个Linux防火墙。①iptables -A INPUT -p tcp --dport 8080 -j REJECT，限制发往本机8080端口的所有请求，对于监听这个端口的服务相当于不可用了；而通过 iptables -D INPUT -p tcp --dport 8080 -j REJECT，删除限制规则，恢复。②iptables -A OUTPUT -p tcp --dport 8080 -j REJECT，限制本机访问其他机器的8080端口，对于该机器就相当于已经和服务隔离了；而通过iptables -D OUTPUT -p tcp --dport 8080 -j REJECT，删除限制规则，恢复。 通过该系统在Hadoop集群做的大量测试，我们模拟日常运维中可能出现的各种故障，制定了针对不同故障类型的Hadoop集群故障处理预案，当故障来临时，我们能够轻松地按照预案内容执行，减少了故障处理时间，降低了对业务造成的损失。 磁盘处理自动化据统计，在数据中心中，硬盘相关的故障占全部硬件故障的85%以上。随着大数据时代的到来，服务器数量大幅度增长，更多的存储需求、更低成本硬盘的使用，以及高温、高存储密度等技术的应用，使硬盘故障及报废规模呈明显的增长趋势，这对业务稳定性和运维效率都造成了严重的影响。我们的Hadoop服务器数量占比很大，且离线计算集群刚开始使用民用级磁盘，每天大量的数据读/写导致磁盘故障率比较高，每天都有大量的故障磁盘需要更换维修。以前都是通过硬件监控或应用监控发现问题，然后由应用运维工程师登录服务器确认磁盘故障，尝试使用工具修复。如果修复失败则摘掉硬盘，再发起故障报修申请。 为了减少人工介入磁盘故障处理的耗时，我们研发了硬盘故障检查报修自动化系统，通过平台提供的API接口和监控系统联动，当监控系统发现硬盘故障后，通过回调接口启动硬盘工具进行软修复，如果修复失败则摘掉硬盘，并在服务管理平台进行记录，自动发起故障维修工单。服务器供应商收到维修工单通知后，根据提供的机房、机柜、硬盘位置，进行集中更换。更换完成后进行通知，再由系统将硬盘分区格式化挂载，开始提供数据存储。 目前磁盘处理自动化主要分为如下几个模块。 （1）磁盘故障检测 磁盘故障检测，针对不同类型的磁盘及故障级别分为4类，即INFO、WARN、ERROR和FATAL。对于WARN，表示磁盘即将故障，而ERROR和FATAL这两种类型，我们定义为已经故障，即对故障通过软件进行修复，如果修复失败，这类磁盘则存在着很大的隐患，对在线的Hadoop业务的影响比直接ReadOnly的影响还要大。 （2）故障处理 硬盘故障检查报修系统从监控系统中获取到磁盘处于WARN、ERRO或FATAL状态后，会向服务器发送磁盘下线指令echo offline &gt; /sys/block/$device/device/state,$device，将磁盘下线并卸载。HDFS Datanode检测到该磁盘为只读，将不再对此磁盘读/写，从而将这块磁盘从业务中下线。 （3）发送故障工单自动化 磁盘故障后，硬盘故障检查报修系统按照磁盘故障进行分类，以及将主机各盘符等信息发送给系统管理员或机房相关处理人员，机房相关处理人员接到工单后，根据机器和盘符位置进行更换，分区格式化。 （4）处理磁盘自动化 硬盘故障检查报修系统通过监控系统获取故障机器的盘符标识为OK状态时，将发送指令echo running&gt;/sys/ block/$device/device/state将该磁盘重新进行上线并挂载，修改权限，然后重启HDFS Datanode进程。 通过如上4步，在Hadoop集群大规模的生产中解决了很大的人力和沟通成本，大大提高了我们在运维中的工作效率。对硬盘的故障检测、修复、上线、下线、报修、结单检测等实现了全方位的自动化运维，同时定期维护和校正硬盘的运行状态信息。 Hadoop服务器环境管理方案Hadoop集群运行时需要很多基础环境和业务类环境支持，比如需要满足集成JCE的JDK、依赖NTP、依赖DNS、Kerberos客户端支持、关闭iptables、部署进程监控工具、关闭SWAP、设定ulimit、集群所有服务的日志需要及时清理等各种各样的需求。开始时处理的方法是通过写脚本在中控机上执行，每上线一批机器，就针对机型以及集群分类对这批机器进行环境初始化。随着机型异构越来越多，需求复杂多样，特别是在某些集群模块需要升级的情况下，要针对不同机型、不同集群、不同需求定制不同的配置文件，这些配置文件都是通过文本文件的方式进行管理的，然后再通过脚本去处理。脚本中控方式的管理逐渐暴露出各种运维管理问题，随后我们开始调研服务器自动化管理工具，选择使用Puppet解决Hadoop集群运维管理的问题。Puppet是一个开源的软件自动化配置和部署工具，它使用简单且功能强大，正得到越来越多的关注，现在很多大型的IT公司都在使用Puppet对集群中的软件进行管理和部署。 Puppet采用C/S星状结构，所有的客户端和一台或几台服务器交互。每个客户端周期性（默认半个小时）向服务器发送请求，获得其最新的配置信息，保证和该配置信息同步。每个Puppet客户端每半小时（可以设置）连接一次服务器端，下载最新的配置文件，并且严格按照配置文件来管理服务器环境。配置完成以后，Puppet客户端反馈给服务器端一个消息。如果出错，也会给服务器端反馈一个消息。然后通过发送邮件的方式发送给Puppet Admin，便于直观地看到每次升级更新的进度。 Puppet的强大功能解决了我们之前遇到的问题，比如通过使用Puppet，解决了Hadoop机型异构管理、Hadoop集群业务支持、软件安装、环境状态保持、用户管理和Crond任务等问题。刚开始时服务器和机房较少，我们只使用了单台Puppet Master，认证和处理都放到同一台机器上。随着机器和机房的数量翻倍增长，CPU到了瓶颈，单台Puppet Master已经不满足所需，经过调研使用了Puppet Master单机多实例，使用Nginx做负载均衡方案，充分利用多核CPU，加快效率。Puppet Master单机多实例架构示意图如图22-5所示。 CA：Puppet认证服务器，处理CA Funtion PM：Puppet服务器端，处理Catalog Request 图22-5 Puppet Master单机多实例架构示意图 随着公司全球化业务的快速扩张，海外机房的数量快速增长，国内到国外的专线已经成了Puppet Agent和Master通信的障碍，经常会出现由于专线带宽较小，导致部署JDK等较大软件包失败的情况。为了迎合多机房在全球的发展，我们经过调研使用了Puppet Master多机房方案，示意图如图22-6所示。在多机房方案中，我们将Puppet CA和Puppet Master拆分，Puppet CA由于安全性要求较高，只部署在国内核心主机房，采用Keepalived走VIP的方式保证CA服务的高可用性。客户端通过配置ca_server指定CA服务器，以达到使用独立CA服务器的目的。Puppet Master要承担主要流量，所以使用Nginx实现Agent请求负载均衡和Master多机多实例。各个机房Agent通过DNS分机房解析进行访问本机房的Master，所有Master的Module我们使用Git统一维护，保持各个机房的Master Module的一致性。 图22-6 多机房方案示意图 Puppet Master多机房配置有以下两大收益。 1. 扩展Puppet Master的SSL传输性能 Puppet使用SSL（HTTPS）协议来进行通信，在默认情况下，Puppet服务器端使用基于Ruby的WEBrick HTTP服务器。由于WEBrick HTTP服务器在处理Agent端的性能方面并不是很强劲，因此需要扩展Puppet，搭建Nginx来处理客户端的HTTPS请求。 2. 横向扩展Puppet Master增加架构的灵活性 有时可能需要提供比单台服务器更多的处理能力，以解决单机单节点故障。在这种情况下，除了纵向扩展外，还可以横向扩展Puppet Master。横向扩展是使用多台服务器提供Puppet Master服务以组成一个集群来获得更多的处理能力。 要提供一个前端的请求处理程序有多种方法和策略可供选择，集群的架构扩展有多种变通与组合方式，我们选择使用HTTP负载均衡技术将客户端的请求直接导向后端服务器。每个Puppet Master都是单独配置。 运维Hadoop集群的常见问题与处理方法HDFS集群不均衡现象HDFS集群非常容易出现服务器之间磁盘利用率不平衡的情况，比如在集群中添加新的数据节点；HDFS集群长时间运行，尤其是在大量的Delete操作后，集群中各个数据节点上的空间使用率可能会存在比较大的差异。防止少数数据节点存储过多的文件。少数使用率过高的数据节点会导致对其的数据访问效率变低，这将引发很多问题，比如MR程序无法很好地利用本地计算的优势，机器之间无法达到更好的网络带宽使用率，机器磁盘无法很好地利用，并且如果该数据节点挂掉了，则需要更多的时间进行恢复，对集群也会造成更大的影响。对于HDFS集群，保证HDFS中的数据均衡是非常重要的。 解决方法：Hadoop中已经提供了均衡机制。 1$ hadoop balancer -threshold &lt;threshold&gt; 控制用户对HDFS资源的使用公共的HDFS集群，在多人使用的情况下，Quota的设定非常重要。尤其是在处理大量数据的环境下，不小心就容易把所有空间用完造成别人无法存取。Hadoop Quota的设定是针对目录，而不是针对用户的，即使目录属主修改了，Quota也依然存在。 - Name Quotas（设定某个目录下文件和目录的总数） 计算公式：Quota -（Dir_count+File_count）= Remaining_Quota - Space Quotas（设定某个目录下使用的空间大小） 计算公式：Space_Quota - Count_size = Remining_Quota 解决方法： $ hadoop dfsadmin -setQuota $Quota $directory # Quota是文件和目录数总和 $ hadoop dfsadmin -setSpaceQuota $SpaceQuota $directory # SpaceQuota是目录使用HDFS空间的大小 $ hadoop fs -count -q $directory # 查看目录的Quota情况 $ hadoop dfsadmin -clrQuota $directory # 取消设定的Quota` HDFS 客户端多集群支持我们现在所有业务的日志收集都是使用Scribe将业务日志打到HDFS集群的，有的业务有一些敏感隐私数据，我们将这些数据放到一个单独的集群中。为了保证所有业务使用同一套Scribe Server，在Scribe Server中使用一个HDFS Client，支持多集群。 解决方法： 使用cluster_nameA集群的Client包。 对cluster_nameA集群的配置文件hdfs-site.xml做如下修改和添加。 修改： 1234&lt;property&gt; &lt;name&gt;dfs.nameServices&lt;/name&gt; &lt;value&gt;cluster_nameA,cluster_nameB&lt;/value&gt; &lt;/property&gt; 添加： 1234567891011121314151617181920212223242526272829&lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.cluster_nameB&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.ha.namenodes.cluster_nameB&lt;/name&gt; &lt;value&gt;host0,host1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.http-address.cluster_nameB.host0&lt;/name&gt; &lt;value&gt;ipB:portB&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.cluster_nameB.host1&lt;/name&gt; &lt;value&gt;ipB:portB&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.cluster_nameB.host0&lt;/name&gt; &lt;value&gt;ipB:portB&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.cluster_nameB.host1&lt;/name&gt; &lt;value&gt;ip:port&lt;/value&gt;&lt;/property&gt;]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二十二章：开源监控系统的选择]]></title>
    <url>%2Funder-the-ops%2F20170923-22-selection-of-opensource-monitor-system%2F</url>
    <content type="text"><![CDATA[监控系统是整个运维环节，乃至整个产品生命周期中最重要的一环，事前及时预警发现故障，事后提供翔实的数据用于追查定位问题。监控系统作为一个成熟的运维产品，业界有很多开源的实现可供选择。在公司刚刚起步，业务规模较小，运维团队也刚刚建立的初期，选择一个开源的监控系统，是一个省时省力、效率最高的方案。之后，随着业务规模的持续快速增长，监控的对象越来越多、越来越复杂，监控系统的使用对象也从最初少数的几个运维人员，扩大为更多的研发、测试、运维人员。这时候，监控系统的容量和用户的“使用效率”成了最为突出的问题，开源的监控系统在功能和性能方面都无法满足业务的需求，所以我们也开始研发自己的监控系统。在本部分，我们会介绍和对比几款有代表性的开源监控软件，以及我们自研监控系统的设计思路与实现。最后，我们会延伸介绍分布式调用跟踪系统以及用户访问质量等相关内容。 开源的或者商业的监控系统很多，具体可以参考维基百科的条目http://en.wikipedia. org/wiki/Comparison_of_network_monitoring_systems。一个完整的监控系统，往简单了讲，主要包括三个主要部分：数据采集、根据策略告警、数据图表展示。我们围绕这三个方面，简单地对现有的几个有代表性的产品做一些介绍和分析。 CactiCacti，最悠久的监控系统之一，2001年9月，一个名叫Lan Berry的高中生，当时他还在为一家小的ISP厂商工作，为了更好地监控网络质量，开发了Cacti的第一个版本，基于RRDtool，提供更友好的使用体验。Cacti的数据采集，采用的是Server端poll的方式，在默认安装情况下，会有一个PHP的Poller，来定期执行相关的数据采集脚本，或者直接采集SNMP的接口输出，采集到的数据会以RRDtool的格式存储到磁盘上，供后续绘图、展示。在要采集的数据较多的情况下，PHP版本的Poller效率较低。为了解决这个问题，Cacti官方提供了一个组件Spine（早期叫Cactid），这是一个用C语言编写的多线程程序，用来代替PHP版本的Poller，有效地提升了数据采集效率。用户可以在Cacti提供的设备管理页，对设备列表进行增、删、改操作，来控制Poller要拉取的任务。Cacti的数据图表展示，底层是基于RRDtool的，Cacti提供了一个用PHP开发的页面，可以让用户很方便地查看每个数据采集项的历史趋势图。Cacti提供了一个树状的graph列表，方便用户高效地组织、管理多个graph。另外，Cacti的graph预览功能很强大，允许在一个页面中放置大量的graph预览图，便于用户了解其所关注的指标全貌。Cacti之所以很强大，原因在于其强大的插件。比如aggregate插件可以对多个采集项进行聚合展示，discovery插件用来自动发现设备，thold插件用来配置告警策略，slowlog插件可以方便地用来分析MySQL的慢查询等。完整的插件列表可以在 http://docs.Cacti.net/ plugins 查看，包括官方支持的和社区支持的。总之，Cacti很强大，用的人很多，社区支持也好，可以作为快速上手的一个不错的选择。 RRDtoolRRDtool，在时间序列数据（time-series data）的存储、展示方面，其独创的round-robin database数据存储格式，基本上是事实上的工业标准了。包括Cacti、MRTG、Collectd、Ganglia、Zenoss等系统，都是采用RRDtool的格式来存储数据，以及使用RRDtool的Graph工具来绘图的。RRDtool使用的数据存储格式，大家也常常称之为环状数据库，其工作方式有三个显著的特点：第一，RRD文件在创建的时候，其文件大小就确定下来了，随着数据的不断写入，RRD文件的大小一直保持不变；第二，数据每次更新到RRD文件的时候，都会触发RRD文件中的归档策略，也就是数据采样策略；第三，查询历史数据的时候，会自动选择最优化的采样数据，而不是全量获取数据，查询效率很高。RRDtool包含了一组工具集，用于创建RRD文件，更新RRD文件，获取RRD文件中的数据，根据RRD文件直接生成相应的图片等，具体可以参考http://oss.oetiker.ch/rrdtool。我们在使用RRDtool的过程中遇到过一些问题，RRDtool的数据是以文件的形式存储在磁盘上，以单机的形式来提供服务的，这样就存在容量上限。该上限的决定因素较多，比如磁盘容量、磁盘IO、CPU等，但是最核心的制约因素就是磁盘IO，用户每push一次数据，都会转化为对相应的RRD文件的一些全量的读/写，磁盘IO会最先遇到瓶颈。在一台普通的Linux服务器上，在1分钟push数据的频率下，一般20万条的Counter上报就会跑满磁盘IO。这显然无法满足较大规模数据下的监控需求。为了在一定程度上缓解磁盘IO压力的问题，RRDtool官方提供了一个组件rrdcached，这是一个常驻内存的后台程序，用户可以把读/写请求通过网络发送给rrdcached，而不是直接操作磁盘。rrdcached内部做了一些优化措施来减轻对磁盘的读/写压力，包括：缓存RRD文件的header部分，每次数据push上来的时候可以减少一次读取操作；对RRD文件的写入，提供了用户态的缓存，即把用户的多次写入操作合并成一次flush到磁盘上，这样有效地提高了写入效率。通过该项优化，使得单机的容量提升不少。不过上述优化，也只能解决一定程度上的问题，整体容量仍然局限于单机的容量上限。 CollectdCollectd相比Cacti、RRDtool来说，较为年轻一些，项目最早是在2005年由Florian Forster开发的，之后便蓬勃发展成为一个开源的项目，很多开发者对其做了大量的改进和扩展。Collectd的定位是收集和传输数据。在告警方面不是Collectd的设计初衷，不过它也支持一些简单的阈值判定，并发送告警信息。要支持更高级的一些告警需求，Collectd可以和Nagios配合使用，有一个名为collectd-nagios 的插件可以很方便地完成这个功能。Collectd是一个用C语言开发的常驻内存的程序，由一堆功能强大的插件组成，其架构示意图如图23-1所示。 图23-1 Collectd架构示意图 插件化是Collectd最重要的一个设计思想，Collectd的所有功能都是通过插件来支持的，Collectd自身没有任何额外的依赖，这使得它几乎可以跑在大多数的操作系统上，包括一些嵌入式系统如OpenWrt。从图23-1来看，用户可以通过各种插件push数据到Collectd，然后通过RRDtool插件存储为RRD的格式，或者通过CSV插件存储为CSV的格式。Collectd支持的上百个插件，可以在https://collectd.org/wiki/index.php/Table_of_Plugins中查阅。 Nagios前面讲到的几个系统，都专注于数据的采集、传输、聚合、存储和展示。说到告警，Nagios可谓是事实上的工业标准，可以用来监控主机和网络基础设施，以及各种应用服务。在监控对象出现问题时，及时发送邮件或者短信通知相关人员；当问题解决后，发送恢复信息。Nagios 从结构上来说，可以分为核心和插件两个部分。Nagios 的核心部分只提供了很少的监控功能，因此要搭建一个完善的监控管理系统，用户还需要在Nagios服务器上安装相应的插件，插件可以从Nagios官方网站http://www.nagios.org 下载，也可以根据实际要求自己编写所需的插件。Nagios可以监控各种网络服务，比如SMTP、POP3、HTTP、NTP、ICMP、FTP、SSH等，也可以监控主机资源，比如CPU、Load、磁盘使用、Syslog等。基本工作模式如图23-2所示。 图23-2 Nagios的基本工作模式 这里介绍两个比较重要的概念：NRPE和SNMP。NRPE的全称是Nagios Remote PluginExecutor，是Nagios的Agent，这可以让Nagios具备监控远程主机和设备的能力。Nagios服务端，通过check_nrpe插件会定期地调用运行在远程主机上的NRPE，执行具体的脚本来获取数据，比如check_load、check_disk、check_ftp等。SNMP（Simple NetworkManagement Protocol，简单的网络管理协议）是一种应用层协议，被路由器、交换机、服务器、工作站、打印机等网络设备广泛支持，主要用于管理和监控网络设备。SNMP的工作方式主要有三种：管理员需要向设备获取数据，SNMP提供了“读”操作；管理员需要向设备执行设置操作，SNMP提供了“写”操作；设备需要在重要状况改变的时候，向管理员通报事件的发生，SNMP提供了“Trap”操作。SNMP的基本思想是：为不同种类的设备、不同厂家生产的设备、不同型号的设备，定义一个统一的接口和协议，使得管理员可以使用统一的方式对这些需要管理的网络设备进行管理。通过网络，管理员可以管理位于不同物理空间的设备，从而大大提高了网络管理的效率，简化了网络管理员的工作。Nagios很好地利用了SNMP的读和Trap功能，很容易地获取各种网络设备的运行数据，达到监控的目的。 Zabbix的使用经验和优化前面介绍了一些常见的、传统的监控系统，我们在初期选型的时候，也都有考虑过，不过最后还是选择了Zabbix。Zabbix作为一款企业级分布式监控系统，功能齐全，用户体验良好，文档完善，API强大，适合于中小规模的公司或者团队使用。Zabbix的主要特点有：◎ 文档齐全，安装、维护、学习成本低。◎ 多语言支持。◎ 完全免费开源。当然，如果需要技术支持的话，Zabbix官方是收费的。◎ 自动发现服务器和网络设备，便于用户配置。◎ 支持告警策略模板的概念，方便用户对一批服务器的监控策略进行管理。◎ 用户体验良好的Web端，用户可以进行集中配置、维护和管理。◎ Zabbix Agent功能强大，默认的采集项足够丰富，且支持用户自定义插件扩展。◎ 不用Agent配合，也可以完成监控任务，支持SNMP、JMX等标准的协议，用户也可以自行推送数据到监控系统中。◎ Web端也提供了良好的Dashboard功能、绘图查看功能。◎ 可以配置历史数据的存储周期，历史数据支持采样存储。◎ 支持分布式监控，比如多个IDC之间，或者有防火墙阻隔。◎ 强大、完善的API支持。在以上特点中，尤其是API功能，完善程度很高，基本上Zabbix的大部分操作都提供了相应的API接口，方便用户编程，和现有的一些系统进行整合。比如以下一些场景。（1）利用历史数据查询API，定期从Zabbix中获取线上服务器的各项资源使用情况，生成每日检查报表；同时，对某些资源使用率不达标的服务器和业务进行筛选，每周进行通报，有效地促进资源利用率的提高。（2）利用Zabbix graph的API，可以对关注的指标获取对应的趋势图，嵌入到各个运维系统中，方便运维人员快速地了解服务情况。（3）利用Zabbix的告警添加API，可以让监控系统和部署系统联动起来。比如某个模块增加了一个实例，那么可以自动添加所需要的监控策略；反之，下线一个实例，可以自动删除关联的监控策略。Zabbix主要由Server、Agent、Proxy和Web-portal几个部分组成。典型的Zabbix的部署模式如图23-3所示。Zabbix的数据采集，主要有两种模式：Server主动拉取数据和Agent主动上报数据。以前者为例，用户在Web-portal中，配置好机器，并给机器应用相应的模板后，Zabbix-server就会定期地去获取Agent的数据，存储到MySQL中，同时根据用户配置的策略，判定是否需要告警。用户可以在Web端，以图表的形式，查看各种指标的历史趋势。在Zabbix中，将Server主动拉取数据的方式称之为active check。这种方式配置起来较为方便，但是会对Zabbix-server的性能存在影响，所以在生产环境中，一般会选择主动推送数据到Zabbix-server的方式，称之为trapper。即用户可以定时生成数据，再按照Zabbix定义的数据格式，批量发送给Zabbix-server，这样可以大大提高Server的处理能力。 图23-3 典型的Zabbix的部署模式 Proxy是Zabbix具备分布式监控能力的一个必备条件，试想我们有一批服务器和网络设备位于防火墙之后，Zabbix-server无法直接访问这些Agent，这时候我们可以选择在防火墙的后面放置一个Zabbix-proxy，那么Proxy就会充当Server的角色，定期收集它所负责的这些Agent的数据，然后定期推送回Zabbix-server。另外，Proxy还可以分担Server的压力，代替Server定期拉取数据，再统一push给Server，这样可以有效地降低Server的开销。在Zabbix的设计中，以下几个概念是最重要的。◎ Host：主机，是Zabbix里面的监控主体，可以是服务器，也可以是网络设备，通过DNS或者IP地址来连接。◎ Item：Host的某个数据采集项，比如hostA的cpu_idle就是一个Item。◎ Template：这是Zabbix的一个很先进的概念，是对具有相同属性和资源的Host的一种抽象。即与某个Template关联的Host，会自动具备该Template所具有的Item、Trigger、Graph等属性。同时Template具备继承的能力。◎ Trigger：触发器，具有三种状态，即unknown、problem和ok。只有当状态从problem变为ok的时候，或者ok变为problem的时候，才会触发相关的Action。当Zabbix-server每次取到Item的值时，与这个Item相关的Trigger都会被检查一次，并生成相应的Event。◎ Action：顾名思义，就是执行动作，Zabbix的Action支持多种动作的执行，用户可以配置满足什么样的条件，做什么样的动作，动作包括发短信、发邮件、执行脚本等。◎ Event：当Trigger的状态每发生一次变化时，就会产生一个Event。Zabbix在业务处于较小规模的时候，效果还是相当不错的。但是当监控的对象超过上千台设备，并且还包括一些服务自身的业务指标也推送到Zabbix的时候，我们遇到了两个严重的问题——Zabbix的性能问题和用户的“使用效率”低下问题。Zabbix的性能问题主要存在两个方面，一是Zabbix-server处理能力有限，尤其当active check模式的采集项较多的时候，会显著消耗Server的Puller线程，使得数据采集延迟，产生堆积，造成报警延迟。我们可以调大Puller的线程数，缓解这个问题，但Zabbix-server自身无法水平扩展，所以不能解决根本问题；二是Zabbix的数据存储引擎存在性能瓶颈，我们线上采用的是MySQL，当数据采集项过多的时候，比如在每分钟大概有20万采集项的规模下，MySQL的写入会达到瓶颈。综上所述，在业务规模较小的前提下，Zabbix是一个很可靠的开源解决方案。在业务规模不断增长的情况下，我们基于Zabbix做了一些优化尝试，在这里分享给各位读者供参考。 测试Zabbix NodeNode是Zabbix官方针对分布式监控扩展需求提出的解决方案。可以通过建立多个Zabbix Node来组成一个层级网络（如图23-4所示），每个Node都可以看作是一个全功能的Server，它负责监控自己所管理服务器的指标，并把历史数据和告警事件同步到自己的主节点。用户可以选择在子节点上来配置策略，也可以集中在主节点上配置好策略，那么策略会被同步到所有的子节点。通过建立节点，可以带来以下好处。（1）监控能力可以大大提高。（2）当子节点和它的主节点网络连通性出现问题时，数据会被暂存在子节点本地，待连接恢复后，子节点会把暂存的数据再次发送给主节点。在图23-4中，所有的节点，其数据和告警事件最终都会被汇聚到节点4上去。 但是遗憾的是，这种Node模式，一方面不够成熟，稳定性较差；另一方面受限于Zabbix当前已有的设计局限，无法很好地在生产环境中使用，且从Zabbix-2.4版本起，这种Node模式已经被官方弃用了。所以，我们对Node模式的尝试，失败了。 Zabbix代码优化和使用模式优化通过分析，Zabbix的性能瓶颈主要体现在两块，一是数据库更新压力大；二是Zabbix的Puller线程忙。针对这两个问题，我们采用了以下解决方案来提高性能。（1）尽量不使用active check模式，而改用trapper模式，主动推送数据给Zabbix-server，来解决Puller线程忙造成的数据采集延迟问题。（2）每次数据更新时，都会更新数据库items表，而items表存在外键约束，导致更新速度太慢，拖累了Zabbix的整体性能。因此，我们通过修改代码，不再依赖于MySQL自身的外键约束来提高整体性能。（3）MySQL所在的服务器采用SSD硬盘来提高数据库的性能。通过这些优化手段，可以充分地提高Zabbix的单机处理能力，但是仍然无法满足业务的快速增长需求，问题仍然在继续。 独立部署多套Zabbix，通过API进行封装整合如果业务需求是成倍地在增长，那么单机版的监控系统无论如何优化，总是存在容量上限。我们迫切需要寻找一个可以水平扩展的方案来支撑业务的快速发展。前面已经讲过了，Zabbix的API功能非常强大，基本上日常用到的所有操作都有相关的API可以调用。因此，我们采用了一个较为稳妥的方案，即部署多套Zabbix，每套Zabbix只单独负责一个业务部门的监控需求。然后通过API，把对多套Zabbix的操作整合到一个Web前端系统中，尽可能地满足运维人员、研发人员的监控配置需求、图表查看需求。这是我们在发展初期最终采用的方案，基本上可以满足业务需求。 OpenTSDBOpenTSDB是目前最优秀的时间序列数据（time-seriesdata）存储和展示的分布式解决方案之一，遵守LGPL开源协议。OpenTSDB具有以下特点。◎ 数据存储：基于HBase，并且存储的都是用户上报的原始数据，不会对数据进行采样和钝化处理，历史数据可以一直保存。支持毫秒级别的数据上报频率。◎ 可扩展性：由于后端的数据存储引擎是HBase，因此可以轻易地支撑每秒上百万次的数据更新操作，并且处理能力随着HBase节点数量的增加而增加。◎ 数据查询：OpenTSDB提供了友好的用户访问界面，方便用户查看相关的数据趋势。另外，也提供了HTTP方式的数据获取接口，方便用户通过编程，自动化地获取HBase中的历史数据，用作其他用途。不仅可以查询单个指标的历史数据，OpenTSDB还提供了强大的聚合功能，比如可以查看多个指标求和后的数据。OpenTSDB的部署结构和工作流程如图23-5所示。HBase为数据存储引擎，TSD是OpenTSDB最核心的组件，和HBase的所有数据交互都通过TSD来完成。TSD是一个常驻内存的进程，是无状态的，可以水平扩展。我们可以通过tcollector主页是https://github.com/OpenTSDB/tcollector来收集每台服务器的各个指标，然后推送给TSD；也可以通过SNMP来获取网络设备的各项指标，推送给TSD。TSD收到数据后，会更新到后端的HBase中。 图23-5 OpenTSDB的部署结构和工作流程图 OpenTSDB提供了Web界面，通过HTTP的接口向TSD查询数据；我们也可以编写一些插件，比如告警插件，从TSD中获取某个指标的数据来判定是否满足阈值，以及是否需要告警。OpenTSDB的出现，让时间序列数据的存储和展示多了一个很好的选择。对于大量写入的场景非常有用。不过也存在一些不足的地方，比如历史数据的查询速度较慢：由于OpenTSDB存储的是原始数据，没有做任何采样，因此在需要查询某几个指标在过去一个月甚至一年的历史数据的时候，TSD真的就会去HBase中扫描相应时段的所有数据。首先，数据量很大；其次，读操作并不是HBase最擅长的；最后，费了好大力气获取到大量的数据，也无法很好地展现给用户，仍然需要应用程序对数据做采样，造成无谓的一些消耗和浪费。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Zabbix</tag>
        <tag>Cacti</tag>
        <tag>RRDtool</tag>
        <tag>Collectd</tag>
        <tag>Nagios</tag>
        <tag>OpenTSDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二十章：数据库备份还原系统]]></title>
    <url>%2Funder-the-ops%2F20170923-20-db-backup-and-recovery-system%2F</url>
    <content type="text"><![CDATA[数据是出于用户对公司产品的喜欢和信任，不断创造和提交的。对于公司来说，这是无比珍贵的资产。如果不幸把数据损坏或者弄丢了，会严重地损害用户的权益，有可能导致严重的用户流失。读者可以想象一下，如果facebook的用户数据一夜之间消失了，用户会有什么反应呢？如果说对facebook的体验还不强烈，可以再想象一下，若此时存放着你所有存款的银行突然告诉你，他们把数据弄丢了，而且没法复原，你的银行存款都只能清零，你还能继续淡定地看下面的文字吗？ 相信通过上面的两个例子你已经理解了数据的重要性，那也就能明白对于整天和数据打交道的DBA来说，数据备份在日常工作中占据了多么重要的位置。对于DBA来说，不怕操作出问题，就怕出问题后没有备份数据进行修复。对数据的任何操作都有风险，一旦出现误操作、数据损坏等意料之外的情况，就可以随时用备份数据把数据恢复到操作前的状态。而如果需要进行数据修复时，却无法找到备份数据进行恢复，这种抓狂和绝望，相信大家或多或少都体验过。为了避免这种虐心的体验，我们把数据库备份还原系统的开发工作放到了第一优先级的位置。经过三个主要的版本开发，备份还原系统已经在生产环境中提供自动创建备份任务、定期进行备份、自动数据恢复测试的功能，保障了业务数据100%的备份成功率和备份数据有效性。本章我们就来介绍数据库备份还原系统的发展过程。 为了避免这种虐心的体验，我们把数据库备份还原系统的开发工作放到了第一优先级的位置。经过三个主要的版本开发，备份还原系统已经在生产环境中提供自动创建备份任务、定期进行备份、自动数据恢复测试的功能，保障了业务数据100%的备份成功率和备份数据有效性。本章我们就来介绍数据库备份还原系统的发展过程。 单机备份工具我们的第一套备份系统是在每台服务器上单独运行一个备份任务，任务管理都是靠DBA手工进行的，或许这个阶段称之为备份工具会更合适。 简单来说，单机备份就是在需要进行数据备份的服务器上安装一个备份工具，比如MySQL官方自带的Mysqldump工具，或者第三方开源的产品Mydumper或Xtrabackup，通过crontab等方式调度，备份数据存储在本地服务器上；需要进行数据恢复时，从存有备份数据的服务器上拷贝一份数据到需要进行恢复的服务器上。 单机备份是最简单的一种方式，靠人工来管理备份任务，并需要定期检查备份任务运行是否正常。这种方式上手快，门槛低，所需的开发工作量也很少，适合用在数据库实例比较少、每个实例体量不大的场景下。 数据库集群数量少的时候，单机备份方式还能满足日常备份需求，但随着数据规模的扩大，问题就凸现出来了。 ◎ 备份数据存储周期不长：毕竟是存储在本地数据库服务器上，一般情况下，为了保证数据库服务的IO能力，磁盘都选用高IO能力、低存储空间的高转速SAS盘或者SSD盘，再加上做了RAID，实际可用的存储空间本来就没有多少，如果还要存储几个周期的备份数据，空间资源捉襟见肘。 ◎ 容易影响数据库运行：因为任务运行和数据存储都与数据库服务在同一台服务器上，每次运行备份任务时，都会同时出现大量的磁盘读/写操作，容易和数据库服务争抢IO资源，影响服务的响应效率；进行数据还原时，拷贝备份数据也会有大量的磁盘读操作，同样也会影响到正在运行的数据库。有的读者也许会说，可以部署一台数据库服务器，专门用来做备份。这样当然可以，不过本着节约的原则，我们还是不推荐这种方式。想想：当你有几百上千个集群时，会有几百上千台服务器专门用作备份，你说这事怎么给老板解释？ ◎ 维护代价高：靠人工维护，不仅容易出错，而且每次变更调整效率也会有所折扣。另外，由于备份任务零散地分布在多台服务器上，只能靠人工记录任务和服务器的对应关系，正确性难以保证，时间一长，容易出现遗漏或错误。 提高备份系统的自动化程度，加强对备份任务的管理，减少DBA在备份管理上的时间投入，这是我们开发第二版备份管理系统的出发点。 集中管理备份系统为了便于统一管理和调度备份任务，我们设计了任务管理模块，把所有备份任务的信息都进行集中记录和维护。管理模块主要有三个功能。 ◎ 任务管理：所有的备份任务管理都由任务管理模块负责，可以添加、删除、修改备份任务，或者激活、停止某个任务的运行，并记录每一次任务运行的状态，比如什么时候在哪台数据库上启动了备份程序，备份过程运行了多长时间，备份策略是全量还是增量，运行结果是否正常，如果正常，备份数据占用了多少空间等，所有信息一目了然，便于DBA每天对备份任务进行常规检查。 ◎ 任务调度：通过时间事件来调度备份任务。当时间事件触发某个备份任务开始运行以后，管理程序先在本机开启一个TCP端口，用于接收客户端发回的备份数据；然后远程调用部署在MySQL服务器上的备份执行程序，执行程序通过Xtrabackup开始数据拷贝，并将备份数据通过流式（stream）发回到备份服务器集中存储。在备份过程中，只有一次数据读取操作，不在MySQL服务器上再次写入数据，减少了对IO的影响。 ◎ 数据存储：所有的备份数据都通过网络传回管理模块所在的服务器统一存储，便于进行查找和管理。 同时我们设计了任务执行模块，主要负责实施备份操作，完成管理模块发送过来的备份任务，并将备份出来的数据发送给管理模块。如果在执行过程中出现任何错误，执行模块还会将错误信息的上下文反馈给管理模块，并停止任务运行。对现有开源的备份工具进行比对并结合生产环境情况综合评估以后，我们选择了Percona Xtrabackup作为基础备份工具。Xtrabackup可以实现InnoDB表的无锁在线备份，降低了备份任务对数据库正常运行的影响。Xtrabackup是执行模块的重要组成部分。 集中管理备份系统的结构示意图 集中化管理可以有效地提高DBA的工作效率，对备份任务的管理（增、删、改、查）只需要在页面上用鼠标点击几下就可以完成，每天的备份任务运行情况也一目了然。这种方式具有开发量小、管理直观快捷的优点，特别适合用在中等规模的数据库集群中。用户可以通过Web管理页面查看和管理所有的备份任务，如图所示。 备份任务管理 备份任务运行报表 随着数据规模的继续增长，这套系统也逐渐难以满足需求，下面是暴露出来的主要问题。 ◎ 存储空间饱和：由于所有的备份数据都要传回存储模块进行存储，在备份任务数量增多的同时，存储空间的争抢问题变得愈加严重。 ◎ 并发控制不便：由于备份任务是通过时间事件进行触发的，有可能出现在部分时间区间大量的备份任务一起运行，备份存储服务器的IO压力非常大；而有些时段又没有备份任务运行，资源白白浪费掉了。 ◎ 无法进行水平扩展：为了便于在备份过程中就将备份数据传回存储模块进行存储，任务管理模块和存储模块是整合在一起的，导致了严重的中心化，要扩展存储模块非常麻烦。 细心的读者可能会发现，由于任务管理模块和调度模块是整合在一起的，而调度模块和存储模块又相互依赖，管理模块的中心化导致了后面两个模块无法做到水平扩展，而出现瓶颈的主要是调度模块和存储模块。如果能把任务管理模块解耦，设计成独立的一个部分，就可以对后两个模块进行扩容了。基于这个思路，我们对第二版备份系统进行了改版，把任务管理模块和调度模块分拆为两个独立部分，并引入了优先级队列，去除了时间事件驱动的方式，形成了第三版备份管理系统——分布调度备份系统。 分布调度备份系统第三版备份系统由三层逻辑组成，第一层是任务管理模块（Task Manager），和第二版的任务管理模块功能类似，用于管理所有的备份任务和记录元信息；主要的改变就是增加了一个优先级队列。每个备份周期开始时（如每天零时），管理模块会根据现有的所有处于激活状态的任务的重要程度，生成一个备份任务优先级队列，然后等待处于第二层的调度模块来请求任务。 调度模块（Scheduler）的调度策略和第二版不同，放弃了时间事件触发的方式，而是采用了自发请求任务的方式。如果调度模块发现当前自己正在运行的任务数N，小于允许运行的最大任务数M，即N&lt;M，就会向任务管理模块发起一个备份任务申领请求，如果管理模块的任务队列中还有待执行的任务，就会将处于队首的一个任务下发给调度模块，调度模块成功接收到新任务后，根据任务信息开始执行一个新的备份操作；如果任务队列中已经没有需要执行的备份任务，调度模块会进入休眠期，休眠结束后，再重新询问管理模块。 执行模块的功能和第二版相同，只负责执行具体的备份操作，没有任何变化。 分布调度备份系统的三层结构示意图如图21-4所示。用户可以继续通过Web页面管理和查看备份任务，也可以通过API的方式和管理模块进行交互。 分布调度备份系统的三层结构示意图 第三版备份系统的主要特点如下： ◎ 任务管理集中化：所有的任务依然是集中管理，通过Web页面或者API的方式，可以管理和查询所有任务的信息和历史运行状态。 ◎ 有效的并发控制：每个调度模块都可以根据服务器的配置，调配允许同时运行的任务数，使备份服务器的压力可控。 ◎ 方便水平扩展：如果磁盘存储空间不足，则可以通过增加服务器的方式快速扩容，新增加的服务器可以自动注册到任务管理模块中，并申请任务运行，管理代价很小。 ◎ 数据分布存储：备份任务是随机在不同的调度模块（存储服务器）上运行的，每个备份任务的备份数据都会分布在多台服务器上，如果其中一台存储服务器挂掉了，损失的也只是一部分备份任务的某几次备份数据，不会出现某个任务的全部备份数据丢失的情况。 ◎ binlog备份：前面介绍的备份都是某个时间点的数据镜像，如果需要恢复到任意时间点，就需要使用binlog。在第三版备份系统中，支持binlog备份的需求。当用户添加了一个日常备份任务以后，备份系统通过获取数据库实例的复制信息（Slave status），找到当前实例的上级主库（或者主库就是该实例自身），然后在备份服务器上启动一个程序，模拟成数据库从库，向主库实时同步binlog任务，达到更新数据实时备份的目的。 ◎ 自动数据还原：由于任务管理模块记录了所有备份数据的信息，这些信息就可以用来实现自动还原。自动还原过程类似于备份过程，只不过数据传输方向是相反的。当用户发起一次数据还原请求时，备份系统会找到一份符合条件的备份数据，并将数据发送给需要恢复的目标服务器，进行恢复操作，如果有需要，还可以自动建立新实例和主库的复制关系，完成数据恢复过程。另外，结合binlog备份，可以恢复数据到任意时间点，供DBA或者研发人员分析、测试使用。 ◎ 自动恢复测试：备份系统会周期性地对备份数据进行恢复测试，检验备份数据的可用性和正确性，如果发现异常，会及时通知相关的负责人员。对备份数据定期进行恢复测试，可以有效地保证备份数据的正确性和有效性，确保在需要的时候能有一份可用的数据副本。 总结本章介绍了数据库备份还原系统发展的主要历程和现状。备份还原系统共经历了三个大版本的发展，从最初的单机备份，发展到目前在役的分布调度备份还原系统，实现了备份管理自动化、任务调度自动化和可扩展、数据恢复测试自动化等重要功能。在保证数据备份覆盖率和成功率的同时，有效地提高了DBA的工作效率，减少了人工操作的时间投入和误（漏）操作的数量。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Database</tag>
        <tag>Backup</tag>
        <tag>Recovery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十九章：数据库自动运维系统]]></title>
    <url>%2Funder-the-ops%2F20170923-19-db-automatic-ops-system%2F</url>
    <content type="text"><![CDATA[在DBA的日常工作中，会有很多重复性的、没有太多技术含量但是又不得不做的事情，比如协调服务器、操作系统环境初始化、部署数据库、添加监控、备份、建立主从复制关系、增减从库、切换主库、上线变更操作等。 这些事情日复一日、年复一年地占用了DBA非常多的时间和精力，以至于只有少量时间，甚至没有时间去做一些更重要的事情，例如参与业务的设计评审、数据库优化、了解数据库底层原理、开发自动化工具等。对于公司来说，这是一种人力资源的浪费；对于DBA团队成员自身发展来说，长期重复这些工作，会感觉到没有收获、没有成长，非常容易产生厌倦、懈怠的情绪。 针对如何提升DBA的工作效率，增加产出质量，我们对DBA的日常工作做了一些分析和总结，如图19-1所示。从服务器上架开始，到数据库服务下线为止，DBA的大部分工作都是重复性的，如果这些事情能由程序自动完成，DBA就可以将更多的精力放在其他更为重要的工作上，而且还能提升操作的正确性和时效性。 图19-1 DBA主要工作细分图 数据库运维发展过程我们的自动化运维系统发展经历了三个主要阶段。 （1）先形成点：手动操作过渡到多点半自动在经过一段时间的纯手工操作后，我们开始把其中的一些操作写成简单的Shell脚本，比如用脚本下载源码包、编译安装，并自动设置一部分配置项；这个阶段开发了一系列Shell脚本，每个脚本相对独立地完成各自的工作。DBA在需要时，调用所需的脚本完成不同阶段的操作；抽象地看，就像一个一个的点，各自没有关联。 （2）点连成线：多点半自动过渡到少量半自动流程一系列自动化脚本虽然能有一些帮助，但是由于相互独立，在操作期间还是需要DBA过多地介入其中，等待一个脚本完成后，检查结果，如果正确，再调度下一个脚本。为了继续减少DBA的介入度，我们开始尝试将一些点连接起来，形成线。例如部署完数据库程序后，会自动添加监控和备份任务。不过这个阶段相比上个阶段没有质的变化，只是提高了聚合度，进一步减轻了DBA的工作量。 （3）线形成面：少量半自动流程过渡到自动化系统积累了一定的自动化处理经验后，我们开始尝试把依然是零散的线段变成一个面，形成一套联动的系统。这套自动化系统也是本章重点介绍的内容。 设计概述在设计数据库自动运维系统的过程中，我们遇到了一些问题和选择，下面简单介绍一下。 数据库集群结构的选择目前市场上可见的公有云实现了下面几种数据库集群结构，各种设计都有自己的优点和不足，没有哪个更好，只有适不适合自己的业务场景而已。 ◎ 单实例：只存在一个实例，没有对应的从属实例。这种结构部署和管理起来非常简单，不涉及集群数据一致性问题；不过缺陷也非常突出，严重的单点，一旦宕掉，恢复可能需要比较长的时间，而且可能会丢失大量的数据。这种结构比较适合对服务在线率和数据完整性要求很低的服务，比如测试场景。 ◎ 一主一备：在单实例的基础上，增加了一个备库（隐藏的，业务不可见），可以缓解单点故障对服务稳定度的影响，也能减少数据丢失的风险；但是由于备库是不可见的，不能分担只读请求，所有的读/写压力还是继续由主库承载，存在一定的资源浪费。这种结构比较适合一些读/写量不大的小服务。 ◎ 一主一从：相对于上面的一主一备方案，一主一从方案把备库放到了前台，可以提供给前端业务读取数据使用，避免了备库不能使用的资源浪费。读/写分离可以通过部署数据库Proxy实现，或者提供一个读/写地址和一个只读地址，由应用端自己进行读/写分离。这种结构适用于大部分读/写量都不是很大的小应用。 ◎ 一主多从：对于一些大型服务，特别是读/写比率很高的服务，如果只用一个从库来承载大量的读请求，则有可能导致从库压力过大，复制延迟增大，请求响应时间变长等，因此需要随着读请求的增长不断地增加读副本。由于我们主要面对公司内部服务，且这些服务的数据库请求量（特别是读请求）都是飞速增长的，因此必须支持扩展从库的功能。为了避免单机房故障，我们还需要将多个从库分布在至少两个机房中，保障如果一个机房出现故障，可以快速地恢复服务，继续为广大互联网用户服务。 复制方式复制有三种方式，即异步复制（asynchronous replication）、同步复制（synchronous replication）和半同步复制（semi-synchronous replication）。异步复制就是MySQL长期以来默认选择的模式，优点是对主库性能影响最小，但是不能保证主库新增的数据能够及时复制到从库上。 同步复制的数据一致性最强，但是对主库的性能损耗也最多，适合一些宁可牺牲一部分性能，也要保证数据完全一致的场景，比如涉及用户交易数据的业务；对于同步复制感兴趣的读者，可以研究一下采用同步复制机制的Percona XtraDB Cluster。MySQL从5.5版本开始，引入了一种折中的复制方式，即半同步复制，能保证MySQL主库的更新日志（binlog）至少有一个强一致的副本，但MySQL中的数据依然是最终一致的。 半同步机制的数据一致性和性能损耗介于异步和同步两种方式之间。经过综合对比和考虑，我们采用了半同步复制方式，通过牺牲不是太多的性能，换来binlog日志的强一致。 MySQL实例和数据库部署方式的选择目前MySQL实例和数据库部署方式只有两种，一种是单实例多库，即在一个MySQL实例中创建多个数据库（database），每个数据库的业务逻辑是独立的。在传统的数据库运维中，这种方式很常见，好处非常明显，减少了运维成本，还可以在一定程度上提升服务器资源利用率；但是缺陷也很明显，不同的库需要维护不同的账号权限，而且非常容易出现资源争抢、同步延迟等问题。 针对单实例多库的情况，MySQL 5.6及以上版本引入了多线程复制，用来缓解一个库的复制延迟导致其他库出现被动延迟的情况。目前原生的MySQL无法对同一个实例中的不同库进行资源的限制和隔离，国内某互联网公司自己修改了MySQL的内核，支持对同实例中的不同库进行独立的资源限制，不过此举代价大，收益小，我们暂不考虑。 另一种是单实例单库，即一个实例中只创建一个数据库，不同业务逻辑的数据库使用不同的MySQL实例。单实例单库的方式可以用较低的成本，监控每个数据库实例的压力、数据增长情况，备份、数据迁移操作不会对其他数据库实例造成影响，对于自动化运维系统来说，这种方式在部署、监控、调度、授权方面都会更加容易实现。 另外，我们在早期制定并推广的《数据库开发规范》中，就强制禁止使用跨库操作（包括跨库事务），所有的读/写操作必须限制在一个库中，程序使用的账号也只授权到一个库，不能有跨库授权，如果对其他数据库也有访问需求，则需要使用另外的数据库访问账号，由程序来进行数据的聚合。有了这个基础，我们才能放心地选择这种部署方式。 资源限制和隔离在自动化运维背景下，一台服务器上部署多个数据库实例成为必然，充分地利用服务器硬件资源，减少单服务的运行成本。 多实例部署带来的一个问题是资源争抢。在资源有限的情况下，为了避免共享同一台服务器资源的多个实例间相互影响，需要限制每一个实例所能使用的资源。资源隔离主要有两种方案，一种是传统的虚拟化技术；另一种就是现在发展得如火如荼的Container技术。 两种技术的主要区别如图19-2所示，具体的细节这里就不展开叙述了。总之，Container相比虚拟机来说，更加轻量，资源的损耗也更低，管理起来方便很多。对比了多种方案后，我们选择了Docker。Docker底层是基于LXC技术的，可以有效地控制进程的CPU、内存、IO、网卡流量等资源的使用，并且可以动态调整限制。 图19-2 Container与虚拟机 单版本和多版本的支持在自动运维系统中是否要支持多版本？这个问题并没有困扰我们多久，经过简单的内部讨论后，就达成了一致，只提供一个数据库版本，节省了运维成本，也避免了过多的选择对业务开发人员造成困扰。 高可用方案（1）主库的高可用在以前的手工运维中，我们一直使用MHA作为主库切换的首选工具（MHA是一个用Perl语言实现的开源数据库主库切换工具，适用于一主多从场景下的切换操作，可以实现故障监测、数据补齐、自动选主等功能），MHA在多次的故障切换（Failover）和在线切换（Online Switch）操作中，保证了切换操作对业务的影响降到最小，减少了切换过程中服务不可写数据库的时间。在长期的使用过程中我们积累了大量的经验，因此在自动运维系统中，我们决定继续采用MHA来辅助完成切换。 在正常方式下，创建一个新的MHA任务，需要先生成一个配置文件，里面记录处于同一个数据库集群的数据库服务器列表和连接信息，然后建立好服务器间的信任关系，最后启动一个守护进程masterha_manager，用来监测主库的存活状态。如果主库出现故障，masterha_manager会自动进行故障切换；或者通过调用在线切换脚本master_iponline change进行在线主库切换。 由于MHA任务的管理改由程序完成，所以我们进行了一些改造。首先增加了一个全局的MHA守护管理模块，用于和自动运维系统进行交互，接收运维系统发送的配置信息和切换命令，并调度MHA进行相关的操作，然后将相关操作结果告诉运维系统。 对于MHA来说，有三点改变。◎ 不提前准备MHA任务配置文件：数据库集群信息由运维系统管理并维护，在需要进行切换的时候再提供给MHA管理模块，减少了配置文件更新和维护的代价。 ◎ 不监测主库的存活：MHA不再启动守护进程来主动监测主库的存活状态和发起切换动作，是否进行切换和什么时候切换由运维系统来判断和决策。 ◎ 不自主选择新主库：运维系统会根据备选实例的实际运行情况，比如服务器压力情况、所在机房、机架等综合信息，选择出一个新的主库，MHA按照新的主从结构进行切换，如图19-3所示。 图19-3 MHA模块结构图 （2）从库的高可用我们在数据库集群上面加入了代理层Proxy，从库的故障切换就由Proxy完成；Proxy将数据库读请求负载均衡到多个从库上，同时对从库进行存活性检查，及时摘掉失效的从库，避免单从库故障影响正常业务的读请求。 （3）数据库Proxy引入数据库Proxy的主要目的如下。 ◎ 统一入口：屏蔽后端数据库服务结构和变动：数据库集群常常会有调整，比如增/减只读实例，集群扩/缩容，某个实例因为某种原因出现长时间延迟等，这些变动应该对前端服务透明，尽量让用户不感知变化，只要连接到一个地址，就可以使用数据库服务。 ◎ 只读流量负载均衡：当有多于一个的只读实例时，将读请求尽量均匀地分配到每个实例上；如果某个实例所在的服务器压力过大，则可以动态地减少此实例的读请求，由其他实例分摊压力，减少整体服务器的压力。 ◎ 用户授权：用户完成认证后，会发起数据库访问请求，Proxy需要验证用户的身份，如果合法，就授予用户相对应的库表访问权限；否则，拒绝访问。 ◎ 流量控制：如果服务流量突增导致数据库集群（特别是主库）压力过大，无法正常提供服务时，Proxy通过降低并发、延迟转发等服务降级机制，可以缓解后端数据库的压力，避免数据库彻底不可用，使服务完全失效。 ◎访问日志：类似于MySQL的general log，由于MySQL自身对IO的消耗比较大，且大量的日志会比较消耗磁盘空间，而Proxy服务基本没有IO操作。另外，Proxy作为流量的总入口，日志记录也是最全的，无须再做合并。 ◎ 故障重试：如果后端只读实例宕掉了，在Proxy层将故障实例摘除前，依然会有一部分请求被转发给故障实例，Proxy需要将响应失败的请求，重新发送给其他健康的实例，减少故障影响。 ◎SQL防火墙：检查将要转发给数据库的SQL，如果不符合预定的规则，则拒绝转发，目的是为了在一定程度上防止拖库，或者防止一些性能有问题的SQL（如全表检索）对数据库造成伤害。数据库Proxy逻辑结构图如图19-4所示。 图19-4 数据库Proxy逻辑结构图 系统结构介绍通过对数据库服务运行生命周期的分析，我们将自动运维系统分成两个模块来实现，一是部署模块，主要功能是管理服务器集群、安装数据库服务以及相关组件（例如监控、备份程序）；二是数据库实例管理模块，主要功能是管理数据库实例的运行状态、角色变更以及权限管理等。 部署模块不涉及任何数据和状态的维护，实现比较简单，业界也有很多比较好的开源解决方案，经过比较，我们选择了Apache Mesos + Marathon + Docker方案，其他的解决方案还有Kubernetes、Flynn等，大家可以自己去了解，这里不做介绍。 如图19-5所示，应用程序部署工作由Marathon调度Mesos完成，程序运行在Docker容器中。关于Marathon、Mesos和Docker的资料网上有很多，各位读者可以自己去了解。下面重点介绍数据库实例管理模块的设计细节和流程。 图19-5 系统结构示意图 部署模块完成相关的应用程序部署后，会反馈对应的部署信息，比如部署任务（Task）运行在哪些服务器上以及对应的端口、健康检查结果等。数据库实例管理模块以这些信息为基础，开始接手数据库集群的维护工作，主要如下。 选择主实例部署模块提供的只有实例的物理信息，对于MySQL集群来说，首先需要选择一个实例作为主库。由于我们使用的服务器集群的硬件配置完全相同，因此选主策略相对简单，目前我们的策略是选择整体资源利用率最低的一台服务器上的实例作为集群主库。后续会考虑加上机柜位置等更加全面的信息，来综合选择一个从物理逻辑上来说更加安全（最少单点）的主库。 建立复制关系主库选择好以后，其余实例的角色就确定了，接下来的步骤按部就班：查看主库的binlog偏移量，然后创建具有复制权限的账号，所有从库建立和主库的复制关系，检查同步状态。不过这些步骤是通过代码来自动完成而已。 访问授权访问授权分为两层，第一层是MySQL对Proxy层的授权，因为Proxy服务器是明确的，所以比较简单，直接使用MySQL的授权就行。 第二层是Proxy层对业务服务器的授权。连接MySQL时，需要提供对应的用户名和密码，MySQL根据用户名、密码、机器名（IP地址）三元组对请求进行认证，如果认证通过，则提供所需要的数据读/写权限。但是在动态部署环境下，需要访问数据库的业务服务器不能提前确定，而且变动会比较频繁，所以认证过程中的机器名（IP地址）是一个不确定变量。为了提升数据的安全性，需要严格限制能访问数据库的列表，这样机器列表的维护代价比较大；如果为了方便而开通网段级别授权，风险又比较高。 前端开发者在部署服务的时候，肯定知道自己所使用的服务器列表，如果他们能在服务自动部署完成以后，自动将服务器列表注册到一个固定的地方，那么数据库就可以以这个列表为基准进行授权了。 基于这个思想，我们把数据库的认证和授权两个过程进行拆分，建立了一个认证中心，当用户新部署一个服务时，会用提前申请的账号在认证中心注册一些基础信息，部署完服务后，相关服务的元信息就会提交到认证中心。如果服务需要访问数据库，会先通过用户名和密码到认证中心认证自己的身份，如果认证通过，服务会拿到一个唯一的认证码，然后拿着认证码到Proxy请求访问数据库，Proxy拿着认证码到认证中心进行确认，得到确认答复后，Proxy会得到一些关于服务的元信息，Proxy根据这些信息授权服务能够访问哪些数据。 通过这个流程，数据库就不关注服务到底从哪台服务器上发起请求了。可能有些仔细思考的同学会问，如果服务B假冒服务A在认证中心进行注册，意图非法访问服务A的数据怎么办？针对这个问题，我们在服务部署的过程中已经通过类似于身份确认的机制，确保不会出现假冒认证。 设置代理将数据库集群的信息注册到Proxy上，便于Proxy将数据库请求路由到对应的数据库实例上。 添加监控数据库实例部署完成后，实例管理系统会自动部署监控Agent，自动将监控数据汇报给监控服务（比如Open-Falcon、Zabbix）。监控数据从操作系统级别和数据库级别进行采集。 操作系统级别：◎ 服务器级别的存活监控：通过Ping等手段检查服务器是否存活。◎ 服务器级别的性能监控：服务器整体的资源使用情况，比如CPU利用率、内存占用率、网卡使用率等；容器级别的资源使用情况。 数据库级别：◎ 数据库存活监控：由专用的监控服务器检测数据库实例是否可以访问；Marathon也会对MySQL实例进行健康检查。◎ 数据库性能监控：数据库内部的运行状态监控，包括但不限于InnoDB引擎数据读/写量、InnoDB锁等待情况、死锁信息；MySQL全局QPS、随机读/写情况等。 添加备份任务数据库实例部署完成后，实例管理系统会发送请求给我们基于Xtrabackup开发的一套数据库自动备份系统，建立定期备份任务。 备份系统可以自动完成数据全备、增备、binlog日志备份，还支持自动将数据还原到任意时间点。具体详见下一章内容。 下线当数据库集群完成它的使命后，由创建者手动触发集群销毁过程。销毁过程开始后，首先会清理掉LVS上的转发配置，关闭入口，防止再有新的数据库请求，然后关闭数据库实例，并将数据推送到离线备份集群中进行长期保存，最后销毁Proxy和MySQL容器，完成集群的清理。 总结本章对数据库自动运维系统的主要结构和设计思路进行了介绍，通过这套系统，把DBA从日常大量重复的体力劳动中解放出来，让他们有时间去思考更重要的事情。毕竟人是有创造力的，如果精力都困在了那些重复性的体力劳动上，那是对人的不尊重。后续我们会继续完善整个系统的功能，比如在Proxy层加入数据分区功能、开发自动化上线系统等，进一步将人力解放出来。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Database</tag>
        <tag>Automatic Ops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十八章：动态调度]]></title>
    <url>%2Funder-the-ops%2F20170923-18-dynamic-scheduling%2F</url>
    <content type="text"><![CDATA[传统部署和动态部署互联网行业一直以来以快为利器，常以天下武功唯快不破为行事原则，功能开发快，版本迭代快，访问速度快，仅快还不足矣，须是快且稳，如同钢丝上的行走艺术；在运维管理方面，更需要适应和支撑这样的标准和要求，由此面临着如下几大方面的挑战： ◎ 快速的业务部署◎ 实时的运行状态监测◎ 准确的故障诊断和处理 在原始的手工运维时期，应付业务的快速部署就占用了大量人力成本，数十分钟甚至数小时的发布上线过程屡见不鲜，在冗长、复杂的发布过程中，往往还伴随着未预期的故障和异常，当发布的结果不符合预期时，又需要再执行复杂的回滚过程；随着规模的变大、流量负载的快速增长、故障率的升高等变化，运维人员更是焦头烂额，深陷泥潭。这其中有业务架构设计的问题，但更多的是缺乏运维规范和框架，放任业务自由发展造成的。 为了摆脱这种局面，发展出了基于各种技术方案的平台化部署系统，摸索出了编译、打包、发布、启停控制等一系列问题的解决方案和运维标准，在很大程度上解决了手工运维的问题，实现了较自动化的业务发布。但传统平台化部署只解决了运维规范和自动化发布问题，在集群容量管理、资源预算分配、故障检测判断上仍然完全依赖于人工或其他系统的辅助。这里并不是说完善的部署系统就应该完全包含这些功能，而是说业务的部署并不是单向地完成发布和启动就算搞定一切，更合理的方式应该是双向的反馈，使得业务部署和线上运行状态形成闭环，从而使系统能满足更多的自动化需求。这便是本章将要讨论的动态部署系统。 动态部署在某种意义上也属于PaaS，对业务架构分层，解除各层之间的强耦合，通过服务发现关联业务架构的各层和各应用模块，实现各业务层和模块都能水平扩展的能力；并基于应用的容量管理和系统资源量化自动计算，确定应用的实例数量和资源分配，减少人工介入，降低运维成本，提高运维效率和业务稳定性，优化资源利用率。 但与PaaS有所不同的是，PaaS系统通常明确地划分了接入、业务逻辑、Cache、存储等各层，只有业务逻辑层对用户可见，其余各类服务只提供服务调用接口，并不直接受业务控制。这种结构虽然有助于使业务更结构化和标准化，降低开发和管理成本，但在实际应用过程中也逐步显现出其自身在规模化和私有环境中应用的一些局限性。 例如：◎ 架构标准化程度要求高, 需要按照PaaS平台结构对业务进行拆分◎ 业务类型受限。由于接入层不受用户控制，通常要求是标准的HTTP服务◎ 业务复杂度受限◎ 商用PaaS平台对开发语言的支持有限，通常只支持部分主流语言 面对私有业务中种类繁多的个性化需求，常见的PaaS平台很难覆盖所有场景，而让所有业务全部改造和匹配PaaS的标准也很难达成。动态部署系统进一步打破了PaaS的限制，让整个业务架构从接入到存储，各个环节都如同PaaS的业务逻辑层一样，从应用角度看到的是模块化、PaaS化和层次化，从运维角度看到的是集群化和标准化。 下面我们就动态部署系统能够解决什么问题，以及如何实施来进行具体讨论。 动态部署系统的目标动态部署系统的目标和意义体现在如下几个方面。（1）减少工作量在传统部署中，大量工作需要人工管理和维护，即使平台化可以帮助记录和存储，也仍然需要人工判断，决定如何使用和分配；而在动态部署系统中，以下资源或行为将转变为自动维护和决断，不再需要人工干预。 ◎ 主机资源分配和预算◎ 服务关联关系的维护◎ 配置的变更◎ 容量的伸缩◎ 异常退出、重启等与实例相关的常见故障处理 （2）提高部署效率快速的部署和故障恢复能力是保证业务稳定运行的关键，动态部署通过自动化的资源分配和部署，使服务容量的伸缩更简单和操作更容易，通过双向反馈机制更能及时发现故障并恢复容量。 （3）提高资源利用率在人工管理容量和资源时，一般的做法是将冗余和突发考虑到资源的分配和使用中，每个部门、每个业务、每个应用分层都会设置冗余，这样是为了提高业务系统在出现异常时的应对能力，但是闲置和浪费了很多资源。 而当资源具备统一的管理和分配能力后，就可以从整体的角度考虑冗余，当某些服务器存在空闲资源时，基于任务的调度和容量的伸缩将空闲资源提供给其他需要的业务使用，提高硬件资源的使用率，并且完全不依赖人工干预。 动态部署系统的设计思路顾名思义，动态部署的关键是“动态”，要达到动态至少满足： ◎ 不需要对业务部署进行提前规划，可实现应用实例动态创建、服务动态发现◎ 不需要在多个环节、多个阶段人工介入或确认，将事务交由系统完成，实现名副其实的一键发布◎ 能根据业务容量、处理能力、故障异常等实时指标，动态触发变更 下面进一步讨论各点的必要性和可行性。在传统部署中，业务部署必须进行提前规划，主要有如下几方面原因。 ◎ 通过规划来分类部署，实行标准和统一化，便于维护。◎ 通过规划来估算资源消耗，评估应用混合部署的可行性，进行资源分配和预算。◎ 通过规划来确定业务的上、下游关联关系，便于进行配置。 然而，提前规划业务部署从运维角度来说却是一把双刃剑。虽然使事情具有相当的计划性和可控性，但是严重依赖人工，使这部分工作很难转换成自动化处理。所以，传统部署的自动化，只是对人工提前规划结果的自动处理，并非完整的部署自动化。当规模达到一定程度时，提前规划也是一项相当繁重的任务。 因此，对业务部署进行系统性的自动化就务必要消除人工的提前规划。相应地，就需要将原来人工规划的资源、人工决策的信息转换为自动的管理方式，包括： ◎ 统一的服务器资源池管理◎ 统一的应用容量管理◎ 可靠的服务自动发现 结合双向反馈的业务部署机制，动态部署系统的整体框架图如图18-1所示。 图18-1 动态部署系统的整体框架图 在这个框架中，资源管理层负责集中统一的实时监测和管理服务器资源的使用情况，并根据业务部署时应用的资源需求进行分配。容量管理层则负责集中统一的管理应用的实例数量，并通过状态检测监控实例的存活，动态地感知应用在线上环境中的运行状态，从而实现服务器资源和应用的自动化管理。 在自动化的基础上，应用实例与服务器的对应关系是动态计算和组织的，因而不再需要人工进行规划。但同时，也正因为应用实例的位置是动态分配的，提前进行配置变得无法进行，因而需要应用实例间通过服务发现机制来感知应用实例的位置变化。 虽然从框架图上来看，改造目标是清晰的，但具体进行传统部署到动态部署的转变并不是只是增加资源管理和业务容量管理就足够了。 前提条件部署只是使业务承载线上流量，具备对外服务能力的其中一环，还有数据、配置、导流等环节的联动和配合，才能正式提供服务。并且要达到7×24小时的高可用性，更需要在架构上对高可用方面进行支持和设计，这是一个庞大的系统工程。因此需要有其他系统的支持和配合，满足一定的前置条件，来支撑动态的业务部署。在应用实例运行成功后没有人工介入的情况下，业务系统能够感知、发现、配置、复查新部署的实例，并自动完成线上流量的接入和处理。 首先，在业务架构方面，应用必须要具备水平扩展能力。水平扩展能力是业务高可用、容量可叠加的基本门槛，在大规模服务集群的运维标准中已进行详细讨论和说明，在此不再赘述。动态部署所依赖的便是基于水平扩展能力的容量叠加，使业务集群能够根据容量需求进行动态调整。 水平扩展能力的核心是去状态化和去差异化，例如，剪除静态的关联配置、分布式的数据存储、无单点的架构设计等。要使应用服务具备水平扩展能力，其中一些需要对架构或业务逻辑进行重新设计；而另一些则可利用第三方系统，仅对应用服务做轻微改造即可实现。接下来所介绍的就是水平扩展能力改造所涉及的常用的系统和平台。 1．外部流量接入系统 流量接入是使业务正式对用户提供服务的唯一入口，是必不可缺的环节。DNS+负载均衡器是较典型和常见的高可用结构，四层/七层负载均衡器通过VS/NAT技术将对外的一个应用映射到多个相同的内部应用实例上，平均分担流量，起到冗余和容量叠加的效果。这个映射关系就决定了流量将被分配到哪些实例，需要能够被配置和管理；例如，LVS中的ipvsadm和Keepalived工具、F5的Web配置界面等。由于动态部署没有了前置的部署规划，因此不能够提前确定映射关系，从而需要负载均衡系统能够接收映射关系的动态注册和注销，通过自动化系统来管理映射关系。 动态部署系统与流量接入系统的联动，才能实现业务前端层的水平扩展并且同步完成流量的引入，对外部提供服务。（流量接入系统的设计和实现，可参考本书中相关章节） 2．内部服务发现系统 在传统部署中，因为部署规划已提前确定，上、下游交互和数据流是可预知的，因此一般直接静态写入到应用的配置文件中，跟随部署系统一起发布到线上环境中；但在动态部署系统中，由于不能提前确定应用实例所在的服务器IP地址及端口信息，无法再写入到应用配置文件中，因此需要动态创建的应用服务具备主动发现所依赖的上、下游服务位置，感知上、下游服务变化的能力，从而达到应用服务实例被动态创建后，便能自动融入到线上环境中，形成一体，如同水滴掉落进水池，平滑顺畅。 要实现应用服务实例的自动接驳，就需要其具备主动通知的能力、动态地感知应用之间的关联关系变化的能力。后者的实现是部分程序逻辑，在此不再深入；而要实现前者就必须将通知广播到有关联的上、下游的所有实例。有两种实现方式： ◎ 应用直接进行广播，如使用Gossip协议。◎ 通过中间服务集中化管理，如服务发现系统。 两种实现方式各有利弊。第一种方式不会引入第三方系统，降低了系统架构的复杂性，减少了故障点，各个实例自己维护全量的关联关系数据，了解全局拓扑；但同时使应用自身的逻辑变得复杂，广播只能确保关联关系的最终一致性，无法保证某一时刻的一致性，增加了网络带宽消耗。 第二种方式引入了第三方集中管理和仲裁机制，在优劣上与广播形式相反，减少了应用的改造和实现成本，能确保应用实例间在某一时刻数据的一致性，应用实例只以单播形式与集中管理系统交互，并按需所取，只有非常少的网络交互和消耗；但劣势也是明显的，作为这样重要的第三方系统，如果出现异常，将直接导致整体业务的崩溃和停服，因此在高可用设计上有严格的要求。（服务发现系统的设计和实现，可参考本书中的相应章节） 3．分布式数据存储系统 前面已经提到，动态部署的基础是水平扩展能力。而实时更新应用数据使应用实例变得状态化，由于需要保证数据的可用性和准确性，因此无法任意创建或销毁应用实例，从而限制了应用的水平扩展能力。通过分布式存储，使数据从应用实例中剥离，统一存储，既能保证原有的多副本和冗余性，也使得实例去状态化，满足水平扩展的要求。 应用数据大体可分为两大类：临时数据和永久数据。对于临时数据，Memcache、Redis等开源的方案在业内已有广泛的成功实践。对于永久数据，分布式文件系统、分布式存储系统方面的解决方案更是多如牛毛，例如分布式文件系统GlusterFS、Ceph等，分布式存储系统HBase、MongoDB等。 4．配置管理系统 配置是应用运行所必需的基本条件，也是控制应用运行逻辑的主要手段，通常以静态文件的形式与应用程序一一对应。但这种形式过于复杂且包含运行逻辑的配置，会给维护管理带来诸多不便，尤其是当规模大到一定程度之后，例如配置项的变更调整、多实例配置的一致性保证等。并且由于业务逻辑的需求，有很大概率会出现同一个应用的多个实例配置存在差异，比如实例ID。差异化造成了应用实例在某种程度上状态化，失去了水平扩展能力。同时差异化使配置管理的自动化实现难度倍增，自动化的可靠性大大下降。 为了使应用进一步轻量化和去差异化，同时也消除传统的人工管理方式，彻底剪除应用的静态配置文件（仅保留实例启动所必需的）是较为理想化的状态。那么如何实现静态化配置的抽离，以及如何管理剩下的配置项呢？有以下几种方式： ◎ 如同服务发现，将配置进行中心化存储和管理◎ 将配置模板化，基于模板自动生成◎ 去除不必需的配置项，转换为自动的应用逻辑，如全局ID的计算 中心化的配置存储管理有一定的改造和开发成本，但也是最为理想的方式，几乎可以托管应用全部的配置项，提供便捷、灵活、快速的中心化控制能力，目前已经有了若干较可靠的开源解决方案，如ZooKeeper、Etcd、Consul等。配置模板化或进行应用逻辑转化的改造成本相对较低，但是对配置格式要求比较严格，只适用于格式统一或有固定规律的配置。 中心化的配置管理系统的主要目的是消除应用配置所造成的差异化，使应用更适合于动态的业务部署，同时也使配置的管理和调整更容易转换成自动化的实现。 5．日志处理系统 日志是统计和分析业务数据的重要数据，日志只在应用正式接入线上流量后才会产生，因此应用实例的运行位置就决定了日志源的获取位置。在日志数据的获取实现上，无外乎pull/push两种方式。pull方式的先决条件是提前知道部署规划和应用实例位置，这与动态部署的设计理念相背，使用该方式大大增加了日志系统的实现难度和成本。而push方式天生与动态部署理念匹配，在这种方式下，应用主动上报日志数据，对应的就需要有日志系统接收、归类和存储日志，再进一步地分析等。 Facebook开源的Scribe、Cloudera开源的Flume等实现，都是push方式的优秀解决方案。 6．监控系统 监控系统在保证业务运行稳定、感知业务运行状态的变化方面，重要性不言而喻。虽然监控系统不直接向用户提供服务，但事关线上业务的服务质量，其与线上业务处于同等重要的地位，且还要关键。 通过监控系统可以采集到主机和业务运行数据，例如服务器负载、服务器资源余量、应用服务的单位时间请求量、请求的平均处理时间等数据，通过这些数据，可以分析和判断出服务器的运行状况、应用服务的运行状况，从而客观评价业务的整体稳定性。同时，通过对特定指标设定阈值，从而实时和自动地检测出超过阀值的指标，辅助判断和分析业务系统的运行是否正常和可控。因此，应用一旦开始承载线上流量后，监控策略的设置就是必不可少的。 由于动态部署没有了提前的部署规划，在不能得知应用服务实例部署位置的情况下，监控项需要跟随实例的动态创建/销毁而动态添加/删除，这就需要利用与日志一样的push机制，也要求监控系统在设计上支持监控对象的动态注册，并通过监控对象在注册时设置的属性动态绑定监控策略。更进一步，动态部署还能够基于监控系统采集的服务器资源数据，分析和计算是否需要增加/减少服务器上的应用实例数量，更合理地调节服务器资源与业务容量间的关系，使服务器利用率更高。 最后，在满足动态部署的诸多前提下，以及经过对应用架构的改造，使其具备水平扩展能力后，便需要对应用的运行环境、程序配置、启停控制方式、外部依赖等确定应用实例运行方式和结果的因素进行抽象和整合，形成部署系统能够发布的最小单元，满足部署的最基本要求。 服务器资源管理如前所述，集中式服务器资源管理是为了消除在部署前期人工介入进行规划，进而支撑动态部署的关键点之一。在拥有服务器全局的资源信息后，便能实现共产主义式的按需分配。只要应用实例所需求的资源不超过服务器单机的剩余最大值，便可以从服务器资源中挑选出适合实例资源需求的位置进行部署；当实例被销毁后，再对资源进行回收和重分配。 相对于传统的人工分配方式，集中式服务器资源管理可以一揽全局，打破业务与业务间的篱障，从业务角度看到更多的资源，需求得到分配和满足的概率更高；从服务器角度看，资源的控制更为精细，减少浪费，利用率更高。 实现服务器资源的集中和精细管理，需要具备以下几个能力。◎ 资源的采集◎ 资源的分配◎ 资源的隔离 1．资源采集 服务的资源信息可以被简单地分为两类：静态不变的硬件或系统资源信息，如CPU数量、内存容量、外部设备、OS版本等；以及在运行过程中会发生变化的动态资源信息，如CPU的使用量、剩余量、外部设备的忙闲程度等。静态资源是准确匹配业务需求，进行资源分配的主要依据；而动态资源则是了解服务器运行状态，判断服务器运转是否正常的主要依据。 传统部署进行部署规划时是以服务器为最小单位，以服务器的CPU、内存、磁盘、网络的信息作为判断依据判断是否满足应用的资源需求、是否适合部署某些应用的。然而，人工进行判断和规划时粒度是比较粗的，例如CPU核数通常是考虑因素，而CPU主频却通常不那么关注。因此，基于粗粒度的规划是不精确的。同时，人工规划时为了便于管理，在粗粒度规划的基础上，会人为地将资源分配进行对齐，相应地会造成一些资源浪费。 所以，更细粒度地规划和使用服务器资源，可以更大程度地提高利用率，减少浪费。但细化服务器资源的粒度就需要深入服务器内部，记录和管理服务器所拥有的各类硬件资源信息。若要通过人工来进行记录和管理，工作量和难度是可想而知的，因此必须要考虑自动化的服务器资源信息采集。 通过监控系统是可行的方法之一，额外的开发成本也较低，但同样也有弊端。监控系统是动态部署外的第三方系统，而资源信息又是动态部署所必需的基础信息，强依赖第三方系统可能会导致部署系统不能正常运转，因此与监控系统解耦，部署系统自行维护会更可靠。幸运的是，内核和系统提供了丰富的工具和接口获取主机的硬件信息，如dmidecode工具、proc信息等，自行实现有限信息的采集和汇总并不复杂。 常见的可被采集的资源信息如下。 静态信息： CPU——Cores、主频、品牌。内存——容量、主频、品牌。磁盘——总容量、硬件类型（SSD、SCSI、SATA）、RAID类型、分区信息、分区的文件系统类型等。网络——带宽、网段。系统——内核版本、系统版本、基础库版本等。 动态信息： 磁盘——容量使用情况。内存——容量使用情况。网络——带宽使用情况。系统——CPU调度相关、磁盘IO相关等。 2．资源分配资源的分配问题，就是资源如何能被有效利用的问题，理想的情况就是在不影响应用性能，出现资源抢占的情况下使用完单机的全部资源。理想总是很丰满，我们知道，在传统部署中，进行业务部署规划时，主要是事先对应用实例在CPU、内存、磁盘上的使用量进行预估或者线下压测，基于评估数据判断应用实例是否适合于部署到某些服务器上，而少量的应用又不能将单机的资源充分利用，因此还要结合应用特性进行混合部署来进一步使用空闲资源，例如CPU消耗型应用和内存消耗型应用共同部署到同一台服务器上。这便是原始的资源分配模型。 这种方式很可靠，但是仍显简陋，原因是：◎ 应用在现实中的资源需求必定是跟随流量、功能实现等方面因素动态变化的。基于预估或者压测数据，只能采用取最大值+冗余值的策略来覆盖应用在线上的所有变化情况。毫无疑问，大多数时间应用的资源消耗不会达到人工设定的最大值，从而出现大量预期内的资源闲置。 ◎ 进行应用的混合部署，通常范围是只在同一个业务之内。而在这样小的范围内，混合部署的组合就相应地少，能完美利用完单机全部资源的可能性就微乎其微。概括来说就是8个字：边界太宽，组合太少。如果缩小应用资源分配的边界，增加混合部署组合的可能性，那么可以肯定的是，利用率会大幅提升。为什么不一开始就这样做呢？接下来我们就分析一下。 ◎ 设定大的资源边界，是为了覆盖所有动态变化的可能性。如果缩小，必然有时候会跑出边界之外，例如流量突发或者在高峰期时，应用的资源就可能短暂地超出边界，而当突发情况消逝后，又会回归边界之内。当超过边界时，需要怎么做呢？扩容，新增应用实例。传统部署新增实例是较慢的，数分钟到数十分钟，所以不得不设定较宽的边界。 ◎ 扩大混合部署的范围，细化组合粒度。那么必然要面临的问题是：①资源抢占；②权限管理；③安全性；④混合部署组合的可能性暴增。如果完全依赖于人工调配和控制，成本和复杂度可想而知，或者说是几乎不可能由人工完成的任务。 因此，首先，要提高实例扩容速度，在判断应用资源的使用率快要达到设定的边界时秒级完成扩容，那么缩短边界就是可行的。其次，对已分配资源进行限定，使其不能超出已设定边界，那么边界就是清楚明确的，才不会出现抢占问题。再次，对运行环境进行隔离，才能保证安全性和进行权限划分。最后，就是基于自动化的混合部署策略计算。 自动化的混合部署策略，可以依照服务器的资源配置生成应用的组合，如同在一个地块上规划能够盖几栋什么规模的楼；也可以依照服务器的剩余资源来选择适合的应用，如同在剩下的空地上看看还适合加盖什么楼。相比较起来，前者同样需要规划，即需要提前知道有哪些应用可以被部署，应用各自需要多少资源；而后者则可以根据实际情况不断调整，更适合于不能提前知道有什么应用将要被部署的场景。 通过前面所述的动态的资源采集，便能够实时地知道线上资源的分配和使用情况，对于新进入的部署需求，便能够选择合适的位置进行部署，从而达到自动化的混合部署。因此，资源的分配和采集密不可分，两者相辅相成、协同工作才能动态地分配和回收资源，完成服务器资源的统一管理。目前已经有开放的解决方案，例如Apache Mesos、Google Kubernetes、Flynn等。其中Apache Mesos是一个比较成熟的开源项目，目前已有一定规模的线上应用，提供了数据中心级别的资源管理和调度功能，也是我们目前选择的解决方案。 3．资源隔离上面已经提到，只有对已分配资源进行限定和隔离，才能缩小边界，增加混合部署的可能性，从而提高利用率。但不同应用对于环境的要求也不尽相同，例如环境变量、基础库、语言版本、第三方软件等，进行混合部署时可能出现冲突；而这些问题正好都属于虚拟化隔离所能解决的范筹，所以通过虚拟化可以一次性解决全部问题。 目前主流的虚拟化技术有两种：虚拟机和Container。这两种技术在实现方式、创建、使用成本、安全性、性能等方面均有较大差异，如表18-1所示。 表18-1 比较虚拟机和Container两种虚拟化技术 从比较中可以看出，虚拟机的隔离性更好，安全性更高，资源量化更精准，但是性能开销和创建成本都非常高，在快速部署的需求场景中，无疑轻量化、低成本的Container更加适合。 正是由于其创建成本低，因此可以更加容易地被处置，如直接销毁，需要时再重新创建。这种方式不仅简单、直接，容易操作，而且不涉及重用和复用，每一次重建提供的都是完全干净和可靠的环境。其特点与动态部署的需求完美契合，实际上Container技术就是为动态部署需求而诞生的。 目前开源的Container解决方案有：Cgroup、LXC、Warden、Docker。Cgroup是Linux内核提供的一种限制和隔离进程使用物理资源的技术，同样也是LXC和Docker实现Container所使用的底层技术。 Cgroup作为最底层技术，直接提供了资源隔离和限制的支持，但直接使用Cgroup创建和配置Container较为复杂，且只具备隔离和限制能力，不具备虚拟化能力。LXC在Cgroup基础上做了封装，并且提供了轻量化的虚拟能力。 Warden是Cloud Foundry下属的一个组件，为其PaaS体系提供资源隔离能力。Docker是云服务提供商dotCloud基于LXC做了进一步封装，完善和提高了虚拟化能力的Container工具，近年逐步被接纳和认可，相关原理和文档在官方文档中已经详细描述，在此不再赘述。 业务容量管理在实现服务器资源的统一集中管理和调度分配后，底层资源已是自动化和动态化的，但此时应用需求还是静态的，需要人工决定。人工处理的低效率和后置性并不能将集中资源进行充分发挥和利用，在应用的管理上同样也应该是自动化和动态化的。 在传统部署中应用的部署是单向的一次发布，应用的容量是提前确定的，在线上运转的过程中部署系统对应用的运行状态是无感知、无控制能力的，只能通过监控系统了解应用的运行状态，当出现容量不足或实例崩溃时，再由人工介入进行恢复或者重新部署，扩容实例恢复容量，这就大大延缓了故障解决的时间。如果触发容量不足或者故障原因恰恰是流量突发造成的，故障或容量问题未被及时解决，还可能导致其他实例的雪崩，使整个业务系统瘫痪。所以通过自动化的动态部署，快速对应用进行重新发布，保证业务容量足够，将极大地提高业务的稳定性。 为了实现容量管理的自动化，需要在传统部署的基础上实现：◎ 应用容量（实例数量）的收集◎ 应用实例运行状态的探测◎ 应用运行条件的管理（消除人工的判断和决定） 如同资源采集一样，应用容量的收集是实时了解应用在线上部署情况的途径，对部署结果以动态形式反馈，在实时掌握线上部署情况后，才能进一步计算应用实例的数量是否符合预期，实例的总容量是否足以承载当前的流量。 应用实例运行状态的检测，在某种程度上接近于实例的存活监控，区别在于监控只是发现，而在动态部署中检测到实例状态发生变化后，随即就可以触发部署。而不需要人工介入，动态发起部署，就需要系统能够知道应用运行所必需的条件和要求，包括： ◎ 所需要的硬件资源（CPU/内存/磁盘空间/网络带宽）◎ 依赖或互斥关系◎ 物理分布要求，如以机架打散或集中 在业务容量管理方面，同样也有成熟的开源解决方案，如基于Mesos的Marathon（由Mesosphere开源）、Aurora（twitter开源）、Kubernets、Fleet等开源软件和实现方案。 运维管理动态调度在很大程度上消除了人工进行提前规划和应用运行过程中的介入，但现实中并不能完全避免和消除人工维护和排除故障的需求。因此需要提供一种方式了解线上应用实例的分布情况，以便确定运行位置，便于人工介入维护或线上调试等。所以仍然需要保留和设置人工的管理和维护方式，在传统运维中，登录服务器是使用最多、最灵活的方法，而API则是工具化管理所依赖的方式。 （1）远程登录最传统和可靠，也是必需的管理接口。当其他自动化接口失效或不能满足需求时，作为最后的控制方式使用。 （2）API接口通过API可快速进行针对应用的控制和查询，如重启、重载、日志查询、数据下载等；并且通过API可再次与其他系统联动，进一步满足其他自动化需求。 权衡和折中动态部署和传统部署在实现目标、运行方式上有着巨大的差异，进而在线上环境的管理方式上也存在明显的不同。前者主要依赖于自动化系统，后者主要依赖于人工。自动化系统不同于人工的灵活和智能，因此需要有固定的流程和规划来指导自动化系统的运行。所以，在从传统部署转向动态部署的过程中，需要在一些方面权衡和折中。 （1）实时性 实时性的折中主要有两方面：①部署实时性；②异常检测和自愈时间。动态部署除了所必需的前置条件和信息外，其他均是在部署过程中判断和生成的，因此会额外消耗时间用于发布启动。从检测到服务异常到重新发起部署需要一个时间周期，在实时性和探测性能上要进行权衡和折中。 （2）小流量预览 小流量预览常被作为避免新程序bug造成大面积故障的可靠方式之一，通过先升级一台特定主机上的程序，然后观察运行是否正常，来判断是否继续。而在动态部署时，由于没有提前确定运行该应用的主机，因此这种方式不再可行。通过特定的预览环境可以折中实现小流量预览需求。 （3）业务间的亲缘依赖由于服务器是动态选择的，有强依赖关系的亲缘应用并不能一定被分配到相同的主机上，造成依赖关系缺失，应用不能正常启动。虽然能够对有亲缘关系的应用通过属性、配置等自动地发现和处理，但相同的会增加一定的管理复杂性。因此，在服务抽象时需要折中考虑有亲缘关系的应用定义，作为一个整体进行打包和发布。 （4）有状态应用和应用数据有状态应用由于其特性通常不能被水平扩展，为了达到高可用性、高稳定性，以及能够进行动态部署，需要在架构方面进行改进，以支持水平扩展。 应用自身运行如果依赖实时和动态更新数据，那么数据无法跟随部署进行发布，同样也限制了此类应用的快速水平扩展。需要在架构上进行改进，采用分布式数据存储等方案剥离逻辑和数据，以支持快速水平扩展能力。 系统解决方案如前所述，在资源管理、业务容量管理、虚拟化方面均有成熟的开源的解决方案，使用开源的解决方案既是解决问题的一个捷径，同样也能够跟随主流的思路和技术，避免重复造轮子。我们的动态部署方案即是以Docker+Mesos+Marathon为基础组件，再根据需求定制实现的。 总体框架结构动态部署方案的总体框架结构示意图如图18-2所示。 图18-2 动态部署方案的总体框架结构示意图 调整和改进开源方案只解决了通用性问题，而针对不同的场景和使用方式，会产生定制化需求。下面介绍的是以这个总体框架结构为基础进行的主要改进点。 首先，Docker作为轻量的虚拟化容器，具有创建、销毁、迁移成本低的特点。但由于Docker并非完全虚拟化，在结合部署的实际应用中发现，Docker与宿主机有较强的耦合，容器不具有完全的独立性。在容器启动后，会动态产生一个宿主机IP+PORT的映射，从宿主机以外访问容器中的应用服务必须要通过该映射作为入口。因此，基于Docker进行部署时，需要在完成部署后获得和记录容器对应的宿主机映射，再将该映射对外开放。对于部署来说，产生了以下一些影响。 （1）应用服务端口的尴尬在Docker默认的几种网络模式中，hostonly模式直接复用宿主机IP地址，从而造成多个容器间不能监听相同的端口。在同一宿主机上相同的image只能有一个容器，这是对动态分配的一大制约。 而bridge模式虽然解决了端口冲突问题，但容器映射的是宿主机上的随机端口，一是并无规律性，需要额外查找宿主机和容器的映射关系；二是只能在宿主机上（Container外）查到该信息，不能由Container内程序直接注册服务。既增加了Container与宿主机的耦合，又使服务发现的实现逻辑变得复杂。 （2）容器网络性能的损耗hostonly模式最接近于宿主机性能，但其损失了虚拟化的部分优势，让容器的分布额外增添了制约因素。bridge模式在NAT过程中，性能有较大程度的损失。 这两者本质上均与容器的网络模式有关，为了使容器具有如同虚拟机的独立性，我们对Docker的网络模式进行了改进，如图18-3所示。 在该方案中，Container环境通过复用物理机的硬件链路，保持与宿主机具有相同的网络结构，因而直接接入到内网。同时避免与宿主机网络环境出现耦合，不造成网络规划的混乱，以及支持动态的Container，Container与宿主机使用不同的VLAN隔离。 图18-3 小米的Docker网络模型 改进后的Docker具有如下特点。◎ 容器与物理机在网络上彻底解耦，每个容器都如同完整的虚拟机，具备与宿主机相同的独立性。◎ 容器与物理机使用不相同的网段，更便于区分两者的不同，在进行授权、访问控制等方面更容易管理。◎ 纯动态化，容器的创建不需要提供任何前置条件或配置，在任意主机上均可随时创建、销毁。◎ 物理机网络仍然是一次性配置的，在运行过程中不需要再动态调整。 相应地，改造也有一定的成本：物理机网络的接入由原来的access模式变为trunk模式，在网络设备上需要调整和规划。为了进一步消除这个成本，将宿主机虚拟为交换设备，虚拟交换设备利用内网相连接，Container与宿主机的虚拟交换设备相连接，从而便可在内网基础上构建虚拟的Container内网。例如，利用OpenvSwitch或Metaswitch的calico等，如图18-4所示。 图18-4 动态部署网络分层 当容器具有与宿主机相同的独立性后，使基于容器的服务部署和运行如同在宿主机上运行一样，因此不需要再进行额外的调整和改造，使业务从传统部署演进到动态部署的成本几乎为零。这也是我们所希望的。 现在剩下的唯一问题就是将容器中的IP地址对外透传，以让外部系统知晓和使用。这便是我们的另一个改动，前后流程对比如图18-5所示。图18-5 容器动态获取IP的过程 从图中可以看出，Container在创建后主动发起DHCP请求，获取属于自己的独立IP地址，并透传给Mesos—Slave，Mesos—Slave确定Container运行成功后，再将该Container的独立IP地址通过status update消息透传至Marathon，使整个部署系统都能知道Container的真实工作IP地址。 其次，从总体框架结构中可以看到，容器作为独立的主机使用时，有基础的环境初始化、对外服务注册、定时任务、远程登录等需求，所以是一个多任务的环境。但在Container实现里，容器中只能有一个主进程，如果让应用实例成为这个主进程，那么前面提到的需求就很难触发和执行，因此开发了专门的watchdog进程作为Container的主进程，由watchdog来完成和触发应用实例运行启动或退出前后的相应动作，并对应用实例进行启动和监视。 例如： 123456watchdog +--- 启动sshd +--- 数据库自动授权 +--- 向服务树注册tag +--- 注册LVS +--- 部署cron任务 +--- 启动应用服务 最后，Docker/Mesos/Marathon分别在主机虚拟化、资源管理、业务容量管理层面提供了较成熟的解决方案，但如同其他的开源方案一样，缺乏与内部系统的对接支持，在容量管理的基础上进行动态的容量调整也是目前开源方案所不具备的功能。因此在系统对接、易用性、容量计算和调整方面，同样也基于总体框架结构的设计做了封装和二次开发的改进。 总结如果说在线服务是互联网行业的生命，那么服务的高稳定性和可靠性便是呼吸，是心跳。随着流量、集群规模、业务复杂度的不断上升，如何在保证高稳定性和高可靠性的基础上提升效率，是一个巨大的挑战，同时也是运维价值的体现。自动化和智能化的运维管理模式是业界共同认可的方向，动态部署则是这条路上的探索和尝试。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Dynamic Scheduling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十七章：部署系统]]></title>
    <url>%2Funder-the-ops%2F20170923-17-deploy-system%2F</url>
    <content type="text"><![CDATA[在运维工作中，线上服务所依赖的环境管理、线上服务的发布和回滚是核心工作之一，支撑这部分工作的一个重要系统就是部署系统。 部署系统是保障研发工程师所开发的代码能否顺利、高效地部署到线上，以及保障线上服务能否正确、稳定运行的重要前提，也是保障线上服务具备良好可运维性的重要条件。 如果把研发人员开发的代码比作培育出来的小羔羊，那么部署系统就是保障农场的合理规划、以及像牧羊犬一样能够管理约束羊群的重要工具，如果没有后者，羊群会四散飘散，这样既不利于羊的安全，也不利于牧羊人收放自如地管理羊群。 因此，部署系统是规范线上环境管理、服务发布和变更，以及服务有序管理的重要基础工具。 部署系统综述互联网公司的项目迭代速度特别快，因此对部署系统的要求除了常规的功能性需求之外，还有高效的要求。在实施自动化部署系统设计、开发和使用推进方面，需要考虑如下几点。 1．在什么阶段开启部署自动化工作在什么阶段开启部署自动化建设，将直接影响部署自动化能够做到的程度和成本。公司早期是各个项目可塑性最强的时候，并且服务还没有上量，是最适宜开启运维自动化工作的阶段。但是大部分公司在创建早期都不会招聘专职的应用运维工程师，而是由系统工程师做基础设施的运维，或者由研发工程师直接维护线上服务。 对此，我们的建议是，前期可以没有很强的平台，但应该制定较强的部署规范，先收敛各类服务需求的多样性，然后借助开源的运维工具进行简单的自动化实施。如果已经错过了公司前期的规范化约束，那么通常后期需要花更多的时间来梳理服务的部署方式和制定推进相关的规范。 2．部署系统的不同阶段的定位是什么在不同的阶段，部署系统关注的需求是不一样的。部署系统前期关注的是如何将线上各类服务管理起来并实现部署动作的自动化，减少手工上线中期关注的是如何将部署系统跟其他运维系统联动，使得部署状态能够自动更新到关联的运维系统，降低人工维护成本后期关注的是如何更好地提升部署的动态化，例如借助资源隔离和动态调度等方法，更好地满足实例混合部署、动态扩容/缩容、动态故障迁移，以及提升服务器资源利用率等需求。 3．设计部署工具时如何预估未来的需求，以及新需求如何支持部署系统除了需要满足当前场景下的需求之外，还需要不断支持新的需求，甚至对未来可能的需求要有一些预估。 对此我们的经验是：一方面，在公司内部建立通畅的需求管理机制，方便使用方快捷提出需求；另一方面，多跟业内其他公司交流，了解其他公司的设计方案和需求情况，来预估未来的需求。此外还有一种比较有效的方法，就是多调研开源的环境管理和部署工具，了解这些工具都满足了哪些需求，都解决了什么问题，借鉴好的设计思想。 运维的很多工作都是需要研发工程师、测试工程师和运维工程师密切配合的，部署工作也不例外。 一方面，线上环境的管理成本、项目迭代的速度等都直接影响着项目上线的时间点，线上服务的稳定性也与开发测试人员息息相关，同时开发人员部署开发环境，测试人员部署测试环境，都会使用到运维自动化的工具 另一方面，部署自动化的推进需要与研发和测试人员密切配合，包括制定发布包的规范、线上路径的规范、配置管理的规范等。 我们的应用运维团队在成立之后立即启动了部署自动化的工作，希望从一开始就将服务部署变更的动作规范起来，避免后期走太多的弯路，期间经历了部署规范的制定、部署系统的开发、与其他关联运维系统整合和全动态部署等不同阶段。 在部署系统发展前期，我们更关注如何将线上百花齐放的服务上线方式集中为统一的方式，这时需要多个规范来约束服务的部署；在部署系统开发设计阶段，更多的是考虑部署系统如何满足运维人员在服务部署各方面的需求，能够满足研发和测试人员的部署需求并提供可靠、方便的使用方式 部署系统全面在线上使用之后，就需要考虑部署系统如何更好地与其他运维系统联动，降低运维管理成本；解决了联动的问题之后，提升资源利用率、动态部署等需求的必要性也随之提升，因此这个阶段需要关注资源隔离和动态调度的需求。下面通过分析不同阶段的工作来串联整个部署自动化的实施过程。 部署系统的前置规范在手工操作阶段，依托的是流程和人的可靠性。为了减少因人为因素导致出现服务部署错误的情况，必须制定很强的流程规范，例如，要通过流程约束服务部署到线上前的测试过程、约束上线申请中的手工操作步骤和回滚步骤等。 上线的操作环节也要求运维工程师具备丰富的经验，并要求运维工程师按照操作步骤进行上线或回滚，操作的方法基本是逐台进行，这其中包含着较多的重复动作，对于实例数量较多的服务，会带来上线过程烦琐、耗时长等问题。 在这个阶段，很多的运维工程师往往都会通过自己实现的一些小脚本来进行操作，但这些脚本往往会带有一些业务服务相关的逻辑，比较难以做到跨产品迁移，因此我们需要确定相应的规范来配合工具进行服务部署。 对于处在这一阶段的服务，可以通过这样几个规范进行约束：软件依赖管理的规范、编译发布联动的规范、线上目录结构的规范。线上服务满足这几个规范之后，既可以提升服务在人工管理时的清晰度和便利性，也可以为后来的自动化部署实现提供基础。 软件依赖问题的解决随着硬件性能的不断提升，为了提高单服务器的资源使用率，我们通常会在单台服务器上部署多个实例。当单机需要部署多个服务实例的时候，每个服务所依赖的软件的版本冲突问题就会突显出来，例如，一台机器上部署了两个Java服务，但使用的Java版本却不相同。如何解决机器上不同服务实例依赖相同软件的不同版本问题，是部署系统必须解决的问题之一。 对于这个问题，原先更多的是通过人工的方式来分隔，也就是将依赖相同软件的服务部署在同一台机器上，依赖有区别的服务只能部署到不同的服务器上，这样虽然可以解决软件依赖冲突的问题，但却增加了管理成本。 在我们的规范中，将服务依赖的所有软件分为两类：基础软件和服务自带软件。基础软件是系统所带的软件，只支持默认版本，不支持定制需求，例如系统自带的JDK 1.6、Python 2.6等（如图17-1所示，基础软件由系统默认提供）。 对于有特殊需求的服务，要求在发布包中自带其所依赖的环境，并将服务配置为依赖自带的软件来启动。如图17-1所示，Nexus服务依赖JDK 1.7，与系统版本不同，就需要在自己的发布包中自带JDK 1.7，而IM服务均使用系统自带的基础软件，因此它不需要自带依赖软件。这个思路类似于Python中的Virtual Env，只是将这个思路应用到各个语言下的各种场景中。 图17-1 服务依赖不同的软件实现示意图 按照上述的规范改造之后，可以解决依赖冲突的问题，并且在后续引入进程管理工具之后，在管理上会非常便捷。这个思路也跟Docker的完全隔离方案一脉相承，在后期引入Docker之后，无须再变更部署方案，可以直接就用上Docker。 编译规范及编译部署的联动编译对于服务发布来说是不可或缺的一个环节，通过编译规范可以约束编译出的发布包的结构，从而减轻部署系统处理发布包的复杂程度。 结合线上服务的情况，我们确定了下面所示的源码包和发布包结构规范，其中deploy目录是源码包中必须包含的，该目录下包含着发布时所需的配置文件，在编译环节直接拷贝到发布包中，供部署工具解析使用。源码包中的build脚本是编译时执行的脚本，允许用户定制编译动作，只要是可执行的脚本语言即可。 源码包结构示意如下： 1234567Service-A |- build |- code_dirs |- other_dirs |- deploy |- 部署配置 |- 配置文件模板 编译后的发布包结构示意如下，deploy目录直接从源码包中拷贝过来，其他目录是项目编译后的产物： 1234567Service-A |- bin |- conf |- other_dirs |- deploy |- 部署配置 |- 配置文件模板 在确定编译环节的规范中，我们既参考了CloudFoundry的buildpack的思路，也考虑到了公司内服务的情况，将buildpack中的detect、compile、release三个脚本合一，使其既能够兼顾较为简单的服务的编译，也能够满足复杂的服务的编译。同时增加了对输出的发布包的目录结构的要求，使得后面的部署系统能够快速对接上。 在确定了编译环节的规范之后，就可以很容易地将编译环节融入到整个服务发布中来。如图17-2所示，进行某个服务的部署，部署系统首先会判断产品库中有没有这个服务指定版本的发布包，如果没有，则走编译系统进行编译，然后发起部署；如果已经存在该版本的发布包，则直接进行部署。 图17-2 编译和部署整体流程 线上目录的规范线上目录的规范用于约束服务在线上运行时程序、日志、数据的位置，对在机器上人工查看和管理来说，这个规范非常重要，没有明确的规范会导致线上目录混乱无法管理；对于部署系统来说，明确的目录规范可以降低系统设计的复杂度。目录规范属于制定较简单，但是推动执行却比较烦琐的工作。 制定此规范主要考虑如下三点。（1）目录结构是明确的，并且足够简单，做到工程师不用思考便可找到所需要的目录。（2）程序（含配置）、数据、日志分离，便于进行程序发布更新时不影响数据和日志，同时日志分离出来之后便于定期清理。（3）能够满足某些特殊场景，比如某些服务的数据对读/写要求较高，目录要挂载SSD盘。 经过综合考虑，我们确定的目录规范如下，能够很好地满足线上的各种需求。可执行文件和配置在一个目录下： 1234/home/work/bin/SERVICE-A |- bin |- conf |- other_dir 日志文件在独立的目录下： 123/home/work/log/SERVICE-A |- xxx.log |- old_log_dir 数据文件和临时文件在独立的目录下： 123/home/work/data/SERVICE-A |- service_data1 |- service_data2 按照上述规范改造后的服务，部署需要关注的二进制程序和配置都在/home/work/bin/SERVICE-A目录下，其他目录部署系统可以不关注，减轻了处理服务日志和数据的逻辑。同时日志的清理可以直接在/home/work/log目录下进行，不必为每个服务的日志目录配置单独的清理任务。独立的数据目录既能够满足挂载不同盘的需求，也能够在服务器上持续保留，不受程序更新发布的影响。 部署系统的前置规范是非常重要的环节，将直接影响到部署系统的设计和后期推进的难度。与此同时，线上服务满足这三个规范之后，对前期手动管理或者借助简单的工具进行管理也能极大地降低难度。 部署系统设计部署系统除了要满足直接的服务部署功能之外，还需要考虑必要的辅助功能，比如，版本的控制，如何确保服务成功启动和正常运行，如何满足服务在不同场景下配置差异的管理，如何进行服务的快速回滚等。 某个单一的开源部署系统很难完全满足公司内这些具体的需求场景，因此我们需要开发适合于自己公司需求场景下的部署系统，并能够在这个框架下来对接公司其他的运维管理系统，甚至是对接测试系统等。 下面先了解我们自研的部署系统结构，然后分析各个组件的细节，来描述部署系统的整体设计思路。 部署系统结构 图17-3 部署系统整体结构示意图 如图17-3是我们自研的部署系统的整体结构示意图。Git为公司统一的代码仓库；Build Server接收部署Web端的指令，执行指定服务的某个版本的编译动作，并将编译结果写入产品库中，如果指定的版本已经编译过，则编译动作不会重复执行，返回成功。 Odin是整个部署系统的控制中心，负责解析Web端传过来的部署任务，并调度需要被部署的服务器上的Frigga执行部署动作。Frigga是每台服务器上的Super Agent，负责管理服务器上其他所有的Agent，并提供对外的RPC接口，接收控制端对服务器的操作指令。 整体的部署流程如下： （1）在Web端选择要部署的服务名称、版本号，以及需要部署的服务器列表。（2）Web端发起编译部署动作，调用Build Server进行编译。（3）编译成功后，Web端调用Odin执行部署任务。（4）Odin控制服务器上的Frigga执行部署动作。 部署系统涉及的组件如下： 1．Odin Odin是具体部署动作的控制中心，其接收的参数包括本次部署的服务名称、服务器列表、发布包的地址，以及本次发布的流程控制参数（例如，部署任务串并发度控制、部署多少个实例之后需要暂停、每个实例部署完成后需要等待多长时间再进行下一个实例的部署等）。Odin会根据流程控制参数分别调用每台服务器上的RPC接口触发具体的部署动作，并周期性地探测和更新部署进度。 为了能够做到开发、测试和线上环境的部署行为一致，除了要求使用同一套部署系统来进行部署之外，还需要保证任意发起的部署任务都能够被正确地记录状态。为此，我们将所有的状态上报放在了各台服务器上，由Agent来完成，从而减轻了中心端的复杂度，并降低了对中心端的强依赖。这样Odin需要处理的需求就是部署串并发度等流程控制，Odin可以设计得很轻量化，我们也确实将Odin设计为一个脚本程序，可以很方便地在任意地方发起部署。 2．Build Server Build Server接收Web端发起的编译请求，所需要的参数包括服务名称、Git地址、Git版本号，Build Server会判断对应的版本是否已经编译过，如果曾经编译过，则直接返回成功，并给出发布包的源地址；如果没有编译过，则执行编译过程，将发布包上传到产品库中，然后返回包地址给Web端。 3．Web 部署系统的Web端，是为了更方便地发起部署任务而开发的，Web端将页面部署配置翻译成Odin的配置，并调用Odin进行部署。对部署系统没有扩展性需求的项目一般都是通过Web发起部署的。Web是引导和限制用户规范性操作的一个重要入口，Web的设计原则是：只需填写必要、简单的配置项即可发起任务。 4．部署Agent 服务器上与部署相关的Agent有Frigga、Thor、God，其中Frigga提供RPC服务，Thor是执行具体部署动作的脚本，God是进程守护工具。Frigga并不是专门为部署系统设计的，而是服务器上的Super Agent，在部署中，Frigga接收Odin的部署指令，并调用Thor执行具体的部署动作，而后接收Odin的部署状态查询指令，返回部署进度。 每次部署服务时，每台服务器上需要具体执行的部署工作由Thor负责，包括发布包的下载、服务目录环境初始化、配置文件生成、目录备份等，完成部署工作后调用God来守护服务的运行状态。 God是一个开源的进程守护工具，是每台服务器上所有服务的Supervisor。它的职责就是维护服务器上所有服务的状态，对于已经启动的服务确保其永远处于running状态，如果服务进程意外退出，它会负责将其重新启动。 接下来，我们详细介绍各个组件的实现。 部署系统组件1．部署控制中心——Odin Odin是每次部署任务的控制中心，形态上它是一个脚本，之所以设计成脚本，是因为在整个部署系统中，状态维护和上报的工作交给了服务器上的Agent，中心端无须再花费更多的精力来记录和维护任务状态，即减轻了中心端的复杂程度。 服务状态的维护管理由每台服务器上的Agent负责，能够确保服务状态信息的准确性，并直接与关联的系统进行交互，避免其他运维系统对部署中心端的强依赖。Odin执行部署任务的流程如图17-4所示。 图17-4 Odin执行部署任务的流程图 （1）部署配置 无论是命令行发起任务还是Web端发起任务，最终都是通过执行odin –f cluster.yml来调用Odin执行部署任务的。所有部署的相关配置都在cluster.yml中指定，下面是详细的配置。 12345678910111213141516171819202122232425262728293031323334### 本次任务的公共配置cluster: ### 本次任务名称 name: cop.xiaomi_owt.miliao_pdl.com ### 本次任务id version: 102315 ### 本次任务部署的环境 env: production-a ### 本次任务包含哪些job，一次任务可以部署多个job，job可以分布在不同服务器上 jobs: - job.a_service.A_cluster.production-a_pdl.com_owt.miliao_cop.xiaomi ### 第一个job的详细配置job.a_service.A_cluster.production-a_pdl.com_owt.miliao_cop.xiaomi: ### job a部署的服务器列表 host: - a-com-x00.bj - a-com-x01.bj ### job a的启动账号 user: work ### job a的并发度 multi: 1 ### 部署多少台机器后暂停：默认的第一批次机器部署完成后会暂停，然后部署多少台机器后暂停就按下面的配置，0表示不暂停 step: 0 ### 每一批次机器部署完成后，暂停时间，对于必须控制部署间隔的任务可以配置此项 sleep: 10 ### job a的版本号 version: b3ec8dbbd ### job a的部署路径 path: /home/work/bin/a ### 部署动作，restart表示需要重启服务，reload表示直接重载服务，update表示只更新文件，无须重启或者重载服务 action: restart ### 包路径，可以是build完成后的包上传的位置，也可以是本地路径 pkg_url: http://scm.pt.xiaomi.com/service.A_cluster. production-a _pdl.com_owt.miliao_cop.xiaomi/job.a_b3ec8dbbd.tar.gz Odin的配置可以由部署Web生成，也可以手动维护（比如开发环境的部署）。对于已经实施持续部署的服务，可以由持续部署工具来生成cluster.yml，并通过Odin命令行发起部署。 （2）发布包下载 下载发布包有两种方案可选：由服务器上的部署Agent直接去所配置的地址（cluster.yml中的pkg_url配置项）下载，或者Odin从pkg_url下载，并启动下载服务供各台服务器上的Agent进行下载。我们采用的是第二种方案，原因有两个：一是对于很多本地发起的部署任务，编译包无须上传到产品库，而是本地编译直接本地发起部署，Odin提供下载服务，可以避免使用方去解决包下载服务的问题；二是如果各个Agent都从产品库下载的话，产品库的压力会无法预估，由Odin来提供，可以根据部署的具体情况评估下载压力，更好地提供下载服务。 （3）部署并发度控制 不同的任务在进行部署的时候会有不同的并发控制，例如，对于服务器量大、影响小的部署任务需要进行快速部署，而对于实例启停对整个集群有中断影响的重要服务需要逐台进行。因此，我们支持配置部署任务的并发度、暂停点和批次间停留时间。通过配置并发度可以控制每批次部署的服务器数量，配置部署暂停点用来控制部署多少台服务器后暂停，方便工程师介入进行检查，批次间停留时间主要提供给启动时间较长的服务，允许每批次机器部署完成后，停留一段时间，待服务完全启动后再部署下一批次。 （4）触发部署和查询部署结果Odin触发部署动作、查询部署状态都是通过Frigga提供的RPC接口进行的，Odin调用Frigga的startdeploy接口触发Frigga启动部署动作，并拿到返回的任务id，然后周期性地调用Frigga的查询接口查询部署进展，部署完成后，Odin可以通过Frigga的getdeploylog接口拿到本次部署的详细日志，便于用户了解整个部署细节。 Odin的设计思想是在任意时间、任意服务器上均可以发起部署任务，包括命令行发起和Web端发起等。 2．服务器上的Super Agent——Frigga出于安全考虑，服务器上开放的任何接口，都需要进行管理。为了减轻服务器上各个Agent的安全防护成本，我们只允许Frigga提供对外交互，其他Agent不允许开放接口与外界直接交互，如果有需求必须通过Frigga进行代理，这样只需要在Frigga上做好安全防护即可，其他的Agent无须关心。另外，Frigga还承担服务器上管理其他Agent的职责，Agent的部署、升级等都通过Frigga进行控制。、 具体到部署任务，Frigga接收Odin的任务调度后，调用Thor执行单机的部署动作，每一次部署任务，Frigga都会生成唯一的部署任务id，并在本地实时记录部署进度，Odin可以通过这个id查询任务进度和部署日志。 3．部署执行工具——ThorThor是真正执行部署动作的工具，它接收的配置与Odin相同，不同的是Thor只关注需要在本机上部署的任务，其他的任务会被忽略。Frigga接收部署指令后，会调用Thor执行本机的服务部署动作，包括解析部署配置、下载对应版本的发布包、在临时目录下准备要部署的服务环境、备份并停止待升级的服务、同步发布包内容到指定目录、重新启动服务等一系列操作。Thor执行部署动作的流程如图17-5所示。 图17-5 Thor执行部署动作的流程图 之所以将执行具体部署动作的模块独立出来，还有一个考虑就是，对于有些没有意愿使用部署系统部署自己的开发环境的开发人员，或者服务器是独立在外的（例如CDN机器与IDC内网不通），或者有些公网的机器考虑到安全风险无法开启对外接口的，可以独立使用Thor进行部署。 在第一阶段的规范要求中，我们要求在源码库中放置一个deploy目录，其下面放置的是部署配置，并要求deploy目录在编译环节被放入发布包中，供服务在各台服务器上部署的时候使用。这部分配置是如何约定的？如何来满足各种不同的需求呢？下面我们来详细分析。 在具体的服务部署执行环节（获取发布包后，执行部署动作，生成线上可运行的最终环境），我们需要关注的是服务的软件依赖环境、程序文件、配置文件等，如何按需生成服务的依赖环境，如何正确放置二进制程序，如何按照服务的运行场景生成服务的配置，以及如何处理服务依赖的数据文件。 在这一环节，部分开源软件（如Capistrano）的做法是提供特定的部署动作模板，或者提供批量功能，方便用户自行描述部署时执行的动作（例如，通过自己编写的bash脚本）。提供特定模板的方式虽然稳定性更好，但灵活性不够，无法满足多样化的需求；而用户自行描述部署动作的方式，灵活性够了，但是稳定性不足。 出于多样复杂的部署动作考虑，在不重复造轮子的情况下，我们选择Puppet来解决部署执行问题。在这里我们并没有使用Puppet常规的Master/Slave模式，而是使用它的Apply模式在本地解析Puppet配置文件做动作执行。 使用Puppet Apply来实现具体部署动作执行的原因有三个：Puppet的DSL（Domain Specific Language）足够优秀，可以约束部署时具体的执行动作，能够保证部署动作描述的准确性；利用Puppet的DSL，能够保证每个部署动作描述和执行的准确性，相比自由编写脚本的方式，更加统一可控；Puppet属于优秀的配置管理软件，业界使用比较广泛，而且运维人员也比较熟悉，学习成本很低。 下面介绍如何通过Puppet解决部署执行环节的几类需求。 （1）满足服务在不同场景下配置不同的方案在服务正式发布到线上环境之前，会有开发环境、Staging测试、Preview环境，每个环境中配置文件里面的参数是不同的，例如，数据库的账号密码在测试环境和线上环境中不同，ZooKeeper地址差异等。这类的配置差异，我们是通过模板和模板值的方式来管理的，具体的替换操作是借助Puppet所提供的erb模板来实现的。通过这种方式，可以减少配置差异性，最终降低线上配置错误的风险。 引入Puppet的模板之后，源码库中deploy目录下的配置详细如下： 1234567891011SERVICE-A |- build.sh |- code_dirs |- other_dirs |- deploy |- manifests |- init.pp |- config.pp | - templates |- service.conf.erb |- cron.erb 在Manifests中，需要配置两个文件：init.pp和config.pp。在config.pp中定义不同场景下的配置模板值，init.pp描述部署执行的动作，包括哪些配置模板需要被替换，在templates下放置配置模板文件。例如，某服务的数据库连接配置，模板文件描述如下： service.conf.erb 1234567891011121314151617181920DB_config: user: &lt;%= scope.lookupvar(＂config::db_config_user＂) %&gt; host: &lt;%= scope.lookupvar(＂config::db_config_host＂) %&gt;在config.pp中定义不同场景下的不同配置：Config.ppclass base &#123; $basedir = “/home/work/bin/$jobname” $db_config_host = “127.0.0.1” $db_config_user = “root”&#125;class staging inherits base &#123;&#125;class production inherits base&#123; $db_config_host = “10.x.x.x” $db_config_user = “service_user”&#125;class preview inherits production &#123; $db_config_host = “10.1.x.x”&#125;class config inherits $&lt;env&gt; &#123;&#125; 在config.pp中定义模板值，利用Puppet中class及class继承的特性。如上配置所述，我们定义了5个class，分别为base、staging、production、 preview和config，除base和config两个class之外，其他几个class分别对应了不同的部署场景，其中名为base的class是默认配置，在staging的class中继承了base的class，即staging环境中直接使用了默认配置，无须修改；在production的class中继承了base，并对db_config_host和db_config_user进行了重新赋值；在preview的class中继承了production的配置，只修改了db_config_host。最后一句配置“class config inherits $”中的env由部署系统赋值，代表的是本次部署的场景（例如部署staging场景的服务，env=staging），这句的含义是通过一个固定名为config的class继承本次部署的场景对应的class，最终部署使用的配置就是config对应的class中的变量。 定义了模板和模板值之后，就可以在init.pp中使用模板替换功能了。 init.pp 12345678class xbox &#123; include config file &#123; ＂$&#123;config::basedir&#125;/conf/service.conf＂: content =&gt; template(＂service.conf.erb＂), &#125;&#125;include xbox 其中“include config”即表示引入了之前在config.pp中定义的config这个class，按照前述，此class中定义的就是本次部署场景下的模板值配置。引入config之后，后面的file描述的就是需要替换的服务配置文件，其中file路径中用到的config::basedir和模板文件中的变量都是出自config.pp中定义的config这个class，也就是对应场景的配置。 （2）解决环境依赖的方法 在前述的第一阶段描述的规范中，限定了服务所依赖的软件如何管理的问题，我们要求在编译环节就将服务所依赖的软件同时打包到发布包中，并在服务启动时明确指定使用包中的软件。这一规范能够满足绝大部分场景，但是如果所依赖的包较为复杂，或者软件包过大不适宜直接放在发布包中，就需要借助其他的方法来支持。在我们的部署系统中，是通过在Puppet中实现自定义的provider来做到的。 对于最为复杂的一种情况，即依赖的软件包也要根据不同场景变更配置，或者其他必须借助部署系统来发布的情况，但是软件自身又不是一个真正的服务，我们在Puppet中增加了自己实现的资源，名称为thor（此处的thor是我们自己实现的一种Puppet资源的名称，实现上直接使用了服务器上的部署组件Thor，因此也取名为Thor）。Thor是通过Puppet定义了一个provider资源，这个provider资源的功能是调用本地的部署工具Thor进行服务部署，在一个部署动作描述文件里面引入了其他的部署动作描述，解决部署依赖问题。 举例如下：init.pp 123456789class xbox &#123; include config thor &#123; ＂$&#123;config::basedir&#125;/resin＂: ensure =&gt; present, source =&gt; ＂puppet-resin-4.x.xx.yaml＂, &#125;&#125;include xbox 上面init.pp配置文件中，描述了XBOX这个服务部署前需要依赖Resin 4.x.xx服务环境，通过增加Thor这个provider字段，描述了Resin的部署动作。部署系统在进行XBOX服务部署前，Puppet会先检查指定目录下Resin及版本是否正确，如果不正确先进行Resin的部署；确定Resin部署正确后，会进行XBOX服务自身的部署。通过Puppet提供的provider功能实现部署依赖描述，从而解决了服务启动前所需的依赖环境问题。 4．进程守护工具——God 为了保证线上服务在服务器上不间断地运行，通常需要借助一些方法或工具，例如，前台运行的服务需要常驻内存执行的话，可以使用nohup将其放到后台执行，通过输入/输出重定向来处理屏幕的输入和输出，使服务能够在后台运行。 很多的开源软件自身已经实现了Daemon化，例如Nginx、Resin等默认就是Daemon方式启动的，并且主进程还具备了简单的守护功能，在发现子进程挂掉之后会重新拉起子进程，但主进程异常退出后无法自恢复。为了确保各类服务能够持续不间断地运行，我们需要进行服务进程守护管理，同时还希望支持服务重启报警、服务统一启停接口、服务运行状态查询等运维需求。下面介绍几类相关的进程守护的方法或工具，重点介绍我们用God做进程守护的使用经验。 早期，在没有进程守护工具的情况下，为了保障线上服务挂掉之后能够修复，部分工程师想到了使用cron定时执行一个脚本来判断服务是否运行，发现服务进程不在之后尝试重启服务。这是一种比较笨重的方法，维护成本较高，且依托于工程师实现脚本的思维缜密程度，容易出错。 后来经过调研我们使用了Supervisord，它是用Python实现的一个进程守护工具，能够管理非Daemon服务，并在服务挂掉之后进行重启。虽然Supervisord能够满足绝大多数场景下的进程守护需求，但由于当时Supervisord无法管理Daemon服务，以及Supervisord升级时被守护的服务进程会重启等问题，无法完全满足我们对于服务进程守护的需求。 在进一步调研后，我们选择了God这个开源的进程守护工具。God是Ruby实现的一个可配置、可扩展的服务守护框架，能够满足进程守护的基本功能，能够管理Daemon或非Deamon服务，提供统一的服务启停接口。另外，God支持插件功能，比如我们添加了服务状态汇报插件，在服务完成部署后，God能够定时给服务树上报服务部署和运行状态（上报的信息描述示例：服务器A上部署了Nginx 1.5.2，Nginx当前处于running状态）。 整体来看，God包括一个Server端（常驻内存）和一个Client端（命令行工具）。日常的操作都通过Client这个命令行工具来完成，Client通过UNIX Domain Socket将指令发给Server端，由Server完成相关动作。Server端包含一个管理线程（Manager）和若干Worker线程，Manager对于其管理的每一个服务，都会启动一个独立的Worker线程进行管理，Manager和Worker线程之间通过消息队列进行通信，所有动作异步执行。 God通过pid的方式判断守护进程是否存活，这样做的好处是God与所守护的进程解耦，即使God异常退出或升级也不会影响其所守护的服务进程。当God重启后，扫描之前记录的pid文件，继续接管进程的守护工作。God的这些特性也是我们选择使用它的重要原因，能够使我们很方便地对它进行升级，而且不会对线上服务造成影响。 God守护进程时执行的操作均来自于一个消息队列，队列中的event产生的方式有两种：Client触发的动作会产生event，服务状态切换时也会产生event。God支持在各类event上添加回调，执行指定的动作。例如执行god load服务配置时，God接收到load事件，加载指定的配置文件，开始管理指定的服务；执行god start服务时，God接收到start事件，启动服务，并开始守护服务；执行god stop服务时，God接收到stop事件，停止服务，并停止守护。 我们在God默认的机制基础上，增加了reload和nuke两个动作，reload可以满足部分服务的重载需求（比如Nginx）；nuke是对God原生的remove动作的升级，支持完整的服务停止和环境清理工作。 God的灵活状态切换和自定义动作支持功能，在后续部署系统的扩展中发挥了极其重要的作用，我们会在后面详细讲解。God进程守护的整体机制示意图如图17-6所示。 图17-6 God进程守护的整体机制示意图 一个典型的God管理的服务配置如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051God.watch do |w| ### 加载配置时是否直接启动服务 w.autostart = true ### 优雅启动和停止的sleep时间 w.grace = 1 ### 服务版本 w.ver = ＂0c5bbe42c＂ ### 服务名称，是God管理服务的名字标识 w.name = ＂job.nginx_service.scribe_cluster.production-lg_pdl.account_owt.miliao_cop.xiaomi＂ ### uid和gid指定服务启动的账号 w.uid = ＂work＂ w.gid = ＂work＂ ### 服务实例的启动命令 w.start = ＂exec /home/work/bin/scribe/bin/scribed -c /home/work/bin/scribe/conf/job.nginx_service.scribe_cluster.production-lg_pdl.account_owt.miliao_cop.xiaomi_scribe.conf 2&gt;&amp;1 ＂ ### 标准输出，错误输出的filename w.log = ＂/home/work/log/scribe/run.log＂ ### 服务日志文件，便于god log查看 w.process_log = [＂/home/work/log/scribe/run.log＂] ### 服务启动的current dirw.dir = ＂/home/work/bin/scribe/＂### 服务启动的环境变量，对于服务自带的runtime环境，可以在这里指定 w.env = &#123;“PATH”: “/bin”&#125; ### 可以限定服务可使用的资源，在我们的场景下用不上 w.keepalive( ) ### 服务所带的定时任务，这个功能是我们内部加上的，暂未开源 w.cron = ＂/etc/god_cron.d/job.nginx_service.scribe_cluster.production-lg_pdl.account_owt.miliao_cop.xiaomi.cron＂ ### 定制nuke时的动作，清理定时任务 w.behavior(:change_cron) ### 定制stop时的动作，清理pid文件 w.behavior(:clean_pid_file) ### 设置停止时，God等待的超时时间 w.stop_timeout = 300.seconds ### 定义God发现服务宕掉之后的重启规则 w.lifecycle do |on| on.condition(:flapping) do |c| c.to_state = [:start, :restart] c.times = 6 c.within = 10.minute c.transition = :unmonitored c.retry_times = 200000 c.retry_within = 1.hours c.notify = &apos;proc_down&apos; end endend### 定义God重启服务失败后的通知规则God.contact(:email) do |c| c.name = &apos;proc_down&apos; c.group = &apos;developers&apos; c.to_email = ＂x@xiaomi.com＂end 可以看到God的配置是Ruby代码，这样可以使得配置的含义更加明确，同时更加灵活。当然，因为这种配置方式并不适合所有的开发和运维工程师关注，并且大多配置项可以固化下来，所以我们对配置进行了精简，并调整为简单的配置方式，这样可以更方便地在服务的源码项目的deploy目录下添加这部分配置，部署系统只需要负责将deploy中的配置生成为最终God可以读取的配置即可。 （1）God的进程守护实现 God的Worker线程通过两次fork的方式启动服务，将其放在后台，并获取服务的pid，后面对服务的控制都是通过该pid进行的。对于Resin、Nginx等Daemon服务，可以通过God原生支持的pid_file读取pid文件的方式获取，考虑到部分服务（如resin4.0以上版本）无法支持pid文件的情况，我们扩展了God，支持了pid_cmd配置，可以通过执行指定的命令来获取服务的pid。上述的pid_file和pid_cmd配置都是God在启动服务之后，异步执行来获取pid的。 获取到服务的pid之后，God周期性地探测服务是否存活，在发现服务进程不存在之后，会对服务进行重启。下面的配置描述的就是God对进程守护时各种状态的配置内容，flapping状态是God的一种内部状态，在God发现服务pid对应的进程不存在之后，标记为flapping状态，守护线程发现flapping状态的服务后，执行start操作重新启动服务。 同时，God通过c.times、c.within、c.retry_in、c.retry_times、c.retry_within五个配置来限制重启的次数，如下面的配置描述的就是在10分钟（c.within）之内，发现服务挂掉之后，最多重启6次（c.times），6次均重启失败后，等待10分钟（c.retry_in），在此期间，服务都处于异常状态，我们的监控系统会报警，同时God也会根据c.notify的配置发送邮件报警。如果等待10分钟（c.retry_in）之后服务还是处于异常状态，会再次尝试重启，并遵循10分钟内最多重启6次的原则。最后通过c.retry_times和c.retry_within来限制服务在1个小时之内最多只能重启200次。 12345678910111213 w.lifecycle do |on| on.condition(:flapping) do |c| c.to_state = [:start, :restart] c.times = 6 c.within = 10.minute c.transition = :unmonitored c.retry_in = 10.minutes c.retry_times = 200 c.retry_within = 1.hours c.notify = &apos;proc_down&apos; end endend 通过上面配置的一系列策略，可以确保服务在宕掉之后，会在第一时间被重启修复，重启几次失败后，通知人工介入修复。 （2）借助God实现统一的服务启停和查看方式引入进程守护工具God之后，不管是人工操作还是部署系统，对服务的启停等操作就统一了。即god start $jobname为启动服务（$jobname代表服务名称，下同），god stop $jobname为停止服务，god status $jobname来查看服务运行状态，god log $jobname来查看服务日志。 在God原生的基础上增加了日志查看功能，即god log $jobname可以罗列出所有配置的日志文件，并通过指定序号来查看日志的滚动（例如：god log $jobname 2）。在上面的God配置中，我们配置了服务的stdout/stderr日志、程序日志的位置，这样就可以通过God来查看这些日志了。 配置如下： 1234567891011121314[ ~]# god statusnginx_nginx_production-lg[21962 root]: running (2015-03-25 13:04:53)nginx_scribe_production-lg[24829 work]: running (2014-11-17 11:59:06) [ ~]# god log nginx_nginx_production-lgUsage: god log job.nginx_service.nginx_cluster.production-lg_pdl.account_owt.miliao_cop.xiaomi [0/1/2..]-----------------Log List ------------------1. God log2. Process log for /home/work/nginx/logs/https_account.xiaomi.com.log3. Process log for /home/work/nginx/logs/http_api.account.xiaomi.com.log4. Stdout log for /home/work/nginx/logs/nginx.run.log## 通过指定的序号查看日志的滚动[ ~]# god log nginx_nginx_production-lg 2 （3）借助God实现定时任务的部署在传统方式下，服务部署和服务相关的定时任务的发布是分离的，服务部署使用部署系统，定时任务通过人为方式管理或者通过另外一套独立的任务管理系统。 在我们设计部署发布规范时，要求定时任务跟随服务部署一起管理，其实现方式是在源码库中维护定时任务配置，在编译和发布包分发环节被带到具体的服务器上，在执行部署时生成最终的定时任务配置文件放置在/etc/god_cron.d/$jobname.cron中，此时定时任务还未生效，在执行god load $jobname后，God开始接管服务，同时将定时任务配置文件拷贝到/etc/cron.d/目录下，即在God开始管理服务的同时会生效定时任务；在执行god nuke $jobname后，会移除/etc/cron.d/$jobname.cron文件，即God移除接管的服务后清理相应的定时任务，从而支持在服务部署的同时管理相应的定时任务。实现定时任务部署的配置如下所示。 在源码的templates目下，需要放置cron对应的模板文件cron.erb： 123456### 每小时清理一次日志01 * * * * &lt;%= scope[＂config::user＂] %&gt; /usr/sbin/tmpwatch --nodirs -m 72 /home/work/log/&lt;%= scope[＂config::jobname＂] %&gt;/在源码的Manifests的init.pp中，配置按模板生成对应的cron文件放到/etc/god_cron.d/目录下： file &#123; ＂/etc/god_cron.d/&lt;%= scope[＂config::jobname＂] %&gt;.cron＂: content =&gt; template(＂cron.erb＂), &#125; 5．Agent稳定性 虽然引入了God进程守护工具，确保了其所管理的服务进程能够被正确守护，但是God自身的存活该如何保障呢？ 在我们的部署系统中，我们通过Frigga和God之间的配合来完成：Frigga自身是通过God进行守护的（这一步是在Frigga初始安装的时候完成的），同时Frigga中有一段逻辑，周期性地判断God的存活情况，一旦发现God挂掉，会立刻重启God，这样通过God和Frigga的互相守护来确保它们的稳定性和存活率。Frigga和God互相守护示意图如图17-7所示。 图17-7 Frigga和God互相守护示意图 部署Web部署Web端最核心的功能是：友好地帮助用户触发编译，管理和生成部署配置文件，调用Odin执行服务部署动作。部署Web端整体结构示意图如图17-8所示。 图17-8 部署Web端整体结构示意图 在部署中，发起部署任务的服务我们称为job。每个job都有独立的部署配置，Web会记录每个job的部署配置，方便后续选择版本和服务器列表就可以简单、快速地发起部署。 用户指定Git地址的版本号及相应的编译参数，部署Web端首先通知编译模块进行编译。编译模块判断该job的指定版本是否已经编译过，如果已经编译过，则直接返回成功；否则执行编译动作。编译成功后，部署Web端会根据页面配置的参数及编译模块返回的包地址组成部署配置文件，通知Odin发起任务。 后续用户在Web上的操作（暂停、取消部署等）会及时通知Odin，同时Odin的任务信息变化也会通知Web。这就是用户发起任务后，Web与周边交互的过程。 Web与周边交互的模块比较多，任务状态较为复杂，如图17-9所示为部署Web端任务时序图。 图17-9 部署Web端任务时序图 图中黑色箭头为部署系统的控制流程，灰色箭头为用户的人工控制行为，粗体黑色箭头为一次常规的部署任务过程。 部署Web端的扩展功能包括管理版本、记录历史部署任务和便捷的回滚方案。另外还增加了一些其他的便捷性功能，比如服务器扩容时的整机job clone，能够在一台新的服务器上快速clone一个相同的服务环境。为了解决“job回滚版本”本身的复杂逻辑，我们重新定义了回滚的含义：一次回滚就是一次正常的部署任务，回滚时Web端自动填充默认配置数据。 Build Server设计使用编译工具的目的是为了提供简单一致的打包方式，提供简单的编译方式，并确保输出的包满足规范，可以供部署系统直接使用。编译工具按照如下规则执行编译动作。 ◎ 如果源码中存在build脚本，则执行build脚本。◎ 如果未发现build脚本，则扫描目录下是否存在pom.xml，如果存在，则按照公司默认的Maven项目编译。◎ 如果未发现pom.xml，则扫描是否存在Makefile；如果存在，则按照公司默认的C类语言编译。◎ 依此类推，如果发现其他默认支持的配置，则按对应方式编译；如果扫描失败，则返回编译失败。 编译工具支持按分支、版本及tag方式获取源码，确保其能够支持多种类型的研发版本管理方式，以及开发、测试和hotfix环境下的分支部署。编译工具的上、下游关系示意图如图17-10所示。 图17-10 编译工具的上、下游关系示意图 Web触发编译任务时，传入jobname、Git地址、Git分支及版本或tag，编译工具执行编译任务，并将发布包上传到产品库中。产品库使用分布式存储系统GlusterFS提供存储，通过HTTP方式提供上传和下载。 部署系统小结在部署系统设计开发阶段，核心的工作是在前置规范的基础上，提供一整套的系统组件，包括部署控制中心Odin、服务器上的Agent（Frigga、Thor、God），以及提升使用便捷性的部署Web端。用户可以借助这套部署系统做到编码完成之后，一键将服务部署到开发、测试或线上环境中。 部署系统扩展支持常规的服务部署之后，运维和开发工程师可以很方便地管理部署的任务，但是对于运维工程师来说，仅仅将服务部署到线上服务器还是远远不够的，接下来还需要去维护状态信息：需要更新前面章节中提到的服务树服务器列表信息，需要更新上、下游服务关联信息，如果部署的服务是LVS后端的Real Server还需要更新LVS配置，另外还需要更新监控配置。 如果上述这些操作都是手动进行的话，服务部署还不是真正意义的自动化，而且如何确保这些信息的准确性和有效性也是个问题。 完整的运维体系不仅包括服务器管理系统、监控系统、部署系统、关联管理系统、域名管理系统，还包括LVS管理系统（管理LVS的VIP及对应的后端Real Server列表）等。在涉及服务维护相关的各个运维系统中，部署系统是多个系统的触发点，因为只有部署系统会第一时间感知线上服务部署变动的情况。 因此，我们必须确保部署系统真正承担起触发的功能。为此，我们扩展了部署系统，使其能够更好地将部署情况第一时间通知到关联的系统。这些扩展功能包括：管理服务树配置、管理服务监控配置、部署对接测试、管理LVS配置、管理服务发现配置。下面先分析多系统联动的基础要素jobname，然后分析各个联动方案的细节。 jobname为了能够实现部署系统与其他运维系统的对接，首先我们需要确定一个在各个系统中都能够被识别并且唯一定位的服务名称，在本书中这个服务名称均称为jobname。有了这个全局唯一的jobname之后，我们就可以在其上关联相应的配置，比如服务对应的监控配置、对应的测试配置等。 jobname在设计的时候必须考虑到满足如下需求。（1）能够唯一地表达jobname对应的服务，既具有可读性，也要便于系统的处理。（2）能够兼容服务树的特性，如果jobname与服务树管理机器列表方式存在差异，必然导致中间交互环节需要进行转化操作，既不利于降低系统复杂度，也不利于人的理解。（3）能够汇聚多维度信息（这一点也与服务树的特性一致），jobname中既要包括团队归属信息，也要标识服务自身信息（如业务名称、归属分组、归属集群等信息）。具备汇聚功能后，就可以抽象到一起进行管理，例如权限的配置可以在汇聚后授予、共性的监控可以汇聚后统一配置等。 综合考虑上述需求后，我们确定了下面的jobname规范，并将其作为服务树和部署系统的管理基础，做到各个运维系统管理的一致性。 1cop.xiaomi_owt.cloud_pdl.search_servicegroup.A_service.a_jobgroup.B_job.b_cluster.c1 核心思想：jobname中应包含标识其属性的多个字段，每个字段为一个tag，每个tag包含一个key和一个value，将所有的tag串接起来组成最终的jobname。 tag包含服务的归属信息和服务自身的属性，表17-1中罗列了在部署系统中使用的各个tag的含义。 表17-1 在部署系统中使用的tag的含义 通过cluster这个tag可以确定服务归属于哪个集群，通过cop、owt、pdl三个tag可以定位到服务的归属，通过两个必选的tag（service、job）和两个可选的tag（servicegroup、jobgroup一般用于分层分列较为复杂的服务）来描述服务自身的属性。通过上述tag可以全方位地描述一个服务。 有了jobname之后，各个运维系统就可以借助它关联到一起了。接下来分析部署系统是如何与其他几个运维系统进行联动的。 对接服务树系统服务树是运维各个系统中涉及服务器的一个基础平台，包括服务器登录权限、监控的报警策略等都需要借助服务树进行管理。在传统的管理方式下，服务树中的节点和服务器列表需要人工进行维护，这种方式极易出现配置信息更新不及时导致失效的情况。 因此，我们对这种方式进行了彻底变革，通过jobname进行关联，每次服务在服务器上部署后，都会将服务的jobname和服务器名称上报到服务树系统，服务树系统将服务jobname和服务器名称的关联关系记录下来。服务树汇总各个服务上报的信息后，就可以根据tag信息绘制出来一棵完整的服务树，示意图如图17-11所示。 图17-11 服务树的合成示意图 每个服务在部署时，都会自动注册到服务树中，这样树的形成就由传统的手工维护方式转为自动生成，并且在服务部署、调整或者下线后，服务器列表能够在树中自动更新，解决了原来手动维护服务树混乱、遗漏和低效的问题。 前面已经讲到，服务部署到线上服务器后，进程守护工具God会开始接管服务。在这里我们充分利用了God的特性，定制了God在加载服务（load事件）配置时的动作，增加了注册服务树的环节，并定制了服务下线（nuke事件）时的动作，增加了从服务树摘除的环节。 注册的动作为调用服务树接口（HTTP接口），将服务器名称（hostname）和服务信息（jobname）上传到服务树系统，实现服务树的挂载动作；在执行god nuke $jobname之后，God会执行清理动作，调用服务树接口，将服务器从服务树对应节点下清除。God注册服务到服务树系统示意图如图17-12所示。God从服务树系统注销服务示意图如图17-13所示。 图17-12 God注册服务到服务树系统示意图 图17-13 God从服务树系统注销服务示意图 对接监控系统公司自研的监控系统，在满足功能需求的同时，还给部署系统提供了丰富的API，确保部署系统在服务部署之后能够及时地更新服务监控。在监控系统中实现服务监控，有两块配置需要完成：数据采集项的上报配置和报警策略的配置。 数据采集项的上报配置包括如下三类。 ◎ 服务器基础监控项：服务器负载及运行状态相关的采集项。这部分在所有的机器上都是一致的，由监控的Agent自动完成采集。◎ 被动采集方式的服务监控项：在服务之外采集监控数据并上报，通常由服务管理的定时任务进行采集。◎ 主动采集方式的服务监控项：服务自身进行监控数据的采集并上报。 在这三类数据采集项的上报配置中，只有被动方式的采集需要额外借助部署系统进行管理，支持的方式为在部署系统部署服务的同时部署监控数据采集的定时任务，即监控采集的任务被封装到脚本中，并配置定时任务周期性采集上报，定时任务及脚本随着服务进行部署。 报警策略的配置包括如下两类。 ◎ 对于已经绑定到tag串的报警策略，部署系统在服务部署之后会自动更新tag串对应的服务器列表（即更新服务树的动作），之后报警策略在新服务器上会自动生效。 ◎ 部分服务需要在部署时配置服务对应的报警策略，即在监控系统中增加报警策略，并增加服务对应的tag串与报警策略的对应关系。部署系统支持将报警策略定义为配置文件，并在部署服务的同时调用监控系统的API添加报警策略。监控系统注册监控项和报警策略的动作均在God的load环节进行，具体示意图如图17-14所示。 图17-14 God配置服务监控示意图 God从监控系统注销监控示意图如图17-15所示。 图17-15 God从监控系统注销监控示意图 具体的监控系统API，可以参考“监控系统”部分中的描述。 对接测试将服务部署到服务器上之后，还有一个重要的环节是确认服务是否正常运行。在传统的服务部署时，需要在部署后去人工观察服务的运行情况，包括进程是否正常、启动的日志是否正常，观察无异常后，再尝试引入小量的请求来观察服务的状态和日志，以此来判断服务运行是否符合预期。 这种人工检查的方式耗时比较长，而且可能会出现无法及时发现服务运行不正常的情况。在服务变更引发故障的案例中，有一部分就是由于部署后的服务运行不符合预期但是没有及时发现所导致的。因此我们在设计部署系统时，专门考虑了服务检查的需求，解决方法是部署系统对接测试。 实际上，在服务研发完成后，研发人员会利用一些测试用例来验证功能是否满足需求，而后提交给测试人员，测试人员会维护服务对应的一套完整的功能和回归测试case。如果这些测试用例能够在线上服务器上再执行一遍，就可以及时发现部署后的服务是否正常运行，唯一的区别是这个环节需要注意去除那些对线上数据有破坏的测试用例。 在部署的配置中，我们增加了测试用例的配置，并扩展了部署系统，支持在部署完成后执行测试动作。 这部分配置包括两类：每个实例运行的测试用例和全局运行的测试用例。◎ 实例测试用例在部署完每个服务实例之后被触发执行。这类测试用例在单机上执行，可以调用服务在本机提供的接口，以测试服务实例自身的运行状况。 ◎ 全局测试用例在部署任务整体执行完成后被触发执行。这类测试用例不在服务实例所在的机器上执行，而是在专门的机器上执行，可以请求整体服务，或者模拟客户端进行调用，甚至可能直接请求服务对外的公网接口，来测试服务整体的运行状态。部署系统对接测试示意图如图17-16所示。 图17-16 部署系统对接测试示意图 通过引入部署系统对接测试之后，能够确保每个服务实例在部署之后的行为符合预期，也能够确保产品整体提供的服务是正常的，这样就极大地减少了服务部署后遗留的潜在问题影响到服务可用的情况。 对接LVS管理系统和服务发现系统在传统方式下，如果服务需要挂载在LVS后面，那么部署完服务后，还需要去LVS管理系统中手动更新LVS配置。另外，服务的上、下游关联关系，也需要手工进行维护。这两件事情也是运维工程师的痛点。在设计部署系统时，我们也想办法解决了部署系统与LVS管理系统联动、与服务发现系统联动的问题。 LVS和服务发现的注册需要在部署配置中增加相应的描述，其中LVS管理系统需要配置注册的VIP:PORT和本地监听的IP:PORT，服务发现系统注册需要配置本地监听的IP:PORT，其目的挂载点即服务jobname对应于服务发现系统中的节点。考虑到LVS管理系统和服务发现系统注册的时候，应该确保服务是存活的，因此注册的动作被设计为God成功启动服务之后进行。相应地，注销的动作在God发现服务宕掉后或者被主动停止前进行。 在部署系统中注册和摘除LVS配置、服务发现配置的示意图分别如图17-17和图17-18所示。 图17-17 God配置服务发现系统和LVS管理系统示意图 图17-18 God从服务发现系统和LVS管理系统中摘除服务示意图 部署系统扩展小结在部署系统扩展方面，主要解决的是部署系统如何与其他关联运维系统对接，包括服务树、监控、测试、LVS、服务发现等系统，增加部署系统与各个系统的联动，保证在服务部署后，相关系统的配置能够及时得到更新，增强各个系统配置的准确性，减轻运维工程师的工作复杂程度。在这个阶段中，一个通行的设计是在服务的项目源码中增加需求所对应的配置，并借助部署动作生成God的配置，最终由God来完成相应的联动需求。 总结本章主要分析了部署系统从零开始到满足基本的服务部署需求的整个历程。前期，部署系统关注的是如何将服务稳定、高效地部署到线上；后期，部署系统会侧重解决与其他系统的联动，确保在感知服务部署状况发生变化后，能够自动更新到关联的运维系统中。 完成上述功能之后，部署系统只是解决了一部分运维自动化的需求，还有一些更智能化的运维需求需要部署系统去继续支持。这些智能化的需求包括：故障实例的自动迁移、自动扩容/缩容、复杂的服务混合部署、彻底的资源隔离，以及全动态的部署调度等。 部署系统为了支持这些需求，引入了Docker进行资源隔离，引入了Mesos进行资源调度，同时与服务发现系统和监控系统进行了更多的交互。由于这一部分涉及的内容较多，因此单独安排了下一章“动态调度”来详细分析。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Deploy System</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十六章：服务发现]]></title>
    <url>%2Funder-the-ops%2F20170923-16-service-discovery%2F</url>
    <content type="text"><![CDATA[在一个分布式系统中，两个服务需要进行RPC交互，主调方如何知道被调方的IP地址和端口呢？在被调方可能随时变更的情况下，这个问题会被进一步复杂化。 最简单的服务间关系如图16-1所示。复杂的服务间关系是，一个主调方需要调用多个不同服务的接口，如图16-2所示。 图16-1 最简单的服务间关系 图16-2 复杂的服务间关系 除了有一个主调方要请求多个被调方之外，每个被调方又可能有多个服务实例在运行，那么管理这些调用关系，并且让服务可靠地运行就是一件很复杂的事情了。 实例一：某公共服务接口层下线几台服务器，当没有服务发现机制时，所有调用该接口服务的服务进程，均需要同步改变连接配置，并重启生效；否则就会出现请求失败，容错做得不好甚至会影响服务可用性。如果请求此接口的服务很多，甚至跨很多产品线，那么配置修改会是个很恐怖的沟通和实施过程，而且所有的配置修改均是人工变更，很容易因出错而造成故障。扩容服务器情况也是类似的，扩容不会影响服务可用性，但可能因为调用方没有同步修改连接配置，而导致被调方流量不均衡。 实例二：中间层服务要PaaS化，如果没有服务发现机制，就是个不可能实现的目标。因为PaaS化的一个重要特性就是，在部署、迁移、扩容时，服务实例的位置（IP地址和端口）会动态变化，随机分配；而且任何故障都可能触发实例迁移，IP地址和端口也随之变化，如果客户端不能同步地自动变更连接，就会一点点丢失服务端实例，无法实现真正意义的PaaS化。 服务发现在众多运维基础设施中是个比较特殊的存在。其他基础设施故障，顶多损失一些功能，如不能发布程序，不能登录服务器，或者监控故障导致工程师不知道线上服务状况等。但如果服务发现系统出了问题，又没有相应的技术来容灾，线上所有主调服务都将变成瞎子，找不到被调服务，从而使所有线上业务同时出现故障。 所以服务发现的业务需求，会对容错、容灾及可用性设计有严格的要求，同时还要考虑如何降低老业务程序接入的成本，即考虑负载均衡和集成方式。我们需要考虑以下问题。容错——如果服务注册失败会发生什么？如果注册超时，或者其他进程突然处于离线状态，又会发生什么？这要求服务发现系统提供心跳机制，以便确定服务的存活性，并且需要有能力可靠地处理失效的服务。 容灾——如果服务发现系统及其所在网络区域发生网络分区，服务发现功能是否能得到预期的结果，而不是完全不确定性地崩溃？可用性——所有应用程序集成服务发现后，是否完全依赖注册中心的可用性？注册中心升级时会中断服务吗？负载均衡——如果多个被调方服务实例被注册到一个服务名字上，怎样让主调方流量均衡到每个被调方服务实例上？集成方式——注册中心如何提供API和SDK？是否需要支持多种开发语言？在集成时，是要将代码嵌入到应用程序中，还是可以选择一个代理进程？ 带着以上这些问题，我们看看传统模式和开源项目是怎样解决的。注：网络分区是指因为某种网络故障，造成原本处于一个局域网的服务器或设备出现无法互访的现象，好像被分割成了两个或多个区域。 传统模式在经典的服务发现解决方案出现之前，服务发现主要有如下三种模式。◎ 硬配置——主调方硬配置被调方的IP地址:端口列表，自行对被调方进行负载均衡和健康检查、容错。在这种模式下，主、被调方的关联与配置强耦合，被调方发生自动的或人为的规模增减或者位置迁移（IP地址或端口变更），都要修改主调方的配置并重启服务生效。随着系统规模的不断扩大，这个问题也被进一步放大，情况变得难以控制。 ◎ 流量代理——LVS、HAProxy、业务总线等均属于这种模式。从主、被调方解耦来看，这种模式解决了问题，每个被调方服务暴露一个IP地址:端口给主调方，主调方不用关心它下面实际对应哪些IP地址:端口列表，甚至不用关心负载均衡问题。看起来很好，但它也有比较大的问题。 首先，配置仍是静态的，需要人工维护，被调方配置集中由LVS、业务总线管理，配置出错可能大面积影响服务，若是配置分散的HAProxy，运维管理成本又太大 其次，新增的流量代理组件，它本身将会成为单点和压力瓶颈，即便可以用OSPF或Gossip等协议让LVS组件集群化，达到多机同时服务，但因流量均要经过此组件，它的可靠性会直接影响依赖它的所有服务，耦合也还是太重，风险过大 最后，无论是LVS的四层转发，还是业务总线的七层转发，都无法避免庞大的内网流量经过流量代理组件带来的负载，业务总线模式在这个问题上更加突出，会在流量代理组件上浪费大量的服务器和内网带宽。 ◎ DNS——用域名来隐藏并解耦多个被调方IP地址。这种模式局限很多，比如还是要主调方维护端口配置，当被调方端口不同时难以处理。而且DNS协议的轮询不能实现完全的负载均衡，会造成被调方负载不均衡。 可见，以上几种模式都不够完善，不适合作为服务发现的通用解决方案。 开源实现服务发现作为分布式系统大规模复杂架构和动态调度部署的重要组件，开源有很多种实现，不同开源软件实现的思路也有很大不同。 从底层实现角度看，主要分为两类。一类是采用异步的Gossip协议，用Agent聚簇（去中心化）的方式完成服务实例注册和查询功能。这类基本只有Serf一个经典实现，为了注册服务，Serf Agent会将自己连接进一个已存在的聚簇中，Agent可以通过自定义标签附带主机角色、环境、IP地址、端口等元数据。一旦某个Agent连接到聚簇，聚簇的其他成员就可以看到这个主机和它的元数据。Serf用弱一致性换取良好的容错能力和可用性，是一个不错的开源服务发现，适合规模较小的场景。 另一类是使用中心化的一致性存储，作为服务注册、变更、查询的载体，业内99.9%的服务发现系统都属于此类。其中比较经典的一致性存储有ZooKeeper、Etcd、Consul、Eureka。 下面分别介绍各个开源服务发现系统的设计细节。 1．ZooKeeper ◎ 老牌一致性存储，采用较复杂的Fast Paxos算法保证一致性，API最丰富完善，但对用户而言也较为复杂。◎ 共有三种角色：Leader、Follower和Observer。前两者参与选举，个数之和应为奇数，数量越多读性能越好，但同时写性能越差（每次写请求都要经过选举）；Observer不参与选举，仅同步数据和接受客户端连接，用于不降低写性能地扩展读性能。◎ 在注册EPHEMERAL类型节点时，客户端与服务端采用长连接保持此节点，当连接断开后此节点自动消失，同时提供Watch功能，当节点发生更新时向客户端触发事件。这两个特性很重要，它们共同提供了早期服务发现系统的基础模型。◎ 支持分布式锁，由于ZooKeeper的设计中有保证时序性，因此可保证先抢到锁的才能持有锁。 2．Etcd ◎ 一致性存储新贵，采用更轻量的Raft算法实现选举一致，因此写性能比ZooKeeper高出不少，单实例可以达到1000 Write QPS。◎ 提供HTTP+JSON的RESTful API，更加易用和方便集成。◎ 通过TTL对已注册的客户端进行健康检查，相比ZooKeeper而言更能容忍网络抖动，TTL时长可根据用户网络情况定义合理值，以便在容忍网络抖动和服务敏感度之间进行折中。◎ 同样提供数据改变监视、多值、目录监听和支持分布式锁等功能。◎ 方便的原子操作——CAS（Compare-and-Swap）和CAD（Compare-and-Delete） 3．Consul ◎ 使用Raft算法来保证一致性，同Etcd。◎ 支持多数据中心，内外网的服务采用不同的端口进行监听。多数据中心集群可以避免单数据中心的单点故障。◎ 支持七层协议的健康检查，允许用户自定义检查协议和策略。◎ 支持HTTP和DNS两种协议接口。◎ 官方提供Web管理界面。◎ Consul的客户端无状态，负责将HTTP和DNS接口请求转发给局域网内的服务端集群。◎ Consul的服务端负责保存配置信息，通过Raft协议形成集群，在局域网内与本地客户端通信，在广域网内与其他数据中心通过Gossip协议聚簇并互相转发请求。每个数据中心的服务器数量推荐为 3或5台。Consul架构示意图如图16-3所示。 图16-3 Consul架构示意图 4．Eureka ◎ Netflix开源的服务发现系统，支持负载均衡和服务发现。 ◎ 并非强一致性，而是最终一致性。各台Eureka服务器使用Gossip协议聚簇构成一个Cluster，彼此之间异步复制状态。 ◎ 如果一台服务器出现问题，Eureka不需要任何类型的选举，客户端会自动切换并连接到一台新的Eureka服务器。当它恢复时，可以自动加入Eureka节点集群。按照设计，它可以在零停机的情况下处理更广泛的网络分区问题，在出现网络分区的情况下，Eureka可以继续接受新的注册并发布，同时可以确保新增服务仍然可以供分区同侧的任意客户端使用。 ◎ Eureka有一个服务心跳的概念，可以阻止过期数据。如果一个被调方服务长时间没有发送心跳，那么Eureka将从服务注册中将其删除。但在出现网络分区、Eureka在短时间内丢失过多客户端时，它会停用这一机制，进入“自我保护模式”。网络恢复后，它又会自动退出该模式。这样，虽然它保留的数据中可能存在错误，但却不会丢失任何进入保护模式前的有效数据。 ◎ Eureka在客户端会有缓存。即使所有的Eureka服务器不可用，服务注册信息也不会丢失。只有当所有的Eureka服务器都没响应的情况下才会用到缓存，因此缓存机制也是可靠的。 ◎ Eureka就是为服务发现而构建的，它提供了一个完整的客户端库，该库提供了服务心跳、服务健康检查、自动发布、缓存刷新以及负载均衡和容错等功能。 ◎ 管理简单，很容易添加和删除节点。官方提供了简洁的UI，能够展现所有的服务及其健康状况。 ◎ Eureka还提供了RESTful API，使用户更易将其集成到其他可能的用途。Eureka架构示意图如图16-4所示。 图16-4 Eureka架构示意图 开源的实现方案，从应用接入角度看，主要分为三类。 第一类是直接使用流量代理模式。这类的特点是，通过一致性存储自动发现注册和变更，并自动编排每个本地流量代理配置，来达到访问一个VIP自动负载均衡到其后配置的多个IP地址:端口的功能。这类的优点是集成简单，完全不需要侵入代码来支持服务发现，且开发者不需要考虑负载均衡问题；缺点是VIP不是一个容易被管理和记忆的媒介，且每个业务程序前多了一个流量代理组件，增加了性能损耗，同时也增加了因流量代理故障造成业务故障的可能性。这类的经典实现有SmartStack、Bamboo（Mesos）。 第二类是使用中心化服务器来承载服务发现系统，提供RESTful API进行服务注册、变更、查询，也是通过增加本地客户端来提升可用性和API易用性的。这类的优点是方便服务端实现更多的复杂功能；缺点是需要侵入业务程序代码完成集成和负载均衡功能，集成难度较大。这类的经典实现很多，如Eureka、Consul、Confd、Discovery等。 第三类也是以中心化服务器来承载服务发现系统，但以DNS协议为基础，通过DNS协议暴露服务注册、变更、查询等功能。这类的优点是方便服务端实现更多的复杂功能，同时通用的DNS协议集成方式比较简单（无论gethostbyname系统调用还是host命令），还可以通过权重方式获得伪负载均衡；缺点是客户端难以随时变更后端IP地址:端口，会造成实际负载不均衡，且主调方也无能力改变负载均衡策略。这类的经典实现有SkyDNS、Spotify，另外Consul也提供了DNS协议的组件及接口。 服务发现的开源实现很多，有一些已经相当成熟，因此在我们的工程实践中，也是基于开源项目进行二次开发，实现公司业务场景上的一些特定需求。 第一版设计我们很早就设计了第一版服务发现系统，这对前、中期的运维平台建设起到至关重要的作用。这一期选型使用了当时比较成熟的ZooKeeper承载服务发现系统。 我们将服务注册（长连接保持）、服务查询功能，通过统一的Java Thrift RPC框架，集成进客户端和服务端应用进程，并由RPC框架负责负载均衡。使用ZooKeeper的长连接和订阅通知机制，来获知被查询服务实例是否存活。 在可用性方面，ZooKeeper的设计目标符合CAP定理中的CP，即满足一致性和分区容忍性，但分区时的可用性无法保证。因此我们规划以逻辑IDC为单位，垂直拆分ZooKeeper集群，每个ZooKeeper集群服务于一个IDC环境，这样就减少了IDC间专线故障造成的分区可能性，变相提高了系统的可用性。 注：CAP定理是指在设计分布式系统时，一致性（Consistent）、可用性（Availability）、分区容忍性（Partition Tolerance）三个属性不可能同时满足，该定理也叫作布鲁尔定理。CAP定理明确了分布式系统所能实现系统的局限性。部署结构示意图类似于图16-5。 图16-5 部署结构示意图 但仅按IDC粒度垂直拆分ZooKeeper集群，仍无法对抗IDC内部的分区问题，也无法对抗ZooKeeper集群本身的故障问题，因此我们在统一的RPC框架中，提供了本地快照机制：保存最后一次成功请求的服务位置信息，当请求ZooKeeper失败时，使用本地快照，在一定心跳周期后，再尝试恢复ZooKeeper请求，这样也就可以部分容忍ZooKeeper集群的故障，发生故障时无法注册新的被调服务实例，但不影响服务的现有连接。 在正常情况下，客户端请求服务端，有变更则写入本地文件缓存中，如图16-6所示。在异常情况下，客户端请求服务端失败，改请求本地文件缓存，直到重新感知服务端恢复，如图16-7所示。 图16-6 客户端与服务器正常交互 图16-7 客户端与服务器正常交互 以上可用性设计，在线上运转4年，有很高的可靠性，基本未出现过因ZooKeeper造成的线上大规模故障。 业务除了服务发现需求外，还有较小部分的附带信息需求，当时为简化设计，也允许这些信息存储在ZooKeeper里。如果想附带额外的信息，如版本号、协议、一致性哈希路由表等信息，RPC框架也提供API自定义数据结构附加在服务名字节点上，可供客户端获取和使用。 此方案简单有效，沿用至今，服务发现的关键需求都能够满足，线上服务覆盖率已高达90%。我们在初期就具备了服务发现能力，这对运维自动化的建设起到了非常大的推进作用。 第二版设计第一版基于ZooKeeper的服务发现系统，在很大程度上帮助工程师提高了对服务管理的掌控力。通过服务发现系统，工程师在服务器扩容、故障屏蔽方面获得了极大的便利，我们不用关心上、下游的服务具体的服务器调整，业务程序通过服务发现可以自动感知。但在第一版的服务发现系统中，也存在着如下一些缺点。 可用性风险——ZooKeeper设计的CP定位，决定了它无法保证分区时的可用性，靠拆分IDC部署和本地快照机制，虽然能变相提升可用性，但无法解决出现双主场景数据不一致的情况；采用ZooKeeper的长连接机制，当服务规模足够大时，网络的抖动可能会带来广播流量的风险。我们希望有更完善的可靠性保障，毕竟服务发现是个典型的AP系统，可靠性是第一要务。 第三方组件难以集成——由于之前仅提供代码嵌入的方式集成，对于Java、C等语言开发的业务App比较容易接入，但一些开源组件如MySQL、Nginx、Redis等，很难嵌入代码实现集成。这些服务在线上环境中使用很广泛，我们希望设计新的服务发现系统，可以更好地兼容应用服务和第三方服务。 注册安全问题——一个业务程序有可能通过一些代码Hack，非法注册为另一个业务程序的服务名字，这样就能够非法截获流量甚至数据。这是比较大的安全风险。 使用混乱——对于ZooKeeper的使用，限制不够严格：允许自定义一些树状结构，也允许在服务节点下自定义配置，还允许部分位置信息是人为静态配置的，因此运维成本很高，尤其是在服务机房间迁移时困难重重。我们希望通过设计规划它的使用方式，带来更多的运维便利性。 服务间关联信息缺失——在第一版的设计中，无法很好地记录谁访问谁这类信息。当一个公共服务公开接口，将服务名字注册到服务发现系统中时，它的维护者无法知晓其被哪些主调方请求。我们希望可以记录被调方的身份及IP地址等信息，供后续挖掘出完整的服务间关联关系。 控制服务间路由和降级开关——我们还希望通过服务发现机制，提供内部灵活调整服务间路由的能力，这项能力在服务机房间迁移时具有超凡的意义。试想：在服务机房间迁移时，我们总是用空间换时间（准备与当前数量相同的服务器集群，在另一个机房完全搭建好服务，切换流量），或者用时间换空间（另一个机房只有几台机器，迁移小部分服务，机器迁移，再迁移部分服务，在中间状态过程中，即使很好地利用了服务发现系统，也依然要频繁地修改配置）。我们可以模拟域名的CNAME，这样就具备了瞬间调整任一层服务路由的能力，同时也具备了提供降级开关的能力。 兼容区域化解析和跨区域访问——区域化解析是指当不指定区域时，默认解析本区域内的服务名字，这对去除配置差异化具有重要意义；跨区域访问是指可以自由解析其他区域的服务名字，这是为部分存在Master的服务架构进行的必要折中。 基于上述问题的考虑，我们重新梳理了服务发现系统的功能和目标，进行了第二版的设计。在第二版中，我们着重考虑服务可用性，提供更丰富的接入方式，解决安全隐患，同时兼顾实现各种高级功能。 整体架构在第二版设计中，我们增加了不少高级功能，如区域化解析、服务间路由切换、关联关系展示、安全注册等。所以在整体功能模块架构上有了较大变化，如图16-8所示。 图16-8 整体功能模块架构示意图 业务流程为：提供两种接入方式，业务App集成SDK直接注册，或者由部署系统的守护组件God监听业务App并代理注册，注册行为都是先注册到本服务器的Eureka Client，再由Eureka Client注册到Eureka Server。Secure Server用来防御用户伪造注册，Dashboard接Eureka Client旁路数据用以绘图。 架构设计重点如下：（1）重客户端，轻SDK为了方便未来开源，把更多的逻辑封装进客户端；同时也降低了SDK的封装工作，尤其在多种开发语言并存的业务环境下，降低了集成成本。另外提供了程序和第三方工具（God，详见下一章介绍）两种集成方式。 （2）Eureka Server通过Gossip协议实现集群化部署在多个IDC机房的Eureka Server，通过Gossip协议聚簇成Cluster后，可容忍网络分区，同时具备超高的可靠性，且可轻松进行水平扩展。 （3）不为业务提供流量代理之前已经分析过流量代理模式的缺点，流量代理将会成为整个系统的负载瓶颈，即使将流量代理完全分散化，也会带来无谓的性能消耗和延迟，延迟又意味着业务程序的性能下降。因此，我们的业务SDK仅去发现服务位置，通过服务位置直连，降低性能消耗和延迟。 （4）客户端旁路上报调用关系数据不额外增加注册中心服务端的负载，而是利用每个客户端的能力，将调用信息有针对性地上报到Dashboard，然后绘出服务关系图谱。上报及绘图带来的负载，均不会对服务发现系统产生影响。 （5）防止假冒身份注册安全问题在Eureka Client和Eureka Server间，增加第三方Secure Server来防范假冒身份，保护请求和数据安全。 注册中心选型对于注册中心的选型，我们放弃了使用类Paxos算法确保存储一致性的ZooKeeper/Etcd/Consul等；而把眼光放在了自行对接形成聚簇，并异步复制状态的Eureka/Serf，最终选定了Eureka。主要考虑如下： 在ZooKeeper中，网络分区中的客户端节点无法到达ZooKeeper的最小法定节点时（如5个节点的集群最小法定节点为3），就会与ZooKeeper失去联系，从而也就无法使用其服务发现机制了。因此，在用于服务发现时，ZooKeeper无法很好地处理网络分区问题。 对于服务发现来说，发现的位置信息中可能包含错误要好于没有信息。虽然我们可以像Pinterest和Airbnb等公司那样，通过客户端缓存和其他技术弥补这种缺陷，但这并不能从根本上解决问题。如果所有节点完全不可用，或者集群分区和客户端都恰好连接到了不属于这个分区但仍然健康的节点，那么客户端状态仍将丢失。 更重要的是，上述做法的本质是试图用缓存提高一致性系统的可用性，即在一个CP系统之上构建AP系统，这是与CAP定理相背离的。而服务发现系统从设计之初就应该针对可用性而设计。 同样的，Etcd和Consul无论如何也仍然是CP系统，用以构建AP系统有些牵强。但若对可用性要求降低一些，Consul其实可以算是个不错的选择，其多机房多实例用Gossip聚簇的设计已经可以满足大多数苛刻的架构要求。 而Eureka的弱一致性设计，让它可以在网络分区时，不会出现整个集群故障，同时Eureka客户端可以自行选择服务的服务端。最后网络分区恢复时，Eureka服务端又可以通过异步数据同步，完成数据的最终一致。 在Eureka和Serf之间选择，考虑的主要是可扩展性，Serf的自动代理对接看似很有趣，每个Serf实例既是Client也是Server，但线上业务毕竟不是实验室，我们要考虑未来可能会有10万台服务器50万个服务实例，组成一个10万节点的Serf集群，对于Gossip协议一定是个不可完成的任务。而Eureka仅是将其Server聚簇，不会产生那么可怕的集群规模和广播流量。 最终我们还是选用Eureka，影响我们判断的主要是可用性和接入成本这两个因素。服务发现系统是一个可以放弃任何功能，唯独不能降低可用性要求的基础组件。 在部署架构方面，Eureka在可用性上做得如此完善，免去了我们几乎所有的可用性设计，我们只需要按官方建议的每个区域部署节点就行了，如图16-9所示。 图16-9 部署架构示意图 高级功能在介绍完所有需求分析和基础架构选型后，下面我们详细介绍一下之前提到的一些高级功能的设计和实现。 1．层级化服务名字我们把名字设计为有序的，且具备层级，主要基于下面的一些考虑。◎ 使用灵活，可使用任意层级的服务名字，获取其对应的位置信息。◎ 和服务树规范定义的服务标识能够保持一致，具备设计继承性。在细节设计时，不需要将服务名字标识进行转化或对应。◎ 层级化服务名字，是实现区域化解析和内部路由切换功能的核心基础。 服务发现系统的名字Tag，与服务树系统保持一致，详见“服务树”一节。其层级顺序为：Job（作业）←JobGroup（作业组）←Service（服务）←ServiceGroup（服务组）←Pdl（产品线）←OWT（部门）←COP（公司）←Cluster（集群）。比如： 1GetListByName(“cop.xiaomi_owt.miui_pdl.bbs_Service.php_job.php” “cop.xiaomi_owt.miui_pdl.bbs_Service.MYSQL_job.master” 是指定查询MySQL主库的位置信息。 而 1GetListByName(“cop.xiaomi_owt.miui_pdl.bbs_Service.php_job.php” “cop.xiaomi_owt.miui_pdl.bbs_Service.MYSQL” 则是指定查询MySQL的所有实例，包括Master和Slave。这就是层级Tag的基础功能，根据不同层级的Tag，获取不同范围服务的位置信息。 2．区域化解析为什么需要区域化解析呢？举个例子。我们有个线上业务，其架构是JobA需要访问本IDC机房的JobB，那么如何配置呢？是不是要根据实际部署情况，JobA部署到不同IDC时，要分别配置连接哪个IDC机房的JobB？新增加IDC机房时，还要新增加配置来适配？想想就很麻烦。 如果我们的服务发现系统有区域化解析功能，服务配置就会被简化：一个服务不必因部署到不同区域，而配置不同的服务名字。这个功能配合服务发现系统的域名区域化解析，就可以彻底做到让服务的配置和部署无关化，一个服务只需一个配置就可以部署到任意IDC机房，自动与本机房的关联服务对接，而无须人工关注太多。 我们可以类似这样使用：GetListByName(“cop.xiaomi_owt.miui_pdl.bbs_Service.NGINX_job.NGINX” “cop.xiaomi_owt.miui_pdl.bbs_Service.php_job.php”以上API调用，代表的含义是，主调方为cop.xiaomi_owt.miui_pdl.bbs_Service. NGINX_job.NGINX，要发现本机房的cop.xiaomi_owt.miui_pdl.bbs_Service.php_job.php这个job的位置信息，客户端会为两个jobname分别自动补齐cluster.IDC1，而Cluster这个信息取自于部署自动化系统为服务打上的标记.JOBNAME。 另外，也支持简单地指定Cluster域以明确使用某区域的该服务，这在如主库存在某一机房，或处于服务迁移阶段等场景下，很有实用价值。 3．CNAME（内部路由切换和降级开关）CNAME功能是为动态路由切换设计的，我们可以提供轻量化的小工具，用来将某个服务的流量从某机房轻松调度到另一机房。这将为运维人员提供一种全新的预案手段，比域名切换更为精细化，带来的带宽、负载迁移也会更小。同时在机房频繁搬迁的场景中具有很大的价值，不必重新发布很多关联服务，才能将流量迁移走。 当CNAME字段设置为空时，服务发现API将直接返回空列表，应用服务进程会得到一个预定义的返回值，我们定义这个返回值为服务的降级开关，服务根据这个开关执行业务绕行、限流等逻辑。 我们提供API，可为任意级JobTag添加或删除CNAME字段，以提供“无痛”切换服务路由功能。例如：SetCNAMEForName（name_tag, CNAME_tag）UnsetCNAMEForName（name_tag）当某个被调方服务的JobTag被设置了CNAME字段，任意主调方查询该服务的位置信息时，会自动改用CNAME_tag进行查询。为避免人为的使用混乱，不支持多次CNAME，以免造成CNAME成环等复杂问题。 UnsetCNAMEForName(name_tag)为服务名字删除CNAME字段，删除后恢复直接查询的逻辑。 4．关系图谱 前面提到过，在主调方调用服务查询API时，必须提供自身的JobTag。Eureka Client接到请求后，会自动将这次调用请求，通过旁路输出到一个汇聚解析服务。 汇聚解析服务将数据处理为符合建模要求的数据结构，导入Neo4j（一个专用图形数据库），然后通过Dashboard进行绘图展现。使用Neo4j提供的查询语句作为接口，以各级Tag为单位，绘制出服务间关联关系的图谱。也可以逐级展开，绘制出服务器级别的服务关系图谱。 通过服务关系图谱，研发和运维人员可以实时了解业务架构，更直观、清晰地了解服务之间的运行关系。在庞大而复杂的服务架构下，通过人工的方式理清服务之间的关系是完全不可能的，借助服务发现系统自动采集服务关系，更加的准确和实时。 5．安全注册 为了解决其他服务假冒身份注册到服务发现系统的安全问题，我们需要对服务实例注册的身份进行认证。要想认证某服务A的实例的身份是否伪造，我们需要有一个第三方认证中心，先由它为服务A颁发一对公私钥，私钥交给A，公钥认证中心留下。A的所有实例，在发起服务注册请求前，先向认证中心发起认证请求，认证请求即用私钥加密的一段密文，认证中心根据A声明的自己的身份，找到对应公钥进行解密，解密成功则认证通过。这时认证中心返回给A一个Access Token，A带着此Token请求服务注册中心，服务注册中心才允许其注册。整个请求示意图如图16-10所示。 图16-10 服务注册请求示意图 与认证对应，还有一个问题需要解决，即授权，A服务是否允许B服务发起请求的问题。这个问题和服务发现无关，需要授权中心和A、B服务共同协作完成，所以在这里就不赘述了。 总结在大规模复杂的系统中，每个服务是不是都具备自动服务发现能力，是考量整个系统架构是否具备可运维性的一个重要标尺。解决服务定位这个问题，从HardCode到HardConf，从HardConf到有服务发现系统，再到可以随意、灵活地路由，服务发现解决方案的演进，在相当程度上决定和体现着运维自动化水平的高低。 本章完整地介绍了在探索服务发现这个重要基础组件上，我们的演进状况和设计思路。本章介绍了一种对于绝大多数中小规模公司完全适用的，经历了近4年线上工程考验的服务发现系统解决方案；也介绍了一种我们正在尝试的新服务发现系统，希望能够尽快和读者分享我们的使用经验。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Service Discovery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十五章：服务管理平台]]></title>
    <url>%2Funder-the-ops%2F20170923-15-it-service-management-platform%2F</url>
    <content type="text"><![CDATA[在前面的章节中，我们讲到了自动化运维的重要性。运维基础信息的管理是自动化运维的前提，自动化水平完全取决于运维基础信息的准确性和完善程度。本章主要讲述我们在服务管理平台建设方面的一些思考和实践。 服务树简介在运维团队成立之初，运维人员只能通过Excel表记录所负责产品线的服务、服务器及之间的关联关系。在手工运维阶段，运维人员对服务或服务器进行操作时，需要经常维护和查询Excel表。随着业务的增长，服务和服务器越来越多，之间的关联关系越来越复杂，维护成本也逐渐增加。这时需要一个简单的系统来记录此表格信息，并提供API给运维人员使用，可以理解为有API功能Web版本的Excel表格。同时，我们也需要把域名、监控、服务部署等信息通过此服务信息进行关联。 根据运维经验，我们设计了一个树状结构的数据管理模型，用来记录运维各个维度信息的管理。这样的系统我们称之为服务树。 服务树主要有三大功能：维护服务与服务器的关联关系；按业务组织成树状结构；与周边业务系统的联动。本节将围绕这三方面来介绍我们设计开发的服务树。主要有如下几个特点：◎ 服务树是整合运维业务系统的核心组件◎ 灵活的服务树结构和展示，满足运维人员的管理需求◎ 多样化的数据记录和支持各种结构化的查询，联动周边系统 服务树的重要性服务与服务器的关联关系，是运维业务中的基础核心数据。服务树管理着这样的关联关系，并且给上层业务提供接口和服务，自然成为了运维平台的核心组件。服务树本身可以理解为一个类名字服务的简易系统，侧重于运维人员的管理需求。通过服务树的节点可以较好地整合上、下游系统与工具，自动变更关联关系，提升自动化水平。 服务树的关联关系数据，是由部署系统自动维护的，上层业务系统只作为使用者。通过如图15-1所示的运维平台关系图，来协助用户理解“服务树作为运维核心组件”这个概念。 图15-1 运维平台关系图 部署系统是服务发布的具体执行者，是服务树数据来源的核心入口，作为服务信息的生产者。其他系统都作为使用者来获取此服务信息，例如：监控根据服务名获取服务器列表。当服务发布完成时，部署系统通知服务树完成关联关系的变更，监控根据服务名即可实时获取到最新的服务器列表。 设计实现在介绍上层业务系统如何使用服务树节点之前，先了解一下服务树的整体设计实现。服务树主要维护服务与服务器的关联关系，因此核心功能是将这些关联关系对外友好地输出。根据公司业务需求及运维积累的经验，为了支持灵活的树视图及查询方式，我们设计了以Tag为核心的服务树。其中Tag是一个类型和值构成的键值对，并分为两大类：服务相关及服务器属性。为方便读者更容易理解，对Tag进行详细说明，如表15-1所示。 表15-2 Tag详细说明 服务相关的Tag需要组合在一起才能独立使用。例如Nginx服务Tag，用户不能区分具体是哪个产品线的Nginx服务，会产生歧义。因此，为了解决“不产生歧义的服务名”，我们通过这样的格式使其具有唯一性：公司部门产品线服务实例。这种格式的Tag组合我们称为节点串。它是贯穿各个系统的通用描述方法。由于服务树功能并不复杂，所以可以把数据表设计得比较简单些。我们是通过如下三个数据表来实现服务树的，如表15-3所示。 表15-3 数据表及用途 服务树视图服务树的一个重要功能是提供Web视图，满足用户的管理需求。视图是和用户交流最直观的一个纽带，同时也是引导和限制用户规范性操作的一个重要入口，进而减少程序的管理成本。服务树视图主要支持：服务层级关系的可视化展示，用户通过点击节点或搜索快速定位。 服务树的展示方式、各层级的先后顺序、节点层级显示与否，都可以由用户自定义。同一棵树，可以根据不同的配置、用户的需求，展示成不同的视图，如图15-4所示。 图15-4 服务树视图 不同的运维业务系统需要的视图也可能不一样。运维业务基本都是基于服务的维度进行管理，因此大部分业务（例如服务树、监控、权限系统）的默认视图树结构是公司-&gt;部门-&gt;产品线-&gt;服务组-&gt;服务-&gt;服务实例组-&gt;服务实例。 在部分业务中，为了减少节点变更带来的管理成本，我们需要引导用户将数据关联至产品线，例如域名管理、LVS管理、部署等业务，视图树结构是公司-&gt;部门-&gt;产品线。在服务器登录权限集中管理的业务系统中，以默认视图进行展示，同时为了支持一些特殊的需求（例如，给系统组授予公司所有故障机器的root登录权限），允许用户在地址栏中修改树结构参数来刷新服务树视图，进而满足对应的特殊需求。 查询功能及命令行工具服务树的另一个重要功能就是给上层业务和运维人员提供查询功能，主要包括两方面：通过服务信息筛选服务器列表和通过服务器反查询服务信息。 筛选服务器列表是查询某个服务部署在哪些服务器上。服务树提供接口给上层业务查询，同时也将此接口封装为一个脚本工具给运维人员使用。这个工具的输入参数是节点串，比如输入corp.A_dep.B_pdl.C_service.nginx，可以得到A公司B部门C产品线下服务属于Nginx的服务器列表。当某个系统版本出现安全漏洞时，运维人员就可以通过类似derp.D_pdl.P_os.centos63来获取本产品线CentOS 6.3版本的问题服务器。 反查询是查询某台服务器上有哪些服务。同样也会提供接口给上层业务查询，并且也将此接口封装为一个脚本工具给运维人员使用。这个脚本的输入参数是服务器名，比如xx-test01.bj，可以查询服务器的所属产品线及状态等基本信息。 目前较多的使用场景是根据节点获取服务器列表，并可以对有登录权限的服务器列表进行相应的操作。同时，公司内部的Ansible获取服务器列表也是基于服务树接口改造的，这样运维人员使用Ansible时就无须维护服务器列表了。 高可用性服务树属于Web型服务，前端使用AngularJS和HTML开发，使用Python开发后台，数据存储在MySQL中。在运维平台业务快速发展的过程中，服务树的可用性将影响到上层业务的稳定性。 作为一个重要的基础服务，需要有非常高的稳定性。在稳定性方面我们是如何设计考虑的呢？首先理解服务树的业务特点：大多数时候绝大部分节点数据很少变动，获取实时数据的要求不是很高，允许有一定的延迟；但有些场景需要获取实时数据，比如发起部署时，部署Agent首先向服务树注册服务器列表，成功后再向LVS注册Real Server。这时LVS就需要向服务树获取实时数据，用于判断服务器是否有权限注册LVS。 因此，根据这样的业务特点，我们做了两件事情：完善服务端API的缓存，以及提供服务树的SDK。 （1）服务端缓存 服务端缓存主要分三个方面。◎ 计算数据结果的缓存。例如获取公司、部门级大节点的服务器列表，不需要实时数据，可以直接返回缓存数据。◎ 数据库配置尽可能多的内存缓存。理论上，可以缓存大部分select语句的结果集。好处就是当数据进行变更时，MySQL自身可以保证缓存数据和实时数据的一致性，进而减少业务代码处理缓存的复杂逻辑。◎ 数据库多机房多实例。程序通过配置，优先获取本机房数据库，当本机房数据库异常时，程序重试连接耗时最短的数据库，尽量保证可以读取到有效的数据库实例。同时MySQL的数据库主从同步功能，目前已非常成熟，进而避免自身业务处理分布式数据的复杂逻辑。 （2）服务树SDK 多机房多实例部署，是目前通用的高可用部署思路，失败时可以重试不同的实例来提高可用度。这里主要给大家分享SDK的思路。SDK的好处在于可以嵌入对方的代码，部署在应用方的机器上，因此可以做一些缓存数据和重试功能。◎ 配置多个域名，默认连接本区域的服务实例，允许轮询多个实例；◎ 默认封装HTTP长连接，给出一些最佳实践的用法，引导用户批量、多次请求时使用长连接模式；◎ 默认缓存响应数据，减少服务端的请求压力，同时当服务端都异常时，还可以使用本地有效的缓存数据，尽量减少因服务端宕机带来的影响。 小结由于关联关系通过部署系统自动化关联起来，相比人工管理阶段，减少了人工维护成本，同时自动化整合上、下游。 服务树本身就是一个很简单的系统，设计和实现时应该简化设计思路。同时我们觉得此系统的难点在于作为平台底层核心，周边系统一旦依赖，改动代价就比较大了。这就要求设计产品形态时需要尽量考虑周全，同时还得考虑支持后续的业务发展。 运维权限系统简介在自动化运维体系中，服务树管理着公司服务器与服务的各维度关系，而运维权限系统维护着公司人员与服务权限的对应关系。通过对人员权限的准确控制，最大限度地减少由于权限管理问题而导致的各类安全风险。运维权限系统经历了两个版本的迭代，下面分享相关的发展过程和经验。 第一版在运维团队成立之初，运维平台的绝大部分功能都只面向运维人员，少部分功能提供给其他部门的同事使用。运维平台主要是由运维人员提炼需求，同时依据此阶段“简单、粗暴、有效、满足80%场景”原则，快速开发完成的。 因此，我们的运维权限系统在此阶段具有两个特点：精简的权限模型和简单有效的人员管理。随着平台业务的发展，运维业务都与服务树节点进行关联，节点成为了贯穿运维相关业务的“公共语言”。因此，我们的权限模型也是基于服务树节点设计的：服务树节点、功能权限、用户，这么一个简单的权限三元组就可以完成权限校验功能（见图15-5）。其中功能权限只有读、写两种。 ◎ 读权限：只对服务信息有查看权限◎ 写权限：对节点下的所有数据（服务、服务器、域名等）有查看、执行、更改、删除操作权限 图15-5 权限模型 这时权限系统提供简单的页面进行展示及配置，如图15-6所示。 图15-6 权限列表 权限系统使用这种方式来管理权限，并给周边系统提供用户权限校验服务，周边系统通过API对用户和节点的权限关系进行合法性校验。从理论上说，这个校验接口可以满足绝大部分需求，继而支撑运维平台的正常运行。 模型设计完成之后，就可以支持运维人员管理用户的需求了。但如果是针对每个人进行单独权限设定，当运维人员面对几十、上百用户规模时，权限管理将是一件非常痛苦的事情，最终无法管理。因此需要引入用户组的概念，我们把节点与权限的组合称为用户组。 此阶段，我们设计了“人员管理”的简单原则：只要用户在用户组中，这个用户就可以增加、删除此用户组的成员。这样的设计简化了权限模型，使人员管理的成本及实现逻辑非常简单。开通权限的整体过程一般是这样的：管理员首次开通用户组权限，后续就可以由组内用户管理了。 第二版随着服务管理维度和运维需求的不断增加，第一版的权限模型逐渐不能满足需求了，因此我们设计了第二版权限系统。 1．新需求 随着公司的发展，产品线越来越多，研发、测试人员等公司员工快速增长，权限需求就逐步发生了变化。（1）部分运维工作需要研发或测试人员共同承担。例如，在测试环境下服务器的运维、系统重装、报警等基础运维工作，逐步由对应的研发工程师进行承担；Preview、Staging环境的程序部署，也可由研发或测试人员负责。 （2）员工的快速增长，业务的快速扩张，需要更细粒度地划分权限，两个简单的读/写权限已不能满足要求。例如，用户可能只需部署系统的发布权限，但基于第一阶段的情况只能授予写权限（可以重启服务器、操作域名、登录服务器权限等），存在一定的安全隐患。运维平台是运维人员提炼需求，并前后参与完成的。研发人员的权限需由运维人员根据需求进行合理授权，避免出现安全事故。 2．设计概述 第二版重新设计了权限模型：服务树节点、角色模板、用户和业务权限点，如图15-7所示。 图15-7 权限模型 根据运维业务各个功能的不同，我们允许权限进行细分。不同的运维业务系统，各个操作都可以设置不同的权限点来表示。比如监控系统，可以有增加监控项、增加报警接收人、查看监控图、删除监控等多个业务权限点。为了减少业务系统的管理成本，我们建议业务系统尽量控制权限点的数量，例如服务器重启、服务器改名、服务器关机等相类似操作都用一个权限点来表示。 为了减少权限点授权管理的成本，我们引入了角色模板的概念。角色模板是一批权限点的集合，根据日常的角色进行划分。为了方便运维人员分配、管理权限点，每个节点都可以绑定多个角色模板。我们的角色模板具有如下两个特点。 （1）继承角色模板：在上级节点绑定的角色模板，在下级节点也可以继承使用。同时允许下级节点调整此角色模板的权限点，其最大权限点由上级节点模板决定。（2）自定义角色模板：权限系统给运维人员提供的公司角色模板，不一定能完全满足所有产品线的权限需求，因此运维人员可根据实际情况，自定义产品线的角色模板。 3．产品形态第二版解决了只有读/写权限的问题，提供了更加丰富、更加细化的权限点，可以根据实际情况定制化角色模板。下面将详细描述这个产品的细节，给读者更加直观的印象。如图15-8所示就是配置完角色模板与权限点后的整体概况。通过此图，方便运维人员给用户分配合适的角色模板，同时也方便管理员调整权限点。 图15-8 角色模板详情 根据工程师的实际使用情况，目前我们公司整体分为两类角色：运维人员和非运维人员（用Dev表示）。权限系统的所有操作都是通过权限点进行控制的，管理员可随时调整每个角色模板的权限。 角色模板配置完成后，就需要给用户配置权限。如图15-9所示，为了方便用户快速查询权限信息，我们以表格的形式进行展示。授权时支持运维人员单个或批量操作，同时也支持用户以正则方式快速搜索权限信息。 权限模板是一批权限点的集合，运维人员可根据用户对服务的掌握程度进行调整。例如小王是运维部新入职的同事，运维管理员可先授予其Dev角色的权限。随着小王对服务的逐渐掌握，运维管理员就可以将部分运维事物交由小王负责，可以通过授予小王运维人员或运维管理员的角色完成。 图15-9 权限列表 如所图15-10所示，运维管理员角色模板关联了几乎所有的权限。节点表示此角色模板所生效的范围。如果填写的是公司节点，此角色模板将适用于公司所有节点；如果填写的是部门，则只能适用于部门内部的节点。 我们允许用户根据自身产品线的实际情况，对上级节点角色模板的权限点进行缩小或调整，如图15-11所示。也允许用户新增产品线的自定义角色模板，此角色模板的权限点可以不受限制并由运维人员自行选择，如图15-12所示。 图15-10 角色模板 图15-11 调整权限点 图15-12 自定义角色模板 与外部系统交互权限系统作为平台的基础组件，管理着公司人员与服务的关联关系。为了保证上层业务的正常运转，运维权限需要对外部提供灵活和高可用的服务。如下是我们对外提供的5个接口： 12345678910# 检查权限check(节点, 权限点, 用户名)# 获取节点对应的成员列表members(节点, 角色模板)# 通过节点发送邮件mail(节点, 邮件标题, 邮件内容, 邮件发送者，角色模板)# 通过节点发送短信sms(节点, 短信内容，角色模板)# 将操作日志发送至服务端统一存储、展示、分析及审计event(事件名, 请求IP, 用户名, 服务器名, 节点串, 详情备注) 我们把用户的运维操作行为通过event函数统一收集起来，如图15-13所示。目前已对外提供简单的文本查询，后续将会收集更详细的数据，并提供友好的界面查询及展示。 图15-13 运维操作审计 同时我们还允许上层业务系统，通过接口和服务账号来自动变更有权限的相关数据。例如，DBA在数据库平台部署数据库实例时，通过调用权限系统的接口就可以对数据库及服务器的登录进行实时授权及变更。也允许公司内部PaaS平台通过接口实时授权相关人员登录Docker容器。 小结在我们公司，依赖运维相关资源（服务器、域名、IP地址等）的系统，基本都会和服务树节点进行关联。服务树和权限系统所构造的运维基础体系，更好地管理和对接各个业务系统。 服务器生命周期管理一台服务器从业务提出预算需求，到最终可以提供线上服务，要经过很多部门的共同协作配合才能完成。随着业务的发展，对服务器的需求也在不断增大。因此采购变得越发频繁，各业务购买的服务器数量也很多。如果整个过程还靠人来协调各部门协作，靠人来完成一些重复过程的操作（如安装系统、系统参数调整、业务依赖基础软件安装等），将会导致效率的极度下降。最终会因服务器交付延期而导致服务出现问题，甚至影响新功能上线，影响业务发展。 随着时间的推移，服务器逐渐老化，故障概率增大，故障频度也会增加。如果服务器规模较大，那么处理服务器故障更是家常便饭。在处理故障服务器时，运维工程师需要预先做很多工作，比如协调备件或新服务器，将故障服务器从集群中摘除，调整监控等。如果服务器只修不换的话，还需要进行持续的状态跟进、记录，以免后续忘记。整个过程非常耗费人力。 正因为服务器的采买、安装、运行、故障、归属调整等时刻都在发生，但过程又是如此的繁杂，所以对服务器平台化管理的需求显得非常急迫。而纵观运维自动化系统，服务器是一种非常明确的实体资源。自动化服务管理的关键能力之一就是通过调度实现服务器资源的调配，当服务器资源不足时，可以自动调配服务器，进行服务部署，并将其注册到服务集群中。而当服务器资源过剩时（如深夜），又能自动减少服务器，将过盛的资源进行归还或调度到其他服务。 服务器在各个阶段会有不同的状态，相关系统会根据状态进行相应的处理。这些状态的变化有些是人通过工单系统触发的，有些是系统执行操作后自动改变的（初始化系统、部署系统、监控系统）。我们将运维场景中产生的各种状态进行梳理，明确每个状态变更点的触发条件、保障流程等，最终形成了对服务器全生命周期的闭环管理，也就是所有状态均在平台中通过特定动作改变，不会出现人为干预而导致的服务器状态错误。这对自动化管理系统非常重要。试想：如果一台服务器正处于“维修”状态，而这时却因为使用系统操作而变为了“可使用”状态，那么它将会被调度系统分配给线上业务进行使用，后果可想而知。 在本节中，我们会介绍如何通过系统管理服务器状态，以及需要哪些前置条件等。对服务器状态的自动维护、状态的触发是工单系统、监控系统自动发现等。前置条件需要备机池，需要产品线临时机器池。 服务器状态服务器从业务提出需求到为线上提供服务，再到故障维修、下线归还，主要经过几大环节，包括：采购、装机、服务准备、服务中、故障、下线。每个环节会有多种子状态，涉及不同的自动化系统。 1．采购预算、采购主要是通过预算系统来完成的。工程师通过该系统提交服务器需求，各团队在此系统中完成审核、审批、生成采购单等工作。采购包括审批中、采购中、已到货三种子状态，这是由人来更新的。运维工程师可以通过此系统跟进服务器的购买进度。 2．装机服务器到货后，会进入装机环节，系统运维工程师会进行网络资源分配、系统安装等工作，对应的子状态为“网络资源分配完成”、“系统安装完成”。这些动作都是通过相关系统完成的，涉及网络资源管理系统、自动装机系统。 3．服务准备当系统工程师将系统装好后，服务器会进入到服务准备环节，此时服务器开始按业务的需求进行服务器命名修改、系统环境私有参数修改、服务程序部署。两种子状态分别为“私有环境初始化完成”、“服务部署完成”。此时，虽然服务器已经搭建好，但并未加入到在线集群中。一般构建好一个服务环境后，还需要进行相应的测试。 4．在线服务线下测试通过后，准备阶段完成，该服务器会被加入到线上集群中，提供线上服务。此时进入到在线服务环节，子状态变为“服务中”。 5．离线如果服务器资源出现过剩，则需要释放资源，我们会将服务器归还公司。服务器进入离线环节，子状态为“离线中”。在调度系统中，这种状态的机器被定义为健康的、可随时使用的资源，是被优先筛选的。 6．故障在服务器出现故障时，我们会将其子状态设置为“故障中”，此时服务器进入故障环节。对于故障的处理方式有两种：停机维修和更换服务器。SRE会根据情况进行选择。如果故障服务器的内存等易换件坏了，我们会选择停机维修，主要是更换机器就涉及应用、数据的迁移，考虑到运维成本，这是没必要的，但这种维修一般需要的时间可能会偏长，需要业务确认是否可以容忍。如果服务器主板等维修起来很麻烦的部件坏了，一般会选择更换服务器。 对于硬件故障的判别，我们主要靠带外监控来主动发现。还有一种情况是从业务上判断服务器整体性能出现下降，不如其他同型号服务器，我们可能不知道是什么原因导致的，这时就会选择更换服务器。具体的故障点系统工程师线下再去追查。当服务器修好后，会自动进入装机环节，重新安装系统等待调度（注：有一种情况不会进入到装机环节，而是进入到服务准备环节，具体场景在业务临时机器池中说明）。 7．报废这种状态一般SRE不太关注，系统工程师会根据服务器的年限、故障情况设置其状态。具体状态如表15-14所示。 表15-14 具体状态 在机器管理中，需要两个服务器池来放置不同状态的服务器，即公共备机池、业务临时机器池。 前置条件在服务器管理系统建设中，需要两个服务器池来存放不同状态、不同业务归属的服务器。 1．公共备机池这里的服务器不属于任何业务，是公共资源。服务器都处于系统安装完成状态。当某个业务资源不够时，调度系统会从中调配服务器。也可以通过系统来进行筛选，并通过工单系统发起备机申请流程，进行服务器借用。最终归还时，也会划分到公共备机池中，继续供其他业务调配使用。进入到备机池的服务器都会被强行重装系统，避免之前业务修改的系统参数影响别的服务。 2．业务临时机器池这里的服务器属于特定的业务，是私有资源。主要是为两种场景设计的：一是特定业务定向采购的服务器，当安装完操作系统后，会自动进入业务临时机器池中，对应的服务器状态为“系统安装完成”，工程师可以随时使用；二是服务器故障，需要停机维修，因为选择了保存业务环境，所以修复后其依然归特定业务所有，此时服务器状态为“故障中”。在服务器维修好后，不会进入到装机环节，而是进入到服务准备环节的“服务部署完成”状态。在测试通过后，会由人触发，将其加入集群提供服务。如果这个步骤接驳自动化测试，则可以自动完成，但目前主要还是人工干预。 服务器管理系统在系统设计阶段，我们对服务器管理系统提出了三大原则，以保证该系统可以作为底层服务可靠地运行。 （1）灵活扩充环节与状态。主要因为公司的组织结构有可能发生变化。这时有可能影响整个服务器生命周期涉及的团队及流程。举例来说，公司出于安全考虑，新出规定要求所有报废的服务器必须由安全团队进行审查（审查可能是人工完成，也可能是系统完成）才能最终得到“报废”状态。这时我们就需要增加安全审查环节，制定相应子状态并与其系统对接。 （2）流程的闭环。服务器状态的正确性是非常重要的。一旦状态出现混乱。我们将不知道哪些服务器可用，哪些不可用。调度系统也会将有问题的服务器提供给线上使用。因此，所有状态的变更必须通过明确的平台流程进行控制。主要是为了确保状态信息的准确，不会出现“遗漏”或“配错”。例如大批服务器到货，系统运维工程师安装完操作系统后，会交付给各业务的运维工程师，如果靠人来通知各业务服务器列表，再由各业务手工改变服务器状态，从历史来看一定会出现遗漏。如果自动装机系统能够自动触发状态变化，运维工程师只要查看自己业务的临时机器池中是否有新机器即可。不需要人在中间进行执行，那将不会出现实施层面的遗漏。 （3）提供统一接口获取资源。无论是实体服务器资源还是虚拟机资源，也不管是服务于公司的私有云还是对外的公有云，都需要通过统一接口进行资源申请，获取到资源后再自行安排使用。因为如果是多套系统或多种申请方式，将导致工程师要进行频繁的工作方式切换，降低工作效率。自动化系统也要考虑各个服务器管理系统的融合（这本不该是自动化系统考虑的）。更重要的是，服务器管理涉及业务使用资产成本的问题，不统一管理，将出现资产的混乱。如图15-15所示是服务器状态图，我们会针对一些关键流程进行说明。 图15-15 服务器状态图 1．私有环境初始化服务器在系统安装完成后，只有一个临时名称，在业务临时机器池中一般命名为music-tmp100。music代表业务线，tmp100代表业务临时机器池内的服务器及数量编号。在公共备机池中的会被命名为sys-buffer199。不管服务器来源于哪里，业务都需要对其进行私有环境的初始化。我们会将其按业务需求进行改名，如music-fe30，规则同上，这个名称确定了其在music业务中用于什么。这里可能有人会问：为什么不在预算环节就定义好服务器名称，这样就可以省了这一步？这是因为在采购的这段时间，线上服务器也在发生着变化，可能会导致与预算服务器定义名称冲突，从而带来未知问题。 服务器改名完成后，开始进行业务私有环境部署，如JDK升级、Python升级等，我们称之为runtime，也就是业务运行时所需要的依赖。我们是通过Ansible自动完成这个动作的。此时服务器状态被设置为“私有环境初始化完成”。基础环境准备好后，就可以通过部署系统部署服务程序了。只要有一个程序部署成功，系统就会将服务器置为服务部署完成状态。这里可能大家会有疑问，一台服务器上可能部署多个程序，为什么只要一个部署完成，即会改变状态？这样设计的原因主要是为后续调度系统考虑，它会根据线上情况自动决定服务器上部署什么，所以哪个程序部署成功，即代表其提供什么服务。 2．服务中测试通过后，就可以提供线上服务了。这种状态目前由人来触发，后续接入自动化测试后，可自动完成。 3．服务器下线下线分为两种场景。（1）服务器归还公共备机池。这类服务器一般是业务较长时间内不再需要这些资源，所以释放给公司，作为公共资源。资源统计系统也不再将其计为该业务成本。（2）服务器放到业务临时机器池中。这种情况是业务短时间内还需要使用这些资源。如果放到公共备机池中，有可能被分配走，所以临时放置在这里。但资源统计系统会定期筛查，超过一段时间没有使用，会自动将其划分给公共备机池。 4．服务器借用当业务自己的临时备机池筛选不出服务器时，会考虑借用，来源统一为公共备机池。即便是两个不同业务线之间借用服务器，也要遵守这个规则。被借方需要将服务器归还公共备机池，需求方再从中借用。业务线之间私自调换服务器的行为一定要控制，否则在资产管理上将会出现混乱。 5．服务器故障故障的处理分为两类。（1）更换服务器。主要适用一些不好修或不确定问题的场景。这个过程比较容易，分配一台新的服务器即可。但需要初始化、部署服务等环节。而旧的服务器下线修好后，会被放置到公共备机池中。（2）故障维修。如果只是硬盘、内存等坏了，一般会选择这种方式。迁移成本比较小。这种处理方式，在系统修好后，还属于原来的业务，启动服务即可。 环境初始化环境初始化的需求是这样产生的：（1）由于基础设施的区域化自治设计，部分配置在不同区域有差异。比如内网DNS区域化自治，不同区域的服务器，在resolve.conf中配置的服务器IP地址不同，需要进行环境初始化。（2）随着运维基础设施的建设，每台服务器上越来越多的Agent需要安装升级，升级频度远比装机包的更新高，因此也必须额外进行环境初始化。（3）业务应用程序，对系统环境有特定的需求（库、语言环境、内核参数等），但这些特定的环境需求又无法统一，这是环境初始化最重要的需求。 传统模式的环境初始化，通常使用业内流行的配置管理工具完成，如Puppet/Chef/Ansible等。常见的批量管理工具大致分为Agent模式和非Agent模式两类，在性能、可靠性、易用性、可维护性等方面各有千秋。基于以下几方面考量，我们选择非Agent模式的开源工具Ansible作为统一的批量管理工具。 ◎ Agent维护成本。Ansible使用SSHD作为Agent，不产生额外维护成本；省去了Agent端的安装、升级、进程守护等额外的管理成本。◎ 权限和认证管理。Ansible的设计模式使其天生就具有认证功能，而中心控制模式容易与现有权限管理系统进行对接，实现基于用户身份的认证和访问控制。◎ 功能可扩展性。模块化实现使扩展功能更容易，并且中心控制模式使扩展功能的升级更加便捷，利用扩展模块与其他自动化系统接驳和联动，使Ansible更加易用。◎ 易用性。Ansible的使用方式更接近于传统SSH，更适合支持传统的运维管理方式。◎ PlayBook。基于PlayBook功能，容易将基础环境配置、常用操作等固化，并结合初始化平台进行主机初始化或统一变更。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Service Mannagement Platform</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十四章：灾备管理]]></title>
    <url>%2Funder-the-ops%2F20170923-14-disaster-management%2F</url>
    <content type="text"><![CDATA[灾备概述灾备是为了有效减轻、抵御自然灾害或其他突发灾难对公司业务生存与发展造成的破坏。之前灾备分为服务连贯性和灾难恢复两类。由于服务连贯性在前面已经做了介绍，更多的是通过多数据中心和系统架构设计来实现的。因此，本章主要介绍离线备份与灾难恢复技术方案部分。 这里首先明确灾备中的两个关键指标。◎ RTO（Recovery Time Objective），指灾难发生后，从系统故障导致业务停止开始，到系统恢复至可以支持业务正常运行的时间。可简单地理解为企业能容忍的恢复时间。◎ RPO（Recovery Point Objective），指灾难发生后，容灾系统能把数据恢复到灾难发生前时间点的数据，它是衡量企业在灾难发生后会丢失多少生产数据的指标。可简单地理解为企业能容忍的最大数据丢失量。 备什么在离线灾备的实施中，其实核心就是保护数据。对于互联网公司来说，数据是支撑业务的关键，是公司的核心资产。灾备也正是为了在发生灾难的时候，实现数据的恢复并维持相关应用正常运转。在运维看来数据分为两种。 （1）生产环境数据。主要是指在线提供外部用户访问服务的相关数据，包括程序、程序运行所需数据、配置、证书、系统环境等。这是为了保证当某个服务因灾难中断时，可以快速进行业务环境及所需数据的重建。 （2）非生产环境数据。这部分数据虽然不直接对外，但却提供了非常重要的管理支持，例如代码仓库、运维管理系统等。主要是为了保证灾难发生时，内部开发过程可以持续，对在线的管理控制可以持续。 备哪里同城备份，是指将生产中心的数据备份到本地的容灾备份机房中。由于和服务机房在同一地区，它的优点是传输速度相对较快；它的缺点是一旦发生大的灾难时，将无法保证本地容灾备份机房中的数据和系统仍可用。 异地备份，是指将生产中心的数据备份到异地。备份时要注意如下几个重要原则。◎ 必须备份到300公里以外。◎ 不能在同一地震带上。◎ 不能在同一电网中。◎ 不能在同一江河流域内。对于大型互联网公司来说，一般同城备份、异地备份两种形式都会有，以分别满足恢复效率和数据安全的问题。 等级划分服务等级 当出现灾难性事件时，会同时影响大批的服务，那么优先恢复哪个服务，恢复速度的要求是什么，备份资源重点分配给哪些服务，这些都需要通过服务等级划分来进行明确。我们可将服务等级分为三类。 （1）重要服务。我们的定义是这类服务发生问题会导致公司出现重大损失，包括影响收入的服务、公司主营服务、全公司业务依赖的生命线服务等，如广告系统、订单系统、支付系统、账号系统等。 （2）一般服务。其定义是和钱没有太多关系，有一定的用户访问量和用户认知度。对公司来说属于增加用户黏性的服务，如音乐、即时聊天、博客等。 （3）不重要服务。一般是指周边很小的衍生服务，用户量小，且与收入无关。这类服务一般处于孵化阶段，出现问题对公司影响非常小。在明确了服务等级后，就需要根据不同服务等级制定其RTO、RPO了。 数据类型 服务中会包含很多种数据，例如访问日志、热词字典、用户上传文件等。在这里我们主要将其划分为隐私数据和非隐私数据。当然，大家可以根据实际情况进行分类调整。针对这两类数据，在数据处理及存储环境上还有很大不同，主要是为了保护用户隐私数据。例如，在机房环境要求上，需要独立区域或者独立持有机柜钥匙等。隐私数据需要提前进行加密处理，再进行备份等，其他数据只要进行压缩即可。 无论是针对服务等级的PTO、RPO，还是面向数据类型的定义，这些最终都会细化为具体的选项（备份频度、保存时间、是否加密、是否压缩等）提供给灾备系统，系统会根据配置项进行自动的备份处理。 备份与恢复成本RTO时间越短，意味着要求在更短的时间内恢复至可使用状态。虽然从管理的角度而言，RTO时间越短越好，但是，这同时也意味着更多成本的投入，即可能需要购买更快的存储设备、网络设备、专线带宽等。对于不同行业的企业来说，其RTO目标一般是不相同的。即使是在同一行业中，各企业因业务发展规模的不同，其RTO目标也会不尽相同。因此，企业在构建容灾备份系统时，首先要找到对企业自身而言比较适合的RTO目标，即在该目标定义下，用于灾难备份的投入应不大于对应的业务损失。 备份成本 为了对各类业务规模进行评估，对成本进行控制，我们根据备份数据的用途，分析了其对硬件的需求及成本特性。主要包括： （1）在线热备存储，主要是为了进行业务快速恢复。这类存储一般对硬盘的读/写性能有较高的要求，并且稳定性要求很好，可以认为和线上业务一样，一旦服务机房出现问题，备份数据随时就可以投入使用。由于是实时热备，很低的RTO值是追求的目标。但它的缺点也很明显，一是成本很高；二是由于是实时备份，一旦源数据因为程序bug被写乱，那么实时备份数据也就损坏了；三是和服务机房距离近，不能应对区域灾难。 （2）离线冷备存储，需要遵循异地备份的原则。很低的RTO值不是离线冷备存储的目标，而数据的完整、可靠保存才是它重点关注的。因此，在硬件选择上，它没有SSD等高性能读/写需求，只要稳定即可。它的优点是成本低，不会因程序bug导致数据完全被写坏；而缺点则是恢复效率低，恢复数据的时效性不高。 （3）离线分析存储，这类存储包括实时数据，也包括非实时数据，完全由业务决定。其所分析的源数据一般都在各个业务中，丢失了一部分也可以进行恢复，有些业务也可容忍偶尔的问题（继续使用前一份数据即可）。因此，其对存储的稳定性要求并不高，而能够进行快速的读/写、计算才是它的主要需求。由于它的特性，不建议使用其应对灾难恢复。表14-1中总结了这三种存储的特性。 表14-1 三种存储的特性 恢复成本 数据备份好了，如果缺少有效管理和技术支撑的话，那么恢复也会非常耗时。在较早的时候，对于离线数据，一般会采用磁带进行备份，并进行入库管理，但是当灾难发生时，数据的恢复过程非常折磨人。库管人员要维护业务归属、业务数据、备份日期、数据日期范围、物理磁带编号等信息，一旦出现偏差，查找起来会非常麻烦。 在恢复时，也要将磁带接入设备，然后读出到本地存储中，读/写效率也让人无奈。情况紧急时，也只能默默等待。当数据拷贝到本地存储中后，还要进行解压、解密等操作，最终才是传输到目标集群进行业务恢复。其效率无法满足我们的要求。随着分布式存储技术的发展，我们开始使用Hadoop进行离线数据的存储。 虽然成本会比磁带高一些，但我们在稳定性、机房位置、是否有外网接入等方面均无特殊需求，因此总体成本也不会高出很多。而在管理成本、恢复效率上却比磁带高了几十倍。因此，就目前互联网的情况来看，分布式存储作为离线灾备是比较可行且常见的方案，而磁带可以作为中、大型企业的保底存储。 灾备演练对于数据备份来说，数据不能备份了就完事，关键时刻不能用，将给公司造成更大的损失。因此，定期（一般建议每季度）进行灾备预案演练、数据恢复测试至关重要。灾备演练主要包括同城灾备机房演练、异地灾备机房演练、海外灾备机房演练。当然，这需要根据公司业务需求和可承受成本而定。 在这个过程中，主要考察是否能在规定时间内拿到可用于直接恢复服务的数据，这里包括备份数据的查找、传输、解压、解密等多个环节。数据恢复测试，则是指数据是否可被程序使用，恢复数据的日期是否符合数据描述等。这里主要关注程序兼容性、备份数据与预期是否一致等。演练及恢复测试完成后，需要给出相应报告。 灾备系统灾备系统是数据备份效率、备份数据有效性、备份管理与恢复的重要保障。目前市场上提供的成熟灾备设备有很多，这里就不再多说了。本节主要介绍自研备份系统。 自研系统 系统目标：◎ 快速、便捷地创建备份任务；◎ 安全地对指定的数据进行备份；◎ 可以查询备份数据的详情和历史版本信息；◎ 便捷地还原所需的数据。设计原则：◎ 不同的备份对象通过Key来区分；◎ 每个Key只能对应一个数据文件；◎ 可以支持将数据备份到多个数据节点，包括同机房数据节点和异地机房数据节点；◎ 提供Web端来创建和查询备份任务；◎ 提供客户端工具来满足备份和还原的需要；◎ 数据传输采用客户端Push方式进行；◎ 客户端不提供压缩、加密等机制；◎ 不提供多个数据备份的带宽占用调配；◎ 对发起备份与获取数据任务的时间、频率做软限制，也就是发邮件提醒；◎ 客户端与仓库需要保证备份数据的一致性（注意非正确性）；◎ Key需要有权限管理，通过关联Tag来与服务树权限保持一致；◎ 不能在同一时间启动两个及以上相同Key的备份；◎ 仓库需要定期进行数据回滚；◎ 仓库中回滚的数据，在管理系统中版本号也清除掉。 图14-2 备份系统功能逻辑图 功能说明 1．创建备份通过在Web端创建一个备份任务，会生成一个唯一的Key作为后续管理、使用的标识。创建的时候需要提供以下信息。备份频率：数据备份的频率预估，比如每天一次。如果超过这个频率会发邮件提示创建者，有强制停止备份任务的风险。数据规模：需要备份数据的大小量级，比如10GB。如果超过会提醒创建者。 副本保留时间：备份数据保留的历史周期，比如一星期。过期数据会通过系统自动删除，对使用者透明。备份集群选择：在默认情况下会选择同机房的数据集群（备份、还原过程会比较快），还可以选择其他机房的数据集群（备份、还原过程耗时较长）。 数据描述：在创建数据备份任务的时候需要对数据做出描述（只是描述，不作为备份配置）。描述包括备份数据在线上的路径、备份数据所属的模块、备份数据的作用描述。数据备份优先级：因为考虑到带宽以及数据集群的承载能力，需要做一定的流控机制，来保证不会因为高峰期造成业务瘫痪。在流量达到上限的时候会根据优先级来调度，优先级低的会等待。 2．执行备份与还原备份系统提供了客户端工具来备份数据。一次备份可以认为是一次数据的Update。备份的实际过程是通过工具将本地数据Push到相应的数据集群上，如果有多个集群，则需要串行上传。执行备份时，指定需要备份的数据文件路径和Key来触发备份动作，首先客户端请求GPS模块获取Key对应的备份任务配置及授权信息，然后将数据同步到任务配置的存储集群上，上传时通过GPS返回的授权信息进行权限验证。在数据恢复的时候，可以通过客户端指定的Key和数据的版本号获取数据。客户端程序先到备份系统的GPS模块查询Key对应的备份数据的存储位置信息，随后向备份存储集群（HDFS、S3等）获取数据。 3．查询备份每一次备份执行时，备份系统会保存本次备份的详细信息，包括备份发起的服务器、文件名、数据版本号、备份时间以及备份结果等，通过Web端、客户端或者API均可以查询备份数据的历史版本、备份详情等信息。 4．过期数据清理备份系统会根据备份任务所配置的副本保留时间，定期清理过期的数据，以便清理出可用的存储空间，减少备份成本。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>disaster Mannagement</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十三章：网站可靠性运维]]></title>
    <url>%2Funder-the-ops%2F20170923-13-website-operation-reliability%2F</url>
    <content type="text"><![CDATA[与系统运维相比，网站可靠性运维更贴近业务，可以说是业务的大管家，每天面对各类业务事务，例如服务上线、报警处理、配合网络调整、服务监控梳理、服务例行排查、预案制定、数据备份等。 网站可靠性工程师起源于Google，在其内部简称为SRE（Site Reliability Engineering，SRE需要掌握很多知识：数据算法、代码编程、大规模分布式架构、网络架构、操作系统和数据库等各个方面，还需要具备较强的故障排查和解决能力。在Google，SRE大多是资深研发工程师转岗，每位SRE都是全栈工程师，甚至是优秀的架构师。但在国内互联网公司多将SRE定义为OP（Operator），主要负责线上服务的各类操作。也正因此，国内能符合Google SRE标准的运维工程师少之又少。 在我们看来，SRE的核心价值是作为全公司最了解整体服务的人，能够敏锐地把关各种变更对服务可用性带来的影响，大到业务架构的变化、网络的调整，小到程序升级、配置调整；设计或改进服务架构，使得在业务功能、可靠性、可扩展性、可运维性方面均衡发展；在服务出现问题时，能够从整体进行排查，快速发现问题点并及时恢复服务。 在我们定义的SRE工作中，有两大技术发展方向：高可用运维和运维自动化。 高可用运维高可用运维的目的是保障业务7×24小时不间断运行，其难度在于业务在不断发展和迭代，在硬件和基础设施的调整、维护、升级过程中，始终要保证用户可以正常地访问和接入。要实现高可用的关键在于整体规划和架构设计，使业务能够支撑功能和性能的扩展，容忍局部故障和异常。SRE在进行整体规划和架构设计时需要做如下工作。 ◎ 与网络工程师共同规划可以自冗余的网络架构，进而从根本上减少网站故障。◎ 规划程序及线上环境的规范，以配合监控、部署等平台，实现自动化的网站维护工作，而不是更依赖人工管理。◎ 规划和设计服务接入方式，提高服务对抗各类天灾人祸的冗余度，并提供方便、快捷的故障恢复预案。◎ 辅助设计程序架构，使其拥有更强的容灾容错能力。这是提高网站可靠性、降低人工故障处理成本的不二法则。 可以看到，称职的SRE，应该能作为互联网各项技术工种的黏合剂，通过推进整体规划、程序架构设计、运维自动化的发展，提高网站可靠性并降低维护网站所需的成本。SRE的工作开展之初，由于各方面建设都不足，各项基础工作会耗费大量的精力。因为既要以规划设计为重点来降低基础工作量，又要保证基础工作的快速推进和当前网站的可靠性。 网站运行是否稳定和可靠，需要用科学、系统的数据指标来衡量，我们称其为可用性指标。它是一个符合产品特性、明确、可自动计算的值。可用性一般用时间来表示，即在单位周期内服务故障时间；当然，也可以通过单位周期损失请求量/总请求量来表示。具体可以根据业务情况来定，这里主要说明一下时间与可用性的关系。 服务可用性常用几个9来表示，一般我们常说服务处于3个9（99.9%）阶段或4个9阶段（99.99%），其最终反映出的是因各种故障导致的服务中断，使得在某个时间范围内业务服务不能被大部分用户所使用。从表13-1中可以看到可用性与时间的对应关系。 表13-1 可用性与时间的对应关系 当前国内大部分互联网公司的服务可用性是在3～4个9之间，也有些大型互联网公司在追求5个9的目标。一般电信、银行、证券类系统的服务可用性都是要求达到5个9的。当然，每增加一个9，所面临的技术挑战是巨大的，所付出的成本是昂贵的。这里包括基础设施、基础服务、应用服务等各个层级的稳定性建设。 在单数据中心的情况下，基础设施的稳定性要大于业务，才能满足业务稳定性的要求。举个简单的例子，一个业务的可用性要求是99.96%，而单数据中心考虑到各类硬件的平均可用性是99.8%，那么业务显然不能达到99.96%的要求。 为了使业务服务达到稳定性的要求，我们就需要考虑同城多数据中心的建设，能够屏蔽由于单数据中心故障导致的服务中断。如果可用性要求高，避免同地域范围内大面积故障带来的服务中断，我们可能还要考虑建设异地多数据中心。比如，3个以上的数据中心，分布在不同的省市，甚至不同的国家。下面列举了一些常见业务的服务可用性指标，以供参考。 1．Web服务 （1）请求拒绝率（基于HTTP协议，Nginx软件）。其计算方法主要是在单位时间内，日志中返回5xx状态码（如500、504等）的数量/单位时间内总请求量。这里我们主要使用5xx状态码，因为这个状态码表明了服务端程序有问题。虽然我们有4xx状态码的监控统计，但不作为可用性计算，主要是因为用户可以主动构造或通过一些行为得到4xx状态码。 （2）请求响应时间的Percentile（统计学术语，一般指百分位数）。计算方法为，将单位时间内所有请求的响应时间进行由小到大排序，取对应第99百分位请求的响应时间，用来评价服务整体响应速度。这里没有采用取平均值的方式，原因是平均值可能会受到某些异常Query的影响，将平均值拉高或者拉低，偏离实际情况。而Percentile更接近请求响应的真实状态和影响用户请求的占比情况。 以上两项都是因业务本身问题产生的拒绝，通过相关日志可以统计到。但如果由于公网故障、网络入口故障、接入层故障等，导致用户请求无法到达服务端，这部分损失将无法被日志记录。针对这种情况，一般采用流量环比的方式预估请求损失。也就是当天故障时段的请求数量与上周同一天、同一时段请求数量之差。当然，也会遇到特殊情况。如果是流量高速增长服务，虽然发生了故障，但环比非但没有损失，反而流量还可能有增长。这时候可以按上一小时或前一天同比推算出增长幅度，得到预期增长量，再进行损失预估。 2．存储服务 （1）读/写成功率。计算方法为，单位时间内读/写失败次数/单位时间内读/写次数之和。（2）数据读/写时间的Percentile。思路同Web服务的请求响应时间的Percentile，这里不再赘述。 3．消息服务 消息发送成功率占比。其定义是，从系统收到消息后，在规定时间内将其成功发送到接收端的数量占比。目前我们规定的时间是1分钟内。由于考虑到用户可能处在各种网络环境中，因此我们要求90%的用户在1分钟内即可。随着系统更加的健壮，规定时间会不断调小。 每个公司不同业务有自己计算可用性的方法，特别是对于复杂系统有很多可用性指标，关键是确定一个能够代表服务整体可用性的指标。但需要注意的是，服务可用性是滞后并且宏观的，它只能反映业务服务一段时间内的整体情况，不代表可以实时和准确地反映服务的故障原因。所以，我们还需要对业务的每个功能，完善详细的服务监控，以便及时发现和定位问题。甚至可以通过对整个业务流进行监控，发现在用户请求处理过程中可能出现的故障等。 服务架构对一个成功的互联网产品而言，产品的创意和功能是吸引用户的关键。同样的，是否能够持续稳定、快速地访问和使用也是产品成功的关键。试想：如果一个互联网产品经常性地不能提供服务，或者对用户请求的响应非常慢，或者总是丢失用户的数据，那么这个产品也一定会失败。因此，可靠的系统是支撑产品持续发展的基本和前提。可能有人认为，业务架构和设计是研发的事情。其实不然，SRE更应该重点关注并参与其中。在这个过程中，主要关注的是异常，针对系统中每个部分拟订可能发生的各种异常情况，评估系统受到影响的程度和是否可控，是否有可执行的解决方案，最终达到让系统满足高可用的设计要求，并保证落实。在本节中，我们就针对服务如何做到高可用来进行说明。 流量接入1．多数据中心接入 接入方式分为多数据中心和多接入点两种，大多是通过域名控制接入位置的。 相对来说，多数据中心的成本更高，因为实体服务、数据等都需要在多个物理数据中心部署，相当于一个完整服务的多个备份，需要大量的服务器。出于成本的考虑，我们对于各个数据中心服务器数量的控制是高峰期30%的冗余，而不是100%。这是因为不可能任何故障都发生在高峰期，没必要为偶发情况准备很多空闲资源。另外，就算故障发生在高峰期，我们也会进行适当的服务降级，以此避免投入过多的服务器资源。 多数据中心的优点在于能适应多种故障场景，包括单数据中心的电力故障、内部网络故障、数据中心出口故障、专线故障、基础服务故障，均可通过将流量调度到其他数据中心而规避问题，属于互联网公司比较通用的解决方案。具体结构如图13-2所示。 图13-2 多数据中心结构 当A数据中心发生故障时，可以通过调整SmartDNS解析，将用户请求调度到具有独立运行能力的B数据中心。在软件架构还不能很好地支持多数据中心接入的时候，运维人员可以考虑采用多接入点的方式，快速搭建，能够解决部分基础设施故障带来的问题。具体结构如图13-3所示。 图13-3 多接入点结构 多接入点结构在本质上仍然是一个数据中心，只是通过让用户就近接入实现快速连接、静态数据缓存加速和动态数据代理转发的能力。这种结构的优点是成本小、部署灵活，各数据中心只要放置一些代理服务器即可；缺点是对专线依赖相对较重。由于是通过代理进行请求转发的，因此只能应对数据中心出口问题，如果数据中心内部出现故障将无法规避。 多点接入使用最多的场景就是通讯类业务，可以让用户就近接入，通过专线将需要转发的请求发往核心数据中心。比如在视频通话业务中，我们在一些地点部署了接入服务器，每个接入点会为一个大的区域提供接入服务，接入服务会将用户请求通过多条线路（专线或公网）向数据中心进行转发，数据中心的服务器只会处理最先到达的请求，次之的将被忽略。 2．接入调度为了将用户请求分配到我们指定的多个数据中心或多个接入点，还需要有对用户请求进行分配和调度的能力。常见的方式如下。 （1）DNS轮询调度这是一种最简单的调度方式，DNS就可以支持，只要将A、B两个数据中心服务器的IP地址配置到域名的A记录中即可。DNS在收到查询请求后，会将该域名下这两个数据中心的IP地址都提供给浏览器，浏览器会优先尝试第一个IP地址，如果第一个IP地址在一定时间内无响应，则会尝试第二个IP地址，在更多IP地址的情况下依此类推。理论上各IP地址位于第一个的概率是均等的。 这种方式有一个不足，就是只能按流量进行分配。当然，如果你想让A数据中心多得到一些流量，就在A记录里加两个A数据中心IP地址、一个B数据中心IP地址。同时，由于Local DNS的存在，无法保证流量的负载均衡，属于伪轮询模式。在DNS轮询调度中，还可以根据用户的IP地址归属，进行更精细化的调度。 （2）DNS智能调度这种调度方式一般根据用户属于哪个运营商、用户在哪个地域进行流量调度。这种基于用户IP地址归属的DNS策略调度系统，我们称为SmartDNS，比较知名的商用DNS服务提供商均支持该功能，如DNSPod。 通过智能解析策略，可以使用户的请求按照用户IP地址属性分配到不同的接入点或数据中心，比如将联通用户落到A数据中心，电信用户落到B数据中心。同样也支持按用户地理位置归属进行调度，如美国的用户调到C数据中心，日本的用户调到D数据中心。 使用SmartDNS解决了区域识别的问题，但同样也存在着不足。SmartDNS是根据用户使用的Local DNS来判别用户的地域属性和运营商属性的，这里面有一个前提假设：用户会设置使用离自己最近的本地运营商Local DNS；但如果用户自己设定了错误的Local DNS，那么就会出现调度错误，出现用户跨网请求的情况，导致访问速度变慢。比如一个用户的DNS设定了8.8.8.8（Google提供的DNS服务），那么SmartDNS就会认为该用户在海外，最终将请求分配到了海外数据中心。所以，如果能够根据用户的真实IP地址进行判别，那么就会比通过Local DNS来判断准确多了。 Google和OpenDNS都支持edns-subnet-clinet协议，它是DNS的扩展协议，会记录用户的IP地址信息传递给在其注册的对应的Name Server服务器（当然，这需要对应的Name Server服务器也支持该协议）。这样Name Server服务器就能获取到用户的IP地址信息，根据用户的真实地域和运营商进行更精确的调度。 无论使用Local DNS还是用户IP地址方式进行判别，IP地址库的准确性都决定了调度结果，所以周期性更新IP地址库，对用户访问服务的效果起着重要的作用。 （3）App/客户端自主调度随着移动互联网的兴起和发展，大量的业务应用又从B/S模式转换为C/S模式，App/客户端不仅是离用户最近的接入层，同时也是业务逻辑的一部分，因此调度策略也在向客户端进一步延伸。例如，通过HTTP请求先从服务端获取服务IP地址列表，然后按用户所属区域、号段（一般是大的区域划分，比如国家）等客户端属性来选择合适的服务端IP地址。更进一步，客户端定期地对服务端IP地址进行质量评估，包括速度、链路和访问质量等，综合服务质量评估结果来选择最优的服务端IP地址。 （4）私有调度策略最常见的场景是业务灰度测试，一般不考虑用户的接入点，而是通过内部二次调度，将具备某些特征的用户调度到测试服务器，进行功能测试验证。比如我们的账号系统，在SmartDNS的IP地址库中增加一个公司IP地址段，将公司内所有用户的账号访问落到测试服务器。再比如广告系统，将某个地域的有车族的用户（业务分析得出的特征）调度到测试服务器，来评估某广告策略效果。 3．流量负载均衡所有的用户请求到达接入数据中心后，我们对接入流量有负载均衡处理的能力。对于TCP协议的服务，我们主要使用LVS集群进行负载均衡。首先，这样能够保证业务接入层的可扩展性与高可用性，能够做到对用户透明、方便地伸缩。这是LVS本身的容错机制实现的。 另外，外网IP收敛后，其变化的频度也会大幅减小，对应的DNS修改也会减少，因为各级DNS延迟生效所带来的问题也会规避。其次，LVS也起到了安全隔离的作用，将内、外网进行隔离，只需要对LVS集群进行安全加固即可。LVS有一定的半连接攻击防御能力。而对于UDP协议的支持，目前还没有成形的解决方案。很多公司大都在一些开源框架或开源软件的基础上自行开发。 系统设计1．多数据中心 多数据中心不仅是数据中心规划的技术问题，对于业务系统架构设计而言，也是一个挑战。网络、数据中心是保障网站提供正常服务的核心基础设施，一旦出现问题，对上游业务的影响将是巨大的。在单数据中心的情况下，如果网络、IDC的服务可用性不能达到99.9%，那么其所承载的业务就算自身再稳定，也无法做到高可用性。即便达到电信级的99.999%的可用性，也一定会出现问题。 这时就需要进行多数据中心建设（多个物理上分开的数据中心，之间通过两条或多条专线进行互联），毕竟两个及两个以上数据中心同时出问题的概率非常小。而在多数据中心结构下，业务系统也需要在软件层面满足应用、数据分区域独立运行的能力，以便做到通过灵活的调度，将故障数据中心屏蔽，使业务服务不过度依赖单数据中心的服务可用性。 对于服务上量比较大的业务，容量扩张也是关键问题。随着流量的不断增长，需要扩充更多的服务器来提升处理能力，而每个数据中心的容量是有限的（很多大厂的服务器数量都在10万台以上），最终结果是单数据中心无法承载全部服务，这时需要向其他数据中心分摊压力。因此，对于这种业务，在运维工程师接手时，要求其必须是多数据中心部署服务，具备同时提供服务的能力。具体要求如下。 （1）服务要有分区域（区域是一个逻辑概念，一般把一个数据中心定义为一个区域）自治能力，即当单一区域出现故障时，其他区域服务不受影响。每个区域有独立的基础设施及服务，如带宽出口、DNS、SNAT、NTP等，业务层面包括ZooKeeper、消息队列、数据存储等。如果数据有一致性要求，则可以定义一个区域作为主区域，数据都写入到这里（单点），其他区域将数据同步到本地，供本区域内读取、查询使用。 （2）尽量规避跨区域的RPC调用。主要有两个原因：一是数据中心间访问网络时延大于同一数据中心；二是数据中心间专线的可靠度差，施工挖断光纤等情况相对比较常见。当然，也可能有些情况不能避免。比如即时通讯类应用，用户是按照组（Partition）区分的，组与组之间的消息通信频繁，就需要保障跨区域交互。研发人员需要认可专线的风险，在出现中断时，系统设计要有能力动态迁移用户。 2．服务解耦 在网站规模由小到大的发展过程中，一般会从功能单一、结构简单逐步发展膨胀为功能众多、结构复杂的巨无霸系统，甚至到后来没人能绘制出系统全貌，最终变成不可运维的状态。特别是当服务扩容时，由于上、下游调用的信息维护不完整，容易导致发生故障。对策应该是在早期就考虑模块拆分、解耦。如果实例间不存在直接的调用关系，那么就不应该耦合在一起，要做到模块的变更不影响上、下游。这里举个解耦例子：利用分布式消息队列来降低系统与系统之间的耦合性。 从图13-4中可以看到，消息队列是发送者生产消息，一个或N个消费者订阅消息的工作模式。在这里，发送者和消费者之间没有直接的调用关系，发送者只是将消息放到分布式队列中，消费者也只需要从队列中获取消息进行处理，不用关心消息从何而来。因此，上游新增业务产生新的消息，对原有其他业务没有影响；而下游新增消费者，对上游也没有任何影响，只要订阅它想获取的消息即可。我们常用的开源软件有RabbitMQ、Kafka等。 图13-4 解耦示例示意图 3．业务分离 如果业务较多，最好能按业务类型在物理上进行归类、分离，以免相互影响。分离的原则一般分为三个维度来进行考虑。 （1）按业务类型分离。最常见的就是把动态资源和静态资源分开，这样可以针对不同的类型进行各类优化，让服务质量更好。比如静态资源，我们可以申请大内存或大磁盘容量的服务器，更多地进行本地缓存，或者使用CDN技术；而动态资源如果是HTTPS的，在硬件上我们还会为其配置SSL加速卡。 （2）按安全等级分离。一般来说，大家都比较关注主站，往往忽略了周边小站的安全性，而入侵事件一般都来自于薄弱处，所以需要按安全等级分离。这种分离不仅仅是服务器层面的，甚至在网络层面也需要进行分离。比如我们使用的开源论坛程序，它的安全风险就比较高，如果论坛服务的服务器与内网其他服务器互通，一旦论坛服务被入侵，后果将不堪设想。 （3）大流量服务分离。比如电商类推广、抢购活动、离线计算集群（数据传输）等，它们的业务可能会带来公网、内网的大规模流量负载，和其他业务共享网络带宽时，可能带来影响。而网络QoS并非解决这类问题的最好办法，最简单、可靠的方式仍然是物理分离，为它们规划独立的区域。 4．容量规划 业务分离后，系统内部也需要考虑服务容量、部署以及服务之间是否会相互影响等问题。考虑这些的出发点主要是成本，一般情况下，一个实例是用不满服务器资源的，因此我们希望通过混合部署来最大化利用服务器的各种资源。但混合部署也需要考虑很多因素，混合部署不周反而会导致服务不易管理，致使服务出现异常。在服务混合部署上需要工程师了解4部分信息。 （1）不同类型的服务器配置及处理能力。比如磁盘IO，需要知道在顺序读/写与随机读/写的场景下，其性能表现。（2）服务的业务特性，主要消耗哪些系统资源等。（3）业务访问量与系统承载的比例关系。（4）明确知道业务系统中的瓶颈点。 根据以上信息，我们会对服务的混合部署做出合理的规划，这对于规避潜在的风险有着重要的作用。由于互联网产品迭代速度非常快，容量及短板也会因此发生变化，因此需要在测试阶段引入压力测试，以便于了解系统容量变化情况，并适时做出调整。根据以往经验，在混合部署过程中我们遵循以下几个原则。 （1）具有相同资源消耗特性的服务不部署在一台服务器上。例如Redis和Kafka，它们消耗内存，也消耗IO；而Nginx和Kafka则可以考虑部署到一起。（2）敏感业务即便资源消耗再少，也要单独部署。比如密钥生成与管理服务、授权认证服务等。（3）在遵循上面原则的情况下，如果服务器容量允许，且属于同一团队管理，则可以尽量将上、下游服务放到同一台服务器上，以减少网络通信的开销。 以上几点主要还是靠人来进行综合评判的，并且需要一定的运维经验。我们正在努力将其转变为系统自动评估，主要依靠虚拟化技术及调度系统来减少人工维护成本。 虚拟化技术主要对资源进行有效隔离，对硬件资源进行划分，并打破路径、端口、账号等资源的限制，工程师不用再根据物理服务器的归属来进行部署规划了。而引入调度系统后，也不再由工程师根据服务器资源使用情况进行部署规划，只要说明每个实例所需要的资源量即可。在本书的部署章节中会进行详细介绍。 5．容错机制 容错机制对集群系统来说是非常重要的保障功能，一方面，服务器经常会出现各种故障，如果集群规模大的话，那么宕机可能是常态的事情；另一方面，互联网的服务每天都会有很多版本更新，这个过程需要重启服务的运行实例。如果集群系统不能对这些异常进行容错，服务可用性将非常低，严重影响用户体验。而且维护成本也将非常大，工程师需要随时准备操作，屏蔽故障服务器与实例。因此容错是集群系统所必需的能力，属于可运维性的最低要求。 线上服务部署，通常是网状交叉互联，需要每一层服务都有容错能力。只有这样才能保证任意层的任意服务实例故障，均可被容错忽略。尽量避免单点服务，由于单点服务往往有状态，因此为其设计高可用方案使其自动恢复的难度也很大。 常见的容错方式有两种。（1）通过定时的心跳检查，判别服务器或实例端口是否存活，如果处于非存活状态，则会将这台服务器或这个实例端口屏蔽，使流量不再分配到这台服务器上。这种方式的优点是可以将故障实例彻底屏蔽，后续请求不会产生过多的开销；缺点是两次心跳检查之间还会有请求落到异常实例上，造成少量损失。 （2）将请求分发到下游服务器，如果在规定的时间范围内没有得到下游服务器的返回，则认为下游的这个服务实例有问题，会将本次请求转发给其他服务实例。这种方式的优点是不会造成流量损失；缺点则是由于多次重试，导致负载增加，上游请求易出现堆积，不进行重试次数控制，还可能造成持续压力下的服务雪崩。重试次数上的控制，一般建议2～3次。 结合上述两种方式的特点，还可以设定一些高级的容错策略，如连续3次到这个实例的请求都失败，则暂时屏蔽5分钟；如果5分钟后依然有问题，则再屏蔽5分钟。 6．减少依赖 这里的依赖主要指两种，一种是对运行环境的依赖，如Java、Python、PHP等，由于不同服务可能依赖的基础环境版本不同，如果服务A需要Java 1.5，而服务B需要Java 1.6，那么它们在混合部署时，路径文件及系统变量都会产生冲突，而物理分离又浪费资源。因此，在概念上最好每个服务都能够做到自包含自己运行所需要的依赖，但冲突肯定很难避免。目前的虚拟化以及资源隔离技术可以很好地解决这类依赖、路径、端口等方面的问题。 另一种是对基础服务的依赖，如DNS、ZooKeeper等。在日常运维中，为了便于记忆，我们会给每台服务器分配一个服务器名，通过名字就能大致知道其在哪个区域、产品线归属、用途等信息，记忆、管理都比记IP地址要方便很多，包括程序的配置文件中也都配置了服务器名。但同时这也是一把双刃剑，在提高了可管理性的同时，也引入了新的耦合。比如当DNS服务出现问题时，由于服务器名解析失败，很可能导致程序运行或通信的异常。这就需要增加本地缓存机制，以防止类似问题的发生。 7．缓存机制 很多网站都会用到缓存技术，该技术主要是为了提高数据的读取速度。常见的开源缓存系统包括MemCache、Redis。当然，如果私有服务支持缓存的话，也有很多的缓存框架可以选择，甚至还可以做成多级缓存，这就需要看业务的具体需求了。 缓存除了可以提升业务系统本身的性能外，也可以提高对下游有状态服务的故障容忍度，乃至提高整个业务服务的可用性。例如一个有读缓存，且缓存命中率高达90%的业务服务，当下游状态服务出现故障时，仍有90%的请求可以正常服务；一个有写缓存队列的服务，在写缓存满之前，写服务的故障也是可以使用户无感知的。 8．服务保护机制 在服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心功能的正常运行。常见的服务保护机制主要有两种。 （1）限流。在网络或应用层面对用户请求进行访问限制，通过这种方式保证一部分用户可以正常访问，而另外一部分请求直接在网络层丢弃，或者在Nginx层被指向到特定的“服务器繁忙”页面。 （2）功能关闭。将某个功能的入口临时去掉，或者在程序中将其关闭，从而使该功能不可访问，以保证主功能的正常。比如关闭写数据库的功能，用户只能查看信息。 限流或功能关闭的开关及策略应该做成可配置的，以便根据情况随时调整，一般这些项都体现在程序的配置文件中。为了能够快速、简便地操作，我们对配置进行了一些规范化要求。首先，我们将配置中的内容分为三类，它们的放置位置有所不同。 （1）程序自身策略描述。比如读取本地字典、加载数据、设定编码等。这类配置在运维过程中很少关注，跟着服务部署即可。 （2）连接类描述。比如配置了连接下游哪些服务器、端口等信息。这类配置和运维直接相关，应该能够很方便地看到，而在服务器和服务数很多的情况下，人工管理又很麻烦，因此在增、删、改上最好能够自动管理。建议注册到ZooKeeper中，当然，如果你有私有的Naming Service更好。如果是开源软件，比如MySQL、MemCache，则可以写个外包程序，负责注册和摘除。 （3）限流、功能开关类描述。比如某个功能的开关项、限制连接数配置等。这类配置一般用于异常情况，操作都是紧急的，所以修改需要方便，生效需要快。建议这种配置最好能够存储在ZooKeeper中，工程师可以快速修改内容，并使其生效。对于Nginx这类开源工具，我们还是使用其本地配置进行控制，如连接数、请求数等。 服务保护的方式分为自动和手动两种。（1）自动。程序创建一个特定线程，对所关心的资源进行定期的健康检查，如连接数、内存使用情况等。当某个指标或多个指标达到一定阈值时，则自动启动服务保护策略，开始限流或关闭某些功能。比如我们的即时通讯系统，接入层程序会判断其当前的用户连接数，如果达到上限，则新的连接请求将被拒绝，直到有资源被释放。还有一种是通过客户端程序进行自动服务保护，当客户端收到服务端的错误返回码（如HTTP协议中的500、503等错误）后，会缓存用户的操作，过一段时间再重试。这种通过客户端服务保护的方式主要面向实时性要求不高的功能场景，比如数据同步到云服务上。 （2）手动。主要是人工判断服务有风险后，进行相关配置修改，或者通过向程序发信号的方式来进行功能开关控制或限流。比如，Nginx这种开源软件可以通过修改limit_conn等进行限流或限制连接数。 9．故障预案 虽然我们的程序有负载均衡、自动容错，自动服务保护等机制，但还不一定能够应付所有情况。比如数据中心级故障时的切换，就需要人工介入。有时还会遇到程序假死、服务器假死、内部网络丢包等情况，这些情况会让服务运行时表现得时好时坏，上游服务的容错等机制无法有效识别，最终导致整个服务异常。我们在运维过程中常说的一句话是：“不怕真死，就怕半死不活”。因为在这种状态下产生的不确定情况最多。 比如内网抖动，对于配置了自动切换的单点业务来说，可能会出现脑裂让系统产生双主，从而导致数据写入发生混乱。因此，在系统设计阶段，我们要识别自动机制不能覆盖的异常场景，针对这些场景应该有对应的规避方案（工具、操作方案等）。 明确预案的原则，当然，这是理想状态，但我们希望尽量向这个目标靠近。 （1）方案执行简单。服务异常时，肯定是越快恢复越好。因此，操作简单才是王道，最好是有工具帮助进行步骤简化。比如主从切换及相关检查动作。当然，对业务也提出了很多要求。例如程序要主动输出重要信息，以满足自动化处理的要求等。 （2）一套方案应对各类灾难问题。这里指的是非服务程序bug导致的问题（一般都是基础设施、基础服务故障）。出现故障时，对于运维来说，不是马上找到问题，而是马上规避问题。因此，我们肯定希望尽量少地判断，能够以一种方式以不变应万变最好。比较常见的就是域名切换，不管数据中心内出现了什么问题，影响范围到底有多大，判断只要不是业务自身问题，就将流量引入非故障区域，然后再慢慢定位。 （3）所有业务执行方式统一。公司的业务非常多，域名也很多，如果在出现故障时，几个人就能快速完成所有域名的流量牵引工作，就可以把故障损失降到最低。我们的域名管理系统对域名中的每个IP地址做了区域tag标记，当某个数据中心有问题时，只需要一个人执行A tag流量切到B tag，快速完成流量切换。 10．水平扩展 在业务流量增加，当前集群不能负载的时候就需要进行资源扩充，所以在服务设计的前期，服务集群是否能平滑地水平扩展，是运维准入的必要条件。在进行服务容量伸缩调整时，主要会涉及数据、应用，这两部分决定了扩展的效率和是否会对用户有损。 （1）应用程序与数据分离数据的概念比较广泛，在这里主要指程序生成的或程序需要读取的文件（配置文件除外），举几个数据的例子，如字典、索引、用户信息记录文件等。数据对于业务的重要性这里就不多说了。 在我们的运维标准中，程序的数据和程序本身是要分开的，主要是为了服务扩容时快速和便捷。对于数据，我们建议优先使用分布式存储系统，如Hadoop、HBase、MFS等，主要是因为它们的稳定性非常高，并且有专门的研发、运维工程师进行管理，不用每个业务再去过多考虑数据的安全性、完整性等问题，在节省人力的情况下还能提高数据服务的可用性保障。 （2）数据读/写分离以MySQL数据库为例进行说明。一般数据库的配置都是一主一从，或一主多从，甚至是级联式的多从。所谓读/写分离，就是基于这种结构，业务程序在功能上支持并且可配置，增、删、改请求只发向Master服务器，而Select等查询操作发向Slave服务器，且读数据库功能支持前面所说的容错策略。数据读/写分离示意图如图13-5所示。 图13-5 数据读/写分离示意图 （3）服务状态在集群系统中，我们希望其中的节点可以随时进行伸缩调整，并且对用户透明。对于程序来说，分为有状态服务和无状态服务两种。无状态服务最常见的就是Web服务，由于Web服务不负责保存访问时序、用户状态等信息，每次请求都是独立的，所以当实例增、删时，不影响请求的正常处理。这里用一个搜索网站的架构来说明，如图13-6所示。 图13-6 一个搜索网站的架构示意图 图13-6中包括Web Service接入层、页面拼装层（A服务）、数据层（B服务）。Web Service有容错机制，当下游服务异常时，Web Service会尝试将请求发给其他同功能实例进行处理。A服务在伸缩过程中，只需要修改上游Web Service配置即可，这便是无状态服务的特性。 在后来的发展进程中，逐渐在无状态化的过程中加入状态化信息，比如Cookie。服务端在响应客户端的请求时，会向客户端推送一个Cookie，其记录服务端上的一些信息。客户端在后续的请求中，会携带这个Cookie，服务端可以根据这个Cookie判断这个请求的上下文关系。Cookie的存在，是无状态化向状态化过渡的一个手段，它借助外部扩展手段，通过Cookie来维护上下文关系。状态化的服务端有更广阔的应用范围，比如即时通讯类软件、网络游戏、支付等服务。 在服务端维护每个连接的状态信息，服务端在接收到每个连接发送的请求时，可以通过本地存储的信息来重现上下文关系。这样，客户端可以很容易使用默认的信息，服务端也可以很容易进行状态管理。比如在即时通讯类软件中，当一个用户登录后，其在线好友能够马上收到该用户上线的通知消息。 无状态服务在处理简单的服务方面有优势，但在复杂的功能方面有很多弊端。状态化服务端在功能实现方面具有更加强大的优势，但由于它需要维护大量的信息和状态，在性能和可维护性方面要稍逊于无状态服务。下面我们用一些应用场景来说明状态化服务端如何满足水平扩展。 试想一款即时通讯类软件，其有上亿用户。由于资源限制和可用性要求，这些用户不可能放在同一台服务器上。最开始的结构是按号段进行划分的，即固定的服务器上承载固定的号段（如A1～A3服务器承载1～1亿号段的用户，B1～B3服务器承载1亿～2亿号段的用户，依此类推），通过人工管理服务器与业务状态的对应关系。这样的架构是经典的垂直扩展，扩展需要人工修改服务器承载的号段范围，而这种修改必然会导致其他号段服务器的配置修改。伸缩过程麻烦，不灵活，易出错。 后续我们引入了一致性Hash，通过算法来自动维护号段与服务器的关系，可以自动计算集群中的服务实例个数，根据一致性Hash算法的计算结果，将请求发到对应服务器上。当某个号段的服务器都宕掉时，可以自动重新计算对应关系，进行自动迁移。在这个案例中，我们通过算法解决了状态化服务的扩展问题，如图13-7所示。 图13-7 通过算法解决了状态化服务的扩展问题 再来看一款社区产品。有很多用户上传数据需要存储，并保持时序，比如用户评论信息的展示，在请求落到不同服务器时，展示顺序需要相同。为了满足这样的要求，最开始的系统设计示意图如图13-8所示。 图13-8 最初的系统设计示意图 每个业务服务均具有ID分配的能力，并各自保持请求的时序化。比如一个评论服务，用户提交评论时，直接由评论服务从自己的ID段分配唯一并具备时序的ID，带着顺序ID的评论，在写入多个存储时，就可以保持一致的顺序，不会发生在不同存储中评论顺序不同的问题。但这样的设计，每个服务都维护自己的ID分配状态，也就使每个服务都无法水平扩展，新增实例时都要面对复杂的配置修改和同步ID过程。为了满足运维和服务的高可靠性要求，我们将系统进行了调整，调整后的系统设计示意图如图13-9所示。 图13-9 调整后的系统设计示意图 在这样的设计中，把ID分配抽取成独立服务，由其保持请求时序化，并将ID及其对应数据分发至下游所有业务服务。这样每个业务服务不再维护状态，也就具备了水平扩展能力；而ID分配服务，通过主备等模式做到一定程度的高可用性。 其实解决水平扩展问题就是去状态化，无论是数据分离还是读/写分离，都是通过将状态逻辑进行剥离，使剩余逻辑无状态化，从而具备水平扩展的能力。剥离出来的有状态逻辑可以作为独立运行的系统服务，再通过主备等模式实现其高可用性。通过这种方式实现服务最大化的无状态，最大程度地满足业务可靠性要求。 运维自动化运维自动化和高可用性是息息相关的，高可用运维会规范化架构设计和业务环境，持续性地提出运维自动化需求，以提高网站可靠性保障，进而降低运维人工成本；运维自动化系统除了提升服务管理的高效和便捷外，反过来还会促进高可用运维的规范化。 我们的自动化运维系统主要包括部署、监控、服务发现、服务管理、服务器生命周期管理、域名管理、资源调度几大基础设施组件。如图13-10所示是我们的总体规划示意图。每个部分都会有对应的章节进行详细介绍。 图13-10 总体规划示意图 在自动化建设过程中，上面提及的这些基础设施组件也不是一次建设完成的，有其规划的先后顺序。首先，我们需要将服务器管理起来，建立服务器及上面所部署服务的关系。然后，需要解决服务的监控问题，如果服务异常都不能及时发现和处理的话，那么更谈不上后续的运维自动了。接着，我们需要重点解决最占日常工作量的事情，即服务变更和部署。解决部署问题能够提升所有技术工程师的工作效率，也方便开展后续的自动化工作，比如服务扩容、服务搬迁等。 最后，就需要着手解决服务间关联关系的管理问题，建设调度系统，实现业务的自动伸缩。自动化系统的实现也需要业务服务具备可容错的能力，同时能够配合自动化运维系统，进行关键运维指标的上报。 运维自动化是整个网站可靠性运维工作中的重中之重，运维自动化建设的程度，在很大程度上决定了高可用运维的工作模式，甚至会改变整个业务体系架构。接下来会详细介绍各个运维组件的功能，以及我们的设计思路和实现方式。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Reliability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十二章：域名和接入]]></title>
    <url>%2Funder-the-ops%2F20170923-12-domain-and-access%2F</url>
    <content type="text"><![CDATA[域名是所有互联网公司服务的入口，也是重要的公司品牌。域名的使用和设计是否合理，不仅影响用户能否快速访问到业务，而且也直接影响内部服务的部署架构、管理复杂度、自动化运维实现等方面。众所周知，域名的解析结果是一个或多个IP地址，利用域名解析的结果，就可以实现：◎ 基本的负载均衡；◎ 引导流量接入到期望的目的地址。 若只使用域名解析来接入服务，则存在以下问题：◎ 受Cache等因素影响，利用域名解析多IP地址进行负载均衡效果有折扣；◎ 域名指向的IP地址必须要直接接入互联网（公网）。 Cache还不足以造成严重的影响，但要求所有业务前端必须接入公网，在小规模情况下问题还不突显，但当业务发展到一定规模时，所引起的副作用就足以让运维人员疲于应付。主要问题有：◎ 服务器接入公网对网络规划的要求；◎ 服务器接入公网后的安全性考虑；◎ 服务器接入公网后的防攻击考虑。 因而，基于VS/NAT的四层负载均衡器和基于URL分流之类的七层负载均衡器应运而生，其核心思想就是只利用少量的、直接暴露在公网的负载均衡层接入用户请求，再通过算法和策略，平均地转发到内网的业务前端进行处理。同样地，也将内网前端的处理结果，原路返回发送给对应用户。负载均衡层在用户和业务之间起到代理和转发的作用。通过图12-1可以看出有/无负载均衡层在结构上的区别。 图12-1 有/无负载均衡层在结构上的区别 负载均衡技术的引入有效地控制了对外暴露服务器的规模，使得安全策略和防攻击都能够被集中和统一地解决。并且与前端服务器规模不成正比关系，无论业务规模如何发展，对外暴露服务器的规模都是有限的。同时，负载均衡技术使前端服务的扩展更加平滑、透明和可控（不受用户DNS Cache影响）。 下面分别介绍我们在域名和负载均衡实践应用中的运维设计和自动化管理方面的经验与考量。 负载均衡及自动化负载均衡器在大规模业务集中发展为必不可少的设施，在业务乃至整个基础环境中都起着至关重要的作用。包括：1．简化网络结构和网络维护成本以最原始的互联网服务为例，对互联网用户提供服务就必须将服务器接入到公网中，如图12-2所示。在小规模业务中，为满足这样的需求进行网络规划和实施并非难事，也不存在太大的成本，甚至内网和外网都可以共享同一套物理设备，所以也是比较常见的形态。 图12-2 增加负载均衡器前的网络结构 但随着业务的发展、规模的扩大，会逐渐地发现：◎ 除了前端服务器（业务的入口），大部分服务器并没有访问外网或被外网访问的需求，因此配置外网实际上是一种浪费，甚至还额外引入了安全风险；◎ 如果内、外网共享物理设备，则有相互干扰的风险。例如，外网被攻击导致设备工作异常，内网也会同时受到影响。但如果拆分内、外网，则又会出现两套网络，浪费物理资源。增加负载均衡器后，网络结构变得更清晰和简洁，如图12-3所示。 图12-3 增加负载均衡器后的网络结构 在增加了负载均衡器的结构中内、外网是分离的，外网流量通过负载均衡器转发进入内网，即使外网发生故障异常也不会不影响内部服务的运转。负载均衡器的规模与流量和转发性能成正比，与内网服务的规模无关。而通常业务功能的扩展和集群规模的扩大都主要发生在内网。 2．减轻在业务升级维护过程中对用户的直接影响 从图12-2中可以看到，用户请求直接与服务器交互，服务器正常的维护、升级、故障等因素都会导致短暂的服务中断，虽然用户端可以设置重试策略，在请求失败后重试到其余服务器，但是对于新用户请求来说仍然会有一部分会访问到异常服务器上，即服务中断期间，有n/m的请求会出现失败和重试（n为服务中断服务器数量，m为提供该服务的服务器总量）。 而从图12-3中可以看到，用户请求与负载均衡器进行交互，再由它进行转发，因此如何进行转发是可控和可配置的。当内网中某服务器异常或维护时，在负载均衡层屏蔽该服务器即可使用外网流量，不再转发到该服务器，从而消除了用户请求存在n/m失败重试的可能性。 3．更集中的安全策略控制和防攻击 增加负载均衡器后，与互联网（公网）互联的服务器由分散的多个入口收敛为集中的一个入口，从而使安全防护策略也跟随集中和统一，更方便地进行维护和变更，消除和避免安全防护策略的缺失、遗漏而造成的隐患和损失。在防攻击方面，图13-2所示的结构是以寡敌众，容易被逐个击破；而图13-3所示的结构通过负载均衡器将内网的服务集群虚拟为外部可见的一台服务器，是以众敌众，强者胜之。 4．支撑业务前端更灵活和实时的容量扩展 如前面所述，负载均衡器将内网的服务集群进行了虚拟，在外部用户看来就是一台服务器，内、外网之间的流量由其决定如何转发，因此内网服务器的扩展和调整是可控制的，变更的生效是更为实时的，集群的规模几乎是不受限制的（例如，使用域名-&gt;IP地址会受到DNS相同域名的最大条目数限制集群规模），并且变更的过程对于用户透明、无感知。 负载均衡的解决方案有多种，如基于四层或七层，基于硬件或软件；在大规模业务场景中历练过的不乏有F5、LVS、Nginx、HAProxy等。作为一个重要的基础设施，负载均衡方案的性能、可扩展性、均衡策略、对业务是否透明等指标直接关系到业务的稳定性、处理能力和接入维护成本。 下面介绍的是以经典代表LVS+Keepalived为基础的负载均衡器接入设计和自动化管理方案。 选择LVS+Keepalived的考虑当前LVS在业内已得到广泛的应用和认可，属于四层负载均衡实现，最初具有NAT、DR、TUNNEL三种工作模式，在随后业内的工程实践中又增加了FULLNAT工作模式，并由taobao开源，使之更适用于大规模业务场景。四层负载均衡如同底层交换设备一样具有天生的业务透明性，接驳到业务流中时对应用层无感知，无须进行额外的设置和调整。 而FULLNAT则是专为大规模工程应用而设计的，消除了原有DR/NAT模式在应用中的诸多限制，比如要求前端服务器在同一广播域或同一网段等。FULLNAT模式使网络结构的设计更为清晰，同时跨网段的转发使服务集群的扩展更加灵活、便捷。LVS可以运行在普通服务器上，有不俗的性能表现，并且安装和配置不复杂，名副其实的低成本和高收益。针对LVS的部署过程已有丰富的文档和手册详细介绍，在此不再赘述。 Keepalived则是基于底层负载均衡框架（如LVS）进一步扩展和增强了负载均衡的高可用性，使之具备构建集群的能力，提供了更适合于维护的配置格式、健康检查能力等。概括来说，LVS实现了内部的业务集群抽象和虚拟，而Keepalived则完成了LVS集群的构建，以及LVS映射关系管理。 负载均衡器接入结构负载均衡器接入结构如图12-4所示。 图12-4 负载均衡器接入结构 在实际应用中基于LVS+Keepalived的负载均衡器采用了如图13-4所描述的结构，LVS的外网侧与交换设备通过OSPF进行接驳，LVS的内网侧与内网交换设备直连。这种结构的特点和优势如下。 1．容量可扩展的LVS集群 在前面已经提到，Keepalived使LVS具备了构建集群的能力，但是Keepalived使用VRRP进行容错备份，而VRRP协议造成了两方面的局限：◎ 在一个集群中，同一时刻只有Master能够工作，若干Slave只能作为冷备。◎ VRRP只能工作在二层网络，因此所有LVS节点必须部署在同一广播域。 通过VRRP协议构建的LVS集群，起到了冗余和备份的作用，但在集群处理性能和容量上仍然是单机性能为上限，并不能通过扩展节点来扩充容量。另外，同属于一个广播域的要求也必须要在网络架构上进行设计和支持，在某种程度上也会增加网络维护的成本和复杂度，其中不能水平扩展在大规模服务的应用场景中是明显的软肋。 所以，必须要使LVS节点具备热备的能力以及水平扩展的能力。通过在LVS与交换设备间建立OSPF邻居关系便是可行的方案之一，OSPF邻居关系建立后，交换设备可以动态发现和感知LVS节点变化，并从LVS节点学习路由。若有多个LVS节点同时宣告了相同路由，交换设备则会通过ECMP（Equal Cost Multipath Routing，等价路由）机制进行分流，而这恰好就是我们所需要的热备机制。在热备的基础上，增加或减少LVS节点数便能够重新分流，达到容量伸缩的目的。 2．独立的LVS节点 不再依赖VRRP作为构建集群的基础后，LVS节点之间便消除了相互通信和选举的需求，不会再发生类似VRRP模式下出现多个Master或无Master的情况。各个LVS节点更具独立性，单节点故障或进行维护不影响其他节点的正常运行。在OSPF+LVS构建的集群化负载均衡结构中，虽然解决了大规模应用中的诸多问题，但也并非十全十美。 首先，单节点故障会造成交换设备路由的变化，造成ECMP机制重新计算。在交换设备中ECMP通常只是简单hash计算，没有一致性hash的特性，因此数据流将会完全打乱（简单hash计算保证了转发的性能和均衡性，而一致性hash除了增加复杂度外，也不能保证绝对均衡，所以当前交换机基本上都不支持一致性hash的特性）。若没有对LVS节点的Session进行同步，打乱后的数据流无法再进行转发，从而造成连接中断。例如，ECMP重新计算前业务流是经过LVS NODE1分流和转发的；而ECMP重新计算后，业务流则可能会被分配到LVS NODE2。由于LVS NODE2上并没有之前会话的任何信息，因而无法正常继续之前的转发和交互。 其次，FULLNAT模块in/out流量都会经由LVS节点穿行，对LVS节点转发性能造成额外的开销。最后，也是由于各节点的独立性，在配置有差异的情况下，各节点仍然能够正常运行，使得配置上错误或不同等小隐患不容易被及时发现。LVS的集群化增强了业务架构体系的稳定性和高可用性，但规模化的集群管理是衍生而来的运维烦恼。如何保证集群的同步一致和可靠运行，便是随之而来的课题。 自动化管理系统Keepalived在LVS的基础上，提供了更为友好的配置形式，便于人工维护。但在大规模应用中，由于配置的条目多，造成人工维护的变更成本高、误操作概率高等衍生问题，从而使得规模越大配置越难维护。而同时，LVS又是内、外网流量接驳的咽喉，一旦出现误操作，就会损失全部用户流量。因此，LVS配置的有效性、变更的准确性至关重要。抛弃人工管理实现自动化，既是顺势而为，又是必然的趋势。 系统结构如图12-5所示。 图12-5 系统结构 首先，系统的核心是Etcd，一款用于配置管理和服务发现的开源的分布式KV存储。Etcd数据是以树状结构存储的，而Keepalived配置同样也是树状的层次关系，DC-&gt;CLUSTER-&gt;VS-&gt;RS-&gt;RS HealthCheck，因此Keepalived经过层次化转换后，可以很容易地保存到Etcd中，并由Etcd自身存储结构来始终保证这样的层次性。同样在读取方面，指定一个节点（如CLUSTER），拉取以该节点为root的子树，经过简单的合并，就可以轻松地将整个节点配置转换为Keepalived的格式。这也是我们选取Etcd作为Keepalived配置存储的考量。 其次，利用Etcd已有的HTTP API接口，封装和完善数据的权限及有效性检查，从而通过UI或工具就能自动化地进行Etcd数据操作，保证配置变更的准确性和可靠性，减少人为误操作。 再次，Etcd作为整个系统的核心，同样也需要保证高可用性和高可靠性，而其自身的分布式设计便能满足高可用性需求。 最后，在Keepalived端，设计采用拉取的策略从Etcd查询得到集群配置并转换为Keepalived配置文件生效。pull策略无疑牺牲了配置生效的实时性，但在稳定性和实时性之间，整个负载均衡器的稳定性是具有更高优先级的。 为什么pull策略会更稳定？主要体现在如下几个方面。 （1）pull动作的触发并非是用户行为，因而是可控的、周期性的，即使有大量的集中的配置变更，也不会造成KPD的频繁加载。 （2）pull形式更便捷地保证了LVS集群配置的一致性，例如某节点在故障或维护后，重新加入集群时，配置可能就已经发生过变化或更新。pull方式在加入集群时强制拉取一次，便可以完成线上配置的同步。而若换作push方式，事情就会变得复杂，在节点故障期间，要记录状态不能往故障机器发布，节点恢复后，再重置状态触发配置的推送和发布。如果状态处理得不正确，就会导致集群配置的不一致，造成隐患。 （3）避免功能模块间的耦合，从图12-6中可以看出，UI、Etcd和LVS是相对独立的业务模块，Etcd通过标准接口接受更新、查询请求，各个模块的升级不影响和依赖其他。 在实现LVS的自动化管理之后，才能进一步整合其他系统，实现体系的自动化。比如通过与部署系统的接驳，实现应用部署完成后自动流量引入；通过与域名管理系统的接驳，实现域名申请后自动进行LVS虚拟IP地址申请等。 总结在互联网行业，高可用性和高性能业务架构是永远不能懈怠的话题，通过负载均衡技术进行流量的接入，不仅提高了业务的可靠性和冗余性，还使资源利用率和请求响应时间得到提升。如何利用好负载均衡这件利器，是持续的实践和探索。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Domain</tag>
        <tag>Access</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十一章：从外包到自建，小中企业CDN运维进阶指南]]></title>
    <url>%2Funder-the-ops%2F20170923-11-cdn%2F</url>
    <content type="text"><![CDATA[CDNCDN是Content Delivery Network的缩写，直译为“内容分发网络”。工作思路是把用户需要访问的内容缓存在边缘节点，就近访问，进而绕开经互联网骨干传输数据的不稳定性和速度瓶颈，起到提升最终用户体验的效果。通过在网络各处放置节点服务器所构成的基于现有互联网基础的一层智能虚拟网络，CDN系统能够实时地根据网络流量和各节点的连接、负载状况，以及到用户的距离和响应时间等综合信息，将用户的请求重新导向离用户最近的服务节点上。其目的是使用户就近获取所需的内容，解决 Internet拥挤的状况，提高用户访问网站的响应速度。CDN分为静态加速和动态加速两部分，分别介绍如下。 静态加速通过缓存静态资源到CDN节点，使用户可以就近访问CDN节点来获取资源，避免用户直接从源站获取数据，提升用户访问速度和质量。如图11-1所示，相比用户直接访问源站，访问CDN节点可以带来以下收益：CDN节点更靠近用户，可以提供更小的网络延迟及更快的访问速度；CDN节点数量远大于源站，可以支撑更大的请求数量；CDN节点多地域部署，可以提供更好的容灾能力。静态加速可以分为小文件类业务、下载类业务。两者之间的区别在于文件大小以及访问方式，以及由此带来的业务不同侧重点。 图11-1 用户访问CDN节点和源站的路径 小文件类业务：主要是通过浏览器访问的网页、图片、JS、CSS等内容尺寸比较小（无严格限制，一般为数KB至几百KB）的文件。特点是单次请求文件较小，请求总数较大，主要消耗CPU和带宽资源。一般都是直接在浏览器中展示，对访问成功率要求较高，要求RTT延时要尽量低，即节点要部署得离用户近。下载类业务：相对于静态资源类业务，下载类业务的单文件尺寸要大很多，需要更多的磁盘空间和带宽资源，对CPU的消耗不如小文件类业务大。并且下载类业务的下载时间会比较长，相对于静态资源类业务更注重于吞吐量，而不是RTT延时。 动态加速静态加速业务是通过将文件缓存至CDN节点来提高用户的访问速度的，而动态加速是通过尽量减少用户到源站之间请求的网络耗时来提高用户体验的。可用的技术有TCP单边加速、多路TCP合并回源、回源压缩。 TCP单边加速：主要原理就是在用户通过公网直接访问源站比较慢时，通过各种方式让用户请求绕过拥堵的网络环节。例如，在用户访问质量良好的区域建立CDN节点，用户访问先经过CDN节点，通过CDN节点中转至源站；或者在某些关键CDN节点使用专线和源站联通，以确保CDN节点到源站的回源质量（见图11-2）。 图11-2 TCP单边加速示意图 多路TCP合并回源：在CDN节点将多个用户的数据合并在一个TCP连接上回源至源站，以减少回源时新建TCP连接带来的额外时间消耗（见图11-3）。 图11-3 多路TCP合并回源示意图 回源压缩：将回源数据流进行压缩，减少在网络上传输的回源数据量。 自建CDN初期公司更多的是使用第三方公司提供的CDN服务，随着业务量和业务需求的增加，第三方CDN在质量和运维效率上逐渐不能满足业务需求。为了提升CDN的质量，提高运维效率，自建CDN是一个不错的选择。在某案例中，使用自建CDN后，不仅业务质量有了明显提升，成本也有了大幅度降低，并且从侧面帮助公司在使用第三方CDN服务时提高了议价能力。 该案例中，自建CDN支持静态加速和动态加速业务，带宽容量达数百Gbps，日常支撑了全公司90%以上的CDN流量，多次支撑了业务的大型发布推广活动（活动带宽相比日常带宽增长数倍）。自建CDN在设计之初就针对公司业务的特性，以及从节点选择、软硬件配置到参数优化都进行了有针对性的调整。 在该案例中，针对公司业务的特点，我们首先开始了大文件静态加速的工作，随后逐步进行小文件静态加速和动态加速的工作。同时，CDN管理和调度系统也随着CDN的发展不断完善。CDN系统整体框图如图11-4所示。 图11-4 CDN系统整体框图 ◎ CDN业务系统：对用户直接提供CDN加速的缓存服务器和代理服务器。◎ 私有调度系统：公司的HTTP调度服务器和302调度服务器。◎ GSLB系统：支持智能解析的DNS服务器。◎ 流量收集系统：采集节点交换机、服务器和各业务的流量。◎ 质量收集系统：对质量评测数据的采集、存储。◎ 日志统计系统：统计CDN访问日志、调度系统日志。◎ 质量分析系统：通过质量评测数据和日志统计数据，汇总统计出质量排序结果。◎ CMDB系统：管理CDN节点的资产信息及状态跟踪。◎ 调度决策系统：根据质量排序结果、流量数据以及CMDB系统的服务器状态，计算出一个最优化的调度方案，并生成相关调度系统的配置文件。◎ 监控系统：对节点各类设备的基础监控及业务可用性监控。◎ 告警系统：CDN告警系统会做两件事情，一是在告警发生时通知运维人员；二是会同时向调度决策系统发送信令，可以实现某些特定故障的自动化修改调度处理。◎ 自动化接口：提供给CDN业务使用者的一些刷新、预加载等接口。◎ Portal展示系统：展示CDN业务运行的各项数据。◎ 配置变更系统：CDN运维人员进行各种日常运维操作的系统。 CDN硬件及节点选型CDN的目的是在尽量低的成本下为用户提供尽可能高的服务质量，所以在硬件选型时成本是一个很重要的考虑因素。针对静态资源类业务，在硬件选型上主要评估在节点出口带宽满载时，所使用的硬件总成本。例如，相同配置的1台万兆服务器和10台千兆服务器都可以跑满10Gbps带宽，这时肯定会选择万兆服务器。当然，在硬件选型时还要考虑实际业务对CPU、内存、磁盘的使用情况。 CDN服务器选型策略静态资源类业务使用的机型规格为E5-2620 2 + 128GB RAM + 2TB SAS 4 + 480GB SSD *6。关键指标：万兆网卡+大内存+SSD存储。使用万兆网卡可以有效避免单机网络吞吐达到瓶颈；大内存可以将更多的数据缓存在内存中，降低磁盘IO的使用；SSD可以提供更好的随机IO读/写性能。总体目标就是避免出现木桶效应。 动态加速类业务使用的机型规格为E5-2620 2 + 64GB RAM + 600GB SAS 2。因为此类业务对带宽使用量较少，主要是大量TCP连接数对CPU的消耗，所以在硬件选型上主要侧重于CPU和内存，对于磁盘和网卡没有要求。 在做硬件选型时，发现相对于Broadcom网卡，Intel网卡无论是在CPU使用率还是小包性能上都要优秀很多，所以无论是千兆还是万兆服务器，都推荐使用Intel网卡。 CDN节点建设策略伴随着业务发展，会需要陆续新建CDN节点，以保证CDN的整体带宽使用率控制在一个合理的水平。显而易见，在带宽缺口最大的区域新建CDN节点是最优的节点建设策略。通过日志统计系统可以得出各个省份运营商的带宽使用量及使用比例（将日志中的客户端IP地址通过IP地址库映射为具体的省份运营商，再将每条请求响应的大小以5分钟为粒度做聚合，除以300秒后即得到各个省份运营商的带宽使用量），再和当前已有节点的分布做对比，即可得到各省份运营商带宽需求及当前带宽供应、当前带宽缺口一览表（见表11-1，非真实数据，仅做示例）。 表11-1 各省份运营商带宽需求及当前带宽供应、当前带宽缺口一览表 确定了在哪些区域新建CDN节点后，后续的工作就是实际的选点了。在选点过程中，主要考核候选节点的成本和质量，节点的成本可以用单位带宽成本来衡量，这个不是本文重点，暂且按下不表。 这里主要介绍如何测量评估候选节点质量。为了使评测结果更接近用户真实体验及更具有横向可比性，我们使用JS测速来评测一个候选节点的质量。简单来讲，就是让部分真实用户去下载被评估的候选节点的一个固定大小的文件，然后记录每次用户下载的成功率及耗时，统计全部用户的平均下载成功率及耗时作为衡量节点质量的依据。从实践上看，此种方法效果良好。 在节点建设方面，我们采用的是模块化方案，一个节点有多少交换机、多少服务器，交换机以及服务器的型号配置如何，网线怎么连、电源线怎么插，全部都是固定统一的。基本可以说，各个节点的区别就只有IP地址是不一样的。并且交换机和服务器都是预先配置好后才发往外地节点上架的。采用这样的模块化方案，不仅可以显著降低使用过程中的运维成本，而且在节点建设中的便利性也是很显著的——无须我们的工程师去现场施工，数据中心现场代维工程师只要参照我们提供的一份标准化上架文档，就可以快速地完成上架施工。并且配置和流程都是高度标准化的，可以最大程度地避免（以及快速发现）上架误操作或漏操作。 CDN缓存系统常见缓存服务器的简要对比◎ Squid：老牌的缓存服务器，但是性能较差，无法满足自建CDN对于高性能的需求。◎ Nginx：性能好，插件丰富，支持广泛，但缓存功能偏弱，在某案例中，动态加速服务就是在Nginx的基础上修改而来的。◎ Varnish：配置方便，性能好，但不支持持久化缓存是其一大缺陷。◎ Apache Traffic Server（ATS）：性能好，对于缓存的支持很完善，但是插件不够丰富，很多业务需求需要自行开发插件来完成。经过综合对比，自建CDN的静态加速服务使用了ATS。 ATS功能介绍◎ 具有完整的正向代理和反向代理功能，并且支持集群工作模式。◎ 使用parent配置时可以自动屏蔽故障源站。◎ 具有内存缓存和磁盘缓存自动淘汰功能，支持直接写裸磁盘。◎ 拥有丰富的日志格式配置。◎ 支持插件开发，可以定制业务专有需求。 ATS配置要点说明1．回源配置回源配置示意图如图11-5所示。 图11-5 回源配置示意图 使用parent配置有以下好处：实现多个源站负载均衡；故障源站自动屏蔽；源站DNS解析和回源Host解耦合。remap和parent配置的区别如下：remap是将用户访问的域名映射成另一个域名（可以是其本身），映射后的域名将用作回源请求和用来生成缓存的key。即，如果两个不同的域名同时remap到同一个域名，则两个域名下的相同的URI指向同一份缓存。而parent则负责将回源请求发送到特定的parent域名（或IP地址），而不是去DNS查询remap后的域名。 record.config文件配置如下：CONFIG proxy.config.http.no_dns_just_forward_to_parent INT 1 #启用parent.config配置CONFIG proxy.config.url_remap.remap_required INT 1 #启用remap.config配置CONFIG proxy.config.url_remap.pristine_host_hdr INT 0 #这个配置项的意思是在remap后是否保留HTTP请求头中的Host项，务必配置成0（不保留） remap.config文件配置如下： 上面的三个remap配置都是很典型的配置，按字面意思理解即可。 2．缓存配置（1）内存缓存配置（record.config文件）CONFIG proxy.config.cache.ram_cache.size INT 1G #配置内存缓存大小CONFIG proxy.config.cache.ram_cache_cutoff INT 50M #配置内存缓存的最大单文件尺寸CONFIG proxy.config.http.record_tcp_mem_hit INT 1 #记录内存缓存命中CONFIG proxy.config.cache.ram_cache.algorithm INT 1 #内存淘汰方式，默认为1（LRU）即可 （2）磁盘缓存配置（storage.config文件）ATS支持两种磁盘缓存方式，一种是以文件的形式缓存在文件系统上，配置项是：/var/cache 500G（路径+存储大小）；另一种是直接写裸盘，配置项是：/dev/sdb（磁盘设备的路径），这时不需要指定存储大小。需要注意的是，ATS使用裸盘时需要更改磁盘设备的权限。在CentOS环境中可以在/etc/udev/rules.d/下新建一个99-trafficserver.rules文件，并添加如下内容：KERNEL==＂sd[c-z]*＂, MODE=＂0660＂,OWNER=＂root＂, GROUP=＂root＂ （3）interim storage缓存配置（record.config文件）为了解决SATA磁盘随机读/写差的问题，ATS支持内存——SSD（interim storage）——SATA磁盘三级缓存。将最常访问的内容放到SSD中，降低对SATA磁盘的随机读数量。相关配置项如下：LOCAL proxy.config.cache.interim.storage STRING /dev/sdb注意：interim storage机制默认是不开启的，请在编译时使用“–enable-interim-cache”参数开启。更详细的资料请参见官方文档：http://trafficserver.readthedocs.org/en/latest/。 CDN调度系统DNS调度DNS调度示意图如图11-6所示。 图11-6 DNS调度示意图 DNS调度是通过对不同的Local DNS服务器返回不同的解析结果来实现智能解析和就近调度的。比如，给黑龙江联通的Local DNS返回黑龙江联通CDN节点的IP地址，给广东电信Local DNS返回广东电信CDN节点的IP地址。 CDN的DNS服务主要实现两个需求：智能解析以及edns-client-subnet支持。也有很多开源的DNS软件支持IP解析调度和edns-client-subnet，如PowerDNS。在某案例中，CDN主要使用DNSPod服务，小部分使用自建DNS服务（后续会逐步迁移到自建DNS服务）。自己研发DNS服务，主要考虑到一些特殊的业务场景和监控需求。 1．一个自建DNS服务实现简述——SmartDNSSmartDNS（智能DNS），根据配置，能够支持针对不同的DNS请求返回不同的解析结果。SmartDNS获取DNS请求的源IP地址或者客户端IP地址（支持EDNS协议的请求可以获取客户端IP地址），根据本地的静态IP地址库获取请求IP地址的特性，包括所在的国家、省份、城市、ISP等，然后根据调度配置返回解析结果。支持A、SOA、NS记录的查询，支持DNS forward功能。 一个开源的Python版本SmartDNS，供大家参考：https://github.com/xiaomi- sa/smartdns。接下来介绍它的部分细节实现。SmartDNS响应DNS请求的处理流程如图11-7所示。IPPool类的初始化和该类中的FindIP函数进行解析处理是SmartDNS中最关键的两个要素，这两个要素在下面详细介绍。IPPool类的初始化示意图如图11-8所示。 图11-7 SmartDNS响应DNS请求的处理流程 图11-8 IPPool类的初始化示意图 ip.csv为IP地址库文件，格式如下： 1200000001，200000010，中国，陕西，西安，电信 其中各个字段的含义分别为IP段起始地址，IP段截止地址，IP段所属国家，IP段所属省份，IP段所属城市，IP段所属ISP。 a.yaml配置文件格式如下： 1234567test.test.com:ttl: 3600default: 5.5.5.5 2.2.2.2中国,广东,,联通: 1.1.1.1 3.3.3.1中国,广东,,电信: 1.1.1.2 3.3.3.2配置中地域信息的key包括4个字段，分别带有不同的权重——国家：8；省份：4；城市：2；运营商：1。在初始化阶段，会生成一个名为iphash的字典，结构如图11-9所示。 图11-9 名为iphash的字典结构 其中，iphash的key为ip.csv每一条记录的起始IP地址，value为一个list，list长度为6，list的前5个字段分别为以该key为起始IP记录的IP段截止、IP段所属国家、IP段所属省份、IP段所属城市、IP段所属ISP，第6个字段是一个hash，key为a.yaml里面配置的域名，value为长度为2的list。iphash[IP段起始地址][6][域名1][0]为域名1在该IP段的最优解析，iphash[IP段起始地址][6][域名1][1]为该最优解析的总权值，该总权值暂时只做参考。 在iphash初始化过程中最关键的是iphash[IP段起始地址][6][域名1]的最优解析的计算，最简单、直接的方式是直接遍历域名1的所有调度配置，挑选出满足条件且总权值最高的解析，即为最优解析。这种方式记录整个iphash的时间复杂度为O（xyz），x为ip.csv记录数，y为域名总数量，z为各个域名的调度配置数。为了优化启动速度，优化了寻找最优解析的方法：事先将每个域名调度配置生成一棵树，这棵树是用字典模拟出来的，这样需要最优解析的时候就不需要遍历所有的调度配置了，而是最多检索15次即可找到最优，即时间复杂度为O（15xy）。具体实现请参考IPPool的LoadRecord和JoinIP两个方法。 有了初始化后的iphash数据结构之后，每次请求处理的时候，只需要定位请求IP地址处于哪个IP段，找到IP段起始IP地址，然后从iphash中取出最优解析即可。具体流程如图11-10所示。 图11-10 计算最优解析的具体流程图 2．edns-client-subnet支持 CDN使用DNS获取查询IP地址，根据IP地址对用户进行地域调度。但这里获取的IP地址是DNS地址，而不是用户真实的IP地址。在大多数情况下，我们假设用户通常会使用离自己网络最近的Local DNS，CDN调度基本还是准确的。但也有很多Local DNS设置错误的情况，或者用户使用Google Public DNS（8.8.8.8/8.8.4.4）或openDNS。 比如国内用户设置了Local DNS为8.8.8.8，我们得到的DNS Query IP地址是74.125.16.208，判断IP地址属于美国加利福尼亚州山景市谷歌公司，这个时候，我们的DNS会返回离美国加州最近的CDN节点IP地址给用户，于是国内用户错误地调度到美国CDN节点上。 为了解决上述问题，Google提交了一份DNS扩展协议，允许Local DNS传递用户的IP地址给authoritative DNS Server。CDN的DNS支持该协议后，就可以获取用户真实的IP地址，进行准确的调度。edns-client-subnet流程示意图如图11-11所示。 图11-11 edns-client-subnet流程示意图 DNS Query会包含header和RR两部分，这里只介绍我们关注的地方，在网上可以搜到很多关于DNS协议的介绍，在这里就不做过多描述了。header会描述本次请求中Questions、Answer RRs、Authority RRs和Additional RRs的数量，RR部分会详细描述每个资源的内容，所有的RR格式是相同的，如下所示： edns-client-subnet是对EDNS协议的扩展，附加在一个DNS请求的Additional RRs区域，这里重点描述edns-client-subnet的结构。EDNS协议Extension mechanisms for DNS（EDNS0），请参考http://tools.ietf.org/html/draft-ietf-dnsind-edns0-01。EDNS0每个字段的结构和描述如下： 1234567891011121314151617Field Name Field Type Description------------------------------------------------------NAME domain name empty (root domain)TYPE u_int16_t OPTCLASS u_int16_t sender&apos;s UDP payload sizeTTL u_int32_t extended RCODE and flagsRDLEN u_int16_t describes RDATARDATA octet stream &#123;attribute,value&#125; pairsOPT的值为41，详细的协议值如下：(A, NS, MD, MF, CNAME, SOA, MB, MG, MR, NULL, WKS, PTR, HINFO, MINFO, MX, TXT,RP, AFSDB) = range(1, 19)AAAA = 28SRV = 33NAPTR = 35A6 = 38DNAME = 39SPF = 99OPT = 41 RDLENGTH描述RDATAD的长度，edns-client-subnet的详细格式存在RDATA中，如下所示： OPTION-CODE：两个字节。OPTION-LENGTH：两个字节，描述它之后的内容长度（byte）。FAMILY：两个字节，1表示IPv4，2表示IPv6。ADDRESS：实际存放IP地址的地方，IPv4长度为4，Google发送过来的长度一般为3，隐藏了IP地址最后一位。理解了以上协议，就可以动手实现了。判断DNS Query是否包含Additional RRs，如果包含，则按照EDNS协议描述获取地址。用获取到的地址进行来源IP地址判断调度，封装结果返回。BIND 9.10版本后自带的dig工具可以支持生成带EDNS扩展的请求来验证EDNS功能是否生效。命令示例：./dig test.com @127.0.0.1 +subnet=1.1.1.1使用WireShark抓取的支持edns-clinet-subnet的DNS请求截图如下。发包： 回包： 私有调度1．HTTP调度 用户通过访问一个HTTP接口，HTTP接口会判断用户的来源IP地址及其他诸如业务类型等信息，然后返回给用户一个JSON字符串，JSON字符串中包含相关的调度结果。客户端通过解析JSON字符串可以得到所需要访问的服务器IP地址列表。 HTTP调度相比DNS调度有以下优势：可以识别用户来源IP地址，调度结果更精确；可以在URL中包含更多的业务信息，功能更强大；可以支持一次返回多个查询的结果。HTTP调度的不便之处就是要依赖客户端支持。 2．302调度 当Web Server收到一个用户请求时，通过识别用户来源IP地址和其URL，返回一个302响应将用户的请求重定向至一个新的URL，用户使用新的URL再去下载真实的资源。302调度可以做到针对单次请求动态调度，比DNS调度粒度更细，变更生效也更迅速，并且可以在URL上添加防盗链等信息。 302调度的缺点是增加了一次HTTP访问的时间，只适用于下载及流媒体等下载耗时较长的业务，静态小文件不适用于302调度。302调度既可以独立使用，也可以作为DNS调度的补充。自建CDN当前就是在混合部署DNS调度和302调度，将访问国外CDN节点的中国大陆用户再重定向至国内CDN节点。 全局调度逻辑为了实现更好的业务容灾，当前公司业务都是在多CDN供应商同时部署的。在需要时可以通过DNS View功能实现区域用户的导向；同一区域用户可以通过第三方和自建互备，提高了业务的容灾能力（见图11-12）。 图11-12 全局调度逻辑示意图 IP地址库维护不管使用什么样的调度方式，一个准确的IP地址库都是调度系统中重中之重的部分，并且IP地址库在诸如日志分析等方面也有着重要的用途。所以维护一份可靠的、高准确度的IP地址库的工作具有重要意义。我们的IP地址库最开始是直接使用网上的纯真IP地址库，这个地址库现在来看是一团糟，地理位置标记很不规范。后来陆续使用业内的收费IP地址库及友商的IP地址接口对此地址库进行了矫正规范，最终得出了一个格式规范、可信度比较高的地址库。 首先，我们将IP地址库的格式及地理位置命名进行了规范，纯真IP地址库大量存在某某学校、某某网吧的地理位置。规范后IP地址库格式为：起始IP地址，结束IP地址，国家，省份，城市，运营商。规范化格式带来的好处就是后期使用及合并裁剪都变得轻而易举。 其次，我们通过收集和购买基础库，采用投票方法进行整合，对IP地址库进行了矫正，得到自己的基础库。矫正的流程是，我们将整个IP地址库拆成以C段为单位，分别和其他的IP地址库进行比对，按照多数服从少数的原则对每个条目的信息做聚合，最后得出一个集各家之长的IP地址库。 在得到集各家之长的IP地址库后，我们和公司业务团队合作，拿到了几千万条IP地址和GPS地理坐标的关系数据（这些数据是通过天气App获取的，可信度很高），通过这些数据校验和进一步修正我们的IP地址库。经过上述一系列校验、矫正后，针对个别仍不能确认的IP地址，我们通过多节点traceroute，拿到各地到此IP地址的路由路径，以此来反推IP地址的归属。 海外CDN建设自建还是第三方当我们考虑海外自建CDN时，有很多现实的问题摆在眼前：服务器使用物理机还是虚拟机？服务器购买还是租赁？数据中心托管怎么解决？现场代维如何做？困难重重。反向比较的话，海外自建CDN相比海外现成的第三方CDN服务有什么优势？访问质量更好，还是成本更低？仔细评估下来，出于快速部署和扩展的考虑，主要采用了成熟的产品。海外是按照流量计费的，所以在数量小的规模下，自建优势不大。后续随着量的增长，我们会在合适的阶段评估自建。 具体到海外自建CDN节点，针对某些用户量大、我们自身掌控力也强的地区（例如中国香港地区），我们采用部署物理服务器的方式将节点直接建设到本地；针对其他用户量大的地区，则使用第三方云服务虚拟机进行节点部署。 海外源站部署在CDN海外部署的过程中，我们遇到最多的问题就是海外回源：海外CDN节点回源到国内质量差，还会由于各种原因导致回源连接被墙。针对上述问题，我们在海外CDN回源上也做了很多优化。针对一些文件总量较小的业务（官网静态页面等），我们将海外源站直接部署到了海外虚拟机上，这样就完全规避了回源的问题。 针对一些文件总量较大或者发布时效性要求比较高的业务，我们在海外源站新增了一个Proxy层，Proxy层会优先将回源请求代理到海外源站，针对暂时没有发步到海外源站的资源会代理到国内源站。针对一些不便进行海外源站部署的业务，我们在中国香港地区做了一组中间层缓存服务器，通过香港地区的缓存服务器再统一回源至国内。总而言之，尽可能地降低海外CDN回源到国内源站的次数，提升回源速度，降低被墙的风险。这是海外源站部署的主体思路。 质量评测系统质量评测的目的是评估CDN服务质量的好坏，而监控系统则是为了及时发现服务中的问题并告警、处理。质量评测关注大层面的服务质量，监控系统关注细节的服务质量，这两个系统的主要目的还是有区别的。当然，质量评测的数据发生异常波动时，也会发送告警、处理。 客户端测试从客户端角度去评测一个服务的质量情况，大体上可以分成以下几种方式。（1）类似于博睿所提供的服务，通过部署在全国各地的测试节点去定期访问我们的被测试业务，统计相应的成功率和访问耗时。此种方式由于可以拿到详细的访问数据，所以在发现问题时更容易定位问题的原因。但是劣势也很明显，需要在全国各地部署节点或者使用第三方的付费服务，并且测试样本偏少，不能有效地代表真实用户，所以此种方式在我们的质量评测中已经很少使用了。 （2）对于有客户端的业务，我们联合业务开发，对一些关键的网络交互环节进行打点，统计访问成功率及访问耗时，这样得出来的结果是业务真实的质量数据，可信度及数据的丰富程度都很高。缺点是需要有客户端支持，不是所有的业务都有此条件。 （3）对于HTTP类的业务，通过在Web页面中嵌入一个JS脚本，使用JS脚本去下载业务的一个资源，统计访问的成功率及访问耗时。相比客户端打点，此种方式具有更广泛的适用性，只要被测试的业务是HTTP类型即可（CDN绝大部分业务都符合此条件）。当前我们大量使用的是此种测试方式。 服务器抓包分析我们开发了一个工具，可以抓取用户访问的TCP流，通过服务器端全流量分析，可以对访问请求分析TCP流各个过程耗时情况（例如三次握手完成后，服务器返回了某个特定信息，累积下载了1MB的文件等）。通过大数据分析，可以得到各用户单元到不同节点的整体图表，对调度表格进行循环优化。下面就简单介绍一下这个抓包工具的实现。Tcpxm已经开源：https://github.com/xiaomi-sa/tcpxm。 Tcpxm是使用Python开发的，调用Pylibcap进行抓包的工具。它共有3个线程：一个负责抓包并分析内容；一个负责写日志；一个用来清除过期数据。Tcpxm可以很方便地抓取和分析TCP请求，打印成所需要的日志形式。 最初我们用它来抓取和分析某个App的登录时间，稍加改造就可以抓取网站访问等时间，计算用户建立TCP连接的时间、第一次发包的时间等。如下是我们之前抓取App登录时间的日志格式和每个字段说明。2012-09-13 21:25:25 tcpxm.py [line:229] [INFO] 221.179.36.189:3103-&gt;xxx. xxx.xxx.xx:2424 [usr:54298295] [login(t6-t0+rtt):2760] [t1:0] [rtt:217] [t3:137] [t4:0] [t5:118] [t6:2069] [t7:193]在此用户的登录时间= t6（发送的时间）– t0（收到SYN的时间）+ rtt（估算出的收到SYN包和发送ACK包的路径时间）。具体每个t代表的时间含义如图11-13所示。日志中的t3=T3-T2，t4=T4-T3，rrt（t2）=T2-T1，…… 图11-13 某个App登录过程时序图 CDN故障处理预案在CDN系统运行过程中，各种大大小小的故障或者突发情况是不可避免的。因此，针对各种已知故障进行归纳总结，整理成故障处理预案，并逐步沉淀成自动化的处理过程。通过这样的不断迭代，可以不断提高服务能力，降低故障发生的概率；在故障发生时可以快速有效地介入处理，减少故障影响的范围及时间。在CDN服务中，某业务突发增长数倍甚至十几倍的情况还是时有发生的。针对这种情况，我们建立了详细的应对预案。 故障场景：业务突发（未提前通知）导致CDN整体带宽利用率超过80%，且有进一步上涨的趋势。关键动作：联系业务同事，确认业务突发原因（推广活动还是受攻击，抑或是其他）；通盘评估，确定对此次突发是进行资源保障还是资源打压，并和业务同事达成一致；发出通告，通知可能受此影响的第三方业务。 CDN故障处理预案：（1）我们的所有CDN业务都是多厂商部署的，并且平时对热门的资源在各厂商都有做预加载，当某一家资源不能满足业务需求时，可以快速灵活地将部分流量调度到其他厂商。（2）我们在CDN交换机上预配置了高、中、低等不同的QoS策略，通过切换业务使用的CDN域名，让突发业务匹配高优先级的QoS策略，可以重点保证此业务的质量。如果需要打压突发业务的流量，则让突发业务匹配低优先级的QoS策略。（3）在极端情况下，会损失突发业务的部分可用性，隔离突发业务。修改调度，将突发业务的流量调度到预配置的有限个节点，将其流量和正常业务从物理上隔离（类似数据中心将受攻击的IP流量牵引到黑洞），只提供有限的服务能力。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>CDN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第十章：传输网建设]]></title>
    <url>%2Funder-the-ops%2F20170923-10-the-construction-of-transmission-network%2F</url>
    <content type="text"><![CDATA[在前面我们提到数据中心采用“双中心”+异地备份的模式，这种模式得以应用的前提首先是三点之间互联专线的稳定性和时效性。对于数据中心间带宽的需求，逐渐会大于数据中心外网带宽的需求，数据中心间传输网的容量和稳健显得尤为重要。传输网除了承担多数据中心间数据库同步、日志传输、容灾切换等需求外，还要保证海外数据中心和国内高质量的通信要求，如运维管理、代码发布、部分业务信息同步等。 传输网的建设也是一个逐步发展的过程，首先是从核心网互联需求开始的，然后经历了由核心到周边，由国内到海外的阶段。为实现核心网的互联，首先规划了两个POP点（汇聚中心），各数据中心都通过双路由与POP点联通，形成星型架构，通过建设波分系统来扩大容量，构建出高可用、高容量的传输体系。 CDN业务需求和用户本地化交互业务需求，推动把外地和海外数据中心也纳入到传输体系中。在中国大陆以外，规划中国香港地区为整个海外到国内的中继，同样规划了两个POP点，这个覆盖全球的传输网络骨架就搭建起来了。 传输网雏形数据中心初期建设时，由于业务规模较小，机柜、带宽等基础资源无法预估，随着业务的快速发展，造成单数据中心基础资源趋于饱和，业务无法进行扩容。另外考虑到单数据中心业务部署可靠性问题，多数据中心之间的数据热备是推动传输网建设的主要因素。开始是简单的专线互联，随着专线越来越多，形成了传输网初期雏形。 传输网建设初期专线带宽量比较小（小于2Gbps），出于维护便利性、低成本因素考虑，专线资源主要依托当前租用机柜服务提供商提供整体解决方案。首先，要求至少提供两条异步路由传输通道保障可靠性；其次，要求提供二层VLAN Tag全透传模式来保障未来业务扩展性（后续增加互联需求时，自行添加SVI互联接口进行路由策略添加即可）。另外，与业务方沟通专线带宽需求，掌握业务系统部署架构，确定专线使用原则，在此阶段尽量减少跨数据中心业务调度，避免业务过度依赖专线。 总结：此阶段传输网处于被动建设，无长远规划，专线资源强依赖第三方服务商。 传输网初期建设在开始进行第三个数据中心规划时出现了很多问题亟待解决，简要分析主要有以下几个方面。 （1）数据中心间专线容量扩展性。随着数据中心服务器数量的线性增长（以Hadoop业务增长为代表），跨数据中心日志传输量、DB同步量对跨数据中心之间的专线容量需求上升到10Gbps级别，迫切需要对线路资源进行扩容。但由于初期多数据中心建设时出于安全风险、商务竞争因素考虑，数据中心基础设施机柜资源是由不同供应商提供的，专线出现质量问题时难以划分责任主体，从而需要协调两家乃至多家供应商共同进行故障处理；开通专线和扩容时，需要涉及的供应商应同时具备资源才能进行。在某真实案例中，就曾遭遇过由于其中一家供应商专线资源容量不足，而导致无法进行扩容的问题。 （2）特殊业务对双运营商专线需求。电商仓储、金融支付类系统需要使用专线与外部第三方公司进行对接，出于可靠性考虑，通常要求采用双运营商线路进行对接，而单运营商数据中心只允许本运营商线路接入，造成在基础设施上难以满足此类业务专线接入需求。 （3）机柜、带宽资源捆绑限制。在初期数据中心选型过程中，业务需要覆盖多运营商用户，互联网线路覆盖能力是考核供应商非常重要的指标之一，供应商本数据中心提供多运营商线路是首要条件，在这种情况下就造成带宽与机柜强捆绑，只能选择与第三方服务商进行资源合作才能满足多线路接入（一般情况下，单运营商数据中心只提供单线路资源，严格限制其他运营商带宽资源引入数据中心）。而这些第三方供应商提供的所谓多线路资源也是通过各种手段拼凑在一起的，总体来说，风险还是比较高的，在使用过程中可能会经常发生非本数据中心运营商线路故障问题。 为了解决以上存在的各种问题，自建传输网提升日程，至此进入传输网初期规划阶段。通过自建传输网，不仅可以从根本上解决上述问题，而且还会带来诸多额外的技术、成本优化价值。下面就介绍传输网基础架构规划。 传输网物理架构在传输网设计规划时，在地域上将传输网划分为城域网节点、广域网节点、海外节点，然后根据每种节点类型进行功能划分，这样就能为基于需求使用特定接入方式提供最优的技术和成本方案。传输节点物理位置的选择直接影响到传输网的可靠性、维护便捷性、未来扩展性。 首先，至少选择两个物理数据中心来保障可靠性，数据中心间距离≥60km，减少光缆接入时同路径路由、区域电力以及灾害等不可抗力因素对传输网的整体影响。 其次，传输节点最好选择自有产权办公楼数据中心，一方面，可以根据自己的需求，对数据中心的物理安全及电力等配套基础设施提出高标准建设要求，如市电故障时，UPS需要能提供至少6～12小时后备电力，进出数据中心需要严格审核等，进一步保障物理安全；另一方面，可以提高进出数据中心维护的便利性、可控性。 最后，传输节点物理线路接入能力，对于传输网建设也起到举足轻重的作用，要求入楼管井及数据中心弱电管路等资源可自行控制，对入楼管井、熔接包、光缆资源拥有产权等措施，可以提高后续光缆接入的可扩展性；在入楼光纤配线架首次施工前，充分评估未来2～3年业务增长量，减少重复施工；入楼管井双入口，要求供应商线路通过不同管井入楼；多数据中心互联对接时，至少有两家供应商，通过两条物理上不同路由的链路，分别接入到两个传输节点，保障可靠性。传输网物理架构示意图如图10-1所示。 在某案例中，通过租用第三方供应商专线的方式联通，遇到了可扩展性及容量问题；另外，由于运营商政策限制，多数据中心间无法进行直接联通。因此必须建设自有传输数据中心，来担当核心数据中心间的汇聚节点。 我们规划了传输网城域网节点（如图10-1中的城域网节点①），将所有运营商数据中心，通过不同路由的两条裸光纤，分别接入至城域网节点A、B。通过使用裸纤线路互联，至少可提供主备各10Gbps容量，未来可通过采购密集波分复用设备（DWDM）进行扩容；为解决特殊业务对双运营商小带宽（容量为2～100Mbps）专线接入需求，两个城域网节点分别引入了电信、联通、移动专线。 图10-1 传输网物理架构示意图 随着多数据中心业务系统架构逐步建设完成，关键业务系统的异地灾备开始提上日程。经过调研业务需求，灾备数据中心与核心数据中心间需要提供大容量（1～10Gbps）专线带宽，并提供服务质量保障。出于安全风险考虑，一般灾备数据中心距离核心数据中心近千公里，于是我们规划了广域网节点（如图10-1中的广域网节点②），用于国内异地数据中心接入。经过调研长途大带宽专线，国内三大运营商成本非常高，相比之下，缆信、中信、铁通线路的资源，可以提供相对较高的性价比，目前长途专线主要租用这三家资源。 随着海外业务的开展，初期业务系统的监控报警、部署、维护等都需要与国内核心数据中心联通，某些业务安全级别要求较高也需要经专线传输。在对接过程中尝试过多种方案，比如搭建IPSec VPN隧道打通，经过验证这种方案的稳定性基本无法保障（主要原因是到海外互联网带宽资源紧张，电信运营商不提供服务质量保障，经常发生不规律、不明原因的TCP Reset造成业务中断），于是我们在中国香港地区建设了国际专线汇聚接入点，中国大陆与国际数据中心以及国际数据中心之间的数据交互通过香港交换，不需要再返回至大陆汇聚节点。 经过综合考虑国际线路成本及业务部署稳定性问题，以及与业务线商讨，确认国际线路使用原则如下：（1）国际专线不作为业务系统使用，仅作为运维使用。（2）对于特殊的高安全级别业务，需求走专线的，需要进行特殊报批申请。（3）非敏感类业务的需求，使用中国香港地区的国际互联网带宽（香港地区的国际互联网访问海外质量较好，规避了GFW影响），尽量降低业务对专线的强依赖。（4）由于国际专线带宽资源有限，在充分复用基础上，部署QoS策略保障关键业务服务质量。 在传输网上规划了中国香港地区国际节点（如图10-1中的国际节点③，主要基于其国际线路资源优势，以及与中国大陆联通性考虑），通过香港地区国际节点将海外资源、国内资源进行联通，即可实现成本优化（国内直接联通海外专线商务成本高，高成本主要是由于国际运营商与中国大陆运营商之间结算费用较高）。 传输网逻辑架构在多数据中心架构设计上，每个数据中心都是内外网物理架构的，传输网的逻辑架构设计也主要基于以上因素的考虑，将跨数据中心间的内外网流量分离，并采用分区域模块化设计，在每个数据中心设计独立的传输网区域，多数据中心间的内外网流量优先通过传输网交互。传输网逻辑架构示意图如图10-2所示。 图10-2 传输网逻辑架构示意图 在传统的网络结构下，虽然是自有业务间的访问需求，但不同运营商的外网间访问均需要跨ISP，带来访问延迟较大、丢包等问题，并且会有额外的带宽成本支出。通过传输网的外网互联模块，打通了多数据中心间外网，不但可以解决延时、丢包等技术问题，而且还可以将多数据中心间自有业务的互访流量控制在传输网内部，提供了专线级别的访问质量，同时减少了出公网带宽成本。 流量模型如图10-2所示，⑤→⑧代表跨数据中心间外网流量交换。多数据中心内网之间的交互主要依靠传输网区域的内网互联模块，通过城域网节点裸纤对接至少可提供10Gbps容量，后续业务增长，只需要部署一套波分设备扩容即可。流量模型如图10-2所示，①→④代表跨数据中心间内网流量交换。 传输网线路容量传输网容量主要受传输节点的物理空间、电力、波分设备档次影响，一旦建设完成，再进行割接改造成本较高，对业务影响范围比较广，因此需要考虑1～3年业务跨数据中心间传输容量需求。在传输网容量需求的不同阶段，使用不同的技术方案，以匹配业务需求。 根据传输网建设经验来看，多数据中心间的传输容量小于1Gbps，主要基于运维简易性考虑，通常租用两家第三方服务商双线路传输通道接入，即可实现高可用，当传输容量需要扩容至10Gbps时，则可以考虑租用裸纤方案，并配套使用长距单模万兆模块对接，同时为未来扩容到大于10Gbps容量做好光纤等基础资源储备。未来传输容量发展到大于10Gbps时，基于裸纤搭建波分网络扩容即可。 波分的选择：首先，根据传输网容量大小，确定采用密波分或粗波分设备；其次，传输网整体架构采用汇聚型结构，在网络数据层面进行冗余保护，减少波分物理环[lw1] 保护建设成本；最后，在波分设备部署架构上优先采用链型方式组网。这样做的好处是，一方面，物理层结构简单清晰，扩容方便；另一方面，利于减少波分设备厂商绑定（波分设备成对配套使用，不能使用不同厂商的设备对接），在设备选型上可以使多家波分设备供应商参与竞标，降低商务成本。关于传输设备厂商的选择，考虑到品牌知名度、可靠性、售后服务等因素，建议优先考虑华为、中兴等本地化厂商。 传输网服务质量虽然专线容量在不断扩充，但仍然会有数据中心间专线、国际专线带宽拥塞丢包的情况发生，造成业务服务质量严重下降，甚至不可用。因此，基于各业务在专线使用中对带宽要求各不相同的考虑，需要对突发流量进行控制，重点保障高优先级业务的服务质量，尽量避免由于专线资源满载造成的业务降级或不可用的问题发生。解决此类问题的关键在于，如何精准识别不同业务类型需求，根据业务需求提供不同的服务质量/带宽保障。 结合上面提出的解决方案，对专线的服务质量给出了QoS保障流程。◎ 定义金、银、铜三类服务级别，通过DSCP区别不同的业务服务级别，金、银、铜按照总带宽容量6:3:1比例（具体比例按实际情况进行划分）分配，在此之上再将不同业务划分入不同的服务级别。◎ 业务服务器出入网卡流量依据服务级别执行DSCP打标签。◎ 传输网区域模块根据DSCP值提供对应的服务质量保障策略。 传输网调度策略上面介绍的主要是传输网的架构和业务的互访需求，接下来我们看几个具体的应用场景和实现方式。 场景一：多数据中心内部不同运营商间流量交互调度（见图10-3）。 图10-3 传输网流量调度路由示意图 多数据中心间外网流量交互访问，通过传输网区域的外网互联模块，根据调度策略进行灵活控制。在正常情况下，多运营商需要多条默认路由保障出入向流量保持一致，使用一张Public路由表会导致默认路由冲突，这是无法满足需求的。 为了解决默认路由问题，需要在同一套物理设备上使用VRF技术进行路由表逻辑隔离。使用VRF多实例（CNC/CTC/BGP）、多进程OSPF（进程号100/200/300）逻辑分离多个运营商。 为了实现单数据中心内的多个ISP之间互联通，传输网区域使用VRFALL_ISP标识并使用OSPF（进程号400）将三类ISP VRF路由打通，并过滤掉默认路由（避免由于默认路由引起的多个运营商之间流量串扰）。 为了实现多数据中心间外网之间互联通，多数据中心传输网外网之间采用BGP协议对接，在每个数据中心传输网边缘将外网路由重分布，进而达到多数据中心间进行路由传递的目的。 场景二：跨数据中心带宽借用。 依靠传输网资源，可以将CDN数据中心带宽，临时调用至其他数据中心使用，通过复用CDN大带宽资源，满足业务突发活动的带宽需求。技术实现原理比较简单，主要是使用VRF技术，将核心数据中心的运营商带宽资源，在传输网外网互联模块上进行延续扩展。在城域网节点，将引入到核心数据中心的运营商，设置相同的VRF，在使用外部调度资源的数据中心，通过动态路由协议，进行IP地址路由通告，即可实现资源的引入（见图10-4）。 图10-4 传输网带宽资源引入示意图 总结：经过传输网的初期建设，传输网的整体架构已经搭建完成。在未来1～2年内，基于现有架构进行波分、网络设备扩容即可实现传输容量的升级扩容。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Transmission Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第九章：网络成长之路]]></title>
    <url>%2Funder-the-ops%2F20170923-09-growth-of-network%2F</url>
    <content type="text"><![CDATA[互联网公司的生存根本是服务安全、高效、稳定，这三个条件也是互联网运维从业者的工作基础，而这些基础工作绝大部分都依赖于底层网络服务的健壮性。互联网行业的网络服务与传统行业的网络服务还是有所区别的。 传统行业由于业务单一。增加趋势平稳，所以对于网络只有安全性和健壮性的需求；但互联网行业的特点是服务多元化、业务无规律增加、热点内容等，所以互联网行业的网络服务除了具备安全性、健壮性特点外，还需要支持较高的扩容能力和灵活的按需变化能力。 初期公司创业初期，一方面没有专职的网络运维人员，另一方面对未来网络容量规划缺少可以参考的历史数据（服务器数量、带宽数据等），所以第一阶段的网络建设是在对未来规划毫无数据支撑的情况下进行的。并且这里有个更重要的因素就是在公司产品还没有盈利的情况下，基础建设往往都会处于滞后状态，这应该也是绝大部分创业公司都会面临的问题。 在一穷二白的时候对网络方案的选择更看重以下三个因素。◎ 组网方案的成熟性。◎ 可靠性（主要依赖产品）。◎ 运维便利性。 组网方案和产品选择初期网络产品自身的稳定性决定了整体网络的稳定性，选择一家业界靠谱的网络设备厂商如H3C、Cisco等，是保障初期网络稳定性的必要条件。初期组网拓扑结构图如图9-1所示。 图9-1 初期组网拓扑结构图 第一阶段的网络结构采用了传统分层的Layer 2组网网络，物理上区分了互联网接入层、核心层、接入层设备。自上而下，我们先看互联网接入层。 互联网接入层互联网接入层设备可以选择路由器，也可以选择三层交换机，这里就不展开介绍这两种设备的功能区别了。在某案例中，第一阶段的互联网接入层设备采用的是三层交换机，主要基于两个因素考虑。 （1）端口密度：在流量未形成规模时，同时对互联网接入层设备没有过多功能要求时（初期只运行简单的静态路由），三层交换机的端口密度指标作为优先考虑的条件。 （2）成本问题：同样的成本，端口使用量是路由器的几倍，三层交换机的性价比更高。通常在数据中心接入互联网线路时，线路提供商可以提供如下几种不同的接入方式（不同的互联网供应商可能提供的方式有所区别）。◎ VRRP主备线路◎ ECMP双主线路 在初期网络建设中，互联网接入层设备采用的是双上联接口VRRP主备方式，在互联网流量较小的情况下基本可以满足对业务带宽和冗余的需求。关于互联网接入线路的选择，由于不同数据中心的规模和承建方、人力投入、技术支持、运营能力都不一样，导致数据中心提供的线路也有所区别。 国内比较常见的现象是，一级运营商直接提供数据中心资源，那么数据中心内只能有一级运营商的线路资源。例如，中国联通的数据中心只能提供中国联通的互联网线路。在由二级运营商承建或由不同的一级运营商联合建设的数据中心（此类较少）是可以提供多线或多种单线线路资源的。 线路资源可以分为如下两类。◎ 单运营商线路：中国联通、中国电信、中国移动等国内一级运营商线路。◎ 多运营商BGP线路：双线、四线、六线、八线国内二级运营商提供的线路。 单运营商线路由于是一级运营商直接提供的，所以单线资源在质量、扩容、故障、成本这4个方面要略优于多线资源。但由于中国各运营商自治和网间结算流量费昂贵，导致业务层面需要使用额外技术手段（如DNS调度、全局负载均衡、CDN等）来提升不同区域和不同运营商用户的访问质量。 多运营商BGP线路是由二级运营商和各一级运营商建立的BGP邻居关系，将自有IP地址通过BGP在已建立邻居关系的一级运营商线路内广播。从应用角度来看，可以轻松地实现IP多线路的运行模式，不受骨干网各运营商之间的访问限制，提升了应用访问的速度和品质。 目前国内家用宽带接入运营商以中国电信、中国联通为主，例如，主营客户端游戏的公司通常选择双线（中国电信、中国联通）的线路为主。但随着2008年中国移动和中国铁通的合并和移动互联网的兴起，移动线路和教育网在各数据中心中所占的比重也越来越重要了。各运营商流量占比如图9-2所示。 图9-2 各运营商流量占比 在用户至上的时代，显然在初期使用多线BGP线路这种复合型线路资源解决用户的问题是立竿见影的。但随着业务规模的扩张、调度系统的完善，以及对成本方面的考虑，多线BGP线路的弊端也逐步显现出来。 首先就是成本的压力，随着用户量的增加，带宽数量也呈现突飞猛进的增长态势，以四线BGP线路和单运营商线路为例，它们的成本差距在5倍左右，在强调“节能环保”的今天，这种成本的差距显然是不可接受的。 其次是故障率和冗余切换能力，由于多线BGP线路主要是二级运营商承建的，从网络角度来看，多一跳就意味着多一个故障的隐患。 再者是冗余切换能力，多线BGP线路在冗余切换上分为两类，业内称为真BGP多线和伪BGP多线，两者的区别在于真BGP多线实现了各大一级运营商线路路由的互相备份，当多线BGP线路中其中一个运营商线路出现故障时，故障运营商线路上的路由可以通过其他线路进行绕行访问；然而，伪BGP多线则只能实现多线路的用户访问覆盖，无法做到各线路路由的互相备份，当伪BGP多线其中一个运营商线路出现故障时，网络工程师能做的只是等待运营商进行故障修复。 还有就是多线BGP线路定向扩容能力。多线BGP线路的优势在于它具备优秀的线路整合能力，将原本复杂的互联网环境简单化。这种复合型线路资源势必会产生多资源配比的问题，例如，我们购买了1Gbps的四线BGP线路，这是不是代表这4个运营商线路我们单独都能跑到1Gbps呢？ 答案肯定不是这样的。通常BGP带宽资源池中的中国电信、中国联通的带宽会比较富裕，但一些相对占比较少运营商的带宽往往会很少。比如某真实案例中，业务移动端流量爆发，导致移动运营商流量突增，从带宽监控图上看带宽Buffer空间仍然有很多富余，但论坛总是能收到用户投诉超时、连接不上服务器等问题。 最后梳理用户投诉发现均为移动用户，协同二级运营商排查后才确定为是由于二级运营商中的移动线路带宽瓶颈导致的。对于这个问题，由于多线BGP线路每家提供商的资源都有所不同，业界也没有一个固定配比原则，所以这部分依然是盲区，不容易规避。 核心接入层和服务器接入层初期在对业务发展毫无概念的情况下，选择一套简单的通用的组网方案和使用相对靠谱的网络设备是保障网络可用性的两个关键条件。 初期核心接入层和服务器接入层采用Layer 2传统二层互联的方式。这种Layer 2传统组网方式，接入交换机后端服务器的默认网关都在核心交换机上运行，核心交换机采用VRRP/HSRP（热备份冗余路由协议）提供一个虚拟地址为服务器默认网关。 核心接入层和服务器接入层运行STP（生成树协议）提供服务器接入层交换机双上联核心交换机冗余性。STP协议需要生成一个无环并且冗余的网络拓扑，基于 STP协议的特性，需要通过算法在核心和接入层之间选择一个接口将其置为Blocking状态，在默认情况下使用Forwarding状态的接口进行数据转发，当Forwarding状态的接口出现故障时，则再次进行STP计算，然后将原Blocking状态的接口置为（默认35秒完成）Forwarding状态后恢复数据传输。 Layer 2传统组网方式提供了便捷的服务器接入能力，服务器只需要接入交换机接口，并将接口归属到指定的VLAN即可完成服务器上线。但如果在此阶段没有良好的IP地址规划，以及在业务爆发期无规律地上线服务器，再加上某些特殊的应用需要在二层环境下才可以工作，如数据集群（Keepalived）、LVS-DR模式等，这样的需求和现状会导致需要频繁的对接口VLAN信息进行变更，工作量巨大。 为了避免此类操作，初期网络采取了比较粗暴的方式进行处理，在核心交换机SVI（Switch Virtual Interface）以辅助地址的形式运行多IP，这样可以解决因服务器上线和接口变更所带来的网络设备的变更问题（不过因此也为后期埋下了祸根）。interface vlan 100ip policy route-map LVS-DR（坑1）ip address 192.168.1.254 255.255.255.0（坑2）ip address 10.0.1.254 255.255.255.0 secondary（坑3）ip address 10.0.2.254 255.255.255.0 secondary大体上来看，这时的网络结构满足了业务需求，并且通过“技术手段”减少了烦琐的服务器上线和其他业务需要的网络设备变更问题，弥补了初期人力不足的短板。但实际上该阶段的网络规划并没有看起来那么美好和健壮。 比如在该案例中，使用了192.168.1.0/24地址给忘了运维带来了困扰。现在做IP地址规划大家一定不会选择B类192.168.0.0/16私有地址作为内网的主要地址。原因不止是B类地址比A类地址可使用的地址数少这么简单。这里面所带来的问题是，如果数据中心存在192.168.1.0/24这个网段的服务器，我们如何通过远程VPN的方式管理这台服务器呢？因为这个地址段默认被D-Link、TP-Link这些家用无线路由器厂商当作初始化的IP地址段。当我们使用PPTP或L2TP远程拨号VPN远程访问目标服务器时，由于本地直连路由比静态路由的优先级更高，所以访问远程目标服务器的数据不会通过VPN发往数据中心目标服务器，数据包会在本地局域网进行二层查询。 上面这个小问题只是让远程办公的同学们有些不爽而已，还没有真正地对生产网络产生影响。下面两个问题对初期网络造成了不可小觑的影响，一个是单SVI多IP工作模式引起的问题，二是LVS-DR工作模式引起的问题。 网络设备对不同的数据处理分别是基于控制层面和转发层面进行的。这部分内容在网络设备厂商官网有大量的介绍，这里就不详细讲解了。简单来说，进入数据控制层面的报文主要靠CPU能力进行处理，转发平面主要靠硬件能力进行处理。控制层面：控制层面就是各种协议工作的层面。 它的作用是控制和管理各种网络协议的运行。控制层面主要处理两类报文，一是抵达设备自身的，如Telnet、SSH、SNMP；二是负责网络选路和拓扑变化所使用的协议，如生成树协议、动态路由协议、组播协议，这些都是由控制层面直接进行处理的。控制层面的处理能力依赖设备自身的CPU性能，所以在网络优化中也需要对控制层面进行特殊的防护，如CoPP（Control Plane Policing，控制层面策略）。总结：控制层面通过软件（CPU）处理，效率低。 转发平面：转发平面的工作主要是对穿越网络设备的报文进行处理，Layer 2数据包、Layer 3数据包、访问策略、QoS这些都属于转发平面的任务范畴。转发平面主要依赖设备接口芯片的处理能力。总结：转发平面通过硬件（芯片）处理，效率高。 在初期，某案例中，应用时常出现超时和访问失败等问题，排查后确定是由于核心交换机CPU负载高引起的（见图9-3），那么到底是什么原因引起的CPU负载高呢？ 图9-3 核心交换机CPU负载高 原因一： 交换机在进行三层转发时，如果路由下一跳在一个二层网段或者在相同的VLAN下，则会产生ICMP Redirects报文，这种报文是直接进入交换机的控制层面处理的，这类流量由交换机CPU直接处理。单SVI多IP工作模式会过度产生直接由交换机CPU处理的报文，这是导致核心交换机CPU负载高的一个主因。High CPU Due to Excessive ICMP RedirectsYou can get ICMP dropped redirects when one VLAN (or any Layer 3 port) receives a packet where the source IP is on one subnet, the destination IP is on another subnet, and the next hop is on the same VLAN or layer 3 segment. 原因二： 由于LVS-DR工作模式的特性，Real Server（后端目标服务器，后续简称RS）在进行响应时，需要使用服务器自身Lo绑定的VIP地址进行回包。LVS后端服务器存在两种部署模式，第一种为后端RS配置公网IP；第二种为后端RS配置内网IP。第一种模式下可以依赖后端RS自身的默认网关完成数据包的回包工作；第二种模式下则需要网络层通过PBR（Policy-Base Route，策略路由）对流量进行特殊的处理。 由于第一种部署模式会将后端服务器直接暴露在互联网上，存在安全风险，所以当初出于安全的考虑，后端服务器采用了第二种部署模式。这就导致需要在网络层使用PBR对这部分特殊的流量进行处理。某些低端核心交换机在开启PBR这个特性时存在一些限制，会导致导致核心交换机CPU负载过高。High CPU Due to Policy Based RoutingPolicy Based Routing (PBR) implementation in Cisco Catalyst 3750 switches has some limitations. If these restrictions are not followed, it can cause high CPU utilization.l You can enable PBR on a routed port or an SVI.l The switch does not support route-map deny statements for PBR.l Multicast traffic is not policy-routed. PBR applies only to unicast traffic.l Do not match ACLs that permit packets destined for a local address. PBR forwards these packets, which can cause ping or Telnet failure or route protocol flapping.l Do not match ACLs with deny ACEs. Packets that match a deny ACE are sent to the CPU, which can cause high CPU utilization.l In order to use PBR, you must first enable the routing template with the sdm prefer routing global configuration command. PBR is not supported with the VLAN or default template. 但随着大数据和分布式计算的应用，业务层面给网络带来的挑战还远远不止这些。分布式计算类型的服务会存在大量的数据交互和复制，其特点是：高带宽、大突发。它们在一定时间里被我们亲切地称之为“内网杀手”。以Hadoop为例，从图9-4可以看出，用户在推送数据块时数据节点之间的复制操作需要频繁地消耗接入层交换机的上行带宽，在初期上行收敛比还是12:1时（8口千兆上行，48口千兆下行），这部分流量必然会给相同机架的其他服务造成影响。 图9-4 分布式计算大量消耗内网上行宽带 针对此类业务的大数据交互的特性，在该案例中，做了以下两个方面的调整。◎ 优化数据节点，减少跨机柜的交互。◎ 扩容接入层交换机，达到核心上行带宽。 刚才我们谈到收敛比时，应该有读者看出初期阶段核心和接入层之间存在的问题。8口千兆上行收敛比应该是6:1，为什么这里写的是12:1呢？就如之前所介绍的一样，核心和接入层之间运行的生成树协议（STP），天生就会浪费50%的带宽资源。STP会带来两个问题，一是资源浪费，二是冗余切换能力，如果当上行接口聚合组其中一个接口出现异常关闭时，那么接入层交换机的上行收敛比就又会减少1/4，并且这时STP并不会重新进行拓扑计算。 我们在处理Hadoop大流量的问题时，意识到STP这种古老的协议已经不适用于当前数据中心网络了。提升收敛比的首要任务就是将STP备份线路充分利用起来。 业界替代STP的组网方式有很多，如Layer 3组网、网络虚拟化、大二层。但这种在原有网络基础上改造的方式，不同于新建，更多的是需要考虑原有设备利用、割接窗口时长等客观因素。 所以在考虑上述因素后，改造方案选择使用网络虚拟化的方式替代STP组网方式（见图9-5）。这样可以在不更换接入层交换机的情况下，将原有收敛比提升一倍，并且很好地规避了前面提到的STP存在带宽浪费、冗余切换的问题。 图9-5 网络虚拟化 网络虚拟化技术有两个技术类别分支，分别是整机虚拟化和接口虚拟化。 整机虚拟化以Cisco VSS(Virtual Switching System) 和H3C IRF(Intelligent Resilient Framework)为代表，这类整机控制层面虚拟化已经是比较成熟的技术，已有很多商用案例。控制层面虚拟化提供了两种工作模式，分别是一虚多和多虚一，分别应对不同的场景。 多虚一的主要作用是简化网络结构，提升网络横向扩展能力。通常是两台设备进行多虚一，但目前也有部分厂商实现了N:1的整机虚拟化，如H3C IRFv2（Intelligent Resilient Framework 2，第二代智能弹性架构）技术目前最多可4台核心设备N:1虚拟化，H3C等较为主流的交换机产品线均支持IRFv2技术S1250、S9500E、S7500E、S5800等。一虚多技术多数用在虚拟化多租户环境下，整体提升硬件资源的利用率。此类技术多使用在金融和虚拟化公有云环境中。 接口虚拟化以Cisco VPC(Virtual Port Channel)和Arista MLAG（Multi-Chassis Link Aggregation）为代表，接口虚拟化和整机虚拟化在网络结构上没有太大的变化，都进行了跨设备的接口聚合操作。但由于接口虚拟化只关注跨设备的接口聚合，所以在核心层面依然需要使用HSRP/VRRP技术提供冗余支持。 在实际部署中对接口虚拟化各家厂商限制的条条框框较多，相比IRFv2整机虚拟化技术部署起来比较复杂，反观整机虚拟化提供最多4台核心设备的虚拟化能力，接口密度为乘的关系，去除了复杂的VRRP/HSRP配置和大量的地址配置，逻辑上只管理和监控一台设备，提供了良好的管理便利性（见图9-6）。 图9-6 整机虚拟化和接口虚拟化部署图 基于以上的分析，在应对初期Hadoop高带宽需求时，我们的解决方法是通过替换核心交换机设备，提升一倍的带宽收敛比，短暂性满足带宽需求，并且尽可能地将Hadoop业务放置在单独的机柜，以防止影响到其他业务。 初期网络运维总结在没有明确需求做数据支撑的前提下，组网方案只停留在了简单的设备堆砌阶段。应用存在不确定增长或大规模爆发情况时，对基础网络造成的冲击非常大。网络运维工程师这时主要承担了救火队的角色。 但这一阶段也有不少收获：第一，逐渐摸清了公司业务流量模型和特点，为后续的规划积累了数据；第二，通过不断的网络平台扩容、改建，总结出以下两条真理。◎ 改造的难度远大于新建。◎ 对于增长不确定的网络规划，核心交换机的规格和投入适当超前。 中期在该案例中，总设备数量的增长，推动了建设新数据中心的需求，让网络工程师有机会在“白纸”上重新构建基础网络，规避初期遇到的问题，重新梳理需求。网络建设和数据中心规模有着密切的关系，如原计划建设一个约500台服务器规模的数据中心，网络的设计容量为支持1000台设备，但如果业务增长超过数据中心规划，这样就导致又要去规划新的数据中心。 这种情况所带来的问题有以下几点。◎ 大量小规模数据中心管理复杂。◎ 数据中心网络设备投入成本大。◎ 小规模数据中心间互联是难题。 当发现这个现象后，为了避免后期出现上述几个问题，采取了“脱壳前进”的数据中心扩容方式。当存在更大规模的数据中心时，将规模小的进行撤离融合到大规模的数据中心去，整体的网络结构、建设成本、运维成本均得到了提升。但这里痛点不在网络层面，而是在业务层面。在这个阶段大量业务还不支持双数据中心冗余切换，需要有损地进行业务迁移。所以需要推动业务架构支持双数据中心部署，为后期双数据中心冗余打好基础。 中期阶段的网络规划在业务分区、边界安全、线路层等方面都进行了相应的优化。同时业务部门也对网络安全提出了更明确的需求，如业务线间安全隔离、互联网访问安全隔离等。下面让我们来看看中期阶段到底为什么这样规划？还遇到了什么挑战？ 组网方案为将来建设支撑万台以上设备的大型网络积累经验，在中期规划了两套不同的组网方案，来对比各自的优缺点。但基础的业务分区、安全需求、LVS区域的实现是一致的。中期组网结构如图9-7所示。 图9-7 中期组网结构图 我们在网络中明确了区域的概念，对公司的不同业务划分了区域。各区域使用不同的物理层设备进行物理和逻辑的双重隔离，做了IP地址段的分区规划，不同IP地址对应不同业务。这样做的好处是逻辑结构清晰，并且便于后续运维及权限划分。可以分为以下三大类：◎ 互联网区域◎ 内网区域◎ DCI（Data Center Interconnection）区域 互联网区域互联网区域示意图如图9-8所示。主要功能是接入多条运营商线路，各线路入网对应不同的负载均衡集群（LVS），出网对应SNAT集群。入网、出网实现统一化、服务化。 图9-8 互联网区域示意图 如前文所述，该阶段，业务对网络的可用性、安全性都提出了更高的要求，所以运营商线路接入都是采用双上联ECMP方式进行的，并且在每条运营商线路的入向接口提供互联网接口白名单的规则。 LVS/SNAT集群和网络一起规划了OSFP的方式联动部署。LVS也升级到了FULLNAT模式，相比DR模式对网络的要求降低很多。从网络角度来看，FULLNAT的数据流是一个正常的从哪里来从哪里回的数据包，在网络层可以不用对这些报文做特殊的处理（策略路由）。这也极大地简化了网络的复杂程度，LVS的部署和详细内容可以参考LVS章节。 在某案例中，此阶段，互联网线路上更多的是关注质量、故障、成本三个因素，所以我们设计多线路的LVS集群方式，并建议应用尽量运行在互联网单线资源上。这里分享一些线路选择和带宽预留的原则。 1．互联网线路使用指引 （1）可调度业务的电信、联通、移动用户使用单线资源覆盖。（2）可调度业务非三大运营商的用户使用多线BGP资源覆盖。（3）SNAT默认只提供BGP出网线路。 2．带宽预留原则 （1）物理接口：物理接口使用总和的50%（单10Gbps接口带宽使用范围为5Gbps，双10Gbps负载接口带宽使用范围为10Gbps）。（2）逻辑带宽：带宽Buffer数量为最近一周带宽峰值的50%。 内网区域在某案例中，初期阶段在收敛比上吃了亏，在中期阶段专门规避了这个问题，收敛比初期设计为1:2，但随着采购量的增加和设备成本的下降，逐步地扩容到1:1。根据监控数据看非分布式计算业务机柜在2:1的收敛比下也可以良好地工作，这和业务自身调优也有比较大的关系。但分布式计算业务如Hadoop机柜则建议1:1收敛。 在实际部署中内网区域使用了两套当时比较主流的组网方案，这主要集中在核心接入层和服务器接入层之间。其中一种是Layer 3互联组网方案；另一种则是以Cisco Nexus系列为代表的Pod Design互联组网方案。这两套组网方案各有优劣势。 1．Layer 3互联组网Layer 3互联组网，顾名思义，是核心层和接入层运行OSPF或者BGP路由协议进行互联。业内一直有一种说法是，Layer 3互联组网是一种临时数据中心组网的过渡方案。这种说法其实没有错。我们可以设想一下，在核心设备不支持网络虚拟化或现在主推的大二层解决方案的年代之前（2009年Cisco Catalyst 6500平台开始支持网络虚拟化技术VSS），除了这种Layer 3互联组网方案，核心设备在当时是没有技术支持横向扩展的。 换个角度来说，就是一个数据中心可承载的服务器数量其实局限于核心设备的接口密度。Layer 3的解决方案在满足扁平化的基础上将数据中心可承载的服务器数量提升了8～16倍（主要参考ECMP的数量决定）。当然，Layer 3互联组网方式也完全规避了STP产生资源浪费和冗余切换时间长的问题。 （1）Layer 3互联组网的挑战Layer 3互联组网方式也对运维管理、成本提升、二层应用的部署带来了一定程度的挑战。◎ 运维管理首先，运维管理的挑战是IP地址管理、基础服务应用、接入交换机上限三个方面。可想而知，IP地址管理将变得很复杂。以4个核心为例，每台接入交换机上都会配置4个P2P（Point-To-Point）互联IP地址和一个服务器网段。如果服务器使用共享ILO（Integrated Lights-Out）管理方式，那么还将有服务器管理卡网段。在正常情况下每个接入交换机上将产生5～6个小网段。假设我们有200台接入交换机，在没有良好的CMDB系统协助管理记录的话，运维这种类型的网络难度可想而知。 其次，是基础服务应用。例如服务器上架和装机，因为每台接入交换机自治的原因，所以就需要服务器上架人员和装机人员对此类网络有充分的了解。但在实际环境中往往是负责上述工作的人员和构建网络人员分属两个不同的组或者部门，所属不同的专业维度，要求他们对网络结构、IP地址与网络工程师有一样的认识显然不太合适。最终就变成一些服务器上架和变更需要主机人员和网络人员配合，费时、费力，效率极低。 最后，是接入交换机上线。就如刚才所提到的IP地址管理的复杂性，在初期没有Netconf和Poap（Netconf和Poap实现网络设备加电后自动下发配置的功能）技术支持的情况下，一台接入交换机的初始化配置涉及大量的人工修改，配置的准确性和上线效率都潜在隐患。 ◎ 接入交换机成本的提升在传输二层环境下，我们进行接入交换机选型主要关注接口密度、接口Buffer、背板带宽、转发能力、MAC地址容量这几个性能指标即可。但对于Layer 3互联组网，我们在关注上述指标之外，还需要关注设备是否支持三层功能、路由表的容量等信息。这样势必会带来接入交换机的成本提升。这部分设备在数据中心基数较大时，产生的成本量级还是比较大的。 ◎ 二层应用部署典型的二层应用就是VRRP（热备份冗余路由协议），如需要提供虚地址方式进行集群冗余的应用，均依赖此协议（如Linux Keepalived）。这样导致依赖此协议的集群服务需要部署在相同的交换机上或者相同的机柜上。显然这样的部署方案在冗余上并不是那么完美。针对这种业务，在Layer 3互联组网方案中，通常的处理方式是将前后排背靠背两个机柜做成多虚一的虚拟机交换机，来满足不同的接入交换机和不同的机柜达到冗余的效果。这里还涉及一个现实的问题是，通常研发或应用运维提交服务器使用申请时是不会标注应用类型的。这样导致负责主机的人员在交付后还要针对使用这种类型的服务进行单独的机柜调整操作，涉及的工作量可想而知。 （2）Layer 3互联组网的优势不可否认，在特定的阶段Layer 3互联组网方案提供了良好的扩展性、冗余性、安全性，这也是此类组网方案的优势所在。扩展性主要体现在可以针对核心层面进行横向扩容；冗余性体现在支持多条冗余上行线路，并且有着良好的切换能力；安全性体现在众所周知的安全性最差的数据链路层上，减少了数据链路层广播风暴的可能性，降低了受广播风暴的影响，但是对特定应用存在一定的部署难题。Layer 3互联组网方式在核心设备与接入层设备品牌选择上提供了差异性支持。接触网络较早的读者一定清楚不同厂商的设备工作在STP环境下会产生莫名其妙的问题。这种组网方式可以规避此类问题，让不同品牌的核心设备和接入层设备工作在网络中。 2．Pod Design互联组网Pod Design互联组网方式，是Cisco Nexus交换平台上推出的组网方案，在物理结构上类似于传统的核心、汇聚、接入三层。实际部署的区别主要在汇聚和接入两层。传统的汇聚和接入采用Layer 2或Layer 3进行互联。Pod Design以Pod概念替代传统区域的概念，在结构上差异并不大，但功能上略有差别，传统区域定义除了划分区域外，区域汇聚可以提供安全、负载均衡、流量控制等功能，这主要依赖汇聚交换机的设备产品形态。但当前数据中心的网络需求已经从多功能向高带宽、低延迟方向转变，这些较为复杂的功能已经不适合当前数据中心的网络需求了。Pod Design互联组网方案在这两层采用了俗称远端板卡接入方式（Fex），类似于在汇聚层和接入层进行纵向虚拟化，Pod内所有接口的操作、监控、变更在Pod两台核心设备上进行，简化网络结构、减少网络中可管理设备的台数。 （1）Pod Design互联组网的挑战Pod Design的设计思想是将网络按模块进行归类，这种思想和我们设计网络时提到的IP关联业务线不谋而合，甚至是天生就实现的。但此类组网方式也同样面临着Layer 3组网所遇到的三个挑战。◎ 运维管理运维管理所带来的挑战是布线系统 、Pod容量、管理风险三个方面。传统的数据中心核心到接入层布线有两种方式，即EoR（该方式指服务器机柜中所有的服务器接口，都通过跳线连接到机柜上的配线架，再由配线架上的铜缆延伸到网络机柜（位于一组机柜尾部）中的接入交换机）和ToR（该方式将接入交换机放置在每个服务器机柜顶部，机柜内服务器直接通过短跳线连接到顶部的交换机，再经由光纤从交换机的上行链路接口连接到核心交换机）这两种方式的优劣势网上分析文章很多，这里就不做过多的分析了。 目前主流的是ToR布线方式。但Pod互联组网模式的布线方式还和ToR方式略有不同，主要是在汇聚交换机提供到接入交换机的接口方式上有所区别。传统的ToR方式汇聚和接入设备分别提供4个10Gbps SFP+线缆接口互联接口，但Pod Design出于机架空间和端口密度的考虑，多数是提供如图9-9所示的40Gbps MPO线缆类型的接口。此时我们需要通过MPO-to-41C 针对汇聚和接入设备线缆进行跳接。 图9-9 40Gbps MPO线缆类型的接口 这带来了一个问题，就是线缆长度的问题。在设计此类型组网方式时，为了避免单一40Gbps接口故障影响过多的接入层上行带宽的容量，在实际环境中40Gbps接口是分别接入4台不同的接入层设备的，这样当汇聚单设备单40Gbps接口故障时，只是4台接入交换机上行带宽从40Gbps减少到30Gbps而已，这个带宽减少的比例还是可以接受的。 但根据机柜位置的不同，线缆长度也不同，在实际采购过程中发现MPO-to-4lc线缆都不提供定做LC侧超过10m以上规格的线缆。在部署和线缆备件的处理上都比较麻烦，所以采取的是汇聚交换机机架顶部配线架的方式，如图9-10所示。 图9-10 汇聚交换机机架顶部配线架 两个配线架以背靠背48端口LC方式对接，在这样的布线方式下，汇聚交换机使用短距MPO-TO-4LC线缆和配线架1对接，接入层设备根据机柜实际位置选择不同长度的线缆与配线架2互联完成对接。 这样的布线方式较为简单地解决了线缆长度和备件所带来的问题，同时也增加了管理成本，并且配线架背靠背对接存在后期维修故障的难度。这样的配线架模式在后期如果其中一对接口出现问题，那么基本上是不可维修的，因为在配线架上直接进行光纤熔接操作非常容易误碰到其他线缆引起不必要的损失。针对这个问题，布线厂商在后期提供了预端接布线方案，大家可以自行百度查看技术细节，主要是以模块化思路处理了这部分对接，出现异常可以通过更换预端接模块进行简单处理。 Pod内服务器容量设计依赖于厂商限定的条件，以Cisco Nexus为例，在主流千兆服务器接入环境下，根据汇聚交换机工作模式的不同，单Pod容量可分为1152台和2304台服务器两种规格。受限于这样的条件，我们面临Pod内部署什么样的业务，以及业务超过Pod容量后Pod间带宽如何计算配比等问题。 此类组网方式减少了网络可管理的设备台数，全网设备管理集中在几台核心设备上。这样带来的管理是一些较为轻量级的操作，如接入层接口配置变更等都需要登录核心汇聚设备进行。但是为后期的权限划分增加了难度。 ◎ 成本问题Pod Design在成本问题上可以分为两块内容，分别是成本提升和成本优化。成本提升主要是在这类组网方案中多了一层汇聚交换机的成本投入。成本优化则体现在业务流量模型清晰化上，可以减少Pod间收敛比、设备板卡及模块的投入，但这个需要视实际业务情况而定。 ◎ 二层应用部署二层应用部署其实面临着跟Layer 3互联组网一样的问题，只是在Layer 3二层环境中以接入交换机为单位，Pod Design以汇聚交换机为单位。 （2）Pod Design互联组网的优势Pod Design采用类似多虚一的虚拟化技术，剥离传统接入交换机的控制层面。这样的优势是以Pod为单位只管理两台汇聚交换机，劣势就是接入交换机只能工作在Pod Design工作模式下，因为它自身不具备控制层面，无法单独使用，如Cisco Nexus 2000系列。 安全的挑战安全是任何一家互联网公司都不能忽视的问题，但安全需求在执行落地时都会给业务发展和基础架构带来一定的挑战，在此阶段安全对网络提出了以下需求。 1．互联网端口白名单随着LVS平台对运维工程师的管理权限下放，以及一些LVS无法支持的业务直接暴露在公网上，需要在互联网入口做防护，其规则制定和访问控制是保障业务安全的第一要素。 网络和安全人员一起制定了默认的白名单端口范围，以及特殊端口的申请及审批流程。在某案例中，这个功能在讨论初期存在两种方案，其中一种是在各LVS集群生效访问控制；另一种是在网络设备如防火墙、交换机生效访问控制。在网络设备层面生效的优势是可以在最初的入口完成访问控制，这也是访问规则就近生效的原则。另外，对于部分直接暴露在互联网上的业务，在网络设备层面统一入口生效也有助于对这类业务的保护。 互联网端口白名单可以选择在出口互联网接入层交换机层面生效，也可以通过上层增加硬件防火墙生效。从安全角度出发，使用硬件防火墙是最好的选择，因为硬件防火墙自身是可以实现如Syn Flood、Ack Flood等攻击的防御的，并且基于会话的硬件防火墙可以非常轻松地实现业务单向访问的需求。反观交换机传输ACL隔离，仅可以实现单纯的访问控制，在单向访问和防御Syn Flood、Ack Flood攻击方面则显得无可奈何。 但最终我们还是选择在交换机层面实施互联网端口白名单，其中一个主要原因是设备性能，因为交换机不记录会话，所以会话的多少是不会直接影响交换机的转发性能的。反观硬件防火墙设备，防火墙设备在选型时考虑的两个重要指标就是并发数和每秒新建连接数。当存在高并发业务时，防火墙设备对此类业务的安全过滤显得有些力不从心。另一个原因是成本，在出口并发数为200万，支持200万并发的场景下，防火墙产品和交换机的价格可是天壤之别。 互联网端口白名单有两种实现模式，其中一种是黑名单即明确拒绝特定端口；另一种是白名单即明确允许特定端口。第二种模式明显限制得更为严格，并且具备完善的审批和记录机制，如果遇到安全问题，可以快速地进行事件追溯（也可以通过全流量分析完成）。但这也在一定程度上带来了人力成本的上升。可以根据实际情况酌情选择。 2．内网隔离内网隔离在金融和军政网络中存在比较多，坦白地讲，金融和军政内网隔离相对较容易实现，因为此类网络对服务器数量和未来流量的预期相对有据可依，所以对这类网络可以选择防火墙隔离或网闸等手段来完成内网隔离。 但互联网公司业务存在很多不确定性和高突发流量的情况，在安全和性能之间很难找到一个平衡点，所以在互联网公司内网隔离就变得难以实现。 需要实施内网隔离一般有以下场景。（1）第三方评审如PCI-DSS，全称为Payment Card Industry（PCI）Data Security Standard，第三方支付行业(支付卡行业PCI-DSS)数据安全标准。（2）非授信区域部署第三方开源应用区域，如Discuz、Phpwind等开源程序。（3）业务线间不同业务线均存在不同类型的敏感数据，并相互不信任其对方的安全性，要求不同业务线间进行内网隔离。 针对不同的场景，应当采用不同的隔离方案。首先，安全合规应用属于第三方评审要求，此类内网隔离建议采用硬件防火墙设备，在这种安全至上的业务面前，性能反而不是第一考虑因素了；其次，针对后两种场景的这类安全需求属于公司内部需求，且对网络性能要求很高，这类需求建议采取交换机ACL方式进行安全隔离。 但采取交换机ACL方式实现内网隔离困难和问题依然很多，这主要体现在功能、容量、管理与变更三个方面。① 功能比如ACL实现单向访问困难。在互联网端口白名单中问题不大，因为互联网涉及的IP较少，类型也比较单一。但应用在内网隔离中这部分缺陷就显现出来，内网应用协议繁多，如FTP、DNS这两类应用是不能通过传统ACL实现单向访问的。所以一个折衷的做法是将此类无法实现单向访问的应用和必要的服务（如YUM/NTP等）进行梳理全部放行，同时将此类服务定义为基础设施白名单，该名单中的服务默认在所有生效的ACL区域放行，进入此名单的服务需要通过完善的安全审核机制和流程，保障其安全性。 ② 容量这方面问题主要是因为后期业务之间调用繁多，超过交换机自身设备所能承载ACL条目的规格。交换机ACL规格和防火墙ACL规格不在一个数量级上，防火墙ACL规格在几万甚至几十万级别，但交换机ACL规格往往只在千级别。 ③ 管理与变更设想一下，上千条访问规则分别应用在不同的设备上，管理与变更这么庞大的访问规则不是件容易的事情。 中期网络运维总结在这个阶段我们完成了数据中心网络标准化的雏形规划和建设，通过与业务磨合，清晰地了解了业务对网络的需求。随着业务从不规律到逐步稳定上升，业务部门可以提供一定的可参考数据，这对后期建设标准化数据中心网络提供了良好的数据积累。业务部分对网络可提供的服务也有了一定的了解，如数据中心容量、线路资源、安全特性等。在这个阶段数据中心网络模型满足当前业务的需求。 总结前两个阶段，是某真实案例中，从初期到当前现状的核心网络的进化史和血泪史，期间所使用的组网技术和方案不是业界先进的技术和解决方案，都是一些传统的技术和组网解决方案，所以在介绍中并没有过多的技术描述，涉及的技术大家可以自行搜索，有大量的技术文档可供参考。希望通过我们的介绍，让大家对处理不同业务需求和初期网络建设少走一些弯路。 近两年随着云计算的发力，公司在初期阶段和对网络自主性要求不高时，云主机的解决方案是一个非常不错的选择。但随着Docker和OpenStack这些虚拟化服务的大量应用，后期需要考虑的是如何在网络架构中更好地支持虚拟化业务。传统的网络架构提供的扩展性和稳定性已经远远不能满足云计算中心对网络的需求，除了扩展性和稳定性这些基本需求外，对网络灵活性和网络快速适配能力都有较高的要求。 业界应对主流云计算网络架构的技术有很多，如SPB、TRILL、FabricPath、VXLAN、OTV、SDN等，希望这些技术在我们正式应用后继续分享给大家。当然，这个阶段的网络还存在一些问题，如监控的精准度、网络管理自动化等。这些问题和业务无关，是网络自身的运维管理问题，我们希望后续通过平台化以及对上述新技术的研究解决这类问题。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第八章：服务器硬件测试选型]]></title>
    <url>%2Funder-the-ops%2F20170923-08-test-type-selection-of-server-hardware%2F</url>
    <content type="text"><![CDATA[面对琳琅满目的服务器硬件品牌和五花八门的硬件型号规格，如何选择高性价比的硬件配置，是系统运维的一项重要工作。系统工程师需要根据产品线的不同需求，测试服务器的各项性能以及功耗，同时结合成本确定出性价比最高的服务器配置。因此，硬件测试便成为了服务器硬件选型的必要依据。 此外，处理器、内存、磁盘、SSD、磁盘阵列、网卡等配件的不同型号或规格，搭配起来存在无数种配置方案。面对产品线提出的各种配置需求，是否存在一种方法既能够满足业务需求，同时又能让系统工程师轻松、高效地预算和管理服务器？在服务器规模达到千台的时候，我们开始了服务器配置套餐化的工作。 随后为了能够持续地优化成本，我们又开始了多品牌服务器、配件选型、硬件性能和功耗优化的工作。随着服务器数量开始呈现出指数级增长，硬件故障处理的工作量也变得十分庞大。为了能够实现自动化检测和管理硬件故障，我们又开始了服务器硬件领域新的探索和实践。 服务器硬件测试引入硬件性能评测，提供选型依据如何将不同硬件之间的差别更加具体化描述呢？比如，如何描述两款不同型号处理器之间的性能差异呢？可视化的性能数据是最直观的。这里就要引入硬件性能评测机制，通过运行权威的性能评测软件将不同硬件之间的差异数据化，为选型提供更强的理性依据。比如E5-2630v2处理器性能评测得分是10000分，而E5-2640v2处理器得分是12000分，那么就能得到后者性能是前者1.2倍的数据性结论。 常用的硬件评测软件有GeekBench、Stream、fio、netperf等。（1）GeekBench主要针对处理器的计算性能进行测试评分，能够分别评估整数计算和浮点数计算能力。（2）Stream单独针对内存提供较详尽的性能测试，比如针对单线程和多线程的内存操作有不同的测试项目。（3）fio是一个灵活性很大的IO评测工具，可以用来评估SAS、SATA磁盘、SATA SSD以及PCIe SSD等各种IO硬件设备的性能。（4）netperf是一个常用的网络性能评测工具，可以用来模拟各种网络流量场景，可以评估网卡在不同场景下的包处理和数据吞吐能力。 如图8-1所示是通过GeekBench和Stream工具对两款处理器（E5-2620v2和E5-2630v2）做计算和内存性能评估收集到的原始数据。 图8-1 通过GeekBench、Stream工具获取到的测试数据 通过测试数据可以发现E5-2620v2的计算能力整体要比E5-2630v2低一些，而且两者之间的性能差异也可以数据化。（1）整数计算性能（Integer Score项目）后者比前者高18%，意味着数据处理能力强18%。（2）单线程混合内存操作（Single_Triad项目）后者比前者高8%，意味着单线程下内存处理效率高8%。（3）多线程混合内存操作（Mutil_Triad项目）则高45%，意味着多线程下内存处理效率高45%，后者更适合多任务环境使用。 如图8-2所示是通过fio工具对不同阵列模式下的SAS磁盘做性能测试后收集到的原始数据。 图8-2 通过fio工具获取到的不同阵列模式下SAS磁盘的原始性能数据 通过数据可以更理性对比不同IO设备在随机读/写、顺序读/写方面的差异，为选型提供理性依据。 同类硬件只考虑一种规格通过硬件性能评测能够准确地评估出处理器、磁盘、SSD等各种硬件的极限性能，便于我们将服务器按照计算、I/O、存储为单元进行拆分并单独评估性价比。在套餐精简的基础上，为了能够形成更强的规模效应，应该从统一配件规格上着手优化。因此，硬件性能便成为了评估性价比的重要指标。 比如，通过评测工具得到候选处理器的性能评分，同时结合处理器的价格来评估性价比。假设处理器A的单价为4000元，性能得分是13000点；处理器B的价格仅为2000元，性能得分是10000点。虽然处理器A的性能是B的1.3倍，但是A的价格比B高一倍。通过性价比计算公式：性价比（点/元）=计算性能（点）/单颗价格（元），得到处理器A的性价比是3.25点/元，处理器B是5点/元，那么处理器B的性价比是处理器A的1.53倍。请参考图8-3。 图8-3 处理器A和处理器B性价比评估 在满足业务性能的前提下，优先选择处理器B，那么就不必考虑在套餐中同时存在两种型号规格的处理器了。此外，同种规格的处理器采购量越大，成本的规模效应越明显，也更容易通过部件进行议价。其他配件也同样适用这个原则。 追求单机性价比在前面的例子中，虽然处理器的性价比是很重要的一项指标，但偶尔也会有意外的情况发生，比如某些注重单机计算能力的业务应用。这时单纯考量单个处理器的性价比就不适用了，应该转向考量整机的计算性价比。 比如，处理器C售价是6000元，是处理器B的3倍，但是C的计算评分是25000点，是B的2.5倍。C的整机成本是25000元，而B的整机成本为20000元。虽然处理器C在单个处理器的性价比上不如B，但如果从单机成本对比来看，C整机成本为B整机成本的1.25倍，因此处理器C的单机性价比是处理器B的2倍。请参考图8-4。 图8-4 处理器C和处理器B性价比评估 因此，选择处理器C来提高业务的单机计算能力，节省成本。对于其他硬件也同样适用该原则。 服务器套餐化在大部分情况下产品线并不知道自己的真实需求：用什么处理器、多大的内存、多少块磁盘。通常都是凭以往的使用经验拍脑袋决定的：今天多加几条内存，明天多加几块磁盘。这样给系统工作带来了极高的服务器配置管理成本。 例如，某公司每个月都会采购新的服务器，某个月产品线提出了共70个配置需求，服务器供应商总共有4家，那么系统工程师等同于要面对和管理共280个服务器配置。从需求收集到落实采购，这期间和产品线、供应商的沟通、确认将耗费大量的精力，时间几乎都要耗在预算采购这一件事情上，而且还容易出错。同时，如此众多的服务器配置也无法形成相同配置的规模效应，无形之中增加了公司的运营成本。因此，服务器配置套餐化十分有必要。 ###什么是套餐化 现在有两家菜馆：A餐馆有100道菜，B餐馆有10个套餐。那么，用户去哪家餐馆点餐最快？显然是B餐馆，而B餐馆就是现在大家熟知的快餐店。在大部分情况下用户并不清楚想吃什么，怎么搭配菜品更合理等。而套餐的存在能够快速将餐品按一定原则分类搭配，给客户提供了预选的方案，同时还精简了菜品数量。 服务器配置套餐化也是同样的道理，按照业务方需求提供预先设置好的配置，能够让业务更容易做出选择，而且套餐数量精简后更容易形成规模效应，增加采购议价能力。 对服务器套餐进行分类建立服务器配置套餐以后，为了方便管理就需要将套餐进行分类。根据业务需求，我们将服务器配置抽象分类成以下几类。（1）计算型：We前端、缓存类业务等；这类配置一般处理器主频规格和内存配置较高，对数据存储的要求较低。（2）计算IO均衡型：业务中间件、数据库等；这类配置通常处理器性价比较高，内存和磁盘配置适中。（3）重IO型：数据库等；这类配置在均衡型的基础上往往会将磁盘更换成SSD，以满足高IO负载的需求。（4）存储型：分布式存储等；这类配置注重单GB成本，通常会使用廉价高容量的SATA磁盘，对处理器和内存的要求并不高。分类后，同一类套餐可能存在多种配置，接下来就是配置近似套餐的抽象合并过程。 精简套餐数量，实现规模效应比如，另外一家公司总共拥有4个套餐配置，也面对4家服务器供应商，每次采购只需要管理和维护共16个配置。这样一来，无论是采购还是资源管理都变得简单、高效起来。同时，套餐数量越少也就意味着规模效应越明显，采购议价能力越强。这就是为什么连锁快餐店总是能把套餐价格做得很低，同时还能保证高效的供应，并且还能拥有较高的利润的原因，一切都归功于规模效应。 套餐配置分类以后总是能够出现配置接近的套餐，尽可能地合并这些配置，将相似的配置抽象成一种套餐，不断地精简套餐数量。比如，X和Y套餐仅仅在内存配置上存在差异：X内存128GB，Y内存64GB，而X需求量是Y的10倍，那么就应该考虑将Y套餐合并成X套餐，因为X套餐的采购价格很可能低于Y套餐。 成本的持续优化服务器套餐化带来了成本的大量节省，但追求成本的持续优化只能另辟蹊径。我们在如下几个方面尝试了一些探索。 多品牌服务器的引入通过多品牌差异化的竞争增加议价能力，降低成本。当然，引入多品牌服务器后很快出现了各种新问题，比如某些品牌服务器不支持共享口，BIOS配置标准各家有差异，管理口对运维支持情况各异等一系列运维可用性问题。很快，运维可用性测试便成为了多品牌服务器引入后的一项重要的测试指标。 通过可用性测试提前发现各种服务器机型的基础运维问题，以便后续要求厂商按照用户的运维习惯进行定制化。入围的品牌我们会按季度来总结故障率，对故障率不达标又没有完成可规避方案的品牌后续减少采购量，甚至不采购。 配件选型减少服务器规模前面提到了同类硬件尽可能要考虑一种规格，而且单机性价比才是最重要的，因此配件的测试选型也就显得格外重要。 同一类配件如SSD，也存在PCIe和SATA两种规格。假设某业务对存储设备的IO需求较高，而使用SATA SSD的方案单机最多能够提供的IOPs能力为20000，现在有一款PCIe SSD能够提供的单机IOPs能力为60000。 虽然同容量PCIe SSD的成本是SATA SSD的2倍，但从整机价格来看，PCIe SSD却只有SATA SSD的1.2倍，那么使用PCIe SSD可以有效减少2/3的服务器采购量，而单机成本仅提高了20%。因此，业务方全面使用PCIe SSD来缩小服务器投入规模，在降低成本的同时还减少了运维的开销。 硬件性能调优以往提升单机性能的方法就是升级硬件，如何在不增加成本的前提下提升硬件的性能呢？通常的做法是软件层面的优化，比如修改BIOS参数、修改内核、升级固件、修改操作系统参数等做法均能达到预期收益。 比如修改BIOS参数，打开处理器的Turbo（睿频）功能能够瞬间提升处理器的运行频率，提升处理器的性能（如图8-5所示是E5-2630v2处理器在开启Turbo前后的性能测试数据，可以看到打开Turbo后整数计算性能得到了15%的提升）。 图8-5 E5-2630v2处理器在打开Turbo设置前后的性能测试数据对比 再比如操作系统参数调优，举的一个例子是在SATA SSD加直连卡的硬件环境下使用中断多核绑定来优化IO性能。业务使用8块SATA SSD加一块直连卡的硬件方案，分别将数据保存在8块SSD中进行读/写。 在优化前业务的QPS仅能够达到27K，SSD的使用率为60%～70%，8块SSD的IOPs总和达到了80K。通过mpstat命令可以发现直连卡产生的IRQ中断全部压在了处理器的第一个核心上（如图8-6所示），产生了82K个中断，这和前面提到的IOPs数量吻合。 图8-6 优化前mpstat情况 这已经达到了单核的处理极限，而SSD的使用率却还有盈余。因此，考虑对直连卡做多核的中断绑定来优化I/O性能。通过/proc/interrupt可以发现直连卡中断号为107～118，因此执行以下命令对中断做多核绑定（如图8-7所示），将不同的IRQ分别绑定到不同的处理器核心上。 图8-7 执行多核中断绑定 优化后业务QPS达到了39K，SSD的使用率也提升至80%～90%，性能提升达到40%之多。硬件性能调优是最理想的成本优化方案，它在不产生任何直接成本的前提下就可得到单机性能提升的收益。 机柜高密度部署的探索业务对服务器的需求规模从几百台一下子增长到了几千台，服务器规模的突增给运维带来了很大的冲击。很快，公司原先的机架资源便面临枯竭。由于原先公司体量不大，在和各数据中心谈合作时很难及时获取到机柜资源，而业务却等着要上线服务。这个时候为了能够缓解机架资源的紧张程度，我们便开始探索机柜高密度部署，很快高密度服务器便进入了我们的视线中。 降低服务器运行功率通过BMC（BMC即服务器带内管理控制卡，它可以获取服务器状态和故障信息，以及控制服务器的停机、开机等电源操作）获取服务器实时运行功率，是另外一种实际且有效地掌握服务器用电情况的方法。结合大数据还能够分析出同配置服务器的功耗运行规律，为单机柜实现更高密度的部署提供真实的数据。 此外，还可以结合各种服务器功耗优化方案来实现单机柜更高密度的服务器部署。服务器功耗优化的方式有很多种，例如：（1）使用高标号电源（比如白金、黄金电源）提高电源转换效率，减少电源谐波干扰；（2）使用低功率电源提高电源负载率，减少转换损耗；（3）使用低功耗硬件设备，比如低电压内存和低功耗处理器等；（4）优化服务器风扇散热策略，在保证散热的同时降低转速，优化功耗。 硬件状态扫描和故障预警服务器达到规模后，故障维护也相应变得困难起来，主要体现在：（1）硬件故障无法提前预见。（2）硬件故障分类判断不精准。（3）硬件故障无法被及时感知。 人工干预的故障排查不切实际，这些棘手问题都给系统运维工作带来困扰。比如，某个服务器阵列类型是RAID 5，一块盘出现故障后，业务仍旧能继续正常运行。但是由于该故障无法被业务应用或运维人员及时感知，因此没有及时更换磁盘。随着时间的推移，继而出现了第二块磁盘的故障，最后导致整个阵列丢失了数据。在实际线上生产环境中，这类问题屡见不鲜。 大部分故障都是显性的在积累了大量服务器硬件故障维护经验后，我们发现大部分故障都是可预见的。可以通过系统、BMC（带内管理卡）日志或者硬件本身记录的日志，以及各种硬件状态检测工具进行故障定位。比如阵列卡本身带有对磁盘设备的管理功能，可以通过专门的工具获取到当前阵列卡、磁盘的状态信息（如图8-8、图8-9所示），并且阵列卡本身也记录了各类日志，可以方便地通过日志判断故障（如图8-10所示）。 图8-8 通过MegaCli工具获取到的阵列卡虚拟盘的状态信息 图8-9 通过MegaCli工具获取到的阵列卡下磁盘或SSD的状态信息 图8-10 通过MegaCli工具获取到的阵列卡终端日志 磁盘的SMART信息也能给诊断磁盘故障带来帮助（如图8-11所示）。此外，处理器、内存等故障也可以通过BMC的SEL日志获取到（如图8-12所示）。很快，我们就可以将故障按照部件进行分类，不同部件的故障定位使用不同的方法应对，工具和日志相结合，综合分析。这便给自动化硬件故障监控提供了技术基础。 图8-11 通过smartctl工具获取到的磁盘SMART信息 图8-12 通过ipmitool工具获取到BMC的SEL日志信息 统一硬件健康检查工具为了能够更加高效地管理监控服务器故障，统一硬件健康监控检查工具的设想很快便被提出来。前面提到大部分故障都是显性且可以分类的，因此如果能够编写一个对服务器不同硬件分别做状态检查的软件工具，就可以及时了解到服务器当前是否存在异常或者故障。 前面提到过不同服务器硬件的状态信息可以用不同的方法查看，而统一硬件健康检查工具可以通过调用相关的硬件检查工具，按照一定的方法逻辑检查硬件状态并分析日志，对服务器硬件整体健康状态做一一检查。其结果就是，工具在任何型号服务器上运行后便能输出当前服务器的健康状态信息：没有问题就不输出任何结果，而存在问题便输出对应的故障分类和具体的故障信息（如图8-13所示）。 图8-13 通过统一硬件健康检查工具输出硬件状态检查信息 常用的检查硬件状态的工具和方法如下。（1）磁盘类：MegaCli、hpacucli、smartctl等。（2）处理器、内存类：mcelog以及通过ipmitool工具查看SEL日志。（3）电源、风扇类：通过ipmitool工具查看SDR（传感器）状态。（4）其他类：通过ipmitool工具查看SEL日志。 再进一步，我们可以将工具与现有的监控系统结合，通过监控代理程序定期运行工具便能准确及时地获取到服务器的硬件故障信息。甚至还可以对故障的严重程度进行分级，以便运维人员在获取到报警信息后快速判断故障的严重程度。 磁盘健康检查工具在所有服务器故障中磁盘故障占了相当大的比例（一般使用SAS磁盘的服务器50%以上均为磁盘故障），因此很有必要对磁盘做自动化健康检查。磁盘系统作为服务器的IO设备是一个复杂且庞大的体系，它并不像内存那样只有相对单一的结构和规格，光是磁盘的接口类型就有SAS、SATA两种，而且还有阵列卡、直连卡等IO中间设备作为磁盘系统的一部分。通常使用阵列卡的服务器较多，下面来讲讲在阵列卡环境下如何实现磁盘的自动化健康检查。 磁盘健康检查工具的实现原理阵列卡作为磁盘系统的一部分，能够将多块物理盘通过不同的阵列算法组合成一个大的虚拟盘；同时阵列卡又承担起管理虚拟盘和物理盘的责任，对于虚拟盘和物理盘出现的错误进行检查和管理。而我们开发的自动化健康检查工具，正是基于能够和阵列卡信息交互的工具来了解虚拟盘和物理盘的健康状况的（如图8-14所示）。 图8-14 磁盘健康检查工具的实现原理图 市面上的服务器大部分使用的阵列卡是基于LSI公司设计生产的芯片，因此和阵列卡的交互都可以通过MegaCli工具实现。 通过MegaCli工具可以获取到虚拟盘和物理磁盘的拓扑结构以及一些基础信息，比如某个阵列卡下有一个虚拟盘VD0由物理盘PD0、PD1、PD2组成，阵列类型是RAID 5，缓存设置为No Read Ahead、Write Back，并且当前状态为Online等；物理盘PD0、PD1、PD2分别对应的槽位ID为slot0、slot1、slot2，并且磁盘状态均为Online；以上信息说明虚拟盘VD0和物理盘PD0、PD1、PD2均处于Online状态，没有任何故障出现。进一步，我们可以单独了解PD0的更详细信息。 比如它的某项错误计数器Media Error Count数量已经大于100，表明已经有超过100个的物理坏道出现，但是磁盘仍旧处于Online状态，这说明磁盘虽然还能正常工作，但是已经处于非健康状态，这块盘很可能会在近期出现故障，因此通过MegaCli工具获取到这些信息提前预见了故障。通过上面的这些判断逻辑，我们将健康检查分成两类：虚拟盘的健康检查和物理盘的健康检查。 定义不同级别的故障信息为了能够区分磁盘故障的紧急程度，我们根据故障紧急程度的不同定义了不同级别的故障信息通知INFO、WARN、ERROR和FATAL。 ◎ INFO：information，即信息级别，通知用户磁盘处于非异常也非完全健康的状态，紧急程度最低；比如虚拟盘正处于修复状态中。 ◎ WARN：warning，即警告级别，通知用户磁盘系统存在一些问题，但是并非完全被定义为故障；比如出现Unconfigured Good状态的磁盘，可能是虚拟盘出现了掉盘的问题。 ◎ ERROR：error，即故障级别，通知用户磁盘系统出现了故障但并不致命，不会出现数据丢失的情况，级别比WARN高，需要用户尽快介入处理故障以避免故障级别升级；比如一组RAID 5的虚拟盘出现了一块磁盘故障的情况，虽然虚拟盘仍旧可以提供IO服务，但是无法再忍受更多的磁盘故障，因此需要尽快更换故障盘。 ◎ FATAL：fatal error，即致命故障级别，通知用户磁盘系统出现了致命的故障，已经无法提供IO服务，并且有极大的概率会出现数据丢失的问题；比如一组使用RAID 0的虚拟盘有一块磁盘出现故障，致使虚拟盘已经无法继续提供IO服务，并且数据全部丢失。 虚拟盘健康检查的设计逻辑根据虚拟盘的不同状态来判断其健康状况，虚拟盘分别有Optimal、Offline、Recovery和Degraded四种状态。 ◎ Optimal为最佳状态，意味着虚拟盘目前处于健康状态，没有任何故障和异常。 ◎ Offline为离线状态，意味着虚拟盘已经出现了严重的故障，比如RAID 0出现一块磁盘故障，或者RAID 5出现两块磁盘故障，表示虚拟盘已经不可用并且数据有丢失。 ◎ Recovery为虚拟盘修复中状态，意味着虚拟盘之前出现了磁盘故障并降级，更换故障盘后阵列卡对新盘进行有效数据填充的过程。 ◎ Degraded为虚拟盘降级状态，意味着带有阵列保护的虚拟盘出现了磁盘故障，但不至于对虚拟盘的数据完整性造成破坏，比如RAID 5出现一块磁盘故障就会导致虚拟盘降级。 通过上面的这些状态我们就可以设计一套判断虚拟盘是否健康的逻辑，具体的设计逻辑图如图8-15所示。如果检查一个虚拟盘状态为Optimal，则不输出任何信息，直接检查下一个虚拟盘。检查状态为Offline，则按照最高级别FATAL输出错误通知；状态为Degraded，则按照比FATAL低一级别的ERROR输出错误通知；状态为Recovery，则表示处于非异常也非健康状态，因此以INFO级别输出错误通知。 图8-15 虚拟盘健康检查的设计逻辑图 物理盘健康检查的设计逻辑物理盘也有类似的一些状态来判断是否健康，如Unconfigured Good、Unconfigured Bad、Online、Offline、Rebuild等。 ◎ Unconfigured Good为未被配置为阵列的健康盘，可能是新加的磁盘或者是某个阵列出现了掉盘都会被阵列卡设置为这种状态。 ◎ Unconfigured Bad为被阵列卡判断为异常状态的磁盘，可能之前属于某个阵列，但是由于出现错误较多而被阵列卡从阵列配置中踢出，也有可能是新加入的磁盘被阵列卡检查健康状态后判断为故障盘。 ◎ Online为在线状态，但并不意味着磁盘本身不存在问题，它仅仅代表该磁盘尚能提供IO服务。某些出现坏道而引发降速的磁盘，由于尚能提供IO服务，仍会被阵列卡判断为在线状态，因此还需要增加其他的因素去判断一块磁盘是否真正健康。 ◎ Offline为离线状态，拔盘或者因为接口通信异常而掉线的磁盘都会被阵列卡设置为该状态，更严重的是磁盘故障到已经无法通信的情况也会被设置为离线状态。 ◎ Rebuild为修复中状态，当一块新盘被用以替换故障盘而参与整个虚拟盘数据修复时，阵列卡会将该盘设置为修复状态。 ◎ 此外，上面讲到即使一块物理盘处于Online状态，也无法判断为健康状态，因此物理盘比虚拟盘还多了一种可以辅助判断是否健康的因素，那就是错误计数器：MEC（Media Error Count）、OEC（Other Error Count）、PFC（Predictive Failure Count）。 ◎ MEC（Media Error Count）为物理坏道计数器，通常指的是磁盘出现无法修复的物理坏道数量，该计数器积累到一定程度后即可判断为磁盘故障。 ◎ OEC（Other Error Count）为其他错误计数器，指的是物理坏道以外的错误数量。由于磁盘是一种比较精密且复杂的机械和电子部件，因此任何部件的异常磁盘都有一种检查机制能够感知并写入磁盘的SMART信息中，因此该计数器可作为判断磁盘是否故障的辅助手段。 ◎ PFC（Predictive Failure Count）为故障预警计数器，它通常集合了一套判断磁盘是否即将故障的逻辑，阵列卡根据磁盘提供的各种错误代码和信息判断磁盘即将故障，该计数器积累到一定程度后即可判断为磁盘故障。 通过上面的这些状态和错误计数器，我们就可以设计一套判断物理盘是否健康的逻辑，具体的设计逻辑图如图8-16所示。如果检查一块磁盘为非Online状态，就会开始一系列检查：Unconfigured Good状态会以WARN级别输出错误通知；Unconfigured Bad或Offline状态均会以ERROR级别输出错误通知；而Rebuild状态则会以INFO级别输出信息通知。 最后，无论该物理盘状态是否为Online，都会对计数器值进行检查判断，如果三个计数器有任何一个超过临界值，都会以WARN级别输出错误通知，以及时通知用户磁盘存在异常。 图8-16 物理盘健康检查的设计逻辑图 故障信息的输出格式关于健康检查的故障通知的输出设计逻辑是：检查到任何异常便输出通知，没有异常不输出任何信息。因此，用户可以专注于出现故障信息的服务器，而不必被一些过多的状态信息所干扰。此外，对于信息的输出格式我们也有精心设计，一条故障信息由故障定级、故障来源和故障原因三部分组成。具体的输出格式演示如图8-17所示。 故障定级：前面提到过错误信息的定级分类，错误输出信息里也应该有这部分内容，以便用户在获取到信息后再次对错误信息按照紧急程度进行分类，增强用户体验。 故障来源：错误信息中应该包含具体哪个虚拟盘、哪块物理盘的消息，甚至可以告知用户物理盘的槽位号，帮助用户精准定位故障来源。 故障原因：有了错误信息的具体来源还不够，还需要告知用户具体的错误原因，比如某块物理盘处于Offline状态，某个虚拟盘由于掉盘而出现Degraded状态，这些具体的错误原因都要在错误信息中输出，帮助用户更好地判断如何处理故障。 图8-17 故障信息输出格式演示]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Hardware</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第七章：资产管理]]></title>
    <url>%2Funder-the-ops%2F20170923-07-asset-management%2F</url>
    <content type="text"><![CDATA[资产管理平台是记录和管理企业IT架构中各种设备配置的信息平台。 在系统工作中，它与所有服务支持和服务交付流程都紧密相关，最大价值是支撑流程的运转。同时又依靠流程关联来保证数据的准确性。对上层服务管理平台，能通过API提供数据查询能力，如设备的硬件配置、机架位等信息；同时也提供服务器的重启、报修和系统重装等接口。 系统运维的早期阶段，主要是用Excel记录各种信息，用邮件来做各种流程审批。 随着业务的快速发展，设备数量爆发式增长，系统工作中的配置变更越来越频繁，信息遗漏变得非常严重，不断产生脏数据，已经不具备准确性和可用性。同时对上层系统的查询需求也无法响应，这种靠Excel维护信息的方式已经不再适用。 我们开始在资产管理平台研发上投入专门的人力，并和工作流程强关联起来，通过流程保证资产平台数据的准确性。资产管理数据的准确性和功能完备性决定了整个系统运维的自动化程度。因此，要支撑公司不断增长的服务器运营，首先需要建立和维护一套基础架构的资产管理平台。 资产管理平台主要规划了配置管理、预算系统、故障管理和自助系统四大部分，如图7-1所示。 图7-1 资产管理平台规划的四大部分 业务通过预算系统提交服务器申请，系统工程师通过配置管理平台录入服务器信息，再通过API的形式将服务器信息同步给服务管理平台。 业务运维工程师可以通过调用自助系统对服务器进行重启、重装等操作。当服务器发生故障时，服务管理平台将服务器标记为故障节点，通过API的形式将服务器故障信息同步到故障管理平台进行维修，维修完成后故障管理平台也通过API将信息同步给服务管理平台。 当服务器要归还时，服务管理平台清除相关信息，通过API的形式将服务器同步给资产管理平台。 配置管理配置管理一直被认为是ITIL服务管理的核心，因此在系统运维中也是核心部分，它在结构上处于最底层。 为什么说配置管理的作用是核心的？举个例子。某服务器已经到了报废期，我们需要下架这台服务器，需要有系统记录这台服务器的数据中心、机架等属性，也需要记录这台服务器的硬件信息，依靠这些信息才能找到这台服务器。 但仅有这些信息还不够，仍需要知道这台服务器的使用者（即业务归属），通知其业务方将业务下线，确认下线后，需要再通知数据中心现场人员下架服务器，同时还需要释放服务器相关的其他资源，比如IP地址、ACL配置、LVS配置等，这一系列信息都需要由配置管理系统来维护和保存。因此可以看到，配置管理是一项非常重要又多信息关联的系统工作。 早期我们同样是用Excel记录数据中心、服务器（资产编号、参数、采购时间、供应商）、配件、网络设备、IP地址、ACL等信息。 随着设备数量的增加，配置变更频繁，又没有统一上线和变更流程，经常出现记录和实际有差别。比如服务器机架信息核对不上，设备实际用途和记录不符，甚至服务器下架了IP地址却没有释放，导致IP地址占用和冲突等问题。 通过配置管理系统的研发，强化了这些资产和配置信息的管理，准确度有显著提高，系统工作以及与外部系统的自动化联动开始向前发展。 在配置管理系统中，我们将各类信息管理定义成小的功能模块，通过API接口串联起来，实现灵活有序的方式管理各类资源。在机柜管理上开发机柜视图功能，把服务器的机架信息、位置占用等录入系统，在系统中可以非常直观地了解服务器的摆放情况、整体机柜使用率。 设备上线前由系统自动分配摆放位置，设备下线后自动释放资源，从而减少了手动分配的工作量。机柜管理还有一些高级功能，在网络规划中按照安全级别划分出了基础服务、普通业务和安全隔离三个区域，系统会按照设定的分配规则给不同的服务器分配不同区域的机柜。 比如，Hadoop等分布式业务对机架摆放有特殊要求，太分散不利于数据节点间的数据同步，太集中冗余度又不够，这些特别的需求都有特殊的分配策略。 通过定期扫描机柜使用率，可以及时调整和关闭空闲机柜，从而避免浪费。开发的iHealth具有运行功率实时监控功能，可以从服务器监控中采集电流信息，能统计出整个机柜的电量使用，进行机柜的超电预警。 在IP管理系统中，把内网、公网、VIP、管理卡都分别设定了不同的IP池，根据类型、机柜、业务的不同，通过管理接口分配和回收IP地址，从而减少了工程师手动分配和回收IP资源的烦琐工作，进一步保证了IP资源管理的准确性。IP管理和ACL管理等模块都有关联功能，在IP地址释放前都会查询并释放相应的ACL记录。 为每台设备设定一个唯一序号，作为主键使用，确保设备的唯一性标识。服务器在记录中的占比最大，网络设备其次。在虚拟化部分中虚拟机也有唯一性标识，并和底层的物理服务器有关联记录，可以快速地查询物理服务器上承载了哪些虚拟机。和虚拟化部分有API关联，虚拟机的飘移会实时地更新记录。 为每台设备粘贴了对应的序号二维码标签。在收货、上下架、资产管理、设备报修等环节，通过扫描二维码的方式进行信息的变更和查询，极大地提高了设备管理的效率和准确性。二维码系统设有权限管理，比如现场的维修人员和系统管理员获取的信息量是不同的，主要是出于安全考虑。 经常会遇到IP地址随着服务器下线释放，但ACL没有及时清理的问题，久而久之，积累的ACL记录很容易达到设备的上限，存在一定的安全隐患。 我们遇到过一个真实的安全案例，两个区域隔离的交换机由于设备BUG的问题，在ACL条目达到上限后并没有任何错误提示，还可以添加ACL记录，但原有的ACL记录完全失效，导致两个区域的隔离失效，带来了安全问题。随后我们上线了ACL管理功能，业务通过系统发起ACL申请，完成审批后由工程师进行变更操作；ACL和IP资源联动，及时清理无效的ACL记录；对ACL记录匹配次数进行监控，对于长期未使用的记录进行人工检查和清理。同时，我们提供API接口，安全人员通过接口获取ACL记录信息，扫描验证ACL记录的有效性。 预算系统在早期运维中，设备采购数量少，申请采购主要依赖邮件审批的模式。系统组负责整理汇总各个需求邮件，再进行统一采购上架，流程非常烦琐，而且容易有遗漏的地方。采购的进度也不透明，业务线无法及时获知进度。 为了简化和规范设备采购的方式，我们上线了预算系统，该系统作为设备资源的唯一入口，包括服务器、网络设备、配件等。 对于服务器资源，支持实体机、基于OpenStack的私有云、公有云以及整机租赁等多种资源的申请。私有云和公有云等通过调用API创建虚拟机，同步录入管理系统；对于租赁的实体服务器通过API下单；自购的服务器先从资产管理系统查询是否有满足需求的备机资源，如果没有再发起采购。 预算系统和监控系统的资源利用率对接，为业务申请采购服务器提供数据说明。当业务运维在采购管理系统发申请之后，审批人根据已有的资源使用情况来审核申请的合理性。 故障管理系统随着服务器数量的增多，每天发生故障的服务器的数量也呈线性增长。早期系统工程师需要逐台定位服务器故障原因，和业务部门沟通停机时间以及跟进供应商的报修，效率非常低下，业务部门也常常因为故障服务器维修周期长而增加烦恼。为了改善故障持续增长造成的维修效率低下，开发了故障管理系统。 通过监控插件来上报故障信息，并完善插件使输出的故障信息详细、准确，能直接将信息发给数据中心现场或供应商来解决，比如磁盘故障会定位到槽位等。 通过服务管理平台将收集到的监控插件上报的数据信息展示在页面中，相应的负责人可以查询到自己的服务器故障信息，在负责人确认服务停止之后直接点报修。 故障管理系统通过API和服务管理平台同步故障信息，系统汇总整理后通过邮件或对接的API分别发送给供应商和数据中心现场。由于邮件中有供应商和数据中心维修需要的各类信息，所以基本上普通故障在两个工作日内就能修复，修复好后也无须人工确认，监控上报信息中自然会消除相应的故障信息。 如有需要手工干预的操作，也会通过监控系统体现出来，比如磁盘需要重新挂载和格式化等。故障都消失后，更改设备状态，即可交付给业务。故障管理系统大大提高了报修的效率，也提高了硬件监控的准确性和有效性，为线上服务的稳定运行提供了底层支持。 故障管理系统建设的上下游是应用运维和供应商，需要大家一起来协调工作，为了配合默契，一起约定梳理了一些标准。 ① 故障信息的描述早期报障需要应用运维人员填写描述信息，不统一、不标准，对诊断和定位故障影响较大。后来约定的方案是先将常见故障分类，实现自动化采集并分类。开发了监控和识别的程序，自动发现和告警给应用运维，由他们判断是否需要报障维修。针对常见故障以外的案例，通过人工分析和识别重新定义到故障类型中。 ② 维修前是否要和应用运维确认早期在维修服务器前要和业务沟通好时间，业务停止服务后再修，花在沟通和跟进进度上的时间非常多。维修前需要确认停服的根本原因是维修的周期较长，很多时候故障服务器仅仅是在排队维修，而这类服务器依然可以正常工作，所以业务方希望仅仅是在维修的时候再暂停服务。 后来达成共识，应用运维要停完服务后提交报障，系统组承诺在一定时间内修复完毕，若是无法完成，需要提供备机。当然，由于承诺了对不同故障类型的响应和维修时间，也就无须反馈维修进度，应用运维也不会再询问，大大减少了沟通时间。 ③ 和供应商定义诊断方式和处理机制早期对于服务器故障定位都是系统组人员和供应商的工程师一起完成的，无法自动化，后来和供应商沟通，让其提供各类故障定位工具，并将相应的日志作为更换配件的标准，也和供应商对不同级别的故障响应和处理时间达成一致，方便批量报修。 自助系统为了更方便地对服务器进行操作，开发建设了一些自助系统，主要有以下功能。 ① 自助查询功能当交换机发生故障或有计划割接时，可以通过交换机维度查询到同一接入交换机的所有服务器列表，从而达到快速的通告效果。业务线可以通过自助查询页面批量输入主机名查询服务器在数据中心中的分布情况，为业务的冗余性部署做准备。还可以通过源IP地址和目的IP地址查询ACL规则等。 ② 自助重装、重启功能在系统运维平台中开发了自动重装和重启功能，逐步地开放给了应用运维，应用运维在工作平台中可以直接调用API完成OS重装工作。发生故障时可以自助地重启、停机，提高了应用运维对故障的响应时间，也避免了系统工程师的重复性机械工作。 ③ 屏幕截图功能程序异常时可能导致kernel panic，在这种情况下一般需要系统运维登录管理卡查看服务器屏幕状态并截图给业务定位故障。后来开发了屏幕截图功能，并通过API给服务管理平台使用，业务运维只要在网页中点击按钮就能看到服务器的屏幕截图，如图7-2所示。 图7-2 自助服务器屏幕截图功能 通过资产管理系统的建设，资产和配置的准确性得到了有效保障，系统运维的效率大幅提升，系统日常运维走出了依靠人工信息维护的初级阶段。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Asset Management</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第六章：系统运维概述]]></title>
    <url>%2Funder-the-ops%2F20170923-06-sys-ops-introduction%2F</url>
    <content type="text"><![CDATA[提到系统运维大家可能较多想到的是和OS（Operating System）类相关的工作，但在这里我们延伸并扩展了它的范围。我们把运维里整个基础资源和基础服务的建设，都涵盖进来，比如数据中心建设、网络规划、CDN服务。 如果能把运维比喻成一栋大厦，每个楼层入驻的是不同业务，系统运维就是大厦的根基和供水供电系统。满足海量业务的资源需求和确保服务的稳定运行是系统运维的核心价值。 数据中心和网络建设移动互联网爆发，快速地占据入口是制胜根本，而一个平台的建成和完善至少需要2～3年的时间。 回顾一些成功的互联网公司的业务发展多数是典型的从无到有，直接到爆发，从业务曲线看，几乎是无平滑的过程。因此系统建设面临了巨大的挑战，这其中最凸显的是数据中心和网络建设。 初创公司前期是不会自建数据中心的，一般租用ISP数据中心，然而中国的ISP是大欺小的垄断状态，根本没有平等对接；政策把控得严，互通只能在骨干做。 推进技术前进的永远是需求，在骨干城市出现SP通过单一IP代播静态实现的BGP链路，在一定程度上曲折地改善了中国的互联互通质量。静态BGP资源满足了业务自身没有用户调度能力的场景。 创业初期，业务功能和未来增长规划不明确，很难做到资源的预估，在数据中心选择上，往往发展不到一年就出现瓶颈，预留又会有过多的成本支出。 有一段时间，我们数据中心的发展一直处于“蜕壳式”成长，通过不断建设新的、更大的数据中心来满足业务。但业务搬迁耗时耗力，服务中断影响用户体验；网络中核心设备不易选型，核心设备如果不投入高端型号，后续扩容升级难度大，替换下来的设备无法再利用。在那段时间里，系统运维的工作是非常痛苦的。 为了解决数据中心和网络建设“蜕壳式”的不足，规划和建设进入了第二阶段。我们把数据中心和网络建设划分成核心数据中心、节点数据中心、传输网三部分。 核心数据中心全部采用“双中心”+异地备份规划，用来承载最主要的业务逻辑单元，关键的数据和内容都放在这里。加强了SmartDNS的建设，通过View支持、HTTP支持、EDNS扩展多种能力，提升浏览器访问质量，同时在客户端层面，通过HTTPDNS来提升稳定性和访问质量。系统运维和应用运维、研发一起把业务做到具有多站点部署的能力，实现业务“双活中心”的架构。 一个新数据中心从选择到可以上线业务需要4～6个月的时间。所以选择的数据中心一定要有能支持规划内和应对突发事件能力的机柜容量，数据中心供应商最好是有二期、三期的建设计划。 规划需要充分调研各业务线的业务增长，我们每年的Q3会把业务线的技术负责人召集到一起，收集至少未来一年的增长计划，突发增长部分的支撑，需要根据历史突发概率的评估来确认，通过商务手段做一些预留。核心网络的规划遵循了一个原则：核心网络设备要适当地超前投入。 双中心间数据要实现同步，逻辑上业务自己解决，物理上要依赖传输网（传输网：数据中心互联网络，也称为DCI（Data Center Interconnection））来打通。 我们规划建设了两个链路汇聚点，汇聚的物理节点位置都是自有的或长期租赁并有物业管理权的，能自由进出光纤。通过光纤把汇聚点和数据中心进行互联，每两个点之间至少链路双路由互备。传输网建设完成后同时也解绑了链路和机柜的关系，更为建设自己的BGP、带宽临时调度做好了铺垫。 另外，还把我们国内的其他几个节点数据中心连接起来。海外业务发展得也很快，在中国香港地区建设了两个汇聚点，国内的核心和所有海外的数据中心专线都和香港地区的连接起来。 成本虽然会比较高，但是完美地解决了全球的联通性问题。之前没有专线的时候，尝试过走广域网、加密和非加密的VPN，服务可用性基本没法保证，各种链路差、被拦截、被丢弃、解包。长途部分因为距离长，光纤断的几率自然会高很多，所以长途专线尽量选择有保护的链路。 节点数据中心的应用场景是CDN Cache类，业务结构就是分布式的，有冗余的能力。地域选择主要在东部沿海地区，华中、东北、西北、西南少量覆盖节点。这和中国的网民用户分布有直接关系。 节点数据中心的选型过程一般是先分析用户分布，然后预选几个点，通过自开发的用户访问质量系统（UAQ），真实用户抽样测试选出两个点做互备。因为节点数据中心会受到自身容量、骨干拥塞的影响，所以质量是动态的，建设完成后会通过在用户访问的接入主机做TCP连接和传输速度的分析，通过数据来周期性地调度优化，也会新建优质节点和撤离质量变差的节点，进而形成了一个持续优化的闭环流程。 数据中心内部的选择也尤为重要，硬件条件主要考量建筑、电器、机械3个部分，都要做到T3标准以上。其他部分考量Internet接入、网络攻击防御能力、扩容能力和空间预留、外接专线能力、现场服务支撑能力几个方面。一般也是预选一批，然后采用分规格和权重打分的模式综合评比。 海外业务的支撑主要选用了相对成熟的AWS云服务。应用较多的是S3存储、EC2、CDN加速几个产品。 资产管理的演进主机和网络的交付是资产管理中主要的内容；系统管理平台的建设要跟上不同量级下的需求。 运维的最早期，采购需求是没有计划的，多是业务增长了就零散地购买。购买完成后设备上架，逐台装机，然后将权限交付给应用运维，资产靠Excel记录。随着采购量的提升，逐渐变成了每周有采购、每天有采购，到货批次混乱、无序，原始的手段已经逐渐不能满足发展的需求。 为了改变这种状态，和业务部门做了深度沟通，把零散采购归纳为预算梳理，按月度采购。再后来通过设备Buffer池的建设，改为按季度采购，但仍不能避免有一些紧急采购。 虚拟化主机推广范围扩大，对业务自身的隔离、成本的优化都有显著成果。主要在预发布、测试、接入层、中间层的场景大量使用。 在物理机方面，根据业务类型差异对机型进行了定制，这里讲的只是基于成熟的品牌型号进行电源、CPU、内存、存储的定制。还需要结合接入交换机的端口数来布置。我们通过使用高密度机型，最高做出过一个机架45个计算节点、13A电、1台48口交换机的标准，但也存在搬迁节点困难的弊端，并非所有场景都适合。 提供资产管理平台，对物理资产信息进行管理记录，对上层平台提供数据信息。按季度采购的主机需要在短时间内集中化交付，所以开发了高并发的自动装机系统。逐渐把系统运维的工作服务化交付，提供给应用运维主机重启、系统重装、Console查看等基础功能。和应用运维管理平台定义交互接口，实现资产管理系统和运维管理系统的对接。比如初始化完成的主机自动Push到运维管理平台的服务树节点中，应用运维可以进行后续环节的操作。 当设备数量上规模后，硬件、软件故障会成为一种常态，故障的响应处理需要分级别，并制定服务响应等级（SLA）。目前我们的系统管理平台将所有的故障报修服务化起来，对其他平台开放API，实现报障自动化。每周、每月会评估故障总量、平均故障处理时长，从而挖掘出不合理的环节并优化。 网络设备的配置实现了标准化版本管理，每台设备的配置可以在系统中查看、复制等。 重新设计了设备申请和交付流程，实现不同资源申请对用户同一接口输入/输出。把使用到的公有云、主机租赁、自购主机多种资源的申请，都封装到统一入口，把现有设备使用率监控系统加入到审批环节，并把交付进度可视化，真正服务化运作起来。 临时性、有突发增长的业务优先使用公有云或私有云。随着公有云和开源私有云系统OpenStack的快速发展，公有云计算、存储服务都已成熟，自建私有云交付能力也大大增强，很多场景完全可以使用云化的资源。 强IO需求的DB、Hadoop服务，仍然跑在自建数据中心。物理主机+混合云的多构成资源模式，将是接下来要重点探索和实践的方向。 基础服务在服务方面，提供稳定的DNS域名解析、LVS负载均衡、SNAT集群出口、CDN加速以及各种基础保障服务（YUM、NTP、SYSLOG）等，并开发对应的管理系统，如LVS配置的Portal和对自动部署提供API接口、DNS调度管理系统等。 同时，系统运维需要配合业务进行服务优化、硬件优化、内核优化来满足不同的业务场景（CDN大小文件、域名解析、LVS）。像DNS、LVS都是上层业务强依赖的服务，服务短时间的中断都会造成大面积故障，服务可用性都需要定义到99.99%以上。要达到高可用性，在架构方面需要设计高可用的集群方案。通过对网络设备以及主机的带外、带内监控及时发现异常。 除了基础设施的建设，安全在系统运维工作中也同等重要。比如网络的公网区域是所有用户访问的第一入口，需要在边界做黑白名单机制，如对22等端口做第一防线的防护。 站点知名度提升后，各种攻击就会成为常态，需要在网络方面做DDoS防护，特殊的业务入口有流量清洗服务，LVS做半连接攻击等防护。内部特殊安全区的业务需要各种ACL，对ACL的管理是非常痛苦的事情，我们把这部分功能都流程化审批和管理，将来希望能够做到自动化线上配置。 我们定义了一个第三方CDN和自建CDN共存的生态环境，选择市场上2～3家成熟的CDN供应商，细致分析自身业务的特性，比如分析总内容量、大小、曲线规律等，让业务能使用到最适合它的资源。同时做到一个源站对应多个前端CDN，当一家的服务出现异常时可以快速实现切换。 在业务CDN使用量95%值达到30Gbps的时候，选择了自建CDN。出发点主要有两个：一是追求质量可控；二是整体成本优化。 在本章中，我们把系统运维涵盖的内容整体串讲了一遍，对系统运维有了一个整体的认识。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Sys Ops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第五章：运维的未来]]></title>
    <url>%2Funder-the-ops%2F20170923-05-the-future-of-ops%2F</url>
    <content type="text"><![CDATA[DevOps的出现相对于瀑布开发模式，敏捷开发过程的一个基本原则就是以更快的频率交付最小化可用的软件。高频率的部署，每天数百个版本的发布，经常会由于运维部署自动化程度的不足，导致部署任务堆积在运维人员的面前。 对于研发人员而言，线上服务运行环境对其属于黑盒，研发、测试和上线时的行为不一致，研发和运维之间的沟通错位，会造成各种部署问题。 为了解决传统意义上的研发行为和运维行为存在的脱节现象，提高持续交付的效率，DevOps的理念应运而生。为了适用与DevOps相关的快速部署节奏，ITIL流程的很多方面，特别是围绕着变更、配置和发布流程方面，需要将所有过程尽量自动化起来。 伴随着DevOps的理念，涌现出一批优秀的开源软件，比如Jekins、Puppet、Chef、SaltSatck、Docker等。Jekins属于持续集成的自动化构建工具；Puppet、Chef、SaltStack属于配置管理和任务执行类工具，方便我们快速同步变更到所需要的环境中。 Docker是近两年来最火的开源项目，它在Cgroup、LXC基础之上封装，基于Linux内核支持的NameSpace进行资源隔离，实现了进程级虚拟化。 早期我们考虑服务整体部署的时候，抽象认为每台服务器类似一个终端设备，每个需要被部署的服务是其上安装运行的App应用。为了达到服务整体部署，不破坏系统纯净的环境，我们制定了一系列部署要求。比如，要求服务所依赖的相关组件自包含、自解决；制定线上目录规范，进行环境隔离；要求所有服务必须有统一的启停接口，方便运维人员或系统进行控制管理等。 在Docker出现后，这些部署需要考虑的前置条件都迎刃而解，Docker逐渐成为我们整个运维体系中一个不可或缺的关键组件，也是部署自动化、动态调度部署的前提。 研发人员通过Docker进行服务封装，能够很方便地在开发、测试和线上环境中部署运行服务，达到部署行为的幂等。在此过程中，运维人员不需要过多的参与，只需要把Docker容器放置在合适的地方启动即可；在有IaaS、PaaS或者自有调度系统支持的情况下，甚至部署Docker容器、启动容器和监控服务的工作都可以由系统自动完成，整个部署工作不需要运维人员参与。 云服务的普及近些年来，随着云服务概念的普及、云服务厂商的持续投入、技术的不断发展，云服务的单位成本在持续下降，云服务已经成为一种不可阻挡的趋势。这给传统的IT建设理念，造成了一定的冲击。 传统的应用部署，需要兼顾应用程序、数据、运行时环境、中间层、操作系统、虚拟化、服务器、存储、网络等很多方面，单位建设成本和维护成本很高，同时对专业人才的需求更多、要求更高，无形中增加了业务运行的成本支出。图5-1描述了传统IT建设和IaaS、PaaS所关注的不同层次。 图5-1 传统IT和IaaS、PaaS方面的对比 IaaS（基础设施即服务），在传统应用部署的基础上，将OS以下的层次都负责起来，即操作系统、资源虚拟化、服务器、存储、网络，减少运维工作量；OpenStack作为领先的开源解决方案，也是我们运维体系建设的重要方向之一。提供商业服务的AWS，也是属于IaaS平台典型的代表。 安全问题是影响我们使用云服务的关键因素，和传统运维不一样，网络设备、宿主机等资源的权限都在云服务厂商手里，网络流量的出入也需要经过他们的设备，云服务厂商自身的安全规范和审计还比较薄弱。 随着HTTPS的大量使用、VPC的成熟、云服务厂商自身安全加固等，这些安全因素也在逐渐减小。当然，由于所有用户使用虚拟机共享宿主机，虚拟机被攻陷获取宿主机的安全问题还依然存在，比如2015年5月份披露的毒液漏洞（VENOM，CVE编号CVE-2015-3456），攻击者可以在有问题的虚拟机中进行逃逸，获取宿主机代码执行的权限。 现在也有一些云服务厂商，比如金山云也提供了混合云的服务，我们可以把非关键服务部署在云服务上，把关键服务部署在物理环境中，由我们控制物理设备的权限，通过公网或者私有专线进行通信，缓解云服务暂时还不能满足的安全需求。 云服务的出现，我们不需要再关注OS层面以下的问题。随着云服务的逐渐成熟，能够提供足够的资源储备和交付效率，我们不需要投入大量的人力在机房建设、服务器采购和网络管理方面，也不需要储备更多的资源应对突发业务、网络攻击等。对于公司而言，资源得到进一步优化，不需要那么多的基础运维人员，而且效率比以往更高了。 PaaS（平台即服务），在传统应用部署的基础上，将应用的运行时环境、中间层通过规范化的方式给管理起来，并实现容量的自动伸缩。本质上，PaaS是作为一种应用部署的规范，并通过相应的机制来保障该规范的实施，以及进一步地实现资源的动态调度，达到容量自动伸缩的目的。 CloudFoundry属于开源PaaS平台的典型代表，只要研发程序遵从一定的约束条件，服务的运行发布、扩容和故障容灾都由PaaS平台统一负责。PaaS由于具有强约束性，主要适用于简单业务，但不能满足比较复杂的业务架构。 我们可以根据PaaS的特性，通过Docker容器将业务进行封装，使业务达到可随意部署和资源隔离的程度，并参考Borg的设计思路定制动态调度系统，通过系统调度服务的部署和监控，减少对应用运维人员的依赖。 云服务厂商还会提供其他各种类SaaS（软件即服务）的服务，比如CloudWatch、EMR（Elastic MapReduce）、RDS（Relational Database Service）、CloudFront等，进一步降低对运维人员的依赖，运维人员不需要搭建和运维相关的基础设施或服务。 比如RDS提供丰富的数据库主从搭建、数据迁移、慢查询监控等功能，不需要数据库运维人员投入过多精力在数据库日常运维方面，可以把更过的精力投入在数据库设计和优化方面。StatHat属于SaaS类的云监控服务，我们自研的监控系统是参考它设计的。 网络虚拟化在传统网络架构中心，根据业务需求部署后，如果发生任何变更，都需要重新修改网络设备（如交换机、防火墙）的配置。这是一个非常痛苦的过程，并且不同品牌网络设备的OS都各不相同，统一配置的难度可想而知。 在移动互联网瞬息万变的今天，高稳定和高性能的网络已经不足以支持业务的需求，灵活性和敏捷性更为关键。传统网络模式的问题在于它的封闭性和自治性很难与应用直接产生关系。 随着SDN（Software Defined Network）技术的成熟，可以让其变得更轻而易举。SDN提出将控制层面和数据层面分离，通过集中或分布式控制器统一管控，通过OpenFlow协议提供网络的可编程能力，让用户可以在网络上定义各种应用程序，通过软件来定义逻辑上的网络拓扑，以满足对网络资源的不同需求，而无须关心底层网络的物理拓扑结构。 在未来，也许网络设备也类似一台台服务器，由软件控制和管理，不需要过多依赖网络运维人员。 总结运维的未来是充满危机和挑战的，从当前虚拟化和云计算的发展趋势来看，未来的互联网服务一定是依托在云端。通过云服务厂商提供的各类服务组件，结合DevOps的理念，将运维所涉及的工作串联起来、自动化起来。 运维人员的工作逐渐由体力密集型向脑力密集型转变，不再需要大量的运维人员从事那些基础建设、服务变更、故障处理等烦琐的体力工作。 运维工作更大的方向是提供平台化的运维产品，提供运维数据的可视化、运维工作的自动化，由操作、响应类转变为优化、规划类的工作内容。甚至连传统的硬件设备管理，也逐渐地通过软件层面去实现，更加的高效和自动化。 作为运维人员，已经不能和以前一样，只关注某个产品、某个领域的专业技能运维，我们需要关注技术的变化趋势，拥抱变化，做好运维转型，成为云服务建设或使用的专家，成为业务规划或性能优化的专家，或许……]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Future</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第四章：服务器资源使用率]]></title>
    <url>%2Funder-the-ops%2F20170923-04-source-usage-of-server%2F</url>
    <content type="text"><![CDATA[服务器数量达到一定规模后，老板们开始担心了：“每季度这么大金额的服务器采购支出，我们的服务器资源使用率如何？” 指标计算了解资源使用率的前提是记录所拥有的服务器资源以及归属，在服务器采购到位后，就开始跟踪服务器的各项资源指标。但每次老板们问到这个问题时，作为运维人员还是很难直接回答。我们记录了每台机器历史的各项资源使用数据，但如何体现整体的资源使用情况，通过简单的指标表示出来还是比较麻烦的。 首先，我们不能把所有服务器罗列出来，逐台给老板汇报。 其次，每台服务器每天的资源占用情况是随时间，随业务特性动态变化的，需要将CPU、内存、磁盘、IO等每个动态变化的资源指标分别换算成一个值。 例如：一台CPU密集型的服务器，每个时间点访问量的不同，CPU使用率也是不同的，如图4-1所示。 最后，每台服务器每天的平均值，由于高峰期或低峰期的差值很大，均值没有任何参考意义。取每台服务器每天的最大峰值，有时候可能由于某一次数据传输，或者跑一个临时MD5运算导致CPU使用率突增，也不能合理代表这台服务器的CPU使用率。 图4-1 CPU使用率 了解到的业界一般的计算方法是： ◎ 定义每天的业务高峰期为 10:00AM—10:00PM，求这段时间平均值表示该台服务器当天的使用率； ◎ 每天取峰值的3个点，求这3个峰值点的平均值表示该台服务器当天的使用率。 对于定义业务高峰期，然后求平均值，我们认为不够精准，很多业务不一定白天时间达到它的峰值。如果每个业务都个性化定义高峰时间，工作量比较大，管理起来也很麻烦。每天取峰值的3个点求平均值，很多时候人工操作或者临时性的计算很容易把这个峰值提得太高。 经过多次讨论后，确定了下面的计算方法： （1）每台服务器的CPU、内存、磁盘、IO、网卡数据每2秒钟采集一次； （2）每天分别取这些数据的TOP n%求平均值。 如图4-2所示为某台服务器某天的监控数据，我们计算出CPU、内存、磁盘和IO的数值如表4-1所示。 图4-2 监控数据 资源项 数值 CPU 98.12% 内存 73.29% 磁盘 58.69% IO 29.05% 表4-1 资源数值 当然，怎么计算这个资源数值都没有问题，只要是统一的计算模型，能够表示所有机器的资源占用指标即可。我们这么计算是希望能够更接近服务器的平均峰值，消除一些临时性尖峰导致数值较大的问题。 指标展现完成了资源使用率计算，但如何整体展现所有服务器的资源使用率也是一个问题。由于业务特性的不同，有消耗CPU的，有消耗内存的，也有类似离线存储消耗磁盘空间的，不可能把所有服务器的某项值加起来，然后求平均值，这样说明不了任何问题，而且值会低得可怜。在一个案例中，公司所有服务器每天的CPU值求平均值，CPU使用率只有23%左右。所有人看到这样的数据都会问一句话：“为什么我们的服务器资源使用率这么低？” 对于如何体现公司整体的服务器资源使用率这个问题，我们也没有什么好的方法，不过可以从另外两个维度去体现。 （1）将整个公司这个维度拆解到最小的相同功能集群的粒度，统计和展现这些小集群的资源使用率情况。这个时候就需要结合之前提到过的“服务树”，我们通过服务树将公司的产品划分到产品线-&gt;子系统-&gt;集群这样的粒度，每个小集群的功能是类似的，这群服务器的资源使用模型也是类似的。研发或运维在提交采购预算时，以最小集群为单位。领导在审批预算时，可以很方便地查看该集群的资源使用情况，决定是否购买。 （2）每月统计出资源使用率不达标的服务器，发给相关的研发主管和运维人员，督促他们尽快利用或者归还资源。比如某个集群每月不达标的服务器数量超过该集群的5%，则认为这个产品线资源使用不达标。 这样侧面解决了如何整体展现公司资源使用率的问题，给出最小集群资源使用率不达标的情况，展现的数据量比较少，而且能够很好地跟进和解决。还可以给出资源使用率不达标服务器和所有服务器的占比曲线，用来观察整个公司服务器资源使用率的变化情况。 这样既满足了工程师需要了解具体信息，执行改进的需求，也满足了老板需要了解整体信息，把握全局的需求。 怎么计算、怎么展现才算合理，每个公司都有自己的做法。当然，在你认为公司整体资源使用率都还不错的情况下，所有服务器的平均值也许可以作为一个基准，用来宏观监控整体资源使用率有没有变好或变坏也是挺不错的。 不达标原因分析完成上述工作后，我们开展了一次资源使用率未达标服务器原因分析，按照未达标的原因进行了归类。如图4-3所示。 图4-3 资源使用率未达标原因归类 每类原因说明如下： ◎ 灾备：该类型服务器属于对重要服务的备份服务器，平时没有或有很少的流量。 ◎ 特殊服务：该类型服务器属于特殊用途需要独占或者多副本存在，如ZooKeeper，或类似博客对外服务又需要安全隔离的业务等。 ◎ 服务部署中：服务扩容、搬迁时，暂时未引入业务流量。 ◎ 流量未达预期：业务已经在线上运行，但由于流量预估不准确，服务上线后流量和计算量未达到预期。 ◎ 开发测试中：新服务小流量运行，处于开发联调阶段。 ◎ 测试环境：研发或测试的线下集群，用于压力测试、功能测试等。 ◎ 计划下线：服务优化、架构调整、业务功能裁剪等，空闲服务器处于待下线归还状态。 ◎ 故障中：服务器故障停止业务，处于维修状态中。 ◎ 高峰预留：为了应对比如业务促销活动、特殊节假日等流量高峰，提前储备的服务器资源。 下面分析导致前几类原因出现的问题。 1．业务线预算不准确◎ 预估的业务流量较大，实际业务流量未达到预期，导致已经上线的服务器资源浪费。 ◎ 项目计划不准确，原计划3月份进行新服务上线或者扩容，结果项目延迟到6月份，导致3个月时间的服务器资源闲置。 2．服务器采购效率低，缺少快速伸缩的能力◎ 随着公司对于服务器需求的数量越来越大，很早以前那种随用随买的模式已经不适合了，转而采用服务器集采的模式，从预算申请，到领导审批、采购招标、供货商送货、服务器上架、系统装机交付等多个环节，在正常情况下会耗时2个月。那么业务部门往往会提前并较多地购买服务器，造成一定时间段的服务器资源浪费。 ◎ 为了应对某次突增几倍的业务促销活动流量，需要提前准备服务器资源。当活动结束后，服务器资源的归往往不及时，归还后其他业务部门消化这部分资源的时间也比较长。 ◎ 服务器采购时资产是划分到各个部门的。服务器资源归还后，还存在部门间资产更新计算的问题。 3．缺少资源监控平台缺少资源监控和审查机制，业务部门浪费的机器也不会及时归还。 资源优化看到这几类原因后，第一时间能够想到的解决方案就是公有云。类似AWS的服务，能够快速地交付或归还虚拟机资源，按时间、按流量收费。虽然大部分业务是可以部署在公有云上的，但还有很多类似HBase、离线计算等对磁盘IO、内网带宽有较高需求的业务，不太适合在虚拟机上部署，成熟公司这部分业务的服务器数量一般占到30%～60%。 先不讨论公有云是否真正省成本，仅就对目前国内云服务商的安全评估，我们还不敢把高敏感、高安全要求的用户数据、现金支付类业务放置在云端。另外，多机房服务冗余、机房间专线互通等各种因素也需要考虑进去。 我们的目标是通过缩短服务器采购周期，资源能够具备快速伸缩的能力，结合预算审批、资源监控等手段，将不达标服务器控制在合理范围内。综合考虑后，可选的解决思路如下： （1）支持物理机租赁模式，提高服务器交付效率； （2）支持私有云和公有云的使用，降低物理机的需求； （3）统一预算平台，限制不达标业务的申请； （4）资源监控，不达标服务器资源能够及时归还。 作为一些互联网公司，不希望投入过多的资源在固定资产上。运维人员也不愿意投入过多的精力在服务器的采购、上架、维保以及后续的报废处理上。当前已经有很多公有云支持虚拟机和物理机的租赁业务，传统的IDC厂商也陆续提供物理机租赁业务，由他们负责服务器的采购、上架等一系列工作，按照我们要求的时间、地点（IDC、机柜位置等）进行交付。公有云或服务器外包公司提供足够的备机池资源，能够应对我们对于服务器快速伸缩的需求，按照实际租赁时间收费。 成本方面，按照服务器3年淘汰，每次物理机采购实际是预付了3年的使用费用。采用租赁的方式按实际使用时间付费，不需要提前支付未来的费用。对于项目计划不合理、预算不准确、高峰期预留等问题，我们可以通过快速的服务器交付、归还的方式缓解这些问题，将成本支出降到最低。当前，足够的备机池、快速的交付时间是有成本支出的。 预算平台和资源监控的结合，限制和发现那些资源使用率不达标的服务器，督促业务线及时归还资源，减少成本支付。 提升服务器资源使用率还有很多方法，比如服务模块混布、资源闲时利用，这些属于运维可控的范围。另外，研发对于代码和功能的优化，效果往往会更加明显。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Usage</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第三章：运维平台]]></title>
    <url>%2Funder-the-ops%2F20170923-03-platform-of-ops%2F</url>
    <content type="text"><![CDATA[服务可管理的前提是运维数据的准确性，而标准化和流程化是保证数据准确性的前提。只有提供准确的运维数据，才能进一步实现服务的运维自动化。所以，一个能够准确记录和管理服务信息的运维平台，对于运维的发展至关重要。 在运维团队组建初期，运维平台建设一直属于运维团队的工作重点。通过标准和流程的约束，保证信息准确地录入到平台，以便能够准确提供运维所需要的各种维度信息，帮助运维人员开发更上层的系统，获取运行状态、资源占用等信息，与部署系统联动进行服务的动态调度部署和故障容错。 一个真实案例中，早期的运维平台有服务器管理、IDC管理、监控（Zabbix）、密码管理、故障记录等这几个模块，更多的是信息记录，更像一个网页版的Excel。没有流程的引入，信息录入完全依赖于人。这个时候的信息仅仅用来对账，滞后不准确的数据无法作为运维工具的基础依据，更谈不上自动化。平台各个功能模块之间没有信息关联，所有信息如一个个孤岛，对于运维的价值非常低。 随着需求场景的进一步明确，平台在不断建设。形成了两个大的运维平台，即：资产管理平台和服务管理平台。 资产管理平台负责记录基础的物理信息，如：IDC、服务器（资产编号、参数、采购时间、供应商）、配件、网络设备、IP地址、ACL等。提供了多个子功能，如：预算管理、自助装机、故障报修、IP地址管理、ACL管理、LVS管理等。资产管理平台作为所有物理资源的唯一出入口，通过流程将预算管理、故障管理这些可能导致资产信息变更的环节打通。新采购的服务器录入到资产管理平台，服务器报废也必须经过它。通过资产管理平台，可以很方便地查询各种物理资源的使用情况。比如，一共有多少服务器、有哪些机房、机房的机柜分布情况、每个机柜摆放的服务器位置等信息。 服务管理平台记录了业务运维所需的逻辑信息，提供一个基于树状结构（注：后续简称“服务树”）和权限绑定的管理模型。基于服务树和权限管理，实现域名管理、监控系统、部署自动化、环境初始化等子功能。服务管理平台记录了多个维度的服务信息，比如，产品线内有多少台服务器；谁具备这些服务器的登录权限；产品线对外使用了哪些域名；服务器上部署了什么服务；服务运行的状态、版本、路径；服务都添加了哪些监控等各维度信息。 可以认为资产管理平台和服务管理平台的信息集合就是ITIL里的CMDB（ConfigurationManagement Database）。由于每个运维子团队的分工不同，平台定位和用户场景不同，出于敏捷建设的考虑，我们将它拆分成了两个平台。资产管理平台的主要用户是系统运维工程师，他们关注设备的出入、维修等管理工作，交付资源给上层业务；服务管理平台的主要用户是应用运维工程师、研发工程师和测试工程师，他们关注服务运行的相关数据。虽然是分开的两个平台，但平台之间通过流程和API接口，实现了数据的相互关联。 资产管理平台负责底层的物理信息管理，提供API供服务管理平台查询和同步。服务管理平台通过API获取新交付的服务器列表及其详细信息，将它们归属到服务树产品线节点，分配对应的权限。应用运维工程师在服务树上领取空闲服务器，进行一系列的环境初始化、服务部署、监控添加等工作。应用运维工程师在服务管理平台提交报修申请、服务器归还等操作，通过API将信息推送到资产管理平台，由系统运维工程师进行相应处理。 两个平台负责所提供信息的准确性，对外提供API接口，可以供更上层的业务使用。基于这些信息，我们可以做更多智能化、自动化的工具开发。下面分享几个实际案例中的应用场景。 场景1 Hadoop数据存储管理我们有大量的数据存储在Hadoop集群上，出于节省成本的考虑，我们将以前的3副本变更为1.5副本，降低一倍存储量。为了避免相同数据存储在同一个机柜的服务器内，降低由于单机柜断电或者同机柜服务器多块磁盘故障导致数据丢失的可能性，我们通过平台提供的API，获取Hadoop集群所有服务器的机房、机柜分布和机架位置信息，在存储数据的时候进行合理的动态调配。 场景2 智能报警合并当服务器死机、机柜断电或接入交换机故障、机房断电或核心网络故障时，往往会收到大量的报警信息。我们可以通过平台提供的信息，对报警信息进行最大程度的聚合，减少报警发送的条目，而且能更好地帮助运维人员快速定位故障。当一台服务器死机的时候，通过监控项与服务器的关联信息，将这台服务器相关的SSHD监控、Nginx监控等报警信息进行聚合，合并成一条服务器宕机报警；当一个机柜断电后，我们可以将该机柜下接入交换机交换机和每台服务器的报警进行聚合，合并成一条机柜或接入交换机故障报警。 场景3 磁盘故障自动报修在互联网业务中大数据应用已经很广泛，Hadoop服务器数量占比很大，大量的数据计算导致磁盘故障率比较高，每天都有大量的故障磁盘需要更换维修。以前都是通过硬件监控或应用监控发现问题，然后由应用运维工程师登录服务器确认磁盘故障，尝试工具修复。如果修复失败摘掉磁盘，再发起故障报修申请。现在我们研发了磁盘故障自动维修系统，通过平台提供的API接口和监控系统联动，当监控系统发现磁盘故障后，通过回调接口启动磁盘工具进行软修复，修复失败后摘掉磁盘，并在服务管理平台进行记录，自动发起故障报修工单。服务器供应商收到维修工单通知后，根据所提供的机房、机柜、磁盘位置，进行集中更换。更换完成后进行通知，再由系统将磁盘分区格式化挂载，开始提供数据存储服务。 在运维平台建设的过程中，我们借鉴ITIL的思想，但没有完全照搬。ITIL能够帮助IT部门提高用户的满意度和运行效率，但它的实施难度比较大，不能满足互联网运维的敏捷要求。我们希望贴近DevOps的理念，管理和提供准确的运维数据，封装各种灵活的运维工具，让运维工作前置到产品研发阶段，帮助研发、测试人员快速完成产品的发布、测试、上线工作，让运维工具在产品的整个生命周期中联动起来。 平台化不等于自动化，我们的平台更多的是通过流程和标准的保证，提供运维数据的可视化，还算不上真正意义的自动化。我们希望研发和运维人员不再需要关心服务具体部署在哪台服务器、哪个IDC中，由调度系统负责服务运行状态的监控，对资源进行合理的调度、伸缩，对一定范围内的故障进行自动处理，实现真正的运维自动化。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Platform</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二章：运维的烦恼]]></title>
    <url>%2Funder-the-ops%2F20170923-02-sorrows-of-ops%2F</url>
    <content type="text"><![CDATA[如前一章所描述那样，随着业务和用户规模越来越大，公司对业务的稳定和质量开始重视起来，这时候公司才意识到需要专职的运维人员介入，而此时的业务系统已经变得非常庞大和复杂。此外，由于互联网产品快速试错的特点，服务架构也在不断地快速变化。 产品研发早期缺少相应的规范和标准，服务的部署方式、启停方式、配置和日志格式等都不统一，服务与服务之间的关联关系错综复杂，服务的各个环节都缺少监控；服务之间的耦合度很高，经常会由于一个小模块的崩溃，导致整个业务系统拒绝服务。 这个时候运维人员更像是保姆、消防员和拆弹专家。运维人员需要细心地呵护服务，让其健康地成长，就像照顾婴儿一样；成长中的服务，经常由于各种不规范带来的历史原因，出现很多意想不到的突发事情，这时候运维人员需要第一时间响应，进行业务的紧急恢复，类似消防员的角色；在日常的服务管理过程中，一个操作顺序或命令的错误，有可能直接让服务中断，这时候运维人员就像拆弹专家，既要细心大胆，又要有耐心，在危机时刻能够快速处理，做出正确决策。 运维人员需要处理各种突发的服务故障，在早期缺少统一规范、缺少业务监控、基础设施不成熟以及业务不断快速变化的时候，运维人员几乎每天都在忙于应付各种大大小小的服务故障。如图2-1所示是一个真实案例中，某个月统计到的服务故障分类和占比。 图2-1 服务故障分类和占比 根据监控项的重要程度对告警进行分级是一个很好的实践，Disaster级别优先级最高，需要立即处理。Warning级别需要24小时内完成处理，如图2-2所示。Warning级别大多数是CPU、内存、硬盘资源超限预警。详细的报警级别定义和划分，请参见后面监控章节的报警分级部分。 图2-2 各报警级别占比 注：根据监控项的重要程度对告警进行分级是一个很好的实践，P0级别优先级最高，需要立即处理。P3级别需要24小时内完成处理，P3级别大多数是CPU、内存、硬盘类资源超限预警。详细的报警级别定义和划分，请参见后面监控章节的报警分级部分。 业务快速变化、缺少统一规范、缺少文档和培训，运维人员基本上是摸黑接手服务。线上服务经常会埋着各种奇奇怪怪的坑，每一次服务变更都如履薄冰。 案例1服务上下游处理超时时间不匹配，上游服务的超时时间设置为5毫秒，而下游服务的超时时间却设置成了10毫秒，下游服务还在正常处理请求中，可上游服务却因达到超时设置而将本次请求丢弃了，最终客户端不断重试，导致服务器端压力增大。 一个真实案例中，遇到过上下游服务超时时间单位不一致的情况，因为系统中的各个服务是不同的研发人员负责的，在联调过程中忽略了一些问题，导致上游服务使用秒作为超时时间单位，下游服务却使用了毫秒。某次下游单机故障时，运维人员发现上游容错机制完全无效，依然导致其堆积了大量请求，最终影响服务整体性能。经过了较长时间的追查，才发现是由于超时时间单位问题引起的。 由于缺少规范化，给运维带来了无形的风险，而且故障定位也比较困难。 案例2集群服务是多台服务器共同完成一个任务，它们之间的调用关系是通过在程序配置文件中配置IP地址或服务器主机名来宣告的。由于IP地址的易读性较差，我们一般会使用内网DNS提供的主机域名。但有些研发人员却通过修改本机/etc/hosts文件的形式自定义域名解析。 这样的修改，使得对目标域名的解析是维护在每台服务器上的，这将极大增加运维管理的风险。试想100台相互存在调用关系的服务器，每台服务器上都需要维护与其他多台服务器的域名关系。当出现服务变更、故障处理、服务迁移时，需要所有上下游服务配合变更，带来很高的操作风险和复杂度。 运维属于技术线的末端（见图2-3），产品研发、测试、上线后将持续不断地在线上运行着。互联网产品很少有产品下线的情况，经常会出现某个产品的产品经理、研发工程师、测试工程师都没有了，而这个产品依然还有运维人员在维护，持续提供服务。上游引入的任何缺陷，最终都由运维去承担，上游往往无法感受到运维的压力。随着业务的增长、服务与主机数量的增加，产品各个阶段的缺陷会被进一步放大，运维压力也越来越大。 图2-3 运维属于技术线的末端 手工操作是初期运维团队的主要方式，渐渐的会形成一些工具或者系统，但都比较零散，适用场景较小，无法产生规模化。运维批量化和自动化所需要的信息非常少，这些信息基本上都靠人工录入，有哪些IDC，放置了什么服务器，服务器部署了什么服务，这些信息都没有自动采集和联动，无法给自动化系统提供必需的基础信息。运维的重复性工作非常多，又较多属于手工操作，不仅效率低，而且手工操作带来的失误率也比较多，几乎无法消除。 运维承受来自于外部不断增长的业务压力，以及快速发展中引入的各种缺陷。同时又面对内部生产力低下，导致工作效率低下和误操作较多的现状。运维是一个比较尴尬的工作，属于技术线的末端，人力、技术和资源的投入也属于末端。运维不出故障是正常，任何由于资源不足、基础设施不稳定、人员误操作导致的问题，都会被业务部门投诉。不过近年来，运维工作的价值越来越被大家认可，运维支持能力成为公司的核心技术竞争力之一。 运维工作需要从两个方向去解决上述提到的问题：提高内部运维效率和降低外部运维压力。 经过统计，运维工作中占比最多的是服务变更、监控管理、容量管理和故障处理。我们需要开发运维工具和平台，在运维数据准确的前提下让所有的工作尽量自动化起来。制定相关的标准和流程，运维人员在项目设计阶段就参与进来，进行设计评审，让研发人员交付的项目符合运维准入的要求。同时，让研发人员使用运维相关的工具，使研发、测试、上线阶段的部署行为一致，监控策略一致，且被测试验证过。 运维标准不是凭空制定出来的，需要满足运维自动化相关工具的最低要求。符合运维标准的产品，能够更加方便地进行一键部署，与监控联动等，这样才使研发人员有动力往运维标准靠拢，更积极地使用运维工具，我们的标准和工具才能进一步得到。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Sorrows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一章：互联网运维工作]]></title>
    <url>%2Funder-the-ops%2F20170923-01-work-of-ops%2F</url>
    <content type="text"><![CDATA[互联网运维工作，以服务为中心，以稳定、安全、高效为三个基本点，确保公司的互联网业务能够7×24小时为用户提供高质量的服务。 运维人员对公司互联网业务所依赖的基础设施、基础服务、线上业务进行稳定性加强，进行日常巡检发现服务可能存在的隐患，对整体架构进行优化以屏蔽常见的运行故障，多数据中心接入提高业务的容灾能力，通过监控、日志分析等技术手段，及时发现和响应服务故障，减少服务中断的时间，使公司的互联网业务符合预期的可用性要求，持续稳定地为用户提供服务。 在安全方面，运维人员需要关注业务运行所涉及的各个层面，确保用户能够安全、完整地访问在线业务。从网络边界划分、ACL管理、流量分析、DDoS防御，到操作系统、开源软件的漏洞扫描和修补，再到应用服务的XSS、SQL注入防护；从安全流程梳理、代码白盒黑盒扫描、权限审计，到入侵行为检测、业务风险控制等。运维人员需要保障公司提供的互联网业务运行在安全、可控的状态下，确保公司业务数据和用户隐私数据的安全，同时还需要具备抵御各种恶意攻击的能力。 在确保业务稳定、安全的前提下，还需保障业务高效的运转，公司内快速的产出。运维工作需要对业务进行各方面优化，比如，IO优化提升数据库性能，图片压缩降低带宽使用量等，使公司提供的互联网业务以较小的资源投入带来最大的用户价值和体验。同时，还需要通过各种工具平台提升内部产品发布交付的效率，提升公司内运维相关的工作效率。 1.1 运维工作分类运维的工作方向比较多，随着业务规模的不断发展，越成熟的互联网公司，运维岗位会划分得越细。当前很多大型的互联网公司，在初创时期只有系统运维，随着业务规模、服务质量的要求，也逐渐进行了工作细分。一般情况下运维团队的工作分类（见图1-1）和职责如下。 （图1-1 运维团队的工作分类） 1.1.1 系统运维系统运维负责IDC、网络、CDN和基础服务的建设（LVS、NTP、DNS）；负责资产管理，服务器选型、交付和维修。详细的工作职责如下： （1）IDC数据中心建设收集业务需求，预估未来数据中心的发展规模，从骨干网的分布，数据中心建筑，以及Internet接入、网络攻击防御能力、扩容能力、空间预留、外接专线能力、现场服务支撑能力等多个方面评估选型数据中心。负责数据中心的建设、现场维护工作。 （2）网络建设设计及规划生产网络架构，这里面包括：数据中心网络架构、传输网架构、CDN网络架构等，以及网络调优等日常运维工作。 （3）LVS负载均衡和SNAT建设LVS是整个站点架构中的流量入口，根据网络规模和业务需求，构建负载均衡集群；完成网络与业务服务器的衔接，提供高性能、高可用的负载调度能力，以及统一的网络层防攻击能力；SNAT集中提供数据中心的公网访问服务，通过集群化部署，保证出网服务的高性能与高可用。 （4）CDN规划和建设CDN工作划分为第三方和自建两部分。建立第三方CDN的选型和调度控制；根据业务发展趋势，规划CDN新节点建设布局；完善CDN业务及监控，保障CDN系统稳定、高效运行；分析业务加速频道的文件特性和数量，制定最优的加速策略和资源匹配；负责用户劫持等CDN日常故障排查工作。 （5）服务器选型、交付和维护负责服务器的测试选型，包含服务器整机、部件的基础性测试和业务测试，降低整机功率，提升机架部署密度等。结合对公司业务的了解，推广新硬件、新方案减少业务的服务器投入规模。负责服务器硬件故障的诊断定位，服务器硬件监控、健康检查工具的开发和维护。 （6）OS、内核选型和OS相关维护工作负责整体平台的OS选型、定制和内核优化，以及Patch的更新和内部版本发布；建立基础的YUM包管理和分发中心，提供常用包版本库；跟进日常各类OS相关故障；针对不同的业务类型，提供定向的优化支持。 （7）资产管理记录和管理运维相关的基础物理信息，包括数据中心、网络、机柜、服务器、ACL、IP等各种资源信息，制定有效的流程，确保信息的准确性；开放API接口，为自动化运维提供数据支持。 （8）基础服务建设业务对DNS、NTP、SYSLOG等基础服务的依赖非常高，需要设计高可用架构避免单点，提供稳定的基础服务。 1.1.2 应用运维应用运维负责线上服务的变更、服务状态监控、服务容灾和数据备份等工作，对服务进行例行排查、故障应急处理等工作。详细的工作职责如下所述。 （1）设计评审在产品研发阶段，参与产品设计评审，从运维的角度提出评审意见，使服务满足运维准入的高可用要求。 （2）服务管理负责制定线上业务升级变更及回滚方案，并进行变更实施。掌握所负责的服务及服务间关联关系、服务依赖的各种资源。能够发现服务上的缺陷，及时通报并推进解决。制定服务稳定性指标及准入标准，同时不断完善和优化程序和系统的功能、效率，提高运行质量。完善监控内容，提高报警准确度。在线上服务出现故障时，第一时间响应，对已知线上故障能按流程进行通报并按预案执行，未知故障组织相关人员联合排障。 （3）资源管理对各服务的服务器资产进行管理，梳理服务器资源状况、数据中心分布情况、网络专线及带宽情况，能够合理使用服务器资源，根据不同服务的需求，分配不同配置的服务器，确保服务器资源的充分利用。 （4）例行检查制定服务例行排查点，并不断完善。根据制定的服务排查点，对服务进行定期检查。对排查过程中发现的问题，及时进行追查，排除可能存在的隐患。 （5）预案管理确定服务所需的各项监控、系统指标的阈值或临界点，以及出现该情况后的处理预案。建立和更新服务预案文档，并根据日常故障情况不断补充完善，提高预案完备性。能够制定和评审各类预案，周期性进行预案演练，确保预案的可执行性。 （6）数据备份制定数据备份策略，按规范进行数据备份工作。保证数据备份的可用性和完整性，定期开展数据恢复性测试。 1.1.3 数据库运维数据库运维负责数据存储方案设计、数据库表设计、索引设计和SQL优化，对数据库进行变更、监控、备份、高可用设计等工作。详细的工作职责如下所述。 （1）设计评审在产品研发初始阶段，参与设计方案评审，从DBA的角度提出数据存储方案、库表设计方案、SQL开发标准、索引设计方案等，使服务满足数据库使用的高可用、高性能要求。 （2）容量规划掌握所负责服务的数据库的容量上限，清楚地了解当前瓶颈点，当服务还未到达容量上限时，及时进行优化、分拆或者扩容。 （3）数据备份与灾备制定数据备份与灾备策略，定期完成数据恢复性测试，保证数据备份的可用性和完整性。 （4）数据库监控完善数据库存活和性能监控，及时了解数据库运行状态及故障。 （5）数据库安全建设数据库账号体系，严格控制账号权限与开放范围，降低误操作和数据泄露的风险；加强离线备份数据的管理，降低数据泄露的风险。 （6）数据库高可用和性能优化对数据库单点风险和故障设计相应的切换方案，降低故障对数据库服务的影响；不断对数据库整体性能进行优化，包括新存储方案引进、硬件优化、文件系统优化、数据库优化、SQL优化等，在保障成本不增加或者少量增加的情况下，数据库可以支撑更多的业务请求。 （7）自动化系统建设设计开发数据库自动化运维系统，包括数据库部署、自动扩容、分库分表、权限管理、备份恢复、SQL审核和上线、故障切换等功能。 1.1.4 运维研发运维研发负责通用的运维平台设计和研发工作，如：资产管理、监控系统、运维平台、数据权限管理系统等。提供各种API供运维或研发人员使用，封装更高层的自动化运维系统。详细的工作职责如下所述。 （1）运维平台记录和管理服务及其关联关系，协助运维人员自动化、流程化地完成日常运维操作，包括机器管理、重启、改名、初始化、域名管理、流量切换和故障预案实施等。 （2）监控系统负责监控系统的设计、开发工作，完成公司服务器和各种网络设备的资源指标、线上业务运行指标的收集、告警、存储、分析、展示和数据挖掘等工作，持续提高告警的及时性、准确性和智能性，促进公司服务器资源的合理化调配。 （3）自动化部署系统参与部署自动化系统的开发，负责自动化部署系统所需要的基础数据和信息，负责权限管理、API开发、Web端开发。结合云计算，研发和提供PaaS相关高可用平台，进一步提高服务的部署速度和用户体验，提升资源利用率。 1.1.5 运维安全运维安全负责网络、系统和业务等方面的安全加固工作，进行常规的安全扫描、渗透测试，进行安全工具和系统研发以及安全事件应急处理。详细的工作职责如下所述。 （1）安全制度建立根据公司内部的具体流程，制定切实可行，且行之有效的安全制度。 （2）安全培训定期向员工提供具有针对性的安全培训和考核，在全公司内建立安全负责人制度。 （3）风险评估通过黑白盒测试和检查机制，定期产生对物理网络、服务器、业务应用、用户数据等方面的总体风险评估结果。 （4）安全建设根据风险评估结果，加固最薄弱的环节，包括设计安全防线、部署安全设备、及时更新补丁、防御病毒、源代码自动扫描和业务产品安全咨询等。为了降低可能泄露数据的价值，通过加密、匿名化、混淆数据，乃至定期删除等技术手段和流程来达到目的。 （5）安全合规为了满足例如支付牌照等合规性要求，安全团队承担着安全合规的对外接口人工作。 （6）应急响应建立安全报警系统，通过安全中心收集第三方发现的安全问题，组织各部门对已经发现的安全问题进行修复、影响面评估、事后安全原因追查。 1.2 运维工作发展过程早期的运维团队在人员较少的情况下，主要是进行数据中心建设、基础网络建设、服务器采购和服务器安装交付工作。几乎很少涉及线上服务的变更、监控、管理等工作。这个时候的运维团队更多的属于基础建设的角色，提供一个简单、可用的网络环境和系统环境即可。 随着业务产品的逐渐成熟，对于服务质量方面就有了更高的要求。这个时候的运维团队还会承担一些服务器监控的工作，同时会负责LVS、Nginx等与业务逻辑无关的4/7层运维工作。这个时候服务变更更多的是逐台的手工操作，或者有一些简单批量脚本的出现。监控的焦点更多的在服务器状态和资源使用情况上，对服务应用状态的监控几乎很少，监控更多的使用各种开源系统如Nagios、Cacti等。 由于业务规模和复杂度的持续增加，运维团队会逐渐划分为应用运维和系统运维两大块。应用运维开始接手线上业务，逐步开展服务监控梳理、数据备份以及服务变更的工作。随着对服务的深入，应用运维工程师有能力开始对服务进行一些简单的优化。同时，为了应对每天大量的服务变更，我们也开始编写各类运维工具，针对某些特定的服务能够很方便的批量变更。随着业务规模的增大，基础设施由于容量规划不足或抵御风险能力较弱导致的故障也越来越多，迫使运维人员开始将更多的精力投入到多数据中心容灾、预案管理的方向上。 业务规模达到一定程度后，开源的监控系统在性能和功能方面，已经无法满足业务需求；大量的服务变更、复杂的服务关系，以前靠人工记录、工具变更的方式不管在效率还是准确性方面也都无法满足业务需求；在安全方面也出现了各种大大小小的事件，迫使我们投入更多的精力在安全防御上。逐渐的，运维团队形成之前提到的5个大的工作分类，每个分类都需要有专精的人才。这个时候系统运维更专注于基础设施的建设和运维，提供稳定、高效的网络环境，交付服务器等资源给应用运维工程师。应用运维更专注于服务运行状态和效率。数据库运维属于应用运维工作的细化，更专注于数据库领域的自动化、性能优化和安全防御。运维研发和运维安全提供各类平台、工具，进一步提升运维工程师的工作效率，使业务服务运行得更加稳定、高效和安全。 我们将运维发展过程划分为4个阶段，如图1-2所示。 （图1-2 运维发展过程） 手工管理阶段：业务流量不大，服务器数量相对较少，系统复杂度不高。对于日常的业务管理操作，大家更多的是逐台登录服务器进行手工操作，属于各自为战，每个人都有自己的操作方式，缺少必要的操作标准、流程机制，比如业务目录环境都是各式各样的。 工具批量操作阶段：随着服务器规模、系统复杂度的增加，全人工的操作方式已经不能满足业务的快速发展需要。因此，运维人员逐渐开始使用批量化的操作工具，针对不同操作类型出现了不同的脚本程序。但各团队都有自己的工具，每次操作需求发生变化时都需要调整工具。这主要是因为对于环境、操作的规范不够，导致可程序化处理能力较弱。此时，虽然效率提升了一部分，但很快又遇到了瓶颈。操作的质量并没有太多的提升，甚至可能因为批量执行而导致更大规模的问题出现。我们开始建立大量的流程规范，比如复查机制，先上线一台服务器观察10分钟后再继续后面的操作，一次升级完成后至少要观察20分钟等。这些主要还是靠人来监督和执行，但在实际过程中执行往往不到位，反而降低了工作效率。 平台管理阶段：在这个阶段，对于运维效率和误操作率有了更高的要求，我们决定开始建设运维平台，通过平台承载标准、流程，进而解放人力和提高质量。这个时候对服务的变更动作进行了抽象，形成了操作方法、服务目录环境、服务运行方式等统一的标准，如程序的启停接口必须包括启动、停止、重载等。通过平台来约束操作流程，如上面提到的上线一台服务器观察10分钟。在平台中强制设定暂停检查点，在第一台服务器操作完成后，需要运维人员填写相应的检查项，然后才可以继续执行后续的部署动作。 系统自调度阶段：更大规模的服务数量、更复杂的服务关联关系、各个运维平台的林立，原有的将批量操作转化成平台操作的方式已经不再适合，需要对服务变更进行更高一层的抽象。将每一台服务器抽象成一个容器，由调度系统根据资源使用情况，将服务调度、部署到合适的服务器上，自动化完成与周边各个运维系统的联动，比如监控系统、日志系统、备份系统等。通过自调度系统，根据服务运行情况动态伸缩容量，能够自动化处理常见的服务故障。运维人员的工作也会前置到产品设计阶段，协助研发人员改造服务使其可以接入到自调度系统中。 在整个运维的发展过程中，希望所有的工作都自动化起来，减少人的重复工作，降低知识传递的成本，使我们的运维交付更高效、更安全，使产品运行更稳定。对于故障的处理，也希望由事后处理变成提前发现，由人工处理变成系统自动容灾。]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Ops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【临时置顶】《运维之下（转载）》目录]]></title>
    <url>%2Funder-the-ops%2F20170923-00-under-the-ops-contact%2F</url>
    <content type="text"><![CDATA[转载自公众号《技术头条》和laiwei的简书博客，乃Open-Falcon开发者laiwei和其小伙伴所著。 前言做技术的同学，始终都是比较单纯，挣钱领工资，只是价值体现的其中一种，小伙伴们同样在意的是自己的工作是不是产生了价值，是不是得到了认可，是不是帮助到了别人。 尤其是做运维，我们用到了业界很多优秀的开源解决方案，Linux、MySQL、LVS、Nginx、OpenResty、OpenSSL、Zabbix、Ansible、Puppet、OpenStack、CloudFoundry、Docker… 正是有了很多业界同仁无私的奉献，让我们借鉴了很多先进的理念，用到了很多现成的优秀解决方案，节省了大量的成本和时间。 饮水思源，夜深人静的时候，努力敲代码、执行命令、拯救世界的时候，我一直在想，索取和给予，应该是对等的，这样才能达到真正的inner peace。 所以，在小伙伴的支持下，我们发起和开源了国内首款互联网企业级监控解决方案Open-Falcon，并聚集了一批志同道合的朋友，努力的撰写文档，书写教程，整理安装步骤，撰写FAQ，甚至翻译为英文文档，组织线下沙龙，线上分享。现在Open-Falcon已经成为国内互联网公司使用最广泛的监控解决方案之一。 然后，有一天，在和小伙伴们聊天的时候，惊讶的发现，在高校没有任何一个专业或者课堂会讲运维是怎么回事，怎么做运维；国内互联网运维蓬勃发展了10多年，却没有一本体系化的运维书籍，告诉我们如何入门和持续提高（或者直接放弃）…… 于是就有了「运维之下」这本合集的众筹创作思路，覆盖系统、网络、数据库、安全、标准化、自动化等多个层面，从创业初期见招拆招到BAT级别的规模化运维，从PaaS、IaaS到公有云，从运维理念到平台实践，都有些许不自量力的阐述。 忘不了老板在这件事上的全力协调，忘不了小伙伴们的支持和陪伴，利用每个夜深人静的夜晚，每个咖啡馆的周六，整理散落的博客，翻阅素材资料，回顾一路走过的坑坑洼洼，静静地码着一个又一个段落，逐字逐句的斟酌，排版，插图…… 30万字倾注了小伙伴们大量的心血。 这本合集以及相关的修订补充，后续会在github开源，并独家授权「技术头条微信公众号」连载发送，遵循知识共享协议（CC-BY-NC-SA 4.0 international），版权归「laiwei和他的小伙伴们」所有（受限于一些原因，完整作者名单不一一列举，后续参考该github项目版权页面）。感谢各位小伙伴的工作，感谢业界各位老师的审阅和推荐（后续参考该github项目致谢页面）。 在这本书中，尽可能的和小伙伴们一起，分享了我们在平淡工作中遇到的不平淡经历，如果对各位同学能有所帮助和启发，促进技术交流，那就心满意足了。 目录第一部分：运维综述 第一章：互联网运维工作第二章：运维的烦恼第三章：运维平台第四章：服务器资源使用率第五章：运维的未来 第二部分 基础运维建设 第六章：系统运维概述第七章：资产管理第八章：服务器硬件测试选型第九章：网络成长之路第十章：传输网建设第十一章：从外包到自建，小中企业CDN运维进阶指南第十二章：域名和接入 第三部分 应用运维 第十三章：网站可靠性运维第十四章：灾备管理第十五章：服务管理平台第十六章：服务发现第十七章：部署系统第十八章：动态调度第十九章：数据库自动运维系统第二十章：数据库备份还原系统第二十一章：Hadoop运维 第四部分 监控系统 第二十二章：开源监控系统的选择第二十三章：互联网企业级监控系统实践第二十四章：分布式调用追踪系统 第五部分 云计算 第二十五章：Iaas平台建设PaaS平台建设AWS实践 第六部分 运维安全 安全体系建设]]></content>
      <categories>
        <category>under-the-ops</category>
      </categories>
      <tags>
        <tag>Under the Ops</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7上简单安装部署GitLab]]></title>
    <url>%2Fcloud%2F20170922-gitlab%2F</url>
    <content type="text"><![CDATA[安装：在安装gitlab之前，记得要安装postfix来作为发送邮件通知的邮件服务： 123yum install postfixsystemctl enable postfixsystemctl start postfix GitLab-CE 可以用脚本一键安装repo文件： 1curl -s https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.rpm.sh | bash 也可以手动编写repo文件： vim /etc/yum.repos.d/gitlab-ce.repo 12345[gitlab-ce]name=Gitlab CE Repositorybaseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gpgcheck=0enabled=1 之后yum安装即可： 1yum install gitlab-ce 简单配置启动根据提示来配置： vim /etc/gitlab/gitlab.rb 找到external_url那项，改为可用的主机名(我的是gitlab.yulongjun.com，并且已做好解析）： 1external_url gitlab.yulongjun.com tips：可以使用sed替换 —— sed -i.bak &quot;s#external_url &#39;http://gitlab.example.com&#39;#external_url &#39;http://gitlab.yulongjun.com&#39;#g&quot; /etc/gitlab/gitlab.rb 修改完后，启动gitlab实例： 1gitlab-ctl reconfigure 设置下次开机自启动： 1systemctl enable gitlab-runsvdir.service 登录浏览器输入gitlab.yulongjun.com。 第一次登录需要更改密码： 然后使用新密码登录，用户名为root： 登录成功后的界面： 设置禁止注册一般公司内部使用的话，是禁止注册的，用的话单独开设账号。 root登录后，在Admin area里可以关掉注册： 去掉勾选 Sign-up enabled： 如果需要注册很多人，而且都是一个公司的邮箱后缀，可以开启邮箱验证功能，并且设置白名单区域，只允许本公司的邮箱后缀的人注册。 详细设置1. 设置时区修改时区为Asia/Shanghai1sed -i &quot;s@# gitlab_rails[&apos;time_zone&apos;] = &apos;UTC&apos;@gitlab_rails[&apos;time_zone&apos;] = &apos;Asia/Shanghai&apos;@g&quot; /etc/gitlab/gitlab.rb 验证是否修改成功： 2. 设置]]></content>
      <categories>
        <category>cloud</category>
      </categories>
      <tags>
        <tag>GitLab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git和GitHub笔记]]></title>
    <url>%2Fcloud%2F20170922-git%2F</url>
    <content type="text"><![CDATA[git安装1. macOS 下安装 GitMac上已经安装了git了 12[yulongjun@MBP ~]$ git --versiongit version 2.13.5 (Apple Git-94) 我们可以用brew命令来更新到最新版。brew命令类似于RHEL的yum和ubuntu的apt-get命令。 OS X没有自带brew，登陆Homebrew网站，找到ruby代码来安装brew目前的安装代码是： 1ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 安装好brew后，就可以用brew来安装git的最新版本了，而且会自动下载依赖包： 1$ brew install git 2. Linux 下安装 GitCentOS/RHEL： 1sudo yum install git Debian/Ubuntu： 1sudo apt install git 3. Windows 下安装Git谁会在Windows下用Git? 还是不写了，官网下个包，下一步下一步傻瓜安装就行了。 https://git-scm.com/ Git初始全局设置1. 设置姓名和邮箱地址终端输入： 12$ git config --global user.name &quot;Firstname Lastname&quot;$ git config --global user.email &quot;your_email@your_example.com&quot; 我用我的名字和邮箱来设置的，命令会在~/.gitconfig文件里生成以下内容： 123[user]name = Yu Longjunemail = tianshoulong@sina.com 2. 高亮命令输出的文字（可选）把color.ui设置成auto 1$ git config --global color.ui auto ~/.gitconfig会增加下面一行： 12[color]ui = auto GitHub设置1. 注册GitHub登陆https://github.com 注册并设置个人信息（这个就不写了） 2. 设置ssh key123456789101112131415[yulongjun@MBP ~]$ ssh-keygen -t rsa -C &quot;me@yulongjun.com&quot;Generating public/private rsa key pair.Enter file in which to save the key (/Users/yulongjun/.ssh/id_rsa): `按回车键`Enter passphrase (empty for no passphrase): `输入密码,可以不填直接回车`Enter same passphrase again: `再次输入密码，可以不填直接回车`Your identification has been saved in /Users/yulongjun/.ssh/id_rsa.Your public key has been saved in /home/yulongjun/.ssh/id_rsa.pub.The key fingerprint is:28:15:a9:38:93:c5:3f:36:b1:6f:76:3f:af:28:45:77 me@yulongjun.comThe key&apos;s randomart image is:+--[ RSA 2048]----+| . .. || o o. || + o.o |`略` 3. 把本机生成的公开密钥添加到github中12$ cat ~/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1...Rrjx6t33i5 me@yulongjun.com 打开github登陆后，点击自己头像下箭头，找到settings（设置），在左边栏目里面选择SSH keys,然后点击Add SSH key，随便填一个Title，把上面把cat出来的内容全部(是全部内容，包括前面的ssh-rsa和后面的邮箱）添加到key文本框里，然后点击add key。 添加成功后，创建github时用到的邮箱会收到GitHub发的一个&quot;A new public key was add to your account&quot;的邮件。 4. 使用私人密钥与GitHub进行认证和通信1234567$ ssh -T git@github.comThe authenticity of host &apos;github.com (192.30.252.130)&apos; can&apos;t be established.RSA key fingerprint is 16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added &apos;github.com,192.30.252.130&apos; (RSA) to the list of known hosts.Enter passphrase for key &apos;/Users/yulongjun/.ssh/id_rsa&apos;: #输入3.1步骤中的密码，没有设置可以直接回车` 出现如下结果，表示成功： 1Hi yulongjun! You've successfully authenticated, but GitHub does not provide shell access. Git基本命令和操作1. git int（初始化仓库）1234$ mkdir Git-GitHub-Note$ cd Git-GitHub-note$ git initInitialized empty Git repository in /Users/yulongjun/Git-GitHub-Note/.git/ 2. git status（查看仓库状态）1234567$ git statusOn branch master #在master分支Initial commit #初始化提交nothing to commit(create/copy files and use "git add" to track) #没有文件提交（创建/复制文件 或者用git add 来添加文件） 创建一个README.md文件后，再查看状态： 123456789101112131415$ touch README.md$ git statusOn branch master #在master分支Initial commit #初始化提交Untracked files: #未追踪的文件： (use "git add &lt;file&gt;..." to include in what will be committed) #用 "git add &lt;file&gt;..."去添加用来提交的文件 README.md nothing added to commit but untracked files present (use "git add" to track)#没有文件添加到提交，但是有未跟踪的文件（用"git add"来追踪） 3. git add（向暂存区中添加文件）把4.2中的untracked files添加到暂存区 123456789101112$ git add README.md$ git statusOn branch master #在master分支Initial commit #初始化提交Changes to be committed: #提交改变： (use "git rm --cached &lt;file&gt;..." to unstage) # 用"git rm --cache &lt;file&gt;..."来去掉暂存文件 new file: README.md #新文件：README.md 4. git commit(提交仓库的历史记录）git commit 后跟的-m的意思是message，意味着添加一段描述性文字描述此次提交。 12345$ git commit -m "First commit"[master (root-commit) 7919beb] First commit 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 README.md 提交完后看状态： 12345$ git statusOn branch master #在master分支nothing to commit, working directory clean#无东西提交，工作目录干净的。 5. git log（查看提交日志）1234567$ git logcommit 7919bebdb5a7635238719556dfc9e2a0d1Author: Yu Longjun &lt;tianshoulong@sina.com&gt;#作者Date: Thu Jul 30 15:35:25 2015 +0800 #日期 第一次提交 单独查看某个文件或某个目录的权限，比如只查看README.md的日志 1$ git log README.md 想查看提交前后的改动，可以用下面命令： 1$ git log -p 当然，单独某个文件或文件夹，也可以查看改动 1$ git log -p README.md 6. git diff（查看更改前后的不同）更改了文件后，未git add，也未git commit，此时用git diff命令可以查看不同之处。 12345678$ git diffdiff --git a/README.md b/README.mdindex e69de29..74ab05b 100644--- a/README.md+++ b/README.md@(Git&amp;GitHub)@ -0,0 +1 @@+Git 教程 更改了文件后，用git add添加到缓存区了，但是未git commit，此时用git diff HEAD命令可以查看不同之处。 12345678$ git diff HEADdiff --git a/README.md b/README.mdindex e69de29..74ab05b 100644--- a/README.md+++ b/README.md@@ -0,0 +1 @@+Git 教程 把图中红框的SSH clone URL复制一下，URL的格式为：git@github.com:用户名/仓库名。 在终端输入git clone+复制内容，就把此仓库clone下来了 例如： 123456789$ git clone git@github.com:yulongjun/A-Byte-of-Markdown.gitCloning into 'A-Byte-of-Markdown'...remote: Counting objects: 33, done.remote: Compressing objects: 100% (31/31), done.remote: Total 33 (delta 13), reused 0 (delta 0), pack-reused 0Receiving objects: 100% (33/33), 25.87 KiB | 6.00 KiB/s, done.Resolving deltas: 100% (13/13), done.Checking connectivity... done. git分支操作1. git branch（显示分支一览表或者创建一个新分支）1234567891011$ git branch* master#显示分支一览表，当前只有一个分支master$ git branch feature-A#创建分支feature-A$ git branch feature-A* master# 显示分支一览表，有两个分支，当前处于master分支 * 分支表示当前所处的分支。 2. git checkout （切换、创建分支）1234567891011121314151617181920212223$ git checkout feature-AM README.mdSwitched to branch 'feature-A'#切换到 ‘feature-A’分支$ git branch* feature-A master#显示当前分支一览表，切换到了‘feature-A’分支$ git checkout -b fix-BM README.mdSwitched to a new branch 'fix-B'#新建并切换到‘fix-B’$ git branch feature-A* fix-B master 让我们看看切换分支编辑的效果 1234567891011121314151617181920212223242526272829303132333435363738$ git checkout feature-AM README.mdSwitched to branch 'feature-A'#切换到分支`feature-A`$ git branch* feature-A fix-B master #查看当前分支一览表 $ vim README.md #在文档里添加一行`- feature-A`$ git add README.md #添加`README.md`到暂存区$ git commit -m "Add feature-A"[feature-A 944df3c] Add feature-A 1 file changed, 1 insertions(+)#提交更改$ git checkout masterSwitched to branch 'master'#切换到`master`分支$ cat README.mdGit教程只显示原来的哪一行$ git checkout -#切换至上一个分支，即分支`feature-A` 3. git merge（合并分支）12345678910$ git checkout master#切换到`master`主干分支$ git merge --no-ff feature-A# `master`分支合并`feature-A`分支,出现的填写信息框，`：wq`保存退出就合并完成，如果想写点合并的原因，可以自行编辑。#出现以下内容就表示合并完毕Merge made by the &apos;recursive&apos; strategy.#用递归策略合并 README.md | 1 + 1 file changed, 1 insertion(+) 4. git log –graph （以图标的形式查看分支）1234567891011121314151617181920212223242526$ git log --graph* commit 64d2d5e9f8f32e890b04ba3c684f803767e650e1|\ Merge: 6ba50f6 399cae8| | Author: Yu Longjun &lt;tianshoulong@sina.com&gt;| | Date: Thu Jul 30 21:56:05 2015 +0800| | | | Merge branch &apos;feature-A&apos;| | | * commit 399cae80e4f5a1ec3a24b697c5222d89763c08a8|/ Author: Yu Longjun &lt;tianshoulong@sina.com&gt;| Date: Thu Jul 30 21:44:57 2015 +0800| | Add feature-A| * commit 6ba50f6beddfa8cc5d0d2c6ead46c8dd293a8f20| Author: Yu Longjun &lt;tianshoulong@sina.com&gt;| Date: Thu Jul 30 21:41:17 2015 +0800| | Add index| * commit a357886ac799f3e7d2ccff329ab1b7ef2a9a9f70 Author: Yu Longjun &lt;tianshoulong@sina.com&gt; Date: Thu Jul 30 21:39:43 2015 +0800 First commit 更改提交的操作1. git reset（回溯历史版本）看上面最后一个git log --graph的图标，我们要把先回溯到feature-A分支创建前，然后创建一个fix-B分支。 回退到“Add index”时候的状态，用git reset --hard 哈希值 123$ git reset --hard 6ba50f6beddfa8cc5d0d2c6ead46c8dd293a8f20HEAD is now at 6ba50f6 Add index 创建fix-B分支创建特性分支fix-B。 12$ git checkout -b fix-BSwitched to a new branch &apos;fix-B&apos; 修改READ.md文件，添加一行- fix-B。 12Git教程- fix-B 提交READ.md文件 12345$ git add README.md $ git commit -m &quot;fix-B&quot;[fix-B 767957a] fix-B 1 file changed, 1 insertion(+) 推进至feature-A分支合并后的状态 git log 只能查看当前状态为终点的历史日志。要查看整个仓库的所有操作日志，要用git reflog，找到回溯历史之前的哈希值，通过git reset --hard来恢复到回溯之前的状态。 123456789101112131415$ git reflog767957a HEAD@&#123;0&#125;: commit: fix-B6ba50f6 HEAD@&#123;1&#125;: checkout: moving from master to fix-B6ba50f6 HEAD@&#123;2&#125;: reset: moving to 6ba50f6beddfa8cc5d0d2c6ead46c8dd293a8f2064d2d5e HEAD@&#123;3&#125;: merge feature-A: Merge made by the &apos;recursive&apos; strategy.6ba50f6 HEAD@&#123;4&#125;: checkout: moving from feature-A to master399cae8 HEAD@&#123;5&#125;: checkout: moving from master to feature-A6ba50f6 HEAD@&#123;6&#125;: checkout: moving from feature-A to master399cae8 HEAD@&#123;7&#125;: commit: Add feature-A6ba50f6 HEAD@&#123;8&#125;: checkout: moving from master to feature-A6ba50f6 HEAD@&#123;9&#125;: checkout: moving from feature-B to master6ba50f6 HEAD@&#123;10&#125;: checkout: moving from master to feature-B6ba50f6 HEAD@&#123;11&#125;: commit: Add indexa357886 HEAD@&#123;12&#125;: commit (initial): First commit 看到第四行64d2d5e HEAD@{3}: merge feature-A: Merge made by the &#39;recursive&#39; strategy.,是feature-A特性分支合并后的状态，对应的哈希值是64d2d5e，我们将HEAD、暂存区、工作树恢复到这个时间点的状态。 12345$ git checkout master$ git reset --hard 64d2d5eHEAD is now at 64d2d5e Merge branch &apos;feature-A&apos; 2. 消除冲突 合并fix-B分支,发现README.md出现冲突 12345$ git merge --no-ff fix-BAuto-merging README.mdCONFLICT (content): Merge conflict in README.mdAutomatic merge failed; fix conflicts and then commit the result. 查看冲突并解决 查看README.md,发现变成这个样子 Git教程&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD- feature-A\=======- fix-B>&gt;&gt;&gt;&gt;&gt;&gt; fix-B> 我们在编辑器将其编程想要的样子 Git教程- feature-A- fix-B 提交结果 12345$ git add README.md $ git commit -m &quot;Fix conflict&quot;[master 3df8965] Fix conflict 3. git commit –amend（修改提交信息）上次提交commit，信息记为”Fix conflict”，不妥，其实是fix-B分支的合并，解决冲突只是过程之一，标记不妥，修改一下 1git commit --amend 执行上述命令后，编辑器启动,出现以下内容 Fix conflict# Please enter the commit message for your changes. Lines starting# with ‘#’ will be ignored, and an empty message aborts the commit.## Date: Fri Jul 31 10:49:28 2015 +0800## On branch master# Changes to be committed:# modified: README.md 修改第一行为Merge branch &#39;fix-B&#39;,保存退出。终端出现以下内容表示修改该信息成功 12 [master 5759a34] Merge branch &apos;fix-B&apos;Date: Fri Jul 31 10:49:28 2015 +0800 4. 远程仓库操作 添加远程仓库 首先在GitHub上创建一个空仓库： 复制GitHub上的仓库路径git@github.com:yulongjun/Git-GitHub-Note.git 运行命令： 1$ git remote add origin git@github.com:yulongjun/Git-GitHub-Note.git 添加远程仓库并把远程仓库设置成origin标识符。 把master推送至远程仓库 1$ git push -u origin master 推送master以外的分支。 12$ git checkout -b feature-D #新建一个feature-D分支$ git push -u origin feature-D #推送feature-D到origin远程仓库]]></content>
      <categories>
        <category>cloud</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>GitHub</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL备份恢复（mysqldump、xtrabackup）]]></title>
    <url>%2Flinux%2F20170911-mysql-mysqldump-xtrabackup%2F</url>
    <content type="text"><![CDATA[备份分为： 冷备（cold backup）：cold backup。闭服务下拷贝文件，属于物理备份。 温备（warm backup）：只读模式下备份，数据可读，但是不能写，属于逻辑备份。 热备（hot backup）：读写模式下备份，在某一时间点拍快照，只备份这时间戳之前的数据库状态，属于逻辑备份。 全量备份、增量备份、差异备份概念： 完全备份（full backup）：完整的数据备份。 增量备份（incremental backup）：仅备份自上一次完全备份或增量备份以来变量的那部数据。 差异备份（differential backup）：仅备份自上一次完全备份以来变量的那部数据。 图示： 周末全备，每天在前一天的基础上打增备： 周末全备，每天针对周末那天打差异备份。 备份工具： mysqldump 全量+binlog xtrabackup 全量+差异+binlog 全量+增量+binlog mysqldump1. 使用方法介绍mysqldump基本指令： mysqldump [options] db_name [tbl_name …]mysqldump [options] –databases db_name …mysqldump [options] –all-databases mysqldump 直接输出到屏幕命令，我们可以重定向到任何一个文件： 1mysqldump --all-databases &gt;/backup/fullbackup-$(date +%Y%m%d).sql MyISAM存储引擎：仅支持温备，不支持热备，备份时要锁定表，mysqldump还需要以下参数： -x, --locak-all-tables：锁定所有库的所有表，即加一个读锁 -l, --lock-tables：锁定指定库所有表。 InnoDB存储引擎：同时支持温备和热备。（能自动提交完成的任务，回滚未完成的任务，然后拍个快照，在此快照基础上创建事务。），musqldump支持以下参数： 热备：--single-stransaction：创建一个事务，基于快照执行备份。 温备：同MyISAM的-x和-l。 mysqldump命令的其他选项： -R， --routines：存储过程和存储函数 --triggers：触发器 -E， --events：事件调度器 --flush-logs：锁定表完成后，进行日志刷新操作。 --master-data[=1|2]：记录备份时候binglog日志所处的位置。 1：记录为CHANGE MASTER TO语句。 2：同样记录为CHANGE MASTER TO语句，但是是注释掉的。（给从服务器用的，要是只有一个主，就可以选择2注释掉，或者不写就什么也没有） 2. 模拟一个完整的温备和恢复过程： 前提条件：/etc/my.conf.d/server.cnf 里加一条 log-bin=master-log,启动二进制文件的记录功能。修改完配置后重启数据库服务，可以看到/var/lib/mysql/下有master-log.xxxxxx,表明启动二进制文件记录功能成功。 （1）温备 mysqldump -x -R -E --triggers --all-databases --master-data=2 --flush-logs &gt;/backup/fullbackup-$(date +%Y%m%d).sql 作者操作日期为20170912，备份的文件需要复制多份到其他服务器，以免服务器崩溃。 （2）对数据库添加或删除表或数据 （3）模拟数据库崩溃 记得先拷贝/etc/my.conf.d/server.cnf和/var/lib/mysql/master-log.xxxxxx到别处。(配置文件，一般配置服务器会保存一份；binlog日志文件，一般是存到不同的服务器做冗余。这里假设都是从其他服务器拷贝过来的，暂时copy一份作为恢复用。你应该知道，这个应该是从其他服务器上拷贝的，这里临时用就不那么麻烦了。） 12systemctl stop mariadb.servicerm -rf /var/lib/mysql/* （4）恢复数据库全备文件 恢复配置文件/etc/my.conf.d/server.cnf、binlog日志文件/var/lib/mysql/master-log.xxxx、数据库备份文件/backup/ vim /etc/my.conf.d/server.cnf,这里注释掉log-bin=master-log，不启动bin_log日志记录功能。 恢复全库备份文件： 第一种方法，本机运行mysql命令，然后： 1MariaDB &gt; \. /backup/fullbackup-20170912.sql 第二种方法：远程或本机执行命令（远程的前提是有远程权限）： 1mysql -u USER -h HOST -p PASSWORD &lt; /backup/fullbackup-20170912.sql 这里USER为用户名，HOST为主机名或IP，PASSWORD为密码。 （5）恢复binlog日志 我们查询到/backup/fullbackup-20170912.sql里日志记录的的CHANGE MASTER TO的位置 1-- CHANGE MASTER TO MASTER_LOG_FILE=&apos;master-log.000002&apos;, MASTER_LOG_POS=245; 我们看到位置是245，也就是开头的位置（因为我们备份时候启动日志滚动--flush-log了，所以会滚动到下一个日志文件的开头），所以我们要把包含master-log.000002和之后的的所有binlog日志文件，用mysqlbinlog命令工具，读取几个binlog日志文件内容，然后重定向到到一个文件里。 12cd /var/lib/mysql/mysqlbinlog master-log.000002 ... master-log.00000x &gt;/backup/binlog-20170912.sql 执行bin_log里的语句进行恢复： 1MariaDB &gt; \. /backup/binlog-20170912.sql 修改配置文件/etc/my.conf.d/server.cnf，把bin-log=master-log的注释去掉。然后重启MariaDB服务，即可正常使用了。 xtrabackup使用方法官网介绍：https://www.percona.com/software/mysql-database/percona-xtrabackup 使用文档：http://ovg31ww7o.bkt.clouddn.com/Percona-XtraBackup-2.4.8.pdf 1. 简介Xtrabackup是由percona提供的mysql数据库备份工具，据官方介绍，这也是世界上惟一一款开源的能够对innodb和xtradb数据库进行热备的工具。特点：(1)备份过程快速、可靠；(2)备份过程不会打断正在执行的事务；(3)能够基于压缩等功能节约磁盘空间和流量；(4)自动实现备份检验；(5)还原速度快。 2. 安装安装：yum install xtrabackup 要用新版本可以去上面的官网下载安装。 3. 命令简介我们主要用封装了的命令innobackupex。 基础用法： innobackupex --user=DBUSER --password=DBUSERPASS /path/to/BACKUP-DIR/ 4. 实际使用(1) 了解备份原理如果要使用一个最小权限的用户进行备份，则可基于如下命令创建此类用户： 1234mysql&gt; CREATE USER ’bkpuser’@’localhost’ IDENTIFIED BY ’s3cret’;mysql&gt; REVOKE ALL PRIVILEGES, GRANT OPTION FROM ’bkpuser’;mysql&gt; GRANT RELOAD, LOCK TABLES, REPLICATION CLIENT ON *.* TO ’bkpuser’@’localhost’;mysql&gt; FLUSH PRIVILEGES; 使用innobakupex备份时，其会调用xtrabackup备份所有的InnoDB表，复制所有关于表结构定义的相关文件(.frm)、以及MyISAM、MERGE、CSV和ARCHIVE表的相关文件，同时还会备份触发器和数据库配置信息相关的文件。这些文件会被保存至一个以时间命名的目录中。 在使用innobackupex进行备份时，还可以使用--no-timestamp选项来阻止命令自动创建一个以时间命名的目录；如此一来，innobackupex命令将会创建一个BACKUP-DIR目录来存储备份数据。 具体使用的一个例子： 1innobackupex --no-timestamp /xtrabackup/$(date +%Y%m%d) 在备份的同时，innobackupex还会在备份目录中创建如下文件： xtrabackup_info —— 总体备份信息。 xtrabackup_checkpoints —— 备份类型（如完全或增量）、备份状态（如是否已经为prepared状态）和LSN(日志序列号)范围信息； LSN：每个InnoDB页(通常为16k大小)都会包含一个日志序列号，即LSN。LSN是整个数据库系统的系统版本号，每个页面相关的LSN能够表明此页面最近是如何发生改变的。 xtrabackup_binlog_info —— mysql服务器当前正在使用的二进制日志文件及至备份这一刻为止二进制日志事件的位置。 backup-my.cnf —— 备份命令用到的配置选项信息。 (2) 准备(prepare)一个完全备份一般情况下，在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此，此时数据文件仍处理不一致状态。“准备”的主要作用正是通过回滚未提交的事务及同步已经提交的事务至数据文件使得数据文件处于一致性状态。 innobakupex命令的--apply-log选项可用于实现上述功能。如下面的命令： 1innobackupex --apply-log /path/to/BACKUP-DIR 实际使用例子： 1innobackupex --apply-log --no-timestamp /xtrabackup/$(date +%Y%m%d) 如果执行正确，其最后输出的几行信息通常如下： 在实现“准备”的过程中，innobackupex通常还可以使用--use-memory选项来指定其可以使用的内存的大小，默认通常为100M。如果有足够的内存可用，可以多划分一些内存给prepare的过程，以提高其完成速度。 (3) 从一个完全备份中恢复数据 注意：恢复不用启动MySQL innobackupex命令的--copy-back选项用于执行恢复操作，其通过复制所有数据相关的文件至mysql服务器DATADIR目录中来执行恢复过程。innobackupex通过backup-my.cnf来获取DATADIR目录的相关信息。 1innobackupex --copy-back /path/to/BACKUP-DIR 实际使用例子： 1innobackupex --copy-back /extrabakup/20170912 如果执行正确，其输出信息的最后几行通常如下： 请确保如上信息的最行一行出现“innobackupex: completed OK!”。 当数据恢复至DATADIR目录以后，还需要确保所有数据文件的属主和属组均为正确的用户，如mysql，否则，在启动mysqld之前还需要事先修改数据文件的属主和属组。如： 1chown -R mysql:mysql /var/lib/mysql/ (4) 使用innobackupex进行增量备份每个InnoDB的页面都会包含一个LSN信息，每当相关的数据发生改变，相关的页面的LSN就会自动增长。这正是InnoDB表可以进行增量备份的基础，即innobackupex通过备份上次完全备份之后发生改变的页面来实现。 要实现第一次增量备份，可以使用下面的命令进行： 1innobackupex --incremental /backup --incremental-basedir=BASEDIR 实际使用的例子： 做一些数据库操作，然后，做一次增量： 1innobackupex --incremental /xtrabackup/20170912-incre-1 --incremental-basedir=/xtrabackup/20170912 然后再做一些操作，在做一次增量： 1innobackupex --incremental /xtrabackup/20170912-incre-2 --incremental-basedir=/xtrabackup/20170912 其中，BASEDIR指的是完全备份所在的目录，此命令执行结束后，innobackupex命令会在/backup目录中创建一个新的以时间命名的目录以存放所有的增量备份数据。另外，在执行过增量备份之后再一次进行增量备份时，其--incremental-basedir应该指向上一次的增量备份所在的目录。 注意：增量备份仅能应用于InnoDB或XtraDB表，对于MyISAM表而言，执行增量备份时其实进行的是完全备份。 (5) 模拟数据库崩溃 注意：实验里没有备份，记得先把那几个binlog master-log.xxxxxx拷贝出来，这个在生产中都是存多份，生产中是从其他机器上copy过来的。 123456# 注意真实环境binlogs是存在其他服务器上的mkdir /xtrabackup/binlogscp /var/lib/mysql/master-log* /xtrabackup/binlogs/# 模拟崩溃systemctl stop mariadbrm -rf /var/lib/mysql/* (6) 恢复备份的数据恢复备份的数据，包括，全量备份，增量备份，还有binlog。 记得要加--apply-log 和--redo-only(指的是该提交的提交，没做完的不回滚，这里和准备备份文件那里不太一样）。 添加开始的全备文件： 1innobackupex --apply-log --redo-only BASE-DIR 接着添加第一个增量： 1innobackupex --apply-log --redo-only BASE-DIR --incremental-dir=INCREMENTAL-DIR-1 而后是第二个增量： 1innobackupex --apply-log --redo-only BASE-DIR --incremental-dir=INCREMENTAL-DIR-2 其中BASE-DIR指的是完全备份所在的目录，而INCREMENTAL-DIR-1指的是第一次增量备份的目录，INCREMENTAL-DIR-2指的是第二次增量备份的目录，其它依次类推，即如果有多次增量备份，每一次都要执行如上操作； 最后一个增量命令完成后，要在全量备份上做“回滚”操作。 1innobackupex --apply-log BASE-DIR 实际的例子： 12345678910# 打全备，该提交的提交，不回滚innobackupex --apply-log --redo-only /xtrabackup/20170912# 打第一个增备，该提交的提交，不回滚innobackupex --apply-log --redo-only /xtrabackup/20170912 --incremental-dir=/xtrabackup/20170912-incre-1# 打第二个增备，该提交的提交，不回滚innobackupex --apply-log --redo-only /xtrabackup/20170912 --incremental-dir=/xtrabackup/20170912-incre-2# 该回滚的回滚innobackupex --apply-log /xtrabackup/20170912innobackupex --copy-back /extrabackup/20170912 (7) 重放binlog直接使用上面的例子来说明: 查看最后一个增量的文件夹里的xtrabackup_binlog_info信息： 1cat /extrabackup/20170912-incre-2/xtrabackup_binlog_info 结果： 1master-log.000003 1244 123456cd /var/lib/mysql/# 真实环境是从其他备份的服务器上copy过来cp /backup/binlogs/master-log* .# 把从master-log.000003 1244位置往后的所有二进制日志，用mysqlbinlog命令生成sql语句，重定向到一个文件里。mysqlbinlog -j 1244 master-log.000003 ... master-log.00000x &gt;/xtrabackup/binlogs/binlog-20170912.sql 登录MariaDB，执行恢复binlog操作： 123MariaDb &gt; set @@session.sql_log_bin=OFF;MariaDb &gt; \. /xtrabackup/binlogs/binlog-20170912.sqlMariaDb &gt; exit 总结 需要随时准备好一台新的机器。 我们的数据库的备份数据（包括全备、增备、差异备份）都要存多份数据。 同时，binlog要同时写入到多个服务器或磁盘。防止崩溃时候，有些数据无法恢复。 还有数据库配置文件，也要在配置分发系统里保存好，一旦服务器崩溃后，可以迅速分发到新的服务器上。 恢复的过程： 关闭连接数据库的周边程序。 从配置分发系统里分发MySQL配置。 恢复全备数据。 恢复增备或差异备份数据。 恢复binlog数据。 测试数据完整性。 打开连接数据库的周边程序。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>mysqldump</tag>
        <tag>xtrabackup</tag>
        <tag>innobackupex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[制作macOS的U盘安装盘]]></title>
    <url>%2Fmacos%2F20170909-create-macos-usbdisk%2F</url>
    <content type="text"><![CDATA[App Store里下载macOS Sierra系统镜像文件。 找一个8G以上的U盘，在磁盘工具里抹掉（抹掉即格式化的意思，一次抹掉不成功，多抹几次）： 终端里df -h可以看到格式化好的u盘，挂载路径为/Volumes/udisk 终端里执行下面命令开始制作u盘： 1sudo /Applications/Install\ macOS\ Sierra.app/Contents/Resources/createinstallmedia --volume /Volumes/udisk --applicationpath /Applications/Install\ macOS\ Sierra.app --nointeraction 直到出现Copy complete和Done，表示U盘制作成功。 重启macbook，开机按住option键，磁盘界面选择名为Install macOS Sierra的一个U盘，进行安装。]]></content>
      <categories>
        <category>macos</category>
      </categories>
      <tags>
        <tag>macOS 10.12.6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS 10.13 U盘安装全过程]]></title>
    <url>%2Fmacos%2F20170909-create-macos-high-sierra-usbdisk%2F</url>
    <content type="text"><![CDATA[App Store里下载macOS High Sierra系统镜像文件。 找一个8G以上的U盘，在磁盘工具里抹掉（抹掉即格式化的意思，一次抹掉不成功，多抹几次）： 终端里df -h可以看到格式化好的u盘，挂载路径为/Volumes/udisk 终端里执行下面命令开始制作u盘： 1sudo /Applications/Install\ macOS\ High\ Sierra.app/Contents/Resources/createinstallmedia --volume /Volumes/udisk/ --applicationpath /Applications/Install\ macOS\ High\ Sierra.app --nointeraction 直到出现Copy complete和Done，表示U盘制作成功。 重启macbook，开机按住option键，磁盘界面选择名为Install High macOS Sierra的一个U盘，进行安装。]]></content>
      <categories>
        <category>macos</category>
      </categories>
      <tags>
        <tag>macOS 10.12.6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-双网络双活模式的Keepalived+Nginx配置]]></title>
    <url>%2Flinux%2F20170904-05-keepalived-nginx%2F</url>
    <content type="text"><![CDATA[Vagrant配置实验环境Vagrantfile： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure("2") do |config| # Vagrant Global Config # `longdream/centos7` is a custom centos7 box made by YuLongjun. config.vm.box = "longdream/centos7" # If this box is add online, set true will check update. # Also set `false` will not update it. # If this box is added locally, this setting is invalid. config.vm.box_check_update = false # you need `vagrant plugin install vagrant-vbguest` # You also need `vagrant plugin install vagrant-hostmanager` config.hostmanager.enabled = true # Allow update `/etc/hosts` file in VMs. config.hostmanager.manage_guest = true # Allow update `/etc/hosts` file in Hosts. config.hostmanager.manage_host = true # Create VM ka1. config.vm.define "ka1" do |ka1| ka1.vm.network "public_network", ip: "172.16.111.10", bridge: "en0: Wi-Fi (AirPort)" ka1.vm.network "private_network", ip: "192.168.111.10" ka1.vm.hostname = "ka1" end # Create VM ka2. config.vm.define "ka2" do |ka2| ka2.vm.network "public_network", ip: "172.16.111.20", bridge: "en0: Wi-Fi (AirPort)" ka2.vm.network "private_network", ip: "192.168.111.20" ka2.vm.hostname = "ka2" end # Create VM web1. config.vm.define "web1" do |web1| web1.vm.network "private_network", ip: "192.168.111.30" web1.vm.hostname = "web1" end # Create VM web2 config.vm.define "web2" do |web2| web2.vm.network "private_network", ip: "192.168.111.40" web2.vm.hostname = "web2" endend vagrant up启动4台虚拟机。 配置Keepalived初始设置（1）时间同步，略（2）关闭SELinux和防火墙，略（3）互相之间/etc/hosts文件添加对方主机名，略（4）确认接口支持多播（组播），略，基本新的网卡都支持。 打开ka1和ka2的ip_forward12echo "net.ipv4.ip_forward=1" &gt;&gt;/etc/sysctl.confecho 1 &gt; /proc/sys/net/ipv4/ip_forward 配置ka1123yum install -y keepalivedsystemctl enable keepalivedcp /etc/keepalived/keepalived.conf&#123;,.bak&#125; vim /etc/keepalived/keepalived.conf： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance External_1 &#123; state MASTER interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125;vrrp_instance External_2 &#123; state BACKUP interface eth1 virtual_router_id 172 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 9d3d15d5 &#125; virtual_ipaddress &#123; 172.16.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125;vrrp_instance Internal_1 &#123; state MASTER interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125;vrrp_instance Internal_2 &#123; state BACKUP interface eth2 virtual_router_id 192 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; 配置ka2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka2@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance External_1 &#123; state BACKUP interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; vrrp_instance External_2 &#123; state MASTER interface eth1 virtual_router_id 172 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 9d3d15d5 &#125; virtual_ipaddress &#123; 172.16.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; vrrp_instance Internal_1 &#123; state BACKUP interface eth2 virtual_router_id 191 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125;vrrp_instance Internal_2 &#123; state MASTER interface eth2 virtual_router_id 192 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; ka1和ka2分别放一个同样的脚本：/etc/keepalived/notify.sh，实现切换通知功能。 vim /etc/keepalived/notify.sh： 1234567891011121314151617181920212223242526#!/bin/bash#contact='root@localhost' notify() &#123; local mailsubject="$(hostname) to be $1, vip floating" local mailbody="$(date +'%F %T'): vrrp transition, $(hostname) changed to be $1" echo "$mailbody" | mail -s "$mailsubject" $contact&#125; case $1 inmaster) notify master ;;backup) notify backup systemctl start nginx # 此处配置后，Nginx服务挂了能自动启动 ;;fault) notify fault ;;*) echo "Usage: $(basename $0) &#123;master|backup|fault&#125;" exit 1 ;;esac 测试keepalived功能ka1： 123systemctl start keepalived.servicesystemctl status keepalived.serviceip a 在ka2没启动前，ka1添加了4个ip192.168.111.100、192.168.111.200、172.16.111.100、172.16.111.200 ka2： 123systemctl start keepalived.servicesystemctl status keepalived.serviceip a ka2一启动，ka1就移除了192.168.111.200、172.16.111.200，ka2就添加了192.168.111.200、172.16.111.200 我们停掉ka1： ka1移除了了192.168.111.100和172.16.111.100 ka2添加了192.168.111.100和172.16.111.100 ,然后拥有了4个vip。 我们还可以通过tcpdump命令来分别查看组播状态。 ka1和ka2都可运行下面命令来查看组播地址检查心跳的状态。 12tcpdump -nn -i eth1 host 224.111.111.111tcpdump -nn -i eth2 host 224.111.111.111 tips：在虚拟机软件里，关闭网卡物理连接，ip也是可以漂移的。 至此，vrrp的的高可用测试完毕。我们继续配置Nginx服务。 安装后端Web服务器web1和web2分别安装httpd或者nginx作为http服务，这里安装的httpd： web1： 1234yum install -y httpdecho "&lt;h1&gt;Real Server 1&lt;/h1&gt;" &gt; /var/www/html/index.htmlsystemctl start httpdsystemctl enable httpd web2： 1234yum install -y httpdecho "&lt;h1&gt;Real Server 2&lt;/h1&gt;" &gt; /var/www/html/index.htmlsystemctl start httpdsystemctl enable httpd 实验环境为了显示不同，故意设置成不同的页面，实际生产中获取的内容应该一致。 测试httpd功能： ka1或ka2上： 12curl 192.168.111.30curl 192.168.111.40 配置Nginx在ka1和ka2上分别yum安装Nginx最新稳定版： 1234567891011121314151617181920212223242526cat &gt; /etc/yum.repos.d/nginx.repo &lt;&lt;EOF[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/\$releasever/\$basearch/gpgcheck=0enabled=1EOFyum install -y nginx mv /etc/nginx/conf.d/default.conf&#123;,.bak&#125; cat &gt;/etc/nginx/conf.d/vhost1.conf &lt;&lt;EOFupstream websrvs &#123; server 192.168.111.30:80; server 192.168.111.40:80;&#125;server &#123; location / &#123; proxy_pass http://websrvs; &#125;&#125;EOFsystemctl start nginx 分别在ka1和ka2上测试： 1for i in `seq 10`; do curl 192.168.111.10; done 至此，Nginx功能也实现了。我们接下来把Nginx和Keepalived结合起来，使Nginx支持高可用。 配置Keepalived使Nginx实现高可用在ka1和ka2的/etc/keepalived/keepalived.conf的全局配置块global_defs下方配置vrrp_script块： 1234567vrrp_script chk_nginx &#123; script &quot;killall -0 nginx&quot; interval 2 weight -10 fall 2 rise 2&#125; 在所有vrrp_instance实例块里，添加track_script块： 123track_script &#123; chk_nginx&#125; 配置完后，重启ka1和ka2的keepalived服务。 12systemctl stop keepalivedsystemctl start keepalived 一开始……实验并未成功 警告：此处测试没有成功，o(╯□╰)o，Keepalived正常，但是停掉Nginx后，不会降权，查了好久配置……，最后准备自己写个脚本shell自检测Nginx的时候，写完脚本运行时候发现，没装killall命令！所以不会降权！……坑啊！ 安装psmisc包，包含killall命令。 1yum install psmisc 安装完killall后，测试完美开一个循环，查看效果：1while True;do curl 172.16.111.100; curl 172.16.111.200;sleep 0.5; done 关闭任意一个Keepalived服务，不受影响。 在Keepalived服务启动的时候，停掉Nginx服务，过一会儿，会自行恢复，如下： 完整配置文件附录附上所有完整配置文件的下载链接：(用的七牛云存储，点击即可下载，或者右键“链接另存为”下载） 实验环境的Vagrant配置文件Vagrantfile：Vagrantfile ka1的Keepalived配置文件/etc/keepalived/keepalived.conf：keepalived.conf(ka1) ka2的Keepalived配置文件/etc/keepalived/keepalived.conf： keepalived.conf(ka2) ka1和ka2的Keepalived调用的通知脚本/etc/keepalived/notify.sh：notify.sh ka1和ka2的Nginx配置文件/etc/nginx/conf.d/vhost1.conf：vhost1.conf]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>VRRP</tag>
        <tag>Keepalived+Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-单网络双活模式的Keepalived+LVS-DR配置]]></title>
    <url>%2Flinux%2F20170904-04-keepalived-lvs%2F</url>
    <content type="text"><![CDATA[Vagrant配置实验环境Vagrantfile： 1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure("2") do |config| # Vagrant Global Config # `longdream/centos7` is a custom centos7 box made by YuLongjun. config.vm.box = "longdream/centos7" # If this box is add online, set true will check update. # Also set `false` will not update it. # If this box is added locally, this setting is invalid. config.vm.box_check_update = false # you need `vagrant plugin install vagrant-vbguest` # You also need `vagrant plugin install vagrant-hostmanager` config.hostmanager.enabled = true # Allow update `/etc/hosts` file in VMs. config.hostmanager.manage_guest = true # Allow update `/etc/hosts` file in Hosts. config.hostmanager.manage_host = true # Create VM ka1. config.vm.define "ka1" do |ka1| ka1.vm.network "private_network", ip: "192.168.111.10" ka1.vm.hostname = "ka1" end # Create VM ka2. config.vm.define "ka2" do |ka2| ka2.vm.network "private_network", ip: "192.168.111.20" ka2.vm.hostname = "ka2" end # Create VM web1. config.vm.define "web1" do |web1| web1.vm.network "private_network", ip: "192.168.111.30" web1.vm.hostname = "web1" end # Create VM web2 config.vm.define "web2" do |web2| web2.vm.network "private_network", ip: "192.168.111.40" web2.vm.hostname = "web2" endend vagrant up启动4台虚拟机。 配置Keepalived初始设置（1）时间同步，略（2）关闭SELinux和防火墙，略（3）互相之间/etc/hosts文件添加对方主机名，略（4）确认接口支持多播（组播），略，基本新的网卡都支持。 配置ka1的keepalived服务12yum install -y keepalivedsystemctl enable keepalived vim /etc/keepalived/keepalived.conf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 60 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance VI_1 &#123; state MASTER interface eth1 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125;vrrp_instance VI_2 &#123; state BACKUP interface eth1 virtual_router_id 192 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; 配置ka212yum install -y keepalivedsystemctl enable keepalived vim /etc/keepalived/keepalived.conf 123456789101112131415161718192021222324252627282930313233343536373839404142434445global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka2@localhost smtp_server 127.0.0.1 smtp_connect_timeout 60 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance VI_1 &#123; state BACKUP interface eth1 virtual_router_id 191 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125;vrrp_instance VI_2 &#123; state MASTER interface eth1 virtual_router_id 192 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; ka1和ka2分别放一个同样的脚本：/etc/keepalived/notify.sh，实现切换通知功能。 vim /etc/keepalived/notify.sh 12345678910111213141516171819202122232425#!/bin/bash#contact='root@localhost' notify() &#123; local mailsubject="$(hostname) to be $1, vip floating" local mailbody="$(date +'%F %T'): vrrp transition, $(hostname) changed to be $1" echo "$mailbody" | mail -s "$mailsubject" $contact&#125; case $1 inmaster) notify master ;;backup) notify backup ;;fault) notify fault ;;*) echo "Usage: $(basename $0) &#123;master|backup|fault&#125;" exit 1 ;;esac 测试keepalived功能ka1： 123systemctl start keepalived.servicesystemctl status keepalived.serviceip a 在ka2没启动前，ka1先添加了ip192.168.111.100，监测不到192.168.111.200，又添加了192.168.111.200。 ka2： 123systemctl start keepalived.servicesystemctl status keepalived.serviceip a ka2一启动，ka1就移除了192.168.111.200，ka2就添加了192.168.111.200 我们停掉ka1的keepalived服务： ka1就移除了了192.168.111.100ka2就添加了192.168.111.100 我们还可以通过tcpdump命令来分别查看组播状态。 ka1和ka2都可运行下面命令来查看组播地址检查心跳的状态: 1tcpdump -nn -i eth1 host 224.111.111.111 tips：在虚拟机软件里，关闭网卡物理连接，ip也是可以漂移的。 至此，vrrp的的高可用测试完毕。我们继续配置LVS相关的配置。 配置LVS相关的配置。配置VS(Virtual Server,也叫Director)我们分别在ka1(ka1也是vs1)和ka2(ka2也是vs2)上安装lvs，然后停止keepalived服务，添加VS配置： 1systemctl stop keepalived 分别修改/etc/keepalived/keepalived.conf,在后面添加Virtual Server相关的配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465virtual_server 192.168.111.100 80 &#123; delay_loop 3 lb_algo rr lb_kind DR protocol TCP sorry_server 127.0.0.1 80 real_server 192.168.111.30 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125; real_server 192.168.111.40 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125;&#125;virtual_server 192.168.111.200 80 &#123; delay_loop 3 lb_algo rr lb_kind DR protocol TCP sorry_server 127.0.0.1 80 real_server 192.168.111.30 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125; real_server 192.168.111.40 80 &#123; weight 1 HTTP_GET &#123; url &#123; path / status_code 200 &#125; connect_timeout 1 nb_get_retry 3 delay_before_retry 1 &#125; &#125;&#125; 上述keepalived设置中的设置了sorry_server为本地的http服务，即要在ka1和ka2上做一个Sorry Server，所以分别提供一个web服务（可采用httpd或者apache都行，这里采用的是httpd）对外say sorry： ka1： 123456yum install -y httpdecho "&lt;h1&gt;Sorry Server@ka1&lt;/h1&gt;" &gt; /var/www/html/index.htmlsystemctl start httpdsystemctl enable httpdsystemctl start keepalivedsystemctl enable keepalived ka2： 123456yum install -y httpdecho "&lt;h1&gt;Sorry Server@ka2&lt;/h1&gt;" &gt; /var/www/html/index.htmlsystemctl start httpdsystemctl enable httpdsystemctl start keepalivedsystemctl enable keepalived 实验环境为了显示不同，故意设置成不同的页面，实际生产获取的内容应该保持一致。 配置RS(Real Server)web1和web2web1和web2分别安装httpd或者nginx作为http服务，这里安装的httpd： web1： 1234yum install -y httpdecho "&lt;h1&gt;Real Server 1&lt;/h1&gt;" &gt; /var/www/html/index.htmlsystemctl start httpdsystemctl enable httpd web2： 1234yum install -y httpdecho "&lt;h1&gt;Real Server 2&lt;/h1&gt;" &gt; /var/www/html/index.htmlsystemctl start httpdsystemctl enable httpd 实验环境为了显示不同，故意设置成不同的页面，实际生产中获取的内容应该一致。 web1 和web2 上添加脚本set_rs.sh： vim ~/set_rs.sh 1234567891011121314151617181920212223242526272829#!/bin/bashvip1=192.168.111.100vip2=192.168.111.200dev1=lo:1dev2=lo:2case $1 instart) echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 1 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce echo 2 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce ifconfig $dev1 $vip1 netmask 255.255.255.255 broadcast $vip1 up ifconfig $dev2 $vip2 netmask 255.255.255.255 broadcast $vip2 up echo "VS Server is Ready!" ;;stop) ifconfig $dev down echo 0 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 0 &gt; /proc/sys/net/ipv4/conf/all/arp_announce echo 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce echo "VS Server is Cancel!" ;;*) echo "Usage `basename $0` start|stop" exit 1 ;;esac 分别运行bash set_rs.sh start。 总体测试测试代码： 1234for i in `seq 5`; do curl 192.168.111.100 curl 192.168.111.200done 全部开启的情况下测试： 关闭web2的httpd服务： 关闭ka2的keepalived服务： 关闭 web1的httpd服务： 完整配置文件附录附上所有完整配置文件的下载链接：(用的七牛云存储，点击即可下载，或者右键“链接另存为”下载） 实验环境的Vagrant配置文件Vagrantfile：Vagrantfile ka1的Keepalived配置文件/etc/keepalived/keepalived.conf：keepalived.conf(ka1) ka2的Keepalived配置文件/etc/keepalived/keepalived.conf： keepalived.conf(ka2) ka1和ka2的Keepalived调用的通知脚本/etc/keepalived/notify.sh：notify.sh]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>VRRP</tag>
        <tag>Keepalived+LVS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-Keepalived各种模式配置]]></title>
    <url>%2Flinux%2F20170904-03-keepalived-models%2F</url>
    <content type="text"><![CDATA[由于图片花费很长时间制作，本文图片转载或使用请务必加上原文链接，图片中的作者信息也请勿删除，谢谢！ 下面仅仅是Keepalived配置里只关于Keepalived部分的相关的配置。从最简单的单网络单主模式，一直到双网络同步漂移的双主模式。 单网络的Master-Backup主备模式（单主模式） ka1 的/etc/keepalived/keepalived.conf配置： 1234567891011121314151617181920212223242526272829global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance VG_1 &#123; state MASTER interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; ka2 的/etc/keepalived/keepalived.conf配置： 1234567891011121314151617181920212223242526272829global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance VG_1 &#123; state BACKUP interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; 单网络的Active-Active双活模式（双主模式） ka1 的/etc/keepalived/keepalived.conf配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance VG_1 &#123; state MASTER interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125;vrrp_instance VG_2 &#123; state BACKUP interface eth2 virtual_router_id 192 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125; ka2 的/etc/keepalived/keepalived.conf配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance VG_1 &#123; state BACKUP interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault" &#125;vrrp_instance VG_2 &#123; state MASTER interface eth2 virtual_router_id 192 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; 双网络（内外网）非同步漂移的Master-Backup主备模式（单主模式）一般生产环境内外网是分开的，所以一般有两个网络，一个内网网络，一个外网网络，内网网络和外网网络不用同步漂移，比如Keepalived+LVS-DR、Keepalived+Nginx、Keepalived+HAProxy，都是不用同步漂移的。（Keepalived+LVS-NAT是需要同步漂移的。） ka1 的/etc/keepalived/keepalived.conf配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_sync_group VG_1 &#123; group &#123; External_1 Internal_1 &#125;&#125;vrrp_instance External_1 &#123; state MASTER interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_1 &#123; state MASTER interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; ka2 的/etc/keepalived/keepalived.conf配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance External_1 &#123; state BACKUP interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_1 &#123; state BACKUP interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; 双网络（内外网）同步漂移的Master-Backup主备模式（单主模式）一般生产环境内外网是分开的，所以一般有两个网络，一个内网网络，一个外网网络，而且内网网络和外网网络要实现同步漂移，比如Keepalived+LVS-NAT模式，那么就用到vrrp_sync_group来设置同步漂移组 ka1 的/etc/keepalived/keepalived.conf配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_sync_group VG_1 &#123; group &#123; External_1 Internal_1 &#125;&#125;vrrp_instance External_1 &#123; state MASTER interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_1 &#123; state MASTER interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; ka2 的/etc/keepalived/keepalived.conf配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_sync_group VG_1 &#123; group &#123; External_1 Internal_1 &#125;&#125;vrrp_instance External_1 &#123; state BACKUP interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_1 &#123; state BACKUP interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; 双网络（内外网）非同步漂移的Active-Active双活模式（双主模式）一般生产环境内外网是分开的，所以一般有两个网络，一个内网网络，一个外网网络，内网网络和外网网络不用同步漂移，比如Keepalived+LVS-DR、Keepalived+Nginx、Keepalived+HAProxy，都是不用同步漂移的。（Keepalived+LVS-NAT是需要同步漂移的。） ka1 的/etc/keepalived/keepalived.conf配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance External_1 &#123; state MASTER interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance External_2 &#123; state BACKUP interface eth1 virtual_router_id 172 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 9d3d15d5 &#125; virtual_ipaddress &#123; 172.16.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_1 &#123; state MASTER interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_2 &#123; state BACKUP interface eth2 virtual_router_id 192 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; ka2 的/etc/keepalived/keepalived.conf配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_instance External_1 &#123; state BACKUP interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance External_2 &#123; state MASTER interface eth1 virtual_router_id 172 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 9d3d15d5 &#125; virtual_ipaddress &#123; 172.16.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_1 &#123; state BACKUP interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_2 &#123; state MASTER interface eth2 virtual_router_id 192 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; 双网络（内外网）同步漂移的Active-Active双活模式（双主模式）一般生产环境内外网是分开的，所以一般有两个网络，一个内网网络，一个外网网络，而且内网网络和外网网络要实现同步漂移，比如Keepalived+LVS-NAT模式，那么就用到vrrp_sync_group来设置同步漂移组，如果要做双活，那么就分别两端加两个vip，互为主备。 ka1 的/etc/keepalived/keepalived.conf配置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_sync_group VG_1 &#123; group &#123; External_1 Internal_1 &#125;&#125;vrrp_sync_group VG_2 &#123; group &#123; External_2 Internal_2 &#125;&#125;vrrp_instance External_1 &#123; state MASTER interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance External_2 &#123; state BACKUP interface eth1 virtual_router_id 172 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 9d3d15d5 &#125; virtual_ipaddress &#123; 172.16.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_1 &#123; state MASTER interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_2 &#123; state BACKUP interface eth2 virtual_router_id 192 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; ka2 的/etc/keepalived/keepalived.conf配置： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id ka1 vrrp_mcast_group4 224.111.111.111&#125;vrrp_sync_group VG_1 &#123; group &#123; External_1 Internal_1 &#125;&#125;vrrp_sync_group VG_2 &#123; group &#123; External_2 Internal_2 &#125;&#125;vrrp_instance External_1 &#123; state BACKUP interface eth1 virtual_router_id 171 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1402b1b5 &#125; virtual_ipaddress &#123; 172.16.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance External_2 &#123; state MASTER interface eth1 virtual_router_id 172 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 9d3d15d5 &#125; virtual_ipaddress &#123; 172.16.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_1 &#123; state BACKUP interface eth2 virtual_router_id 191 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 0702f7ab &#125; virtual_ipaddress &#123; 192.168.111.100 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125;vrrp_instance Internal_2 &#123; state MASTER interface eth2 virtual_router_id 192 priority 95 advert_int 1 authentication &#123; auth_type PASS auth_pass 85c9a27b &#125; virtual_ipaddress &#123; 192.168.111.200 &#125; notify_master "/etc/keepalived/notify.sh master" notify_backup "/etc/keepalived/notify.sh backup" notify_fault "/etc/keepalived/notify.sh fault"&#125; notfiy.sh脚本内容见上一节。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Keepalived</tag>
        <tag>VRRP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-Keepalived配置]]></title>
    <url>%2Flinux%2F20170904-02-keepalived-configuration%2F</url>
    <content type="text"><![CDATA[Keepalived HA cluster的配置前提(1) 各节点时间必须同步：可以使用ntp(CentOS6&amp;7)或者chrony(CentOS 7)。(2) 确保iptables及selinux不会成为阻碍。(3) 各节点之间可通过主机名互相通信（对KA并非必须）：建议使用/etc/hosts文件实现(DNS服务如果有问题，还不如hosts文件好用）(4) 确保各节点的用于集群服务的接口支持MULTICAST通信：多播或叫组播，使用D类地址(224-239)。（多播地址最好不要使用默认的，手动修改一下。因为如果好多个集群服务都是用默认的，虽然有认证机制，但是也会互相发送信息，虽然因为认证机制丢弃掉了，但也影响性能，也会产生无用的日志。） （1）所有服务器都要做时间服务器同步。 第一种方法： ntp 1ntpdate TIME_SERVER_IP TIME_SERVER_IP换成时间服务器IP。 第二种方法：chrony vim /etc/chrony.conf 12345#server 0.centos.pool.ntp.org iburst#server 1.centos.pool.ntp.org iburst#server 2.centos.pool.ntp.org iburst#server 3.centos.pool.ntp.org iburstserver TIME_SERVER_IP iburst TIME_SERVER_IP换成时间服务器IP。 systemctl restart chronyd.service chrony sources可以查看同步时间情况。TIME_SERVER_IP换成时间服务器IP。 （2）iptables、SELinux、/etc/hosts自行设定。 （3）Keepalived集群的组播地址不采用系统默认的。 组播地址定为224.111.111.111。 Keepalived配置详解Keepalived的配置文件：/etc/keepalived/keepalived.conf man keepalived.conf可以查看配置详情解析。 配置分为三大项配置块： Global configuration，全局配置，包含两个子配置块： 全局定义：global_defs。 静态地址和路由：static_ipaddress、static_routes VRRP configuration，VRRP配置，包含两个子配置块： VRRP同步组：vrrp_sync_group。 VRRP实例：vrrp_instance。 LVS configuration，LVS配置。如果要用Keepalived+LVS的话，需要使用这段配置，如果是用其他的如Keepalived+Nginx的话，是不需要配置的。LVS的话，包含两个子配置块： 虚拟服务器组： virtual_server_group。可选项,大型LVS集群才会用。 虚拟服务器：virtual_server。每个虚拟服务器里面又包含多个真实服务器real_server。 Global Configuration12345678910global_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from ka1@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 # 前面这几段随意配置，一般不会用邮件通知，一般会有监控软件来监控 router_id ka1 vrrp_mcast_group4 224.111.111.111 # 多播地址段，最好自定义，不要用系统默认的&#125; VRRP ConfigurationVRRP同步组配置块：vrrp_sync_group定义一个vrrp同步组，当同步组内的instance挂了一个，组内的所有instance都Failover（故障转移）到另外一个节点。 123456vrrp_sync_group GROUP_NAME &#123; group &#123; INSTANCE_NAME INSTANCE_NAME &#125;&#125; 常见的一个案例是，在有连接外网的地址和内网的地址都有，并且两个VIP做了ip_forward，需要两个VIP同时漂移。如下面例子，External_1为外网的VRRP实例，Internal_2为内网的VRRP实例。 123456vrrp_sync_group VG_1 &#123; group &#123; External_1 Internal_2 &#125;&#125; VRRP实例配置块：vrrp_instance定义vrrp实例。 12345678910111213141516171819202122232425262728293031323334353637383940vrrp_instance INSTANCE_NAME &#123; state MASTER|BACKUP # 当前节点在此虚拟路由器上的初始状态；只能有一个是MASTER，余下的都应该为BACKUP； interface IFACE_NAME # 绑定为当前虚拟路由器使用的物理接口；IFACE_NAME是网卡名，如`eth0` virtual_router_id VRID # 当前虚拟路由器的惟一标识，VRID代表一个数字，范围是0-255； priority PRI # 当前主机在此虚拟路径器中的优先级；PRI范围是1-254； advert_int INT # vrrp通告的时间间隔；INT单位为秒，比如`1`，代表通告间隔为1秒 # 认证模块，同一个实例的主和备，要用同一个认证方式和密码 authentication &#123; auth_type AH|PASS # 认证方式：PASS为简单字符串密码，推荐使用；AH为IPSEC方式，不推荐使用 auth_pass &lt;PASSWORD&gt; # 密码，只有8个字符被使用，写多了会截取前8位 &#125; # 虚拟地址，即Floating IP virtual_ipaddress &#123; # 格式为：&lt;IPADDR&gt;/&lt;MASK&gt; brd &lt;IPADDR&gt; dev &lt;STRING&gt; scope &lt;SCOPE&gt; label &lt;LABEL&gt; # 可以简写为单个地址，系统会默认计算掩码和设备，也可以写label别名。 192.168.200.15 # 192.168.200.16/24 # 192.168.200.17/24 dev eth1 # 192.168.200.18/24 dev eth2 label eth2:1 &#125; # 配置要监控的网络接口，一旦配置中的一个接口出现故障，这个实例就转为FAULT状态 track_interface &#123; eth0 eth1 ... &#125; # 抢占模式相关配置 nopreempt # 定义工作模式为非抢占模式 preempt_delay 300 # 抢占式模式下，节点上线后触发新选举操作的延迟时长（在没有nopreempt的时候才能使用） # 定义通知脚本 notify_master &lt;STRING&gt;|&lt;QUOTED-STRING&gt; # 当前节点成为主节点时触发的脚本 notify_backup &lt;STRING&gt;|&lt;QUOTED-STRING&gt; # 当前节点转为备节点时触发的脚本 notify_fault &lt;STRING&gt;|&lt;QUOTED-STRING&gt; # 当前节点转为“失败”状态时触发的脚本 notify &lt;STRING&gt;|&lt;QUOTED-STRING&gt; # 通用格式的通知触发机制，一个脚本可完成以上三种状态的转换时的通知&#125; 下面为一段通用格式的通知脚本： 12345678910111213141516171819202122232425#!/bin/bash#contact='root@localhost' notify() &#123; local mailsubject="$(hostname) to be $1, vip floating" local mailbody="$(date +'%F %T'): vrrp transition, $(hostname) changed to be $1" echo "$mailbody" | mail -s "$mailsubject" $contact&#125; case $1 inmaster) notify master ;;backup) notify backup ;;fault) notify fault ;;*) echo "Usage: $(basename $0) &#123;master|backup|fault&#125;" exit 1 ;;esac 在vrrp_instance配置块里定义通知脚本的方法： 123notify_master "/etc/keepalived/notify.sh master"notify_backup "/etc/keepalived/notify.sh backup"notify_fault "/etc/keepalived/notify.sh fault" 检测机制程序自带的检测示例： cat /usr/share/doc/keepalived-1.2.13/samples/keepalived.conf.vrrp.localcheck 1234567891011121314151617181920212223242526272829303132333435363738394041424344vrrp_script chk_sshd &#123; script "killall -0 sshd" # cheaper than pidof interval 2 # check every 2 seconds weight -4 # default prio: -4 if KO fall 2 # require 2 failures for KO rise 2 # require 2 successes for OK&#125;vrrp_script chk_http_port &#123; script "&lt;/dev/tcp/127.0.0.1/80" # connects and exits interval 1 # check every second weight -2 # default prio: -2 if connect fails&#125;# .............vrrp_instance VI_1 &#123; interface eth0 state MASTER virtual_router_id 51 priority 100 virtual_ipaddress &#123; 192.168.200.18/25 &#125; track_interface &#123; eth1 weight 2 # prio = +2 if UP eth2 weight -2 # prio = -2 if DOWN eth3 # no weight, fault if down &#125; track_script &#123; chk_sshd # use default weight from the script chk_haproxy weight 2 # +2 if process is present chk_http_port chk_https_port chk_smtp_port &#125;&#125;track_script &#123; chk_sshd # use default weight from the script chk_haproxy weight 2 # +2 if process is present # ......... &#125;# ............... 检测分为两个地方：vrrp_script(示例里通过track_script调用定义的vrrp_script)和track_interface： vrrp_script，跟踪脚本返回值： script &quot;SCRIPT&quot;，这里写检测脚本。返回值是0，不执行后续操作，返回值是1，执行后续操作。 例子1：script &quot;killall -0 sshd&quot;。是检测服务是否存在，如果服务存在，返回值是0，无操作；服务不存在，返回值是1，执行后续操作。 例子2：script &quot;&lt;/dev/tcp/127.0.0.1/80&quot; 检测端口80是否打开，如果端口打开，返回值是0，无操作；端口未打开，返回值是1，执行后续操作。 interval 2， 检测间隔为2秒。 weight -4，如果检测返回值为1（失败），则权重-4 weight 2，如果检测返回值为0，则权重+2 fall 2，需要2次失败才认为是失败 rise 2，需要2次成功能认为是成功 track_script，定义调用哪个vrrp_script： chk_sshd,直接使用脚本里的默认权重。 # use default weight from the script chk_haproxy weight 2，自定义权重，如果进程存在，则权重+2 # +2 if process is present 有了vrrp_script，我们就可以基于服务层面来做Floating IP了，也就能实现其他服务，如Nginx或HAProxy的高可用。 track_interface，跟踪网卡状态： eth1 weight 2，如果是up状态，则权重+2(prio = +2 if UP)。 eth2 weight -2，如果是down状态，则权重-2(prio = -2 if DOWN)。 eth3，没有写权重的话，如果是down状态，则失败(no weight, fault if down)。 下一节我们来具体配置Keepalived的各种模式，包含： 单网络的Master-Backup主备模式（单主模式） 单网络的Active-Active双活模式（双主模式） 双网络（内外网）的Master-Backup主备模式（单主模式） 双网络（内外网）的Active-Active双活模式（双主模式）]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Keepalived</tag>
        <tag>VRRP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-Keepalived介绍]]></title>
    <url>%2Flinux%2F20170904-01-keepalived-introduction%2F</url>
    <content type="text"><![CDATA[HA（高可用，High availability）里的几个概念SPOF(single point of failure)单点故障 MTBF(Mean Time Between Failures)：平均故障间隔MTTR(Mean Time To Repair)：平均修复时间A(Availability)：可用性A=MTBF/(MTBF+MTTR)：99%、99.9%、99.99%、99.999%、99.9999% 用高可用的方案可以减少无故障时间。 Failover(故障转移)：即当活动的服务或应用意外终止时，快速启用冗余或备用的服务器、系统、硬件或者网络接替它们工作。Failback(故障恢复)：是将系统，组件，服务恢复到故障之前的组态。 解决高可用中网络分区的方法两节点集群容易出现网络分区（Network Partition）。网络分区是一种在系统的任何两个组之间的所有网络连接同时发生故障后所出现的情况。发生这种情况时，分裂的系统双方都会从对方一侧重新启动应用程序，进而导致重复服务或脑裂（Brain Split）。如果一个群集中配置的两个独立系统具有对指定资源（通常是文件系统或卷）的独占访问权限，则会发生裂脑情况。由网络分裂造成的最为严重的问题是它会影响共享磁盘上的数据。 在两节点集群中（偶数节点也有这种情况，也要考虑），由于网络分区导致集群的节点之间无法正常通信，无法判断对方是否故障，各持有一票无法裁决。这时候就需要引入观察者，一个拥有投票权，但不拥有被投票权的设备。 我们可以引入前端路由作为观察者，能跟前端路由通信的就获得一票，那么能通讯的集群节点就对外服务，前端的路由也叫做一个ping node(ping节点，只用来做探测和投票的，没有被投票权）。 我们可以引入一个仲裁盘（quorum disk），活动节点周期性的写入磁盘，备用节点接受心跳，并能周期性的读到磁盘上的写入的数据。如果不能接受到心跳，也无法读到周期性的数据，那么就可以认为活动节点挂了，这时候就可以让备用节点变为活动节点对外服务。 多节点集群，只有一个对外服务，会导致资源大量的浪费（尤其节点很多的情况下），为了避免资源浪费，那么我们就可以让多个主机同时工作起来。但是需要故障时候，ip地址可以漂移到另外一个节点。这种方式叫做双主模型。 STONITH（shoot the other node in the head）： Heartbeat软件包里提供了这样的一个STONITH组件，它允许使用一个远程或“智能的”连接到健康服务器的电源设备自动重启失效服务器的电源，stonith设备可以关闭电源并响应软件命令，运行Heartbeat的服务器可以通过串口线或网线向stonith设备发送命令，它控制高可用服务器对中其他服务器的电力供应，换句话说，主服务器可以复位备用服务器的电源，备用服务器也可以复位主服务器的电源。 HA Cluster实现方案：VRRP协议的实现：KeepalivedAIS（完备HA集群）：RHCS(corosync + pacemaker)、heartbeat VRRP协议VRRP：Virtual Router redundancy Protocol(虚拟路由器冗余协议) 相关术语： 虚拟路由器(Virtual Router)：由一个 Master 路由器和多个 Backup 路由器组成。主机将虚拟路由器当作默认网关。 VRID：虚拟路由器的标识。 有相同 VRID 的一组路由器构成一个虚拟路由器。 Master 路由器：虚拟路由器中承担报文转发任务的路由器。 Backup 路由器：Master 路由器出现故障时，能够代替 Master 路由器工作的路由器。 虚拟 IP 地址(VIP: Virtual IP)：虚拟路由器的 IP 地址。一个虚拟路由器可以拥有一个或多个IP 地址。 IP 地址拥有者：接口 IP 地址与虚拟 IP 地址相同的路由器被称为 IP 地址拥有者。 虚拟 MAC 地址(VMAC: Virutal MAC)：一个虚拟路由器拥有一个虚拟 MAC 地址。虚拟 MAC 地址的格式为 00-00-5E-00-01-{VRID}。通常情况下，虚拟路由器回应 ARP 请求使用的是虚拟 MAC 地址，只有虚拟路由器做特殊配置的时候，才回应接口的真实 MAC 地址。 优先级(Priority)：VRRP 根据优先级来确定虚拟路由器中每台路由器的地位。 VRRP详细内容可见：VRRP技术白皮书需要细读一下。了解VRRP工作原理。 VRRP工作模式VRRP全称 Virtual Router Redundancy Protocol，即 虚拟路由冗余协议。可以认为它是实现路由器高可用的容错协议，即将N台提供相同功能的路由器组成一个路由器组(Router Group)，这个组里面有一个master和多个backup，但在外界看来就像一台一样，构成虚拟路由器，拥有一个虚拟IP（vip，也就是路由器所在局域网内其他机器的默认路由），占有这个IP的master实际负责ARP相应和转发IP数据包，组中的其它路由器作为备份的角色处于待命状态。master会发组播消息，当backup在超时时间内收不到vrrp包时就认为master宕掉了，这时就需要根据VRRP的优先级来选举一个backup当master，保证路由器的高可用。 在VRRP协议实现里，虚拟路由器使用 00-00-5E-00-01-XX 作为虚拟MAC地址，XX就是唯一的 VRID （Virtual Router IDentifier），这个地址同一时间只有一个物理路由器占用。在虚拟路由器里面的物理路由器组里面通过多播IP地址 224.0.0.18 来定时发送通告消息。每个Router都有一个 1-255 之间的优先级别，级别最高的（highest priority）将成为主控（master）路由器。通过降低master的优先权可以让处于backup状态的路由器抢占（pro-empt）主路由器的状态，两个backup优先级相同的IP地址较大者为master，接管虚拟IP。 单主模式（主备模式）： 单主模式仅仅一个虚拟IP，仅实现高可用。 多活模式（或叫多主模式、负载均衡的高可用模式）： 对外表现为多个虚拟IP，这样就可以同时实现负载均衡和高可用。 上面两图是针对正向代理的，不过和反向代理的原理一样，大家只要理解VRRP原理就可以了，不比纠结于图示。 Heartbeat、Corosync、Keepalived对比Heartbeat、Corosync、Keepalived这三个集群组件我们到底选哪个好，首先我想说明的是，Heartbeat、Corosync是属于同一类型，Keepalived与Heartbeat、Corosync，根本不是同一类型的。Keepalived使用的vrrp协议方式，虚拟路由冗余协议 (Virtual Router Redundancy Protocol，简称VRRP)；Heartbeat或Corosync是基于主机或网络服务的高可用方式；简单的说就是，Keepalived的目的是模拟路由器的高可用，Heartbeat或Corosync的目的是实现Service的高可用。 所以一般Keepalived是实现前端高可用，常用的前端高可用的组合有，就是我们常见的LVS+Keepalived、Nginx+Keepalived、HAproxy+Keepalived。而Heartbeat或Corosync是实现服务的高可用，常见的组合有Heartbeat v3(Corosync)+Pacemaker+NFS+Httpd 实现Web服务器的高可用、Heartbeat v3(Corosync)+Pacemaker+NFS+MySQL 实现MySQL服务器的高可用。总结一下，Keepalived中实现轻量级的高可用，一般用于前端高可用，且不需要共享存储，一般常用于两个节点的高可用。而Heartbeat(或Corosync)一般用于服务的高可用，且需要共享存储，一般用于多节点的高可用。这个问题我们说明白了。 又有博友会问了，那heartbaet与corosync我们又应该选择哪个好啊，我想说我们一般用corosync，因为corosync的运行机制更优于heartbeat，就连从heartbeat分离出来的pacemaker都说在以后的开发当中更倾向于corosync，所以现在corosync+pacemaker是最佳组合。 KeepalivedKeepalived是一个基于VRRP协议来实现的服务高可用方案，可以利用其来避免IP单点故障。 一个WEB服务至少会有2台服务器运行Keepalived，一台为主服务器（Master），一台为备份服务器（Backup），但是对外表现为一个虚拟IP，主服务器会发送特定的消息给备份服务器，当备份服务器收不到这个消息的时候，即主服务器宕机的时候，备份服务器就会接管虚拟IP，继续提供服务，从而保证了高可用性(主备模式）。当然，也可以对外表现为多个虚拟IP，这样就可以同时实现负载均衡和高可用。（两台的话叫主主模式，或者叫双活模式） 但是它一般不会单独出现，而是与其它负载均衡技术（如LVS、HAProxy、Nginx）一起工作来达到集群的高可用。 Keepalive是vrrp协议的软件实现，原生设计的目的为了高可用ipvs（LVS、Nginx、HAProxy）服务： 基于vrrp协议完成地址流动(IP Floating)。 为集群内的所有的节点生成ipvs规则（在配置文件中预先定义）。 为ipvs集群的各RS做健康状态检测。 基于脚本调用接口通过执行脚本完成脚本中定义的功能，进而影响集群事务。 组件： 核心组件： vrrp stack：VRRP协议的实现 ipvs wrapper：为集群内的所有的节点生成ipvs规则 checkers：为ipvs集群的各RS做健康状态检测。 控制组件：配置文件分析器 IO复用器 内存管理组件]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Keepalived</tag>
        <tag>VRRP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keepalived系列文章]]></title>
    <url>%2Flinux%2F20170904-00-keepalived-content%2F</url>
    <content type="text"><![CDATA[✔01-Keepalived介绍 ✔02-Keepalived配置 ✔03-Keepalived各种模式配置 ✔04-单网络双活模式的Keepalived+LVS-DR配置 ✔05-双网络双活模式的Keepalived+Nginx配置]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat主配置详解]]></title>
    <url>%2Flinux%2F20170830-03-tomcat-server-config%2F</url>
    <content type="text"><![CDATA[/etc/tomcat/server.xmlserver.xml是Tomcat的主配置文件。我们来看一下配置文件里各个条目的意义： &lt;Server port=&quot;8005&quot; shutdown=&quot;SHUTDOWN&quot;&gt;一个Server代表tomcat instance，即表现出的一个java进程；监听在8005端口，只接收“SHUTDOWN”。各server监听的端口不能相同，因此，在同一物理主机启动多个实例时，需要修改其监听端口为不同的端口。 通常我们把这个关闭：&lt;Server port=&quot;-1&quot; shutdown=&quot;480f5eca50a06b1e063165c5e30ab0c3415dd5e0&quot;&gt; tips：那串字符串是通过openssl rand -hex 20生成的。 &lt;listener className=&quot;xxxxx&quot;&gt;侦听器，监视资源的改变，并做相应的操作，多数情况不需要改变。 &lt;GlobalNamingResources&gt;全局命名资源。 子项&lt;Resource&gt;就是一个个的资源信息。指向了资源的路径(pathname)。 系统默认提供了一个认证用的资源信息,名为UserDatabase, 后面的Realm会调用这个资源： 12345&lt;Resource name=&quot;UserDatabase&quot; auth=&quot;Container&quot; type=&quot;org.apache.catalina.UserDatabase&quot; description=&quot;User database that can be updated and saved&quot; factory=&quot;org.apache.catalina.users.MemoryUserDatabaseFactory&quot; pathname=&quot;conf/tomcat-users.xml&quot; /&gt; &lt;Service name=&quot;Catalina&quot;&gt;提供的服务名，默认为Catalina。可以用来实现一个或多个Connector关联至一个engine组件。 Connector负责接收客户端请求。常见有3类：HTTP连接器，HTTPs连接器，AJP连接器。 进入tomcat的请求可分为两类：(1) standalone : 请求来自于客户端浏览器；(2) 由其它的web server反代：来自前端的反代服务器。前端反代又分为以下几种模式： nginx –&gt; http connector –&gt; tomcat httpd(proxy_http_module) –&gt; http connector –&gt; tomcat httpd(proxy_ajp_module) –&gt; ajp connector –&gt; tomcat httpd(mod_jk) –&gt; ajp connector –&gt; tomcat 默认配置里有几项示例： HTTP connector： 普通HTTP连接器： 123&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; 使用共享线程池的http连接器： 1234&lt;Connector executor=&quot;tomcatThreadPool&quot; port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot; connectionTimeout=&quot;20000&quot; redirectPort=&quot;8443&quot; /&gt; HTTPs connector 123&lt;Connector port=&quot;8443&quot; protocol=&quot;org.apache.coyote.http11.Http11Protocol&quot; maxThreads=&quot;150&quot; SSLEnabled=&quot;true&quot; scheme=&quot;https&quot; secure=&quot;true&quot; clientAuth=&quot;false&quot; sslProtocol=&quot;TLS&quot; /&gt; AJP connector 1&lt;Connector port=&quot;8009&quot; protocol=&quot;AJP/1.3&quot; redirectPort=&quot;8443&quot; /&gt; 上述例子没有的几项属性参数： address：监听的IP地址；默认为本机所有可用地址。 maxThreads：最大并发连接数，默认为200。 enableLookups：是否启用DNS查询功能。 acceptCount：等待队列的最大长度。 EngineServlet实例，即servlet引擎，其内部可以一个或多个host组件来定义站点； 通常需要通过defaultHost来定义默认的虚拟主机。 &lt;Engine name=&quot;Catalina&quot; defaultHost=&quot;localhost&quot;&gt; 内部有个相应的Host是localhost 如果要设置负载均衡，要加一个jvmRoute Realm认证模块，这里调用的GlobalNamingResources里名为UserDatabase的Resourse。用来给Manager和Host-Manager提供认证服务。 1234&lt;Realm className=&quot;org.apache.catalina.realm.LockOutRealm&quot;&gt; &lt;Realm className=&quot;org.apache.catalina.realm.UserDatabaseRealm&quot; resourceName=&quot;UserDatabase&quot;/&gt;&lt;/Realm&gt; Host位于engine内部用于接收请求并进行相应处理的主机或虚拟主机。 示例： 123&lt;Host name=&quot;localhost&quot; appBase=&quot;webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt;&lt;/Host&gt; 常用属性说明： (1) appBase：此Host的webapps的默认存放目录，指存放非归档的web应用程序的目录或归档的WAR文件目录路径；可以使用基于$CATALINA_BASE变量所定义的路径的相对路径；(2) autoDeploy：在Tomcat处于运行状态时，将某webapp放置于appBase所定义的目录中时，是否自动将其部署至tomcat； 示例： 12345# 事先创建好目录# mkdir -pv /appdata/webapps# mkdir -pv /appdata/webapps/ROOT/&#123;lib,classes,WEB-INF&#125;# touch /appdata/webapps/ROOT/index.jsp# 添加测试内容 index.jsp： 123456789101112131415161718&lt;%@ page language=&quot;java&quot; %&gt;&lt;html&gt; &lt;head&gt;&lt;title&gt;TomcatA&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;h1&gt;&lt;font color=&quot;red&quot;&gt;TomcatA.yulongjun.com&lt;/font&gt;&lt;/h1&gt; &lt;table align=&quot;centre&quot; border=&quot;1&quot;&gt; &lt;tr&gt; &lt;td&gt;Session ID&lt;/td&gt; &lt;% session.setAttribute(&quot;yulongjun.com&quot;,&quot;yulongjun.com&quot;); %&gt; &lt;td&gt;&lt;%= session.getId() %&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Created on&lt;/td&gt; &lt;td&gt;&lt;%= session.getCreationTime() %&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/body&gt;&lt;/html&gt; 修改server.xml： 123&lt;Host name=&quot;tc1.yulongjun.com&quot; appBase=&quot;/appdata/webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt;&lt;/Host&gt; 浏览器访问tc1.yulongjun.com:8080即可访问到这个页面（记得hosts文件里要添加对应的ip，要么部署dns服务） Context包含在Host标签里。 如果在一个主机上部署多个app的话，Context就可以用来定义不同app对应的路径。 1&lt;Context path=&quot;/PATH&quot; docBase=&quot;/PATH/TO/SOMEDIR&quot; reloadable=&quot;&quot;/&gt; 把/PATH/TO/SOMEDIR映射到 request URI /PATH 如果/PATH/TO/SOMEDIR不带/，则是相对路径，相对的是Host的appBase。 如果/PATH/TO/SOMEDIR带/，则是操作系统的绝对路径。 reloadable：是否支持主机装载。 关于reloadable：http://blog.csdn.net/blueheart20/article/details/40074115 如果在上面Host tc1.yulongjun.com 内部加入一个Context： 12345&lt;Host name=&quot;tc1.yulongjun.com&quot; appBase=&quot;/appdata/webapps&quot; unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt; &lt;Context path=&quot;/test1&quot; docBase=&quot;test1&quot; reloadable=&quot;&quot;/&gt; &lt;Context path=&quot;/app1&quot; docBase=&quot;/app/app1&quot; reloadable=&quot;&quot;/&gt;&lt;/Host&gt; ValveValve存在多种类型： 定义访问日志：org.apache.catalina.valves.AccessLogValve 定义访问控制：org.apache.catalina.valves.RemoteAddrValve 123&lt;Valve className=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory=&quot;logs&quot; prefix=&quot;localhost_access_log.&quot; suffix=&quot;.txt&quot; pattern=&quot;%h %l %u %t &amp;quot;%r&amp;quot; %s %b&quot; /&gt; &quot;%h %l %u %t &amp;quot;%r&amp;quot; %s %b&quot;为日志格式，&amp;quot表示的是引号。 1&lt;Valve className=&quot;org.apache.catalina.valves.RemoteAddrValve&quot; deny=&quot;172\.16\.100\.67&quot;/&gt;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat管理页面]]></title>
    <url>%2Flinux%2F20170830-02-tomcat-manage%2F</url>
    <content type="text"><![CDATA[在http:tomcat_ip:8080页面，我们可以看到3个按钮，分别的功用为： Server Status：tomcat服务器状态，包括内存信息，进程信息等等。manager App：Web App的管理页面，可以开、关、重载、部署、卸载 Web AppHost Manager：虚拟主机的管理页面，可以对虚拟主机进行添加删除修改操作。 Tomcat Web Application Manager （简称 manager）这个工具Tomcat里一个启动（Start）\关闭(Stop)\重载(Reload)\部署(Deploy)\卸载(Undeploy) Web App的一个非常好用的工具，可以实现热部署。 但是点开会提示权限禁止，需要配置： manager-gui - allows access to the HTML GUI and the status pages manager-script - allows access to the text interface and the status pages manager-jmx - allows access to the JMX proxy and the status pages manager-status - allows access to the status pages only 我们可以看到，要增加页面的manger权限，需要manager-gui角色，然后赋予相应的用户这个角色，这个角色同事也支持状态页，也就是页面上的Server status。 vim $CATALINA_BASE/conf/tomcat-users.xml,在tomcat-userstag里写如下内容： 12&lt;role rolename="manager-gui"/&gt;&lt;user username="tomcat" password="LongDream" roles="manager-gui"/&gt; tomcat 7里不需要再修改任何东西就可以访问，tocmat 8 做了权限控制，只有tomcat本机才能访问，所以还需要去相应的manager的context.xml中的valve的权限选项：vim $CATALINA_HOME/webapps/manager/META-INF/context.xml修改其中的Valve阀门中的allow=&quot;127\.\d+\.\d+\.\d+|::1|0:0:0:0:0:0:0:1&quot;为allow=&quot;192\.168+\.111+\.\d+|::1|0:0:0:0:0:0:0:1&quot;context.xml修改完之后的全部的内容为： 123456&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;Context antiResourceLocking="false" privileged="true" &gt; &lt;Valve className="org.apache.catalina.valves.RemoteAddrValve" allow="192\.168+\.111+\.\d+|::1|0:0:0:0:0:0:0:1" /&gt; &lt;Manager sessionAttributeValueClassNameFilter="java\.lang\.(?:Boolean|Integer|Long|Number|String)|org\.apache\.catalina\.filters\.CsrfPreventionFilter\$LruCache(?:\$1)?|java\.util\.(?:Linked)?HashMap"/&gt;&lt;/Context&gt; 重启tomcat，systemd管理的tomcat 7 ，重启用systemctl restart tomcat 手动安装的tomcat 8.5.x，可以catalina.sh stop 然后 catalina.sh start来重启。 下面就是 Manager的界面： Tomcat Virtual Host Manager （简称 host manager）可以实现虚拟主机的热管理。 vim $CATALINA_BASE/conf/tomcat-users.xml,在tomcat-userstag里再添加一条，如下： 123&lt;role rolename="manager-gui"/&gt;&lt;role rolename="admin-gui"&lt;user username="tomcat" password="LongDream" roles="manager-gui,admin"/&gt; Server Status上面个两个配置任意一个配置好了后，都可以启用Server Status的GUI界面： JVM Memory StatusMAX memory：大约为服务器最大内存的四分之一左右。可以自定义，最大不超过32G。 Heap 内存区分为：Eden Space（伊甸园）、Survivor Space(幸存者区)、Tenured Gen（老年代-养老区）。 非Heap 内存区分为：Code Cache（代码缓存区）、Compressed Class Space（压缩的类区）、Metaspace（源数据区） AJP 服务 和HTTP服务（默认HTTPS服务没有启用）AJP(默认监听在8009端口）、HTTP（默认监听在8080端口） 服务阶段（Stage）P: Parse and prepare request （解析和准备请求）S: Service （服务中）F: Finishing （完成服务）R: Ready （准备好）K: Keepalive（保持连接）]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat基础知识]]></title>
    <url>%2Flinux%2F20170830-01-tomcat-basic%2F</url>
    <content type="text"><![CDATA[介绍Tomcat之前先介绍下Java相关的知识。 JavaJava简介Java代码的运行： *.java(source code) –&gt; javac –&gt; *.class(bytecode) –&gt; JVM *.java源代码经过javac编译之后，生成字节码*.class，然后发送给JVM来运行。 JVM（Java Virtual Machine）：class loader，加载程序的类文件，及程序的类文件依赖到的其它的类文件而后运行； 整个运行表现为一个jvm进程。 Java Applet 、Java Servlet、在浏览器或客户端运行的Java程序就叫：Java Applet 在服务端运行的的Java程序就叫：Java Servlet Applet和Servlet都不是独立的程序，都需要Java运行环境。 JSPJava Server Pages(JSP)是一种实现普通静态HTML和动态HTML混合编码的技术，JSP并没有增加任何本质上不能用Servlet实现的功能。但是，在 JSP中编写静态HTML更加方便，不必再用println语句来输出每一行HTML代码。更重要的是，借助内容和外观的分离，页面制作中不同性质的任务可以方便地分开：比如，由页面设计者进行HTML设计，同时留出供Servlet程序员插入动态内容的空间。 此段转载自：http://blog.csdn.net/yasi_xi/article/details/22071099如果想要详细了解，可以阅读原文。 jasperjasper程序的功能，就是把JVM不认识的JSP文件解析成java文件，然后由javac程序编译成class文件提供使用。目前有很多的JSP解析引擎，Tomcat中使用的是Jasper。如下： *.jsp –&gt; jasper –&gt; *.java –&gt; javac –&gt; *.class –&gt; JVM JDKJDK（Java Development Kits）：Java 开发套件，一共分为两种： OpenJDK Oracle JDK 安装 OpenJDK 1.8yum list|grep openjdk发现包含1.6、1.7、1.8，我们可以过滤一下，只看1.8的x86_64版本的： 1yum list|grep java-1.8.0-openjdk.*x86_64 openjdk是The OpenJDK runtime environment，只有运行环境; openjdk-devel是The OpenJDK development tools,是开发工具包组。所以我们来安装openjdk-devel，这样有一系列的开发工具包组可供使用。 1yum install -y java-1.8.0-openjdk-devel 这条命令会安装java-1.8.0、java-1.8.0-poenjdk-devel、java-1.8.0-openjdk-headless以及其他相关依赖。 java -version可以看到java版本： 安装 Oracle 的 JDK 8Oracle 官方也有对应的JRE和JDK。可以去官网下载安装：http://www.oracle.com/technetwork/java/javase/downloads/index.html 包含3个版本，这里选择JDK版本下载，包格式选择rpm 用rpm命令进行安装： 这里单独下载安装的JDK的rpm，是无法用rpm -ql查询的，我们可以在/usr/java下找到： 默认latest指向jdk的版本的文件夹，default指向latest文件夹，如果后来升级了，可以把指向自行更换。 我们还需要写一下环境变量，让系统可以找到java程序1234567# 写到环境变量里，下次启动生效。cat &gt; /etc/profile.d/java.sh &lt;&lt;EOFexport JAVA_HOME=/usr/java/latestexport PATH=\$JAVA_HOME/bin:\$PATHEOF# source一下，在当前环境下立即生效。source /etc/profile.d/java.sh 这里推荐用OpenJDK，免去了配置环境变量的操作。而且JDK属于Oracle公司未开源产品，万一你公司做大了，Oracle像告谷歌一样告你呢？:-D TomcatTomcat简介Tomcat 服务器是一个免费的开放源代码的Web 应用服务器，Tomcat是Apache 软件基金会（Apache Software Foundation）的Jakarta 项目中的一个核心项目：tomcat.apache.org，它的项目名称为catalina，后来由Apache、Sun 和其他一些公司及个人共同开发而成，因为在O’Reilly家出的书的封面是一只汤姆猫，所以软件更名为Tomcat。Tomcat 是一个小型的轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP 程序的首选，因为Tomcat 技术先进、性能稳定，成为目前比较流行的Web 应用服务器。Tomcat是应用（java）服务器，它只是一个servlet容器，是Apache的扩展，但它是独立运行的。 Tomcat架构 图片来自于：http://www.blogjava.net/honzeland/archive/2010/05/10/320458.html 它由一组嵌套的层次和组件组成，一般可分为以下四类： 顶级组件：位于配置层次的顶级，并且彼此间有着严格的对应关系。Server 服务类组件：位于Server的下一级。Service 连接器组件：连接客户端（可以是浏览器或Web服务器）请求至Servlet容器。http，https， ajp 容器类组件：包含一组其它组件。Engine，Host，Context 被嵌套的组件：位于一个容器当中，但不能包含其它组件。valve，loger，realm，loader，manager，… 集群类组件： listener，cluster，… 各常见组件： 1、服务器(server)：Tomcat的一个实例，通常一个JVM只能包含一个Tomcat实例；因此，一台物理服务器上可以在启动多个JVM的情况下在每一个JVM中启动一个Tomcat实例，每个实例分属于一个独立的管理端口。这是一个顶级组件。 2、服务(service)：一个服务组件通常包含一个引擎和与此引擎相关联的一个或多个连接器。给服务命名可以方便管理员在日志文件中识别不同服务产生的日志。一个server可以包含多个service组件，但通常情下只为一个service指派一个server。连接器类组件： 3、连接器(connectors)：负责连接客户端（可以是浏览器或Web服务器）请求至Servlet容器内的Web应用程序，通常指的是接收客户发来请求的位置及服务器端分配的端口。默认端口通常是HTTP协议的8080，管理员也可以根据自己的需要改变此端口。还可以支持HTTPS ，默认HTTPS端口为8443。同时也支持AJP，即（A）一个引擎可以配置多个连接器，但这些连接器必须使用不同的端口。默认的连接器是基于HTTP/1.1的Coyote。同时，Tomcat也支持AJP、JServ和JK2连接器。 容器类组件： 4、引擎(Engine)：引擎通是指处理请求的Servlet引擎组件，即Catalina Servlet引擎，它检查每一个请求的HTTP首部信息以辨别此请求应该发往哪个host或context，并将请求处理后的结果返回的相应的客户端。严格意义上来说，容器不必非得通过引擎来实现，它也可以是只是一个容器。如果Tomcat被配置成为独立服务器，默认引擎就是已经定义好的引擎。而如果Tomcat被配置为Apache Web服务器的提供Servlet功能的后端，默认引擎将被忽略，因为Web服务器自身就能确定将用户请求发往何处。一个引擎可以包含多个host组件。 5、主机(Host)：主机组件类似于Apache中的虚拟主机，但在Tomcat中只支持基于FQDN的“虚拟主机”。一个引擎至少要包含一个主机组件。 6、上下文(Context)：Context组件是最内层次的组件，它表示Web应用程序本身。配置一个Context最主要的是指定Web应用程序的根目录，以便Servlet容器能够将用户请求发往正确的位置。Context组件也可包含自定义的错误页，以实现在用户访问发生错误时提供友好的提示信息。 被嵌套类(nested)组件： 这类组件通常包含于容器类组件中以提供具有管理功能的服务，它们不能包含其它组件，但有些却可以由不同层次的容器各自配置。 7、阀门(Valve)：用来拦截请求并在将其转至目标之前进行某种处理操作，类似于Servlet规范中定义的过滤器。Valve可以定义在任何容器类的组件中。Valve常被用来记录客户端请求、客户端IP地址和服务器等信息，这种处理技术通常被称作请求转储(request dumping)。请求转储valve记录请求客户端请求数据包中的HTTP首部信息和cookie信息文件中，响应转储valve则记录响应数据包首部信息和cookie信息至文件中。 8、日志记录器(Logger)：用于记录组件内部的状态信息，可被用于除Context之外的任何容器中。日志记录的功能可被继承，因此，一个引擎级别的Logger将会记录引擎内部所有组件相关的信息，除非某内部组件定义了自己的Logger组件。 9、领域(Realm)：用于用户的认证和授权；在配置一个应用程序时，管理员可以为每个资源或资源组定义角色及权限，而这些访问控制功能的生效需要通过Realm来实现。Realm的认证可以基于文本文件、数据库表、LDAP服务等来实现。Realm的效用会遍及整个引擎或顶级容器，因此，一个容器内的所有应用程序将共享用户资源。同时，Realm可以被其所在组件的子组件继承，也可以被子组件中定义的Realm所覆盖。 此段来自于：http://www.ttlsa.com/tomcat/tomcat-install-and-configure/ 安装Tomcat安装好OpenJDK或JDK之后，就可以安装tomcat了。 yum 安装方式1yum install tomcat-lib tomcat tomcat-admin-webapps tomcat-webapps tomcat-docs-webapp tomcat-webapps 依赖的tomcat-libs、tomcat api和其他组件会自动安装上。 CentOS7 默认安装的tomcat版本是7： 1systemctl start tomcat 浏览器打开http://tomcat_ip:8080，这里的tomcat_ip换成你tomcat服务器的ip 手动安装过程看官方版本图：http://tomcat.apache.org/whichversion.html 我们可以看到，在目前来说，8.0.x属于superseded(废弃）状态，9.0.x属于alpha版，不考虑,8.5.x是目前的主流版本，故下载8.5.x版本，目前版本是8.5.20，笔者到清华镜像下载的。 123456789101112131415wget https://mirrors.tuna.tsinghua.edu.cn/apache/tomcat/tomcat-8/v8.5.20/bin/&#123;apache-tomcat-8.5.20.tar.gz,apache-tomcat-8.5.20-fulldocs.tar.gz,apache-tomcat-8.5.20-deployer.tar.gz&#125;tar -xvf apache-tomcat-8.5.20.tar.gz -C /usr/localcd /usr/locallink -sv apache-tomcat-8.5.20 tomcatcat &gt; /etc/profile.d/tomcat.sh &lt;&lt; EOFexport CATALINA_BASE=/usr/local/tomcatexport PATH=\$CATALINA_BASE/bin:\$PATHEOF. /etc/profile.d/tomcat.shuseradd -r tomcatchown -R tomcat:tomcat /usr/local/tomcat/su - tomcat -c &apos;catalina.sh start&apos; 手动安装的话，设置完环境变量后，手动安装的版本可以使用catlina.sh来进行各种操作。 catalina.sh start 后，浏览器打开http://tomcat_ip:8080，这里的tomcat_ip换成你tomcat服务器的ip: Tomcat 目录结构tomcat的目录结构： bin：脚本，及启动时用到的类。 conf：配置文件目录。 lib：库文件，Java类库，jar。 logs：日志文件目录。 temp：临时文件目录。 webapps：webapp的默认目录。 work：工作目录。存放编译后的字节码文件。 tomcat的配置目录文件构成： server.xml：主配置文件； web.xml：每个webapp只有“部署”后才能被访问，它的部署方式通常由web.xml进行定义，其存放位置为WEB-INF/目录中；此文件为所有的webapps提供默认部署相关的配置； context.xml：每个webapp都可以专用的配置文件，它通常由专用的配置文件context.xml来定义，其存放位置为WEB-INF/目录中；此文件为所有的webapps提供默认配置； tomcat-users.xml：用户认证的账号和密码文件； catalina.policy：当使用-security选项启动tomcat时，用于为tomcat设置安全策略； catalina.properties：Java属性的定义文件，用于设定类加载器路径，以及一些与JVM调优相关参数； logging.properties：日志系统相关的配置。 JSP Webapp的组织结构： webapps的根目录： index.jsp：主页； WEB-INF/：当前webapp的私有资源路径；通常用于存储当前webapp的web.xml和context.xml配置文件； META-INF/：类似于WEB-INF/； classes/：类文件，当前webapp所提供的类； lib/：类文件，当前webapp所提供的类，被打包为jar格式； webapp归档格式： .war：webapp .jar：EJB的类打包文件； .rar：资源适配器类打包文件； .ear：企业级webapp；]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自制的 Vagrant box —— longdream/centos7]]></title>
    <url>%2Fcloud%2F20170829-vagrant-custom-box%2F</url>
    <content type="text"><![CDATA[自己做了一个Vagrant镜像，上传到Vagrant官方的Vagrant Cloud上，方便自己和读者使用。 目前为2.0.0版本。 项目地址：longdream/centos7 Vagrant可以使用下面命令安装： 1vagrant box add longdream/centos7 关于Vagrant使用，可参考：Vagrant–快速搭建实验环境利器。 In English:longdream/centos7 Change Log@Version 2.0.0 change CentOS official box centos/7 1707.1 to centos/7 1708.1(CentOS’s Version is updated from 7.3.1611 to 7.4.1708)。 Modify /etc/ssh/sshd_config：PasswordAuthentication no –&gt;PasswordAuthentication no (To enable argument Login with Password). add useful tools: expect. longdream/centos7 IntroductionBase on CentOS official box centos/7 1708.1. the following changes have been made: Disabled SELinux. yum updateupdate to the latest version. Modify /etc/ssh/sshd_config：PasswordAuthentication no –&gt;PasswordAuthentication no (To Open Login with Password). Some useful tools: vim, tree, wget ,bash-completion, net-tools ,tcpdump ,ab, expect. CentOS-Base.repo and epel.repo change to aliyun’s repos(which more fast in China). Add VirtualBox Guest in box. (Need plugin vagrant-vbguest in Host：vagrant plugin install vagrant-vbguest.) 中文：longdream/centos7 变更日志@Version 2.0.0 更改CentOS官方boxcentos/7 1707.1 到 centos/7 1708.1（CentOS的版本从7.3.1611 更新为7.4.1708）。 修改/etc/ssh/sshd_config：PasswordAuthentication no –&gt;PasswordAuthentication no （打开密码登录参数）。 添加有用的工具: expect。 longdream/centos7 介绍基于CentOS官方box centos/7 1708.1。做了如下的更改： 禁用了SElinux。 加了一些有用的工具：vim、 tree、 wget 、bash-completion、 net-tools 、tcpdump 、ab、expect。 yum update更新到最新版。 修改/etc/ssh/sshd_config：PasswordAuthentication no –&gt;PasswordAuthentication no (为了打开密码登录)。 CentOS-Base.repo 和 epel.repo 更换为 阿里云的 repo （国内更快） 在box里添加了VirtualBox的虚机客户端。（需要在宿主机上安装插件vagrant-vbguest：vagrant plugin install vagrant-vbguest。）]]></content>
      <categories>
        <category>cloud</category>
      </categories>
      <tags>
        <tag>Vagrant</tag>
        <tag>Vagrant Cloud</tag>
        <tag>Vagrant box</tag>
        <tag>longdream/centos7</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-HAProxy 统计页面]]></title>
    <url>%2Flinux%2F20170828-04-haproxy-stats%2F</url>
    <content type="text"><![CDATA[HAProxy的统计页面，可以定义在frontend listen backend中，不过一般都定义在listen段中，专门开辟一个端口作为监听端口，然后对外提供状态页面。 12345678listen stats mode http bind 192.168.20.222:9999 stats enable stats hide-version log global stats uri /haproxy-stats stats auth haadmin:haadmin123 重启服务，systemctl restart haproxy，输入统计页地址192.168.20.222:9999/haproxy-stats，输入上面定义的用户名和密码： 如果在添加一个参数：stats admin，可以提供后端主机的管理功能，还可以限定只允许某台主机进行管理： 1stats admin &#123; if | unless &#125; &lt;cond&gt; 例如不设置限制：1stats admin if TRUE # 任何主机都可以管理 设置完重启服务，可以看到每个服务器前面会多一个勾选用的复选框，可以直接设置服务器状态： 还可以设置页面自动刷新间隔： 1stats refresh 15s]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>HAProxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-HAProxy 具体配置参数]]></title>
    <url>%2Flinux%2F20170828-03-haproxy-params%2F</url>
    <content type="text"><![CDATA[具体配置参数可以查看这里： HAProxy 1.5：http://cbonte.github.io/haproxy-dconv/1.5/configuration.html HAProxy 1.7：http://cbonte.github.io/haproxy-dconv/1.7/configuration.html 下面捡一些用的比较多的参数来说明： server：后端具体服务器1server &lt;name&gt; &lt;address&gt;[:[port]] [param*] name：跟服务器相关的内部名称，会出现在log日志和报警里。 [:[port]]：具体服务器ip地址和端口，不指定端口话默认为80。[param*]：参数有很多，如backup、check inter fall rise等。 1234567frontend myweb bind *:80 default_backend websrvsbackend websrvs server nginx-web1 192.168.20.10:80 check server nginx-web2 192.168.20.20:80 check backup：标记为备用主机标记为backup的主机，不负载均衡，在服务器全挂了的时候才会调用，一般用来做sorry Server。 1server sorry-server 192.168.20.44 backup balance：负载均衡算法12balance &lt;algorithm&gt; [ &lt;arguments&gt; ]balance url_param &lt;param&gt; [check_post] 负载均衡算法有以下几种： roundrobin：(动态)轮询。可运行时调整权重，支持慢启动，最多支持4095个后端主机。（不过这种限制不算是限制了，已经很大了） static-rr：静态轮询。运行时不可调整权重，不支持慢启动，无后端主机数量限制。 leastconn：最少连接，按最少连接来调度，此处是加权最少连接，支持指定权重，是动态的，所以也就支持权重的运行时调整和慢启动。 first：制定先调度至一个主机，主机连接限制到了，再调度到另外一台。 source：source ip hash，源地址hash。根据请求的源地址进行hash，同一个来源ip调度到同一台主机。 uri：uri hash。根据请求的uri的hash值调度，这样可以提高缓存的命中率。 url_param：根据url里的某个参数的hash来调度。读取url地址里给定的参数的hash值，来调度。 hdr(&lt;name&gt;)：根据HTTP header 的hash值进行调度。如果name不存在，或者如果不包含任何值，则采用roundrobin算法。 上述的source、 uri、 url_param、 hdr(&lt;name&gt;)采用的默认hash算法都是map-based，。还可以更改为另外一种算法consistent。 map-based: 静态映射，可以分配权重，但是一旦服务启动后，无法动态调整权重，也不支持慢启动。而且一旦服务器发生变动，影响是全局的，不适合用来做缓存代理。这种算法虽然不好，但是占用系统资源少。consistent: 一致性hash，服务器发生变动，只影响局部有限的服务器，可以运行中动态调整权重，支持慢启动。这种算法很好用，但是占用系统资源多。 举几个例子： 1、想要对缓存比较友好，缓存命中率更高，使用基于uri的一致性hash算法。同一个uri地址，无论请求地址是多少，返回的信息始终来自于同一个后端主机： 123456789frontend myweb bind *:80 default_backend websrvsbackend websrvs balance uri server nginx-web1 192.168.20.10:80 check server nginx-web2 192.168.20.20:80 check hash-type consistent 2、想要基于HTTP 头信息的浏览器来分类： 123456789frontend myweb bind *:80 default_backend websrvsbackend websrvs balance header(User-Agent) server nginx-web1 192.168.20.10:80 check server nginx-web2 192.168.20.20:80 check hash-type consistent compression：压缩12compression algo &lt;algorithm&gt;compression type &lt;mime type&gt; 用在前段的例子例子： 1234567891011frontend myweb bind *:80 default_backend websrvs compression algo gzip compression type text/html text/plain application/xml application/javascriptbackend websrvs balance source server nginx-web1 192.168.20.10:80 check server nginx-web2 192.168.20.20:80 check hash-type consistent mode：负载模式（四层TCP或7层HTTP）负载模式： tcp：默认模式，四层TCP负载均衡，被用于四层基于端口的协议，可以用来转发各种端口。 http：七层HTTP负载均衡，这是HAProxy的价值所在，可以做很多比较复杂的七层策略。 123frontend myweb bind 192.168.20.111:80 mode http default_backend websrvs backend websrvs mode http balance source server nginx-web1 192.168.20.10:80 check server nginx-web2 192.168.20.20:80 check server 中的params1. check、inter、fail、raise：（健康检查相关） check：是否启用健康检查。不写则默认不启用健康检查。 addr：可以不占用服务器主用的ip，换一个ip进行检测。 port：同样，端口也可以换一个。 inter &lt;delay&gt;：两个连续的检查之间的延时，默认为2000ms。 fall &lt;count&gt;：在连续的几次健康检查中失败，则视为dead（当机），默认为3次。 rise &lt;count&gt;：在连续的几次健康检查中成功，则视为operational（可操作的），默认为2次。 123frontend myweb bind 192.168.20.111:443 mode http default_backend websrvs backend websrvs mode http balance source server nginx-web1 192.168.20.10:443 check port 80 inter 1000 fall 3 rise 5 server nginx-web2 192.168.20.20:443 check port 80 inter 1000 fall 3 rise 5 option httpcheck：基于http协议做健康检查可以指定method、uri、version 1234option httpchkoption httpchk &lt;uri&gt;option httpchk &lt;method&gt; &lt;uri&gt;option httpchk &lt;method&gt; &lt;uri&gt; &lt;version&gt; 例子： 123456# Relay HTTPS traffic to Apache instance and check service availability# using HTTP request &quot;OPTIONS * HTTP/1.1&quot; on port 80.backend https_relay mode tcp option httpchk OPTIONS * HTTP/1.1\r\nHost:\ www server apache1 192.168.1.1:443 check port 80 还有 smtpchk、mysql-check、pgsql-check 、ssl-hello-chk不做赘述，也是应用层的复杂检测方法。 maxconn：最大并发连接数 maxqueue：最大队列数12maxconn &lt;maxconn&gt;maxqueue &lt;maxqueue&gt; 可以指定单台后端服务器的最大并发连接数。 1server nginx-web1 192.168.20.10:80 check maxconn 10000 maxqueue 2000 redir ：302临时重定向到另外一台服务器。123frontend myweb bind 192.168.20.111:443 mode http default_backend websrvs backend websrvs mode http balance source server nginx-web1 192.168.20.10:443 check port 80 inter 1000 fall 3 rise 5 server nginx-web2 192.168.20.20:443 check port 80 inter 1000 fall 3 rise 5 redir http:///www.baidu.com 对192.168.20.10的访问，临时重定向到http://www.baidu.com weight：权重服务器的权重]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>HAProxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-HAProxy简单配置和基本用法]]></title>
    <url>%2Flinux%2F20170828-02-haproxy-simple-configuration%2F</url>
    <content type="text"><![CDATA[环境一台HAProxy，两个Nginx Web Server。 Vagrantfile配置如下： 123456789101112131415161718192021222324252627# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure("2") do |config| config.vm.box = "longdream/centos7" config.vm.box_check_update = false config.hostmanager.enabled = true config.hostmanager.manage_guest = true config.hostmanager.manage_host = true # Create VM haproxy config.vm.define "haproxy" do |node| node.vm.network "private_network", ip: "192.168.20.222" node.vm.hostname = "haproxy.yulongjun.com" end # Create VM nginx-web1 config.vm.define "nginx-web1" do |node| node.vm.network "private_network", ip: "192.168.20.10" node.vm.hostname = "nginx-web1.yulongjun.com" end # Create VM nginx-web2 config.vm.define "nginx-web2" do |node| node.vm.network "private_network", ip: "192.168.20.20" node.vm.hostname = "nginx-web2.yulongjun.com" endend 配置web服务器端1yum install nginx 更改两个服务器的/usr/share/nginx/html/index.html文件： nginx-web1更改内容为： 1Backend Server 1 nginx-web2更改内容为： 1Backend Server 2 这样做事为了区分开两个服务器显示页面，生产环境下一般是两台都是一样的内容。 在浏览器上访问下http://nginx-web1.yulongjun.com/和http://nginx-web2.yulongjun.com,如果能访问成功，证明没问题。 配置HAProxy1yum install -y haproxy 程序环境： 主程序：/usr/sbin/haproxy 主配置文件：/etc/haproxy/haproxy.cfg Unit file：/usr/lib/systemd/system/haproxy.service 配置文件/etc/haproxy/haproxy.cfg的结构主要分为全局配置段和代理配置端： global：全局配置段 进程及安全配置相关的参数 性能调整相关参数 Debug参数 用户列表 peers proxies：代理配置段 defaults：为frontend, listen, backend提供默认配置 fronted：前端，相当于nginx, server {} backend：后端，相当于nginx, upstream {} listen：同时拥前端和后端 1. 配置日志在/etc/haproxy/haproxy.cfg里，日志定义为： 1log 127.0.0.1 local2 所以我们要开启rsyslog，并且添加local2的定义项： vim /etc/rsyslog.conf： 1234567# Provides UDP syslog reception$ModLoad imudp$UDPServerRun 514# Save boot messages also to boot.loglocal7.* /var/log/boot.loglocal2.* /var/log/haproxy.log 重启rsyslog服务，并且确保udp的514端口打开：12systemctl restart rsyslogss -unlp |grep rsyslogd # 确保514端口处于监听状态 HAProxy的基本用法（简单配置）默认的全部注释掉： 然后开始手动添加： 12345678frontend myweb bind *:80 default_backend websrvsbackend websrvs balance roundrobin server nginx-web1 192.168.20.10:80 check server nginx-web2 192.168.20.20:80 check 测试一下： 1234567891011[root@haproxy ~]# for i in `seq 10`;do curl 192.168.20.222;done&lt;h1&gt;Backend Server 1&lt;/h1&gt;&lt;h1&gt;Backend Server 2&lt;/h1&gt;&lt;h1&gt;Backend Server 1&lt;/h1&gt;&lt;h1&gt;Backend Server 2&lt;/h1&gt;&lt;h1&gt;Backend Server 1&lt;/h1&gt;&lt;h1&gt;Backend Server 2&lt;/h1&gt;&lt;h1&gt;Backend Server 1&lt;/h1&gt;&lt;h1&gt;Backend Server 2&lt;/h1&gt;&lt;h1&gt;Backend Server 1&lt;/h1&gt;&lt;h1&gt;Backend Server 2&lt;/h1&gt;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>HAProxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-HAProxy简介]]></title>
    <url>%2Flinux%2F20170828-01-haproxy-introduction%2F</url>
    <content type="text"><![CDATA[我们yum info haproxy可以看到HAProxy的描述，翻译一下： 摘要：HAProxy是在高可用环境下提供TCP/HTTP proxy和Load Balance的工具。描述：HAProxy 是一个适用于HA环境的 TCP/HTTP reverse proxy（译者注：本身没有HA功能，但是有proxy和LB功能，可以这样理解，HA的proxy…… ），HAProxy可以： 路由依赖静态分配cookies的HTTP请求。 在多个服务器之间扩展负载，同时确保通过使用HTTP cookie来保持服务器的持久性。 在主服务器fail时候，可以切换到备用服务器。 接受连接到特定端口的专用服务监控 在不断开已有的链接的情况下，停止接受新的链接请求。 双向添加，修改，删除HTTP headers。 锁定匹配特定模式的请求。 从一个被应用解析的URI来报告详细状态给认证过的用户（译者注：相当方便和强大） HAProxy是一个纯粹的reverse proxy，能够实现基于TCP的4层和基于HTTP的7层负载均衡功能，但是和Nginx不一样，是没有Web Server功能的，所以HAProxy后端通常会接Nginx或Apache httpd等Web Server。 通常HAproxy会和Keepalived配合使用，实现高可用的负载均衡(Keepalived后面会讲到）： 更多详细的介绍可以看官方文档：Introduction to HAProxy 下一节，我们来简单配置一个HAProxy的集群。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>HAProxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10-ngx__stream_*_module详解]]></title>
    <url>%2Flinux%2F20170823-10-ngx_stream_module%2F</url>
    <content type="text"><![CDATA[Nginx的ngx_stream_module提供了伪四层代理功能。但是功能不强，用的不多。 捡几个常用的模块说明一下作用，就不做详细展开，想研究的可以详细的去看官方文档。 http://nginx.org/en/docs/ ngx_stream_core_module ：核心模块listen 监听的端口。默认为tcp协议，加上udp就监听udp协议的端口。 stream stream段里面定义一个个的server的具体信息。（每个sever监听一个端口） ngx_stream_proxy_module四层代理功能proxy_pass定义的每个端口的server对应的后端主机或主机组。 proxy_connect_timeout和被代理的服务器建立连接的超时时间。 proxy_next_upstream当代理服务器和后端服务器不能建立时，是否把客户端连接转移到到下一个后端服务器。默认打开 proxy_next_upstream_timeout转移到下一个服务器的连接的限制时间，如果设置了值，过了这个值，就会再次转移到下一个服务器。如果不设置，那么默认为0，即不限制，一直要求建立连接。 proxy_next_upstream_tries在超时后，尝试的次数，超过这个次数，会转移到下一台。默认为0，无限制尝试。 proxy_timeout 代理服务器处理客户端和后端服务器两次读或写成功的之间的时间内，超过多少时间，就断开连接。默认为10分钟。（即建立连接后，10分钟内没有数据交互，则断开连接） ngx_stream_upstream_moduleupstream stream的upstream和http的upstream没什么区别，都可以设置多个后端服务器，也可以设置备份后端服务器，还可以设置权重和算法。 123456upstream backend &#123; hash $remote_addr consistent; server backend1.example.com:12345 weight=5; server 127.0.0.1:12345 max_fails=3 fail_timeout=30s; server unix:/tmp/backend3;&#125; ngx_stream_ssl_module和http的ssl相似。 12345678910111213141516171819worker_processes auto;stream &#123; ... server &#123; listen 12345 ssl; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers AES128-SHA:AES256-SHA:RC4-SHA:DES-CBC3-SHA:RC4-MD5; ssl_certificate /usr/local/nginx/conf/cert.pem; ssl_certificate_key /usr/local/nginx/conf/cert.key; ssl_session_cache shared:SSL:10m; ssl_session_timeout 10m; ... &#125;&#125; ngx_stream_log_modulelog_format，是日志格式，可以定义多种，写在stream段。access_log可以用在stream和server段（使用log_format）。 如下，定义一个名为basic 的log_format，access_log调用basic的日志格式 1234log_format basic &apos;$remote_addr [$time_local] &apos; &apos;$protocol $status $bytes_sent $bytes_received &apos; &apos;$session_time&apos;;access_log /spool/logs/nginx-access.log basic buffer=32k; ngx_stream_access_module同http的访问控制模块。 例子： 12345678server &#123; ... deny 192.168.1.1; allow 192.168.1.0/24; allow 10.1.1.0/16; allow 2001:0db8::/32; deny all;&#125; ngx_stream_geo_module： 自定义ip地理位置。 用的少，一般用geoip，是有ip地址库的,见下。 `ngx_stream_geoip_moduleyum install yum install GeoIP GeoIP-datageoip是库文件，geoip-data是具体的数据文件。不过不是最新版，要想要最新版还是要到官方网站去下载：官方提供免费版和付费版，官网地址:www.maxmind.com。 rpm -ql GeoIP-data： 1234/usr/share/GeoIP/GeoIPASNum-initial.dat/usr/share/GeoIP/GeoIPASNumv6-initial.dat/usr/share/GeoIP/GeoIPCity-initial.dat/usr/share/GeoIP/GeoIPCityv6-initial.dat 123456789101112stream &#123; geoip_country /usr/share/GeoIP/GeoIPASNum-initial.dat; geoip_city /usr/share/GeoIP/GeoIPCity-initial.dat; map $geoip_city_continent_code $nearest_server &#123; default example.com; EU eu.example.com; NA na.example.com; AS as.example.com; &#125; ...&#125; 官方的一个示例： 1234567891011121314151617181920212223242526272829303132333435363738394041worker_processes auto;error_log /var/log/nginx/error.log info;events &#123; worker_connections 1024;&#125;stream &#123; upstream backend &#123; hash $remote_addr consistent; server backend1.example.com:12345 weight=5; server 127.0.0.1:12345 max_fails=3 fail_timeout=30s; server unix:/tmp/backend3; &#125; upstream dns &#123; server 192.168.0.1:53535; server dns.example.com:53; &#125; server &#123; listen 12345; proxy_connect_timeout 1s; proxy_timeout 3s; proxy_pass backend; &#125; server &#123; listen 127.0.0.1:53 udp; proxy_responses 1; proxy_timeout 20s; proxy_pass dns; &#125; server &#123; listen [::1]:12345; proxy_pass unix:/tmp/stream.socket; &#125;&#125;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Nginx FastCGI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09-ngx_http_upstream_module详解]]></title>
    <url>%2Flinux%2F20170823-09-ngx_http_upstream_module%2F</url>
    <content type="text"><![CDATA[LB Cluster（负载均衡集群）方案： 硬件： F5公司：BigIP Citrix公司：NetScaler A10公司：A10 软件： 四层调度：lvs, nginx(stream module), haproxy(mode tcp) 七层调度：nginx(http_upstream module), haproxy(mode http), httpd, ats, … 这节我们学习Nginx的七层调度模块ngx_http_upstream_module。 我们可以构建一个后端服务器组(Backend Server Group),里面包含多个相同作用的服务器，通过Nginx的负载均衡功能，可以实现调度。 ngx_http_upstream_module模块用来定义服务器组，这个服务器组可以被proxy_pass、fastcgi_pass、uwsgi_pass、scgi_pass和memcached_pass等指令引用。实现负载均衡的调度功能。 默认算法为randrobin，另外支持least_conn、ip_hash算法，nginx plus还支持更多的调度算法，这里不做讨论。 指令upstream upstream只能在http上下文里，定义了一组上游服务器组。 server server字段只能在在upstream上下文里，定义一个个的上游服务器，也叫后端服务器（Backend）. 参数有很多： weight=number：设置服务器的权重，默认情况下为1。 fail_timeout=time：与后端服务器通信失败的超时时间，超过时间，则算一次失败。 max_fails=number：设置与服务器通讯的最大失败次数，超过此数将不再调度。 backup：将服务器标记为备份服务器。当一个组里所有非备份的服务器都挂了的时候，将会反代备份服务器对外服务。 down：将备份服务器标记为不可用；通常在维护服务器的时候使用，比如蓝绿发布的时候。（配置为down，然后reload Nginx服务，维护好了，再去掉down，再reload Nginx服务） resolve：当后端服务器地址写的是域名的时候，如果修改了ip地址，可以自动监控修改 蓝绿发布：https://www.v2ex.com/t/344341 调度算法(默认不写是wrr)least_conn、ip_hash、hash 最少连接算法 源地址哈希算法：同一个ip绑定在一个服务器上 hash自定义的key。该key可以包含文本，变量，以及它们的组合,consistent表示是否启用一致性hash算法。 例如hash $request_uri consistent，就是根据request URI来做调度，即意味着无论哪个客户端，访问同一个资源，都会到固定的后端服务区上去找。如果后端是缓存服务器，可以提高缓存命中率。 关于一致性hash算法，可以看这里详细了解：http://www.jianshu.com/p/e8fb89bb3a61 实验基础虚拟机环境一个proxy负责前端调度，后面接两个backend做负载均衡，一个作为backup。 proxy反代服务器：192.168.1.200backend1后端服务器：192.168.1.101backend2后端服务器：192.168.1.102backup备份后端服务器：192.168.1.103 Vagrant的Vagrantfile配置文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure("2") do |config| # Vagrant Global Config # `longdream/centos7` is a custom centos7 box made by YuLongjun. config.vm.box = "longdream/centos7" # If this box is add online, set true will check update. # Also set `false` will not update it. # If this box is added locally, this setting is invalid. config.vm.box_check_update = false # you need `vagrant plugin install vagrant-vbguest` # You also need `vagrant plugin install vagrant-hostmanager` config.hostmanager.enabled = true # Allow update `/etc/hosts` file in VMs. config.hostmanager.manage_guest = true # Allow update `/etc/hosts` file in Hosts. config.hostmanager.manage_host = true # Create VM `proxy`. config.vm.define "proxy" do |proxy| proxy.vm.network "private_network", ip: "192.168.1.200" proxy.vm.hostname = "proxy" end # Create VM `backend1`. config.vm.define "backend1" do |backend1| backend1.vm.network "private_network", ip: "192.168.1.101" backend1.vm.hostname = "backend1" end # Create VM `backend2`. config.vm.define "backend2" do |backend2| backend2.vm.network "private_network", ip: "192.168.1.102" backend2.vm.hostname = "backend2" end # Create VM `backup`. config.vm.define "backup" do |backup| backup.vm.network "private_network", ip: "192.168.1.103" backup.vm.hostname = "backup" endend 安装Nginxproxy的Nginx程序作为反向代理服务，backend1、backend2、backup的Nginx程序作为HTTP服务。 分别安装Nginx软件： 写一个install_nginx.sh脚本，在四台机器上运行： 123456789101112131415#!/bin/bash## set up yum repo.cat &gt;/etc/yum.repos.d/nginx.repo &lt;&lt;EOF[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/\$releasever/\$basearch/gpgcheck=0enabled=1EOF# installyum install -y nginx [ $? -eq 0 ]&amp;&amp;echo -e "\033[32;1mInstall nginx successfully.\033[0m"||echo -e "\033[31;1mInstall nginx failed. Please check the Network.\033[0m" 配置各个服务器backend1: 1234mv /usr/share/nginx/html/index.html&#123;,.bak&#125;echo "&lt;h1&gt;Backend Web Server 1&lt;/h1&gt;" &gt; /usr/share/nginx/html/index.htmlsystemctl enable nginxsystemctl start nginx backend2: 1234mv /usr/share/nginx/html/index.html&#123;,.bak&#125;echo "&lt;h1&gt;Backend Web Server 2&lt;/h1&gt;" &gt; /usr/share/nginx/html/index.htmlsystemctl enable nginxsystemctl start nginx backup： 1234mv /usr/share/nginx/html/index.html&#123;,.bak&#125;echo "&lt;h1&gt;Backup Server&lt;/h1&gt;" &gt; /usr/share/nginx/html/index.htmlsystemctl enable nginxsystemctl start nginx proxy： 123456789101112131415161718mv /etc/nginx/conf.d/default.conf&#123;,.bak&#125;cat &gt; /etc/nginx/conf.d/upstream1.conf &lt;&lt;EOFupstream backend &#123; server 192.168.1.101; weight=2 max_fails=2 fail_timeout=3; server 192.168.1.102:80 weight=1 max_fails=2 fail_timeout=3; server 192.168.1.103 backup;&#125;server &#123; location / &#123; proxy_pass http://backend; &#125;&#125;EOFsystemctl enable nginxsystemctl start nginx 测试1for i in `seq 10`;do curl 192.168.1.200;done down掉backend1，只能访问到backend2 down掉backend1和backend2，就负载均衡到backup备份服务器上了：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Nginx FastCGI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08-ngx_http_fastcgi_module详解]]></title>
    <url>%2Flinux%2F20170823-08-ngx_http_fastcgi_module%2F</url>
    <content type="text"><![CDATA[有两种Web架构： LNMP( Linux+Nginx+MySQL+PHP-FPM+MySQL)：需要FastCGI模块 LNAMP(Nginx+HTTPD+MySQL+PHP_Module)：需要HTTP相关模块 LNMP是使用的php的fpm功能，而不再是一个依赖httpd的库模块。LNAMP里面，是httpd+php库模块。 ngx_http_fastcgi_module和ngx_http_proxy_module很像，在proxy里的用的proxy_pass，在fastcgi里就变成了fastcgi_pass，定义缓存也换成了是fastcgi_cache_path，调用缓存也换成了fastcgi_cache等等。基本用法都差不多。 这里给出一个示例1： 在192.168.10.40上安装php-fpm 1yum install -y php-fpm 修改/etc/php-fpm.d/www.conf 12345678listen = 0.0.0.0:9000# listen.allowed_clients = 127.0.0.1 # 注释掉，让Nginx可以连接到php-fpmpm = dynamic # 可以修改为静态# 下面的参数生产中都会调大。pm.max_children = 50 # 最大子进程数pm.start_servers = 5 # 起始子进程数pm.min_spare_servers = 5 # 最小空闲子进程数pm.max_spare_servers = 35 # 最大空闲子进程数 在Nginx(192.168.10.10)上的/etc/nginx/config.d/里在上一节的基础之上设置vhost1.conf： 12345678910111213141516171819202122232425262728293031323334353637383940server &#123; server_name www.yulongjun.com; listen 80; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location / &#123; root /usr/share/nginx/html; # 此处增加php index index.php index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; location /blog/ &#123; proxy_pass http://192.168.10.20/; &#125; location /bbs/ &#123; proxy_pass http://192.168.10.30/; &#125; location ~* \.(jpg|gif|png|jpeg|svg)$ &#123; proxy_pass http://192.168.10.40; proxy_cache pgcache; proxy_cache_key $request_uri proxy_cache_valid 200 302 10m; proxy_cache_valid 301 1h; proxy_cache_valid any 1m; proxy_cache_use_state error timeout invalid_header; &#125; location ~* \.php$ &#123; fastcgi_pass 172.16.0.69:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME /data/fpm/$fastcgi_script_name; include fastcgi_params; &#125;&#125; 配置示例2：通过/pm_status和/ping来获取fpm server状态信息； 12345location ~* ^/(pm_status|ping)$ &#123; include fastcgi_params; fastcgi_pass 127.0.0.1:9000; fastcgi_param SCRIPT_FILENAME $fastcgi_script_name;&#125; 访问下列不同地址，可以看到不同格式和内容的状态信息： www.yulongjun.com/pm_statuswww.yulongjun.com/pm_status?fullwww.yulongjun.com/pm_status?jsonwww.yulongjun.com/pm_status?xml ngx_http_uwsgi_module和fastcgi接口一样，uwsgi也是一种Web服务器网关接口。它是一个Web服务器（如nginx，uWSGI等服务器）与web应用（如Python的Flask、Django、Tornado等框架写的程序）通信的一种规范。Nginx的ngx_http_uwsgi_module实现了跟uwsgi服务器的通信。 ngx_http_scgi_module同上，也是一种Web服务器网关接口，略]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Nginx FastCGI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07-ngx_http_proxy_module详解]]></title>
    <url>%2Flinux%2F20170823-07-ngx_http_proxy_module%2F</url>
    <content type="text"><![CDATA[ngx_http_proxy_module 里面包含反向代理(Reverse Proxy)相关指令和缓存(Cache)相关指令。 proxy_pass前端代理服务器Nginx：192.168.10.10后端准备两个被代理的服务器192.168.10.20和192.168.10.30，开启httpd服务 192.168.10.20上： 123456789yum install httpdcat &gt;/var/www/html/index.html &lt;&lt;EOF&lt;h1&gt;BLOG Server&lt;/h1&gt;&lt;h2&gt;Upsteam Server 1&lt;/h2&gt;&lt;h3&gt;IP: 192.168.10.20&lt;/h3&gt;EOFsystemctl start httpd 192.168.10.30上： 123456789yum install httpdcat &gt;/var/www/html/index.html &lt;&lt;EOF&lt;h1&gt;BBS Server&lt;/h1&gt;&lt;h2&gt;Upsteam Server 2&lt;/h2&gt;&lt;h3&gt;IP: 192.168.10.30&lt;/h3&gt;EOFsystemctl start httpd 192.168.10.10上 mv /etc/nginx/confi.d/{,.bak} vim /etc/nginx/conf.d/vhost1.conf 12345678server_name www.yulongjun.com;listen location /blog/ &#123; proxy_pass http://192.168.10.20/;&#123;location /bbs/ &#123; proxy_pass http://192.168.10.30/;&#125; 在proxy_pass 后面的url，加斜线和不加斜线是有区别的：如果加了斜线，如/bbs/ --&gt; http://192.168.10.20/ 指的就是访问www.yulongjun.com/bbs/即访问的http://192.167.10.20/。如果不加斜线，如/bbs/ --&gt; http://192.168.10.20，指的就是访问www.yulongjun.com/bbs/即访问的http://192.168.10.20/bbs。如果location定义其uri时使用了正则表达式的模式，或在if语句或limt_execept中使用proxy_pass指令，则proxy_pass之后必须不能使用uri; 用户请求时传递的uri将直接附加代理到的服务的之后: 123location ~* \.(jpg|gif|png|jpeg|svg)$ &#123; proxy_pass http://192.168.10.40;&#125; 如果我们访问http://www.yulongjun.com/img/bluesky.jpg即访问http://192.168.0.40/img/sky.jpg proxy_set_header 设定发往后端主机的请求报文的请求首部的值。 proxy_set_header X-Real-IP $remote_addr; # 真实client地址proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # 代理服务器代理的真实地址 定义各个段用的Cache之前，要先在http 上下文定义缓存路径： proxy_cache_path 定义的keys_zone的名字，要被嵌套的各个子段所引用，即各子段缓存都定义在这个缓存路径下。 1234http&#123; proxy_cache_path /data/nginx/cache levels=1:2 keys_zone=one:10m; ...&#125; 记得创建目录mkdir -pv /data/nginx/cache /data/nginx/cache为缓存路径。 levels=1:2的意思是一级目录为hash值的倒数第一个数，二级目录再切两个数字。 keys_zone=one:10m：one为缓存的名字,10m为缓存的大小 那么，缓存的里的文件名就类似于这样： /data/nginx/cache/c/29/b7f54b2df7773722d382f4809d65029c proxy_cacheproxy_cache定义在哪，就在哪生效，在server里写，就对server生效，在location里写，就对location生效。 proxy_cache_valid设置不同的响应码的缓存时间。 123proxy_cache_valid 200 302 10m;proxy_cache_valid 301 1h;proxy_cache_valid any 1m; proxy_cache_use_stalestale(腐烂，过时） 在后端服务器出故障或找不到的时候，哪些情况可能会使用过时缓存进行相应。 1proxy_cache_use_stale error timeout invalid_header; proxy_cache_methods proxy_hide_header隐藏响应首部的filed，默认隐藏了“Date”，“Server”，“X-Pad”和“X-Accel -…”，如果还想加隐藏项，就可以写在这里。 proxy_pass_header正好相反，想要传递客户端的header里的某些field，可以在这个字段添加。 proxy_set_header设定发往后端主机的请求header添加某些字段。 1proxy_set_header X-Real-IP $remote_addr; 修改后端服务器的日志格式： 1LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%&#123;Referer&#125;i\&quot; \&quot;%&#123;User-Agent&#125;i\&quot;&quot; combined 为1LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%&#123;Referer&#125;i\&quot; \&quot;%&#123;User-Agent&#125;i\&quot; %&#123;X-Real-IP&#125;i&quot; combined 或者这样修改： 1proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 延伸知识，X-forwarded-For可以记录多层代理的信息，如：X-Forwarded-For: 1.1.1.1, 2.2.2.2, 3.3.3.3而X-Real-IP只能记录单层代理的上一级Client的信息，多级只会记录隔一级的代理。详细区别可见：http://www.cnblogs.com/mypath/articles/5239687.html proxy_connect_timeout代理服务器与后端服务器建立连接的超时时间。应该注意的是，这个超时通常不能超过75秒。 proxy_read_timeout代理服务器从后端服务器读取响应的超时时长。超时仅在两个连续读操作之间设置，而不是传输整个响应。如果后端服务器在此时间内没有传输任何内容，则连接被关闭。 proxy_send_timeout请求发送给后端服务器的超市是长。超时仅在两个连续写操作之间设置，而不是传输整个响应。如果后端服务器在此时间内没有传输任何内容，则连接被关闭。 proxy_limit_rate限制从后端服务器读取相应的速度。 总的配置实例：mkdir -pv /data/nginx/cache vim /etc/nginx/nginx.conf 在httpd段里添加：proxy_cache_path /data/nginx/cache levels=1:2 keys_zone=one:10m; 全部配置为： 1234567891011121314151617181920212223242526272829303132user nginx;worker_processes 1;error_log /var/log/nginx/error.log warn;pid /var/run/nginx.pid;events &#123; worker_connections 1024;&#125;http &#123; include /etc/nginx/mime.types; default_type application/octet-stream; proxy_cache_path /data/nginx/cache levels=1:2 keys_zone=one:10m; log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /var/log/nginx/access.log main; sendfile on; #tcp_nopush on; keepalive_timeout 65; #gzip on; include /etc/nginx/conf.d/*.conf;&#125; 移走默认的/etc/nginx/config.d/default.conf ，以免造成干扰 mv /etc/nginx/config.d/default.conf{,.bak} 添加vhost1.confvim /etc/nginx/config.d/vhost1.conf 123456789101112131415161718192021222324252627282930313233server &#123; server_name www.yulongjun.com; listen 80; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; location / &#123; root /usr/share/nginx/html; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root /usr/share/nginx/html; &#125; location /blog/ &#123; proxy_pass http://192.168.10.20/; &#125; location /bbs/ &#123; proxy_pass http://192.168.10.30/; &#125; location ~* \.(jpg|gif|png|jpeg|svg)$ &#123; proxy_pass http://192.168.10.40; proxy_cache pgcache; proxy_cache_key $request_uri proxy_cache_valid 200 302 10m; proxy_cache_valid 301 1h; proxy_cache_valid any 1m; proxy_cache_use_state error timeout invalid_header; &#125;&#125;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Nginx Reverse Proxy</tag>
        <tag>Nginx Cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06-Nginx的HTTP相关的杂项模块]]></title>
    <url>%2Flinux%2F20170823-06-ngx-http-other-modules%2F</url>
    <content type="text"><![CDATA[主要介绍下面几个模块： ngx_http_access_modulehttp_access_module包含了http的访问权限控制的一个模块，前面一节的limit_except指令里用过，还可以用在http, server, location等地方。 allow deny 示例： 1234567location /admin &#123; deny 192.168.1.1; allow 192.168.1.0/24; allow 10.1.1.0/16; allow 2001:0db8::/32; deny all;&#125; ngx_http_auth_basic_module实现基于用户的访问控制，使用basic机制进行用户认证 auth_basic auth_basic_user_file 示例： 123yum install -y http-tools # htpasswd工具在这个包里htpasswd -c -m /etc/nginx/.htpasswd admin1 # 之后输入admin1的两次密码htpasswd -m /etc/nginx/.htpasswd admin2 # 之后设置admin2 的密码 修改nginx的配置文件： 1234location /admin &#123; auth_basic &quot;Admin Area&quot;; auth_basic_user_file /etc/nginx/.htpasswd;&#125; ngx_sub_status_module用于输出nginx的基本状态信息。 stub_status 1234567location/basic_status &#123; stub_status; allow 172.16.0.0/16; deny all;&#125; Active connections: 291server accepts handled requests 16630948 16630948 31070465Reading: 6 Writing: 179 Waiting: 106 Active connections: 活动状态的连接数 accepts：已经接受的客户端请求的总数 handled：已经处理完成的客户端请求的总数 requests：客户端发来的总的请求数 Reading：正在读取客户端请求报文首部的连接的连接数 Writing：正在向客户端发送响应报文过程中的连接数 Waiting：正在等待客户端发出请求的空闲连接数 ngx_http_log_module指定http访问日志格式和路径的模块。 log_format string可以使用nginx核心模块及其它模块内嵌的变量。Nginx内嵌变量：http://nginx.org/en/docs/varindex.html 我们可以看到/etc/nginx/nginx.conf的默认的format格式： 123log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; 具体分析一下： $remote_addr：指的就是客户端地址。 $remote_user：Basic认证时候的用户名 [$time_local]： 中括号括起来的本地时间(通用的日志格式的那种） $request：request URI $status：响应码 body_bytes_sent：返回的body大小 http_referer：跳转链接，从哪个网页跳转过来，可以用来看外链。 http_user_agent：用户浏览器类型 http_x_forwarded_for：简称XFF头，它代表客户端，也就是HTTP的请求端真实的IP，在通过了HTTP 代理或者负载均衡服务器时会添加该项。 httpd的日志格式默认使用的combined模式，combined格式的默认定义为：LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%{Referer}i\&quot; \&quot;%{User-Agent}i\&quot;&quot; combined这里写了各种参数代表的意义：http://blog.csdn.net/hytfly/article/details/11209909官方详细参数解析：http://httpd.apache.org/docs/2.4/mod/mod_log_config.html#formats access_log 可以在不同的区域自定义访问日志文件路径和文件名，还有日志格式，以及相关的缓冲的配置： buffer=size flush=time open_log_file_cache 缓存各日志文件相关的元数据信息。 max：缓存的最大文件描述符数量 min_uses：在inactive指定的时长内访问大于等于此值方 可被当作活动项 inactive：非活动时长 valid：验正缓存中各缓存项是否为活动项的时间间隔 ngx_gzip_modulegzip 是否开启gzip。 gzip_comp_level gzip压缩级别。级别1-9。 gzip disable 禁止IE6之类的浏览器压缩。 gzip_min_length 不能多大的文件都压缩吧，那样效率很低，这里规定了gzip压缩的最小响应报文大小。 gzip_buffers 支持实现压缩功能时为其配置的缓冲区数量及每个缓存区的大小。 gzip_proxied nginx作为代理服务器接收到从被代理服务器发送的响应报文后，在何种条件下启用压缩功能的。 off：对代理的请求不启用。no-cache, no-store，private：表示从被代理服务器收到的响应报文首部的Cache-Control的值为此三者中任何一个，则启用压缩功能。 gzip_types mime-type 压缩过滤器，仅对此处设定的MIME类型的内容启用压缩功能。 示例： 12345gzip on;gzip_comp_level 6;gzip_min_length 64;gzip_proxied any;gzip_types text/xml text/css application/javascript; ngx_http_ssl_modulessl 开启或关闭ssl。 ssl_certificate 当前虚拟主机使用PEM格式的证书文件。 ssl_certificate_key 当前虚拟主机上与其证书匹配的私钥文件。 ssl_protocols 支持ssl协议版本，默认为1、1.1、1.2，为了安全可以改为1.3，1.2是有安全漏洞的。 ssl_session_cache builtin[:size]：使用OpenSSL内建的缓存，此缓存为每worker进程私有。[shared:name:size]：在各worker之间使用一个共享的缓存。 ssl_session_timeout 客户端一侧的连接可以复用ssl session cache中缓存的ssl参数的有效时长。 ngx_http_ssl_module参数示例： 生成自签名证书123456789101112cd /etc/pki/CAopenssl genrsa -out private/cakey.pem 4096chmod go= private/cakey.pemopenssl req -new -x509 -key private/cakey.pem -out cacert.pem -days 365touch index.txtecho 01&gt; serialcd /etc/nginxmkdir sslcd ssl(umask 077; openssl genrsa -out nginx.key 2048)openssl req -new -key nginx.key -out nginx.csropenssl ca -in nginx.csr -out nginx.crt 配置ssl模块1234567891011server &#123; listen 443 ssl; server_name www.yulongjun.com; root /vhosts/ssl/htdocs; ssl on; ssl_certificate /etc/nginx/ssl/nginx.crt; ssl_certificate_key /etc/nginx/ssl/nginx.key; ssl_session_cache shared:sslcache:20m; ssl_session_timeout 600s; &#125; ngx_http_rewrite_module（URL重写）有下面几种url重写： http://bbs.yulongjun.com/ –&gt; http://www.yulongjun.com/bbs/http://www.yulongjun.com/ –&gt; https://www.yulongjun.com/http://www.yulongjun.com/login.php;username=tom –&gt; http://www.yulongjun.com/tom/ http://www.yulongjun.io/bbs/ –&gt; http://bbs.yulongjun.io/ 将用户请求的URI基于regex所描述的模式进行检查，而后完成替换。 rewrite 将用户请求的URI基于regex所描述的模式进行检查，匹配到时将其替换为replacement指定的新的URI。 注意：如果在同一级配置块中存在多个rewrite规则，那么会自下而下逐个检查；被某条件规则替换完成后，会重新一轮的替换检查，因此，隐含有循环机制；[flag]所表示的标志位用于控制此循环机制。 如果replacement是以http://或https://开头，则替换结果会直接以重定向返回给客户端。 [flag]： last：重写完成后停止对当前URI在当前location中后续的其它重写操作，而后对新的URI启动新一轮重写检查；提前重启新一轮循环(类似于continue)； break：重写完成后停止对当前URI在当前location中后续的其它重写操作，而后直接跳转至重写规则配置块之后的其它配置；结束循环； redirect：重写完成后以临时重定向方式(302 Moved Temprarily)直接返回重写后生成的新URI给客户端，由客户端重新发起请求；不能以http://或https://开头； permanent：重写完成后以永久重定向方式(301 Moved Permanently)直接返回重写后生成的新URI给客户端，由客户端重新发起请求； 12rewrite /(.*)$ https://www.yulongjun.io/$1;rewrite /bbs return 停止处理，然后返回一个特殊响应码给客户端。还可以指定返回的url页面。 rewrite_log 是否开启重写日志 if 引入一个新的配置上下文 ；条件满足时，执行配置块中的配置指令。只支持在server, location。 condition： 比较操作符： == != ~：模式匹配，区分字符大小写； ~*：模式匹配，不区分字符大小写； !~：模式不匹配，区分字符大小写； !~*：模式不匹配，不区分字符大小写； 文件及目录存在性判断：-e, !-e-f, !-f-d, !-d-x, !-x 示例： 12345678910111213141516171819if ($http_user_agent ~ MSIE) &#123; rewrite ^(.*)$ /msie/$1 break;&#125;if ($http_cookie ~* &quot;id=([^;]+)(?:;|$)&quot;) &#123; set $id $1;&#125;if ($request_method = POST) &#123; return 405;&#125;if ($slow) &#123; limit_rate 10k;&#125;if ($invalid_referer) &#123; return 403;&#125; set ![](/images/15043208822285.jpg) 设置用户自定义变量。 ngx_http_referer_module访问有通过地址直接访问，或者从其他网站跳转过来。 用过滤referer头域的方法可以来防止盗链。 valid_referers 定义referer首部的合法 可用值。 none：请求报文首部没有referer首部。 blocked：请求报文的referer首部没有值。 server_names：参数，其可以有值作为主机名或主机名模式。 arbitrary_string：直接字符串，但可使用*作通配符。 regular expression：被指定的正则表达式模式匹配到的字符串；要使用~打头，例如 ~.*\.yulongjun\.com。 示例： 12345valid_referers none block server_names *.yulongjun.com yulongjun.* ~\.yulongjun\.;if($invalid_referer) &#123; return http://www.yulongjun.com/invalid.jpg;&#125; 1234567valid_referers none blocked server_names *.example.com example.* www.example.org/galleries/ ~\.google\.;if ($invalid_referer) &#123; return 403;&#125;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Nginx ACL</tag>
        <tag>Nginx Status</tag>
        <tag>Nginx Log</tag>
        <tag>Nginx Gzip</tag>
        <tag>Nginx SSL</tag>
        <tag>Nginx Rewrite</tag>
        <tag>Nginx Upstream</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-ngx_http_core_module详解]]></title>
    <url>%2Flinux%2F20170821-05-ngx_http_core_module%2F</url>
    <content type="text"><![CDATA[ngx_http_core_module,规定了一些http功能相关的核心配置。 与套接字相关配置1、server设置虚拟服务器全局配置 配置一个虚拟主机的全局配置 12345server &#123;listen address[:PORT]|PORT;server_name SERVER_NAME;root /PATH/TO/DOCUMENT_ROOT;&#125; 2、listen设置监听端口和IP default_server：设定为默认虚拟主机 ssl：限制仅能够通过ssl连接提供服务 backlog=NUMBER 设置调用中限制挂起连接队列的最大长度的backlog参数 listen()。默认情况下， backlog在FreeBSD，DragonFly BSD和macOS上设置为-1，在其他平台上设置为511。 rcvbuf=SIZE 设置所监听端口的接收缓存大小（SO_RCVBUF选项） sndbuf=SIZE 设置所监听端口的发送缓存大小（SO_SNDBUF选项） 注意：(1) 基于port：listen PORT指令监听在不同的端口(2) 基于hostname：server_name指令指向不同的主机名(3) 基于ip的虚拟主机listen IP:PORT&gt; IP 地址不同 例如： 1234567listen 192.168.0.100:8000; # 监听192.168.0.100的8000端口listen 192.168.0.100; # 监听192.168.0.100，不写端口则默认为80端口listen 8000; # 监听本地8000端口listen *:8000; # 同上listen localhost:8000; # 同上listen [::]:8000; # 监听本地的ipv6的8000端口listen [::1]; # 监听本地的ipv6地址，不写端口则默认为80 3、server_name设置虚拟服务器名字 虚拟主机的主机名称后可跟多个由空白字符分隔的字符串。 支持*通配任意长度的任意字符 server_name *.yulongjun.com www.yulongjun.* 支持~起始的字符做正则表达式模式匹配 1server_name ~^www\d+\.yulongjun\.com$ \d 表示 [0-9] 匹配机制: 首先是字符串精确匹配 如:www.yulongjun.com 左侧*通配符 如:*.yulongjun.com 侧*通配符 如:www.yulongjun.* 正则表达式 如: ~^.*\.yulongjun\.com$ 都没写，则匹配default_server 4、tcp_nodelay on | off; 在keepalived模式下的连接是否启用TCP_NODELAY选项。off时，延迟发送，合并多个请求后再发送 默认On时，不延迟发送。 5、sendfile 零拷贝是否开启 是否启用sendfile零拷贝功能，即在内核中封装报文直接发送。默认Off。 定义路径相关的配置1、root 设置web资源的路径映射;用于指明请求的URL所对应的文档 的目录路径，可用于http, server, location, if in location ```server { …root /data/www/vhost1;}1234567891011 &gt; 例如：网络上`http://www.yulongjun.com/images/logo.jpg`,实际是服务器上的`/data/www/vhosts/images/logo.jpg`### 2、`location`![](/images/15034969496035.jpg) 在一个server中location配置段可存在多个，用于实现从uri到 文件系统的路径映射;ngnix会根据用户请求的URI来检查定义的所有 location，并找出一个最佳匹配，而后应用其配置 示例:``` server &#123;... server_name www.yulongjun.com; location /images/ &#123; root /data/imgs/; &#125; &#125; 例如：网络上的http://www.yulongjun.com/images/logo.jpg，实际是服务器上的/data/imgs/images/logo.jpg =：对URI做精确匹配。 1location = / &#123; ... &#125; 可以匹配http://www.yulongjun.com/ ，不能匹配http://www.yulongjun.com/index.html ~：对URI做正则表达式模式匹配，区分字符大小写 ~*：对URI做正则表达式模式匹配，不区分字符大小写 ^~：对URI的最左边部分做匹配检查，不区分字符大小写 不带符号：匹配起始于此uri的所有的uri 匹配优先级：=&gt;^~&gt; ～&gt; ～*&gt;不带符号。 示例： 1root /vhosts/www/htdocs/ http://www.yulongjun.com/index.html –&gt; /vhosts/www/htdocs/index.html 123456server &#123; root /vhosts/www/htdocs/ location /admin/ &#123; root /webapps/app1/data/ &#125;&#125; http://www.yulongjun.com/admin/index.html–&gt; /webapps/app1/data/admin/index.html 3、alias PATH; 路径别名，文档映射的另一种机制；仅能用于location上下文 示例： 123location /bbs/ &#123; alias /web/forum/;&#125; http://www.yulongjun.com/bbs/index.php–&gt; /web/forum/index.html 相比之下，root是这样的： 123location /bbs/ &#123; root /web/forum/;&#125; http://www.yulongjun.com/bbs/index.php–&gt; /web/forum/bbs/index.html 注意：location中使用root指令和alias指令的意义不同 (a) root，给定的路径对应于location中的/uri/左侧的/ (b) alias，给定的路径对应于location中的/uri/右侧的/ 4、index file指定默认网页资源，注意,这个是在ngx_http_index_module模块。 5、error_page 错误页404 12error_page 404 /404.html; error_page 500 502 503 504 /50x.html; 这将导致内部重定向到uri 客户端请求方法指定的内部重定向更改为“GET”（对于除“GET”和“HEAD” 之外的所有方法）。 此外，可以使用=response语法将响应代码更改为另一个，例如： 1error_page 404 = 200 /empty.gif; 如果代理服务器或FastCGI/uwsgi/SCGI服务器处理错误响应，并且服务器可能返回不同的响应代码（例如200、302、401或404），则可以使用返回的代码进行响应： 1error_page 404 = /404.php; 如果在内部重定向期间不需要更改URI和方法，则可以将错误处理传递到命名位置： 123456location / &#123; error_page 404 = @fallback; &#125; location @fallback &#123; proxy_pass http：//后端; 如果uri处理导致错误，则将最后发生的错误的状态代码返回给客户端。 还可以使用URL重定向进行错误处理： 12error_page 403 http://example.com/forbidden.html; error_page 404 = 301 http://example.com/notfound.html; 在这种情况下，默认情况下，将响应代码302返回给客户端。它只能更改为重定向状态代码之一（301,302,303,307和308）。当且仅当没有error_page 在当前级别上定义指令时，这些指令才能从上一级继承 。 6、try_files 以指定的顺序检查文件的存在，并使用第一个找到的文件进行请求处理; 该处理在当前上下文中执行。文件的路径是file根据根和别名指令的参数 构建的 。可以通过在名称末尾指定斜杠来检查目录的存在，例如“ $uri/”。如果没有找到任何文件，则会uri进行最后一个参数中指定的内部重定向 。例如： 1234567location /images/ &#123; try_files $uri /images/default.gif;&#125;location = /images/default.gif &#123; expires 30s;&#125; 最后一个参数也可以指向一个命名的位置，如下面的例子所示。从0.7.51版本开始，最后一个参数也可以是 code： 123location / &#123; try_files $uri $uri/index.html $uri.html =404;&#125; Drupal/FastCGI示例： 12345678910111213141516171819202122232425location / &#123; try_files $uri $uri/ @drupal;&#125;location ~ \.php$ &#123; try_files $uri @drupal; fastcgi_pass ...; fastcgi_param SCRIPT_FILENAME /path/to$fastcgi_script_name; fastcgi_param SCRIPT_NAME $fastcgi_script_name; fastcgi_param QUERY_STRING $args; ... other fastcgi_param&apos;s&#125;location @drupal &#123; fastcgi_pass ...; fastcgi_param SCRIPT_FILENAME /path/to/index.php; fastcgi_param SCRIPT_NAME /index.php; fastcgi_param QUERY_STRING q=$uri&amp;$args; ... other fastcgi_param&apos;s&#125; 在上面的例子中， 123location / &#123; try_files $uri $uri/ @drupal;&#125; 该try_files指令相当于下面两端代码的结合： 1234location / &#123; error_page 404 = @drupal; log_not_found off;&#125; 123456789location ~ \.php$ &#123; try_files $uri @drupal; fastcgi_pass ...; fastcgi_param SCRIPT_FILENAME /path/to$fastcgi_script_name; ...&#125; try_files 在将请求传递给FastCGI服务器之前检查PHP文件的存在。 Wordpress和Joomla的示例： 12345678910111213141516171819location / &#123; try_files $uri $uri/ @wordpress;&#125;location ~ \.php$ &#123; try_files $uri @wordpress; fastcgi_pass ...; fastcgi_param SCRIPT_FILENAME /path/to$fastcgi_script_name; ... other fastcgi_param&apos;s&#125;location @wordpress &#123; fastcgi_pass ...; fastcgi_param SCRIPT_FILENAME /path/to/index.php; ... other fastcgi_param&apos;s&#125; 定义客户端请求的相关配置1、keepalive_timeout设定长连接的超时时长 第一个参数设置保持连接的超时时长，0表示禁止长连接。默认为75s，推荐120s 可选的第二个参数在“Keep-Alive：timeout = time”响应头域中设置一个值。两个参数可能不同。 Mozilla和Konqueror可以识别 Keep-Alive：timeout = time头域。MSIE在大约60秒内自行关闭保持连接。 2、keepalive_requests长连接允许请求的资源的最大数量 在一次长连接上所允许请求的资源的最大数量。默认为100 3、keepalive_disable对哪种浏览器禁用长连接 4、send_timeout设置用于向客户端发送响应的超时时间 设置用于向客户端发送响应的超时时间。超时仅在两个连续的写入操作之间设置，而不是传输整个响应。如果客户端在这段时间内没有收到任何内容，则连接被关闭。 5、client_body_buffer_size 设置读取客户端请求体的缓冲区大小。如果请求体大于缓冲区，则整个身体或仅将其部分写入 临时文件。默认情况下，缓冲区大小等于两个内存页面。这是x86上的8K，其他32位平台和x86-64。其他64位平台通常为16K。 6、client_body_temp_path定义用于存储持有客户机请求主体的临时文件的目录。最多可以在指定目录下使用三级子目录层次结构。例如，在以下配置中： 1client_body_temp_path /spool/nginx/client_temp 1 2; 临时文件hash： 85c9d32c3526a1bbb3996525ec80b3e0f7aa83dd 12ls /spool/nginx/client_temp/d/3d/a83/85c9d32c3526a1bbb3996525ec80b3e0f7aa83dd 我们可以看到就是逐级从右边截取hash值 目录名为16进制的数字； 1 1级目录占1位16进制，即2^4=16个目录 0-f 2 2级目录占2位16进制，即2^8=256个目录 00-ff 2 3级目录占2位16进制，即2^8=256个目录 00-ff 对客户端进行限制的相关配置1、limit_rate 限制响应给客户端的传输速率，单位是bytes/second 默认值0表示无限制 19、limit_except 限制客户端使用除了指定的请求方法之外的其它方法，只能用在location上下文。 Method:GET, HEAD, POST, PUT, DELETE MKCOL, COPY, MOVE, OPTIONS, PROPFIND, PROPPATCH, LOCK, UNLOCK, PATCH 12345limit_except GET HEAD POST &#123; deny 192.168.111.200 allow 192.168.111.0/24; deny all;&#125; 表示除了GET、HEAD、POST方法其他方法都限制，主机范围为：禁止192.168.111.200、允许192.168.111.0/24、禁止所有。即仅允许192.168.111.0网段访问，但是禁止192.168.111.200的地址访问。 文件操作优化的配置1. aio 是否启用Linux的aio功能。 2. directio 是否同步写磁盘，在Linux主机启用O_DIRECT标记，意味文件大于等于给定的大小时使用，例如directio 4m 3. open_file_cache nginx可以缓存以下三种信息： 文件元数据：文件的描述符、文件大小和最近一次的修改时间 打开的目录结构 没有找到的或者没有权限访问的文件的相关信息 max=N：可缓存的缓存项上限；达到上限后会使用LRU算法实现管理。inactive=time：缓存项的非活动时长，在此处指定的时长内未被命中的或命中的次数少于open_file_cache_min_uses指令所指定的次数的缓存项 即为非活动项，将被删除。 4. open_file_cache_errors 是否缓存查找时发生错误的文件一类的信息。 5. open_file_cache_min_uses open_file_cache指令的inactive参数指定的时长内，至少被命中此处指定的次数方可被归类为活动项。 6. open_file_cache_valid 缓存项有效性的检查频率。 综上来个例子： 123456aio on;directio 4m;open_file_cache max=1000 inactive=20s;open_file_cache_valid 30s;open_file_cache_min_uses 2;open_file_cache_errors on; 其他模块暂时没时间写，有时间再写。可以去查看官方文档的Module reference(模块参考），用的比较多的是NGINX主配置文件里讲到的那些模块，这里再次列出来。 核心模块： core module 常用的标准模块： HTTP modules： ngx_http_core_modules http核心功能模块（重要） ngx_http_ssl_module http信道加密模块（重要） ngx_http_upstream_module http定义服务器组模块（重要） ngx_http_fastcgi|uWSGI|SCGI_module http web api接口模块（重要） ngx_http_proxy_module http反向代理模块（重要） ngx_http_gzip_module http gzip压缩传输模块（次一级） ngx_http_log_module http日志模块（次一级） ngx_http_referer_modulehttp防盗链模块（次一级） ngx_http_rewrite_module http重定向模块（次一级） ngx_http_access_module http权限控制模块 ngx_http_auth_basic_module http认证模块 ngx_http_stub_status_module http状态模块 ngx_http_headers_module http首部信息模块 Mail modules： 用的少 Stream modules： ngx_stream_core_module http的伪四层负载均衡模块]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Nginx HTTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-Nginx的默认http配置选项]]></title>
    <url>%2Flinux%2F20170821-04-nginx-default-httpd-config%2F</url>
    <content type="text"><![CDATA[http配置结构http配置，可以写在/etc/nginx/nginx.conf的http配置字段里。也可以单独来写，一般是单独写在/etc/nginx/conf.d/xxx.conf文件里，不同的虚拟主机写在不同的配置文件里，清晰明了。 在http的配置文件里，和主配置一样，分为{}括起来的，我们称之为段，比如http{}、server{}、location {}、if CONDITION {} 12345678910111213141516171819202122http &#123; # http block: http/https 协议相关的公共配置块 ... server &#123; # server虚拟主机配置段，可以定义多个server # server block: 某一虚拟主机的详细配置块 ... location /URI1/ &#123; # 资源位置location的配置段，可以定义多个location # location block: 某一资源位置的详细配置块 if CONDITION &#123; # if in location block: 在location配置段里的if条件判断块。 &#125; ... &#125; location /URI2/ &#123; ... &#125; location ... # 多个location &#125; server &#123; &#125; server ... # 多个server &#125; http各种配置段的的详细配置选项信息见： http://nginx.org/en/docs/http/ngx_http_core_module.html 与http相关的配置指令仅能够放置于http,server,location,upstream,if CONDITION段里。而且每个指令都有对应的可以放的段，并不是所有段都可以放。 比如default_type配置指令，只支持http，server，location段： Syntax: default_type mime-type;Default: default_type text/plain;Context: http, server, location 通过默认的http配置来简单了解http配置文件。 1.10版本时候，http默认配置还是写在/etc/nginx/nginx.conf中，新版本（1.12）的http默认配置是在`/etc/nginx/conf.d/default.conf 我们通过理解nginx.conf里默认的http配置来管中窥豹。后续再去理解一些更精细的参数。 HTTP协议的配置： 123456789101112131415161718192021222324252627282930313233343536373839404142http &#123; # 日志格式 日志别名 log_format main '$remote_addr - $remote_user [$time_local] "$request" ' '$status $body_bytes_sent "$http_referer" ' '"$http_user_agent" "$http_x_forwarded_for"'; # 定义日志路径和使用的日志格式（用上面定义的别名） access_log /var/log/nginx/access.log main; sendfile on; # sendfile零拷贝 tcp_nopush on; # 在sendfile模式下，是否启用TCP_CORK选项 tcp_nodelay on; # 在keepalived模式下的连接是否启用TCP_NODELAY选项 keepalive_timeout 65; # 设定保持连接的超时时长，0表示禁止长连接；默认为65s types_hash_max_size 2048; include /etc/nginx/mime.types; # 支持的MIME类型文件 default_type application/octet-stream; # # Load modular configuration files from the /etc/nginx/conf.d directory. # See http://nginx.org/en/docs/ngx_core_module.html#include # for more information. include /etc/nginx/conf.d/*.conf; # 载入confi.d下的子配置文件 server &#123; listen 80 default_server; # 监听地址；default_server表示是设定为默认虚拟主机 listen [::]:80 default_server; # ipv6的监听地址 server_name _; # 列出所有服务器名称。 root /usr/share/nginx/html; # http服务器的根目录相对于系统的路径 # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; # 载入default.d下的配置文件 location / &#123; # 根据请求URI设置配置，这里请求的URI是/ &#125; error_page 404 /404.html; #设置404错误页面为/404.html, location = /40x.html &#123; # 这里又缩进了一层location，表示404的error_page实际指向是http root目录的40x.html,根据上文root路径，可得出是系统下/usr/share/nginx/html/40x.html &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; 原理同上，是把500，502 503，504的error_page都定位到/50x.html上 &#125; HTTPS(HTTP with TLS) 12345678910111213141516171819202122232425262728293031# Settings for a TLS enabled server. server &#123; listen 443 ssl http2 default_server; # 端口为443，开启了ssl，协议为HTTP2.0标准，设置为默认服务器。 listen [::]:443 ssl http2 default_server; # ipv6的 server_name _; #服务器名字列表 root /usr/share/nginx/html; # https服务根目录 ssl_certificate "/etc/pki/nginx/server.crt"; # 服务器证书 ssl_certificate_key "/etc/pki/nginx/private/server.key"; # 服务器认证私钥 ssl_session_cache shared:SSL:1m; # SSL会话缓存模式为共享，时间为1分钟。 ssl_session_timeout 10m; # SSL会话超时时间10分钟 ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; # Load configuration files for the default server block. include /etc/nginx/default.d/*.conf; # 下同http，略 location / &#123; &#125; error_page 404 /404.html; location = /40x.html &#123; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; &#125; &#125;&#125; 术语：关于sendfile零拷贝，见Linux网络编程–sendfile零拷贝高效率发送文件。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>default http config</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-Nginx主配置文件]]></title>
    <url>%2Flinux%2F20170821-03-nginx-global-config%2F</url>
    <content type="text"><![CDATA[Nginx的程序架构Nginx进程： 进程为为master/worker模式 一个master进程：负载加载和分析配置文件、管理worker进程、平滑升级 一个或多个worker进程：处理并响应用户请求 缓存相关的进程： cache loader：载入缓存对象 cache manager：管理缓存对象 Nginx模块： 高度模块化，但其模块早期不支持DSO机制；近期版本支持动态装载和卸载； 模块分类： 核心模块： core module 常用的标准模块： HTTP modules： ngx_http_core_modules http核心功能模块（重要） ngx_http_ssl_module http信道加密模块（重要） ngx_http_upstream_module http定义服务器组模块（重要） ngx_http_fastcgi|uWSGI|SCGI_module http web api接口模块（重要） ngx_http_proxy_module http反向代理模块（重要） ngx_http_gzip_module http gzip压缩传输模块（次一级） ngx_http_log_module http日志模块（次一级） ngx_http_referer_module http防盗链模块（次一级） ngx_http_rewrite_module http重定向模块（次一级） ngx_http_access_module http权限控制模块 ngx_http_auth_basic_module http认证模块 ngx_http_stub_status_module http状态模块 ngx_http_headers_module http首部信息模块 Mail modules： 用的少 Stream modules： ngx_stream_core_module http的伪四层负载均衡模块 第三方模块 Nginx的功用： 静态的web资源服务器；(图片服务器，或js/css/html/txt等静态资源服务器) 结合FastCGI/uwSGI/SCGI等协议反代动态资源请求； http/https协议的反向代理； imap4/pop3协议的反向代理； tcp/udp协议的请求转发； Nginx的安装yum安装最新稳定版： vim /etc/yum.repos.d/nginx.repo 12345[nginx]name=nginx repobaseurl=http://nginx.org/packages/centos/$releasever/$basearch/gpgcheck=0enabled=1 yum install -y nginx epel源安装次新的稳定版： yum install epel-release这是安装的官方的epel源地址。 也可以安装国内的epel镜像源： 如阿里： 12mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backupwget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo NGINX程序组成 主程序文件：/usr/sbin/nginx systemd服务：nginx.service 配置文件(/etc/nginx下）： 主配置文件：nginx.conf（include了conf.d/*.conf） fastcgi， uwsgi，scgi等协议相关的配置文件：fastcgi_params、uwsgi_params、scgi_params 支持的MIME类型配置文件：mime.types NGINX主配置文件/etc/nginx/nginx.conf 主配置文件各种配置段的的详细配置选项信息见： http://nginx.org/en/docs/ngx_core_module.html 我们要来理解配置文件的结构： 段：如http{} 、event{}、server{}、location{}等，段可以并列，也可以嵌套。 块：指的就是在段里定义的一组directives（配置指令）。 directive：指的就是块里的一条条配置指令。指令定义方法：directive value [value2 ...];。 注意：(1) 指令必须以分号结尾。(2)嵌套的段是有层级结构的，比如http段里，遵循http{}–&gt;server{}–&gt;location{}这种嵌套结构。(3) 支持使用配置变量： 内建变量：由Nginx模块引入，可直接引用。 自定义变量：由用户使用set命令定义。 定义变量：set variable_name value;。 引用变量：$variable_name 主配置文件结构： 1234567891011121314151617181920212223# main block：主配置段，也即全局配置段...events &#123; # 事件驱动相关的配置段 ...&#125; http &#123; # http block：http/https 协议相关的公共配置段 server &#123; # server block：虚拟主机公共相关配置段 location /xxx &#123; # location block：资源位置相关公共配置段 &#125; ... # 其他location &#125; ... # 其他server&#125; mail &#123; ...&#125;stream &#123; ...&#125; main block主配置段，也即全局配置段main block常见配置指令分类： 正常运行必备的配置 优化性能相关的配置 用于调试及定位问题相关的配置 事件驱动相关的配置 正常运行必备的配置： 1、 user USERNAME; 指定worker进程的运行身份，如组不指定，默认和用户名同名。如果不写user字段，默认为nobody用户和nobody组。 2、pid /PATH/TO/PID_FILE; 指定存储nginx主进程进程号码的文件路径 3、 include file | mask; 指明包含进来的其它配置文件片断 4、load_module file /usr/share/nginx/modules/*.conf 指明要装载的动态模块路径。例如geoip.conf文件里：load_module &quot;/usr/lib64/nginx/modules/ngx_http_geoip_module.so&quot;; 性能优化相关的配置： 1、worker_processes NUMBER | auto; worker进程的数量；NUMBER通常应该等于小于当前主机的cpu的物理核心数。auto：自动为当前主机物理CPU核心数。例如：worker_processes 4 2、worker_cpu_affinity cpumask ...; worker_cpu_affinity auto |CPUMASK;nginx进程的CPU亲缘性，配置cpumask可以指定绑定CPU 123456CPU MASK：0000 0001：0号CPU0000 0010：1号CPU0000 0100：2号CPU... ...0000 0011：0和1号CPU 例如如果nginx进程为4个的话，worker_cpu_affinity 0001 0010 0100 1000;就是把4个nginx进程分别绑在0123号cpu上。例如如果nginx进程为4个的话，worker_cpu_affinity 0101 1010;就是把第一个nginx进程绑在0和2号cpu上，第二个cpu进程绑在1和3号cpu上 3、worker_priority NUMBER; 指定worker进程的nice值，设定worker进程优先级：[-20,20] 4、worker_rlimit_nofile NUMBER; 单个worker进程所能够打开的文件数量上限,如65535 调试、定位问题相关配置： 1、daemon on|off; 是否以守护进程方式运行nignx，默认是on，守护进程方式 2、master_process on|off; 是否以master/worker模型运行nginx；默认为on，off将不启动worker 3、error_log PATH/TO/LOGFILE [LEVEL]; PATH/TO/LOGFILE 为日志文件路径LEVEL为日志级别,一般不修改，出于调试需要，可设定为debug。系统默认为：error_log /var/log/nginx/error.log; 事件驱动相关的配置: 写在events里面 events { ... } 1、worker_connections number; 每个worker进程所能够打开的最大并发连接数数量，如 10240总最大并发数：worker_processes * worker_connections 2、use METHOD 指明并发连接请求的处理方法 ,一般不用设置，Nginx会默认自动选择适合系统的最优方法。 如果系统是Linux，会自动使用epoll方法；如果是FreeBSD和macOS，会自动使用kqueue；如果是Solaris和HP/UX，会自动使用/dev/poll。有两个普通方法select和poll，已经被淘汰，不做讨论。 3、accept_mutex on | off 是否接受互斥。指的是处理新的连接请求的方法；on指由各个worker轮流处理新请求 ，Off指每个新请求的到达都会通知(唤醒)所有的worker进程，但 只有一个进程可获得连接，造成“惊群”，影响性能，默认on]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>IO model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-IO模型]]></title>
    <url>%2Flinux%2F20170821-02-io-model%2F</url>
    <content type="text"><![CDATA[转载自：我本善良 几大模型： Blocking IO（阻塞型IO） Non-Blocking IO（非阻塞型IO） IO multiplexing（IO复用型）：select，poll signal driven I/O（事件驱动型IO）：epoll（Linux）、Kqueue（BSD）、/dev/poll（Solaris） Asynchronous I/O（异步IO） 概括来说，一个IO操作可以分为两个部分：发出请求、结果完成。如果从发出请求到结果返回，一直Block，那就是Blocking IO；如果发出请求就可以返回（结果完成不考虑），就是non-blocking IO；如果发出请求就返回，结果返回是Block在select或者poll上的，则其只能称为IO multiplexing；如果发出请求就返回，结果返回通过Call Back的方式被处理，就是AIO。 Blocking IO这个最好理解了，在Blocking IO模式下，函数调用只有在操作完成后才会返回。下图是它调用过程的图示： 重点解释下上图，下面例子都会讲到。首先application调用 recvfrom()转入kernel，注意kernel有2个过程，wait for data和copy data from kernel to user。直到最后copy complete后，recvfrom()才返回。此过程一直是阻塞的。 Non-Blocking IONon-Blocking 是Blocking的反，也就是说，即使操作没有完成，函数也可以返回。调用过程如下： 可以看见，如果直接操作它，那就是个轮询。。直到内核缓冲区有数据 AIO也是这样啊？对！这是Non-Blocking IO 和AIO的共同点。其实从概念层面来说Non-Blocking IO 就是AIO，他们没有什么区别。但是Non-Blocking IO是对文件描述符（*nix）或者Handle（Windows）的设置，在执行操作时不需要特殊的数据结构。Non-Blocking IO提交请求后只能通过提交的操作函数来查询操作是否完成，这是一个很大的限制。而AIO往往会提供多种通知或者查询机制，也就是说用Non-Blocking IO时只能轮询，而AIO有更多选择。所以是否支持轮询外的其他机制是AIO和Non-Blocking IO的区别。 Non-Blocking IO和Blocking IO的区别仅仅在操作是否能够立刻完成，如果能够立刻完成，IO函数的行为是一样的；如果不能立刻完成，Non-Blocking IO会返回EAGAIN或者EWOULDBLOCK，而Blocking IO会一直阻塞。 IO multiplexing (select and poll)最常见的I/O复用模型，select。 select先阻塞，有活动套接字才返回。与blocking I/O相比，select会有两次系统调用，但是select能处理多个套接字。 signal driven IO (SIGIO) 与I/O multiplexing (select and poll)相比，它的优势是，免去了select的阻塞与轮询，当有活跃套接字时，由注册的handler处理。 Asynchronous IOAIO让应用发起一个操作请求，让这个请求被异步地执行。应用可以选择在操作完成时被通知到或者不被通知。所以通知机制并不是AIO的核心，但是需要提供几种方案的选择。 完全异步的I/O复用机制，因为纵观上面其它四种模型，至少都会在由kernel copy data to application时阻塞。而该模型是当copy完成后才通知application，可见是纯异步的。 Nginx官方并没有实现AIO模块，Linux官方提供了AIO库函数来实现AIO，但是用的很少。目前有很多开源的AIO库，如libevent、libev、libuv都很不错。 下面是以上五种模型的比较 可以看出，越往后，阻塞越少，理论上效率也是最优。 下面可以把select,epoll,iocp,kqueue按号入座。 select和iocp分别对应第3种与第5种模型，那么epoll与kqueue呢？其实也于select属于同一种模型，只是更高级一些，可以看作有了第4种模型的某些特性，如callback机制。 那么，为什么epoll,kqueue比select高级？ 答案是，他们无轮询。因为他们用callback取代了。想想看，当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。 提供一致的接口，IO Design Patterns 实际上，不管是哪种模型，都可以抽象一层出来，提供一致的接口，广为人知的有ACE,Libevent这些，他们都是跨平台的，而且他们自动选择最优的I/O复用机制，用户只需调用接口即可。说到这里又得说说2个设计模式，Reactor and Proactor。有一篇经典文章http://www.artima.com/articles/io_design_patterns.html值得阅读，Libevent是Reactor模型，ACE提供Proactor模型。实际都是对各种I/O复用机制的封装。 总结一些重点： 只有IOCP是asynchronous I/O，其他机制或多或少都会有一点阻塞。 select低效是因为每次它都需要轮询，但低效也是相对的，视情况而定，也可通过良好的设计改善。 epoll, kqueue是Reacor模式，IOCP是Proactor模式。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>IO model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-Nginx简介]]></title>
    <url>%2Flinux%2F20170821-01-nginx-introduction%2F</url>
    <content type="text"><![CDATA[Nginx （是engine x的缩写） 是一个HTTP server（HTTP服务器）和HTTP reverse proxy server（HTTP反向代理服务器），一个mail proxy server（邮件代理服务器）, 一个 通用 TCP/UDP proxy server（TCP/UDP反向代理服务器）。 Nginx工作在OSI模型的七层，也可以提供伪四层负载均衡。 Nginx是一个scalable event-driven（asynchronous）architecture，即可伸缩的事件驱动型（异步）架构。 国内的基于Nginx的二次开发版本： Tengine OpenResty Nginx三大核心功能： http Web服务器（类似于httpd的功能） HTTP reverse proxy Server（httpd也有类似的功能） mail Mail proxy server，支持icmp/pop3/smtp stream TCP/UDP proxy server（类似于lvs，不过是伪四层代理，效率没有lvs高） HTTP的几个概念术语URI统一资源定位符（Uniform Resource Identifier） scheme:[//[user[:password]@]host[:port]][/path][?query][#fragment] 例子： fragment,可以通过访问下面网站来实验： PV、UVPV: Page ViewUV: User View http事务：request&lt;–&gt;response Headers: 通用头部（General Headers） 请求头部（Response Headers） 回应头部（Request Headers） Method：GET/HEAD/POST(常用）, PUT/DELETE, TRACE, OPTIONS 常用Status Code(状态码）： 2xx：成功类响应码，200（OK）等 3xx：重定向类的响应码，301（永久重定向,permenent）、302（临时重定向,termina）、304（Not Modified，从上次访问后未发生改变） 4xx：客户端错误：403(Forbidden)，404(Not Found) 5xx：服务器端错误，502(Bad Gateway)、503 （Service Unavailable） 认证（Auth）： 基于IP认证 基于用户认证：basic（基于htpasswd生成的用户文件认证）/digest（摘要认证） httpd MPM： prefork：进程模型，两级结构，主进程master负责生成prefork子进程，每个子进程相应一个请求。 worker：线程模型，三级结构，主进程master负责生成子进程worker，每个worker子进程负责生成多个worker线程，每个worker线程相应一个请求。 event：主进程master负责生成子进程event，每个event子进程基于事件驱动机制相应多个请求。 文件描述符（file descriptor，简写fd）内核（kernel）利用文件描述符（file descriptor）来访问文件。文件描述符是非负整数。打开现存文件或新建文件时，内核会返回一个文件描述符。读写文件也需要使用文件描述符来指定待读写的文件。 在设备读写（IO）、网络通信、进程通信，fd可谓是关键中的关键。 IO类型“阻塞”与”非阻塞”与”同步”与“异步”不能简单的从字面理解，提供一个从分布式系统角度的回答。 1.同步(Synchronous)与异步(Asynchronous)同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。 换句话说，就是由调用者主动等待这个调用的结果。 而异步则是相反，调用在发出之后，这个调用就直接返回了，但是没有返回最终结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 典型的异步编程模型比如Node.js，Python的asyncio。 举个通俗的例子：你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，我查一下”，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。 2. 阻塞(Blocking）与非阻塞IO(Non-Blocking)阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态。 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 还是上面的例子:你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。 3. 同步阻塞、同步非阻塞、异步阻塞、异步非阻塞 老张爱喝茶，废话不说，煮开水。出场人物：老张，水壶两把（普通水壶，简称水壶；会响的水壶，简称响水壶）。 （1）老张把水壶放到火上，立等水开。（同步阻塞）老张觉得自己有点傻。 （2）老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有。（同步非阻塞）老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音。 （3）老张把响水壶放到火上，立等水开。（异步阻塞）老张觉得这样傻等意义不大。 （4）老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶。（异步非阻塞）老张觉得自己聪明了。 所谓同步异步，只是对于水壶而言。普通水壶，同步；响水壶，异步。虽然都能干活，但响水壶可以在自己完工之后，提示老张水开了。这是普通水壶所不能及的。同步只能让调用者去轮询自己（情况2中），造成老张效率的低下。 所谓阻塞非阻塞，仅仅对于老张而言。立等的老张，阻塞；看电视的老张，非阻塞。 情况1和情况3中老张就是阻塞的，媳妇喊他都不知道。虽然3中响水壶是异步的，可对于立等的老张没有太大的意义。所以一般异步是配合非阻塞使用的，这样才能发挥异步的效用。 – 来自知乎：怎样理解阻塞非阻塞与同步异步的区别？ 下一节我们来了解一下IO模型，IO模型不光Nginx有设计，其他服务也会涉及。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx系列文章]]></title>
    <url>%2Flinux%2F20170821-00-nginx-content%2F</url>
    <content type="text"><![CDATA[✔01-Nginx简介 ✔02-IO模型 ✔03-Nginx的主配置文件 ✔04-Nginx的默认http配置 ✔05-ngx_http_core_module详解 ✔06-Nginx的HTTP相关的杂项模块 ✔07-ngx_http_proxy_module详解 ✔08-ngx_http_fastcgi_module详解 ✔09-ngx_http_upstream_module详解 ✔10-ngx_stream*_module详解]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS/NAT实验]]></title>
    <url>%2Flinux%2F20170819-03-lvs-nat%2F</url>
    <content type="text"><![CDATA[LVS/NAT模式，在VS和RS之间有路由。 上图中，假设172.16.111.0网段都是外网。 实验采用Vagrant配置网络和主机信息。Vagrant的用法可参考Vagrant–快速搭建实验环境利器。 服务器 IP client 172.16.111.123（假装是外网） vs 172.16.111.200（假装是外网）/192.168.111.200（内网） rs1 192.168.111.101（内网，网关指向vs） rs2 192.168.111.102（内网，网关指向vs） Vagrantfile配置文件： 12345678910111213141516171819202122232425262728293031323334353637Vagrant.configure("2") do |config| # config为全局配置文件 config.vm.box = "longdream/centos7" # 这里是我自定义的centos7模板 config.hostmanager.enabled = true # 启用hostmanager插件 config.hostmanager.manage_guest = true # 允许更新虚拟机上的hosts文件 config.hostmanager.manage_host = true # 允许更新主机上的hosts文件 # 定义Client config.vm.define "client" do |client| client.vm.network "private_network", ip: "172.16.111.123" client.vm.hostname = "client" client.vm.provision "shell", inline: "sudo bash /vagrant/client.sh" end # 定义VS config.vm.define "vs" do |vs| vs.vm.network "private_network", ip: "172.16.111.200" vs.vm.network "private_network", ip: "192.168.111.200" vs.vm.hostname = "vs" vs.vm.provision "shell", inline: "sudo bash /vagrant/vs.sh" end # 定义RS1 config.vm.define "rs1" do |rs1| rs1.vm.network "private_network", ip: "192.168.111.101" rs1.vm.hostname = "rs1" rs1.vm.provision "shell", inline: "sudo bash /vagrant/rs1.sh" end # 定义RS2 config.vm.define "rs2" do |rs2| rs2.vm.network "private_network", ip: "192.168.111.102" rs2.vm.hostname = "rs2" rs2.vm.provision "shell", inline: "sudo bash /vagrant/rs2.sh" endend Vagrantfile里每一台机器都运行了相应的脚本。 Client: client.sh 1234#!/bin/bashecho &gt;&gt; /etc/sysconfig/network-scripts/ifconfig-eth1 &lt;&lt;EOFGATEWAY=172.16.111.200EOF VS: vs.sh 12345#!/bin/bashecho "net.ipv4.ip_forward=1" &gt;&gt; /etc/sysctl.confecho 1 &gt; /proc/sys/net/ipv4/ip_forwardyum install -y ipvsadmbash /vagrant/vs-nat-rr.sh start vs.sh引用的vs-nat-rr.sh来启动VS。 vs-nat-rr.sh： 123456789101112131415161718192021#!/bin/bashvip=172.16.111.200mode=m # m为NAT模式，g为DR模式，i为tun模式schdule=rrrip1=192.168.111.101rip2=192.168.111.102case $1 instart) ipvsadm -A -t $vip:80 -s $schdule ipvsadm -a -t $vip:80 -r $rip1 -$mode ipvsadm -a -t $vip:80 -r $rip2 -$mode ;;stop) ipvsadm -C ;;*) echo "Usage: `basename $0` start|stop" exit 1 ;;esac RS1 rs1.sh： 123456789101112#!/bin/bashecho 'GATEWAY=192.168.111.200' &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth1ifdown eth1 &amp;&amp; ifup eth1yum install -y httpdcat &gt;/var/www/html/index.html&lt;&lt;EOFReal Server 1EOFsystemctl enable httpdsystemctl start httpd RS2 rs2.sh： 123456789101112#!/bin/bashecho 'GATEWAY=192.168.111.200' &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth1ifdown eth1 &amp;&amp; ifup eth1yum install -y httpdcat &gt;/var/www/html/index.html&lt;&lt;EOFReal Server 2EOFsystemctl enable httpdsystemctl start httpd vagrant up启动所有机器后，在Virtualbox里关掉所有虚机的eth0（vagrant创建虚机时候，默认的一个NAT网络，默认在eth0上，关闭它以防止对实验造成影响）： 1ifdown eth0 然后从Client虚机里运行： bash /vagrant/client-test.sh进行测试： 1234567#!/bin.bash# 测试LVSvip=172.16.111.200for i in `seq 100`;do curl --connect-timeout 1 $vip sleep 1done]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>LVS</tag>
        <tag>LVS/NAT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS/DR实验]]></title>
    <url>%2Flinux%2F20170819-02-lvs-dr%2F</url>
    <content type="text"><![CDATA[LVS/DR模式,数据流来的时候走VS，回去的时候调度到RS上，然后用VIP的作为源地址返回回去。 实验采用Vagrant配置网络和主机信息。Vagrant的用法可参考Vagrant–快速搭建实验环境利器。 Vagrantfile配置文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344Vagrant.configure("2") do |config| # config为全局配置文件 config.vm.box = "longdream/centos7" # 这里是我自定义的centos7模板 config.hostmanager.enabled = true # 启用hostmanager插件 config.hostmanager.manage_guest = true # 允许更新虚拟机上的hosts文件 config.hostmanager.manage_host = true # 允许更新主机上的hosts文件 # 定义Client config.vm.define "client" do |client| client.vm.network "private_network", ip: "172.16.111.123" client.vm.hostname = "client" client.vm.provision "shell", inline: "sudo bash /vagrant/client.sh" end # 定义Router config.vm.define "router" do |router| router.vm.network "private_network", ip: "172.16.111.222" router.vm.network "private_network", ip: "192.168.111.222" router.vm.hostname = "router" router.vm.provision "shell", inline: "sudo bash /vagrant/router.sh" end # 定义VS config.vm.define "vs" do |vs| vs.vm.network "private_network", ip: "192.168.111.100" vs.vm.hostname = "vs" vs.vm.provision "shell", inline: "sudo bash /vagrant/vs.sh" end # 定义RS1 config.vm.define "rs1" do |rs1| rs1.vm.network "private_network", ip: "192.168.111.101" rs1.vm.hostname = "rs1" rs1.vm.provision "shell", inline: "sudo bash /vagrant/rs1.sh" end # 定义RS2 config.vm.define "rs2" do |rs2| rs2.vm.network "private_network", ip: "192.168.111.102" rs2.vm.hostname = "rs2" rs2.vm.provision "shell", inline: "sudo bash /vagrant/rs2.sh" endend Vagrantfile里每一台机器都运行了相应的脚本。 Client: client.sh 123#!/bin/bashecho "GATEWAY=172.16.111.222" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth1ifdown eth1 &amp;&amp; ifup eth1 Router: router.sh 123#!/bin/bashecho "net.ipv4.ip_forward=1" &gt;&gt;/etc/sysctl.confecho 1 &gt; /proc/sys/net/ipv4/ip_forward VS: vs.sh 12345#!/bin/bashecho "GATEWAY=192.168.111.222" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth1ifdown eth1 &amp;&amp; ifup eth1yum install -y ipvsadmbash /vagrant/vs-dr-wlc.sh start vs.sh引用的vs-dr-wlc.sh来启动VS。 vs-dr-wlc.sh： 123456789101112131415161718192021222324#!/bin/bashvip=192.168.111.200mode=g # m为NAT模式，g为DR模式，i为tun模式schdule=wlcrip1=192.168.111.101rip2=192.168.111.102dev=lo:1case $1 instart) ifconfig $dev $vip netmask 255.255.255.255 broadcast $vip up ipvsadm -A -t $vip:80 -s $schdule ipvsadm -a -t $vip:80 -r $rip1 -$mode -w 3 ipvsadm -a -t $vip:80 -r $rip2 -$mode -w 1 ;;stop) ipvsadm -C ifconfig $dev down ;;*) echo "Usage: `basename $0` start|stop" exit 1 ;;esac RS1 rs1.sh： 1234567891011121314#!/bin/bashecho "GATEWAY=192.168.111.222" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth1ifdown eth1 &amp;&amp; ifup eth1yum install -y httpdcat &gt;/var/www/html/index.html&lt;&lt;EOFReal Server 1EOFsystemctl enable httpdsystemctl start httpdbash /vagrant/rs-config.sh start RS2 rs2.sh： 1234567891011121314#!/bin/bashecho "GATEWAY=192.168.111.222" &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth1ifdown eth1 &amp;&amp; ifup eth1yum install -y httpdcat &gt;/var/www/html/index.html&lt;&lt;EOFReal Server 2EOFsystemctl enable httpdsystemctl start httpdbash /vagrant/rs-config.sh start 两个RS都调用的一个脚本 rs-config.sh 12345678910111213141516171819202122232425#!/bin/bashvip=192.168.111.200dev=lo:1case $1 instart) echo 1 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 1 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 2 &gt; /proc/sys/net/ipv4/conf/all/arp_announce echo 2 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce ifconfig $dev $vip netmask 255.255.255.255 broadcast $vip up echo "VS Server is Ready!" ;;stop) ifconfig $dev down echo 0 &gt; /proc/sys/net/ipv4/conf/all/arp_ignore echo 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_ignore echo 0 &gt; /proc/sys/net/ipv4/conf/all/arp_announce echo 0 &gt; /proc/sys/net/ipv4/conf/lo/arp_announce echo "VS Server is Cancel!" ;;*) echo "Usage `basename $0` start|stop" exit 1 ;;esac vagrant up启动所有机器后，在Virtualbox里关掉所有虚机的eth0（vagrant创建虚机时候，默认的一个NAT网络，默认在eth0上，关闭它以防止对实验造成影响）： 1ifdown eth0 然后从Client虚机里运行： bash /vagrant/client-test.sh进行测试： 1234567#!/bin.bash# 测试LVSvip=192.168.111.200for i in `seq 100`;do curl --connect-timeout 1 $vip sleep 1done]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>LVS</tag>
        <tag>LVS/DR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS原理、模式、调度算法]]></title>
    <url>%2Flinux%2F20170817-01-lvs-introduction%2F</url>
    <content type="text"><![CDATA[我们先来讲一下集群和负载均衡。 一、集群和负载均衡1. scale up和scale out当业务量越来越大的时候，我们就要对系统进行扩展，扩展有两种方式： scale up: 纵向扩展，即增强服务器的性能。买买买！但是性能越好的服务器价格成几何倍数增长…… scale out: 线性扩展，即增加服务器的数量，然后用调度来解决分配资源，这就是集群（Cluster）的由来。 2. 集群和分布式集群（Cluster）定义：为解决某个特定问题将多台计算机组合起来形成的单个系统 Cluster类型： LB：Load Balancing，负载均衡。 HA：High Availiablity，高可用。 HPC：High-Performance Computing，高性能计算。（www.top500.org） 与集群相对应的是分布式系统（distributed system）。 集群和分布式的区别，简单来说就是： 集群：同一个业务，部署在多个服务器上。 分布式：一个业务分拆多个子业务，部署在不同的服务器上。 举个例子大家更能明白： 一个小饭店原来只有一个厨师，切菜洗菜备料炒菜全干。后来客人多了，厨房一个厨师忙不过来，又请了个厨师，两个厨师都能炒一样的菜，这两个厨师的关系是集群。为了让厨师专心炒菜，把菜做到极致，又请了个配菜师负责切菜，备菜，备料，厨师和配菜师的关系是分布式，一个配菜师也忙不过来了，又请了个配菜师，两个配菜师关系是集群链接。–本段文字来源。 3. 负载均衡集群负载均衡集群的实现： 硬件： F5: BIG-IP Citrix: NetScaler A10: A10 软件： LVS HAproxy Nginx ats(apache traffic server) perlbal 软件中，LVS, HAproxy, Nginx用的比较多，重点学习就是这3个。 我们重点学习头3种：LVS, HAproxy ,Nginx 关于这三种的优缺点可以看这篇文章：Nginx/LVS/HAProxy负载均衡软件的优缺点详解 本文主要讲集群中的负载均衡中的一个：lvs（Linux Virtual Server）,LVS基于ISO七层模型中的4层传输层。 二、LVS基本术语和组成1. LVS基本术语LVS集群类型中的术语： VS：Virtual Server，Director，Dispatcher(调度器)，Load Balancer RS：Real Server(lvs里)， upstream server(nginx里)，backend server(haproxy里)、Replica（副本） CIP：Client IP 客户端ip VIP: Virtual server IP VS外网的IP DIP: Director IP VS内网的IP RIP: Real server IP 2. LVS软件组成LVS 由2部分程序组成，包括 ipvs 和 ipvsadm。 ipvs(ip virtual server)：一段代码工作在内核空间，叫ipvs，是真正生效实现调度的代码。 ipvsadm：另外一段是工作在用户空间，叫ipvsadm，负责为ipvs内核框架编写规则，定义谁是集群服务，而谁是后端真实的服务器(Real Server)。 LVS基本工作原理 当用户向负载均衡调度器（VS或者叫LB）发起请求，调度器将请求发往至内核空间。 PREROUTING链首先会接收到用户请求，判断目标IP确定是本机IP，将数据包发往INPUT链。 IPVS是工作在INPUT链上的，当用户请求到达INPUT时，IPVS会将用户请求和自己已定义好的集群服务进行比对，如果用户请求的就是定义的集群服务，那么此时IPVS会强行修改数据包里的目标IP地址及端口，并将新的数据包发往POSTROUTING链。 POSTROUTING链接收数据包后发现目标IP地址刚好是自己的后端服务器，那么此时通过选路，将数据包最终发送给后端的服务器。 LVS各种模式原理lvs集群的类型： LVS/NAT： NAT模式。修改请求报文的目标IP,多目标IP的DNAT。 LVS/DR：DirectRouting（直接路由）。操纵封装新的MAC地址。 LVS/TUN：Tunneling（隧道）。在原请求IP报文之外新加一个IP首部。 LVS/FULLNAT：Full NAT。修改请求报文的源和目标IP。 1. LVS/NATLVS/NAT 最基本的LVS策略 如图: 客户端(Client) -&gt; LB（VS） -&gt; replica1(RS1),replica2(RS2),replica3(RS3) 详细的数据包的流转图： (a). 当用户请求到达Director Server，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP(b). PREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链(c). IPVS比对数据包请求的服务是否为集群服务，若是，修改数据包的目标IP地址为后端服务器IP，然后将数据包发至POSTROUTING链。 此时报文的源IP为CIP，目标IP为RIP(d). POSTROUTING链通过选路，将数据包发送给Real Server(e). Real Server比对发现目标为自己的IP，开始构建响应报文发回给Director Server。 此时报文的源IP为RIP，目标IP为CIP(f). Director Server在响应客户端前，此时会将源IP地址修改为自己的VIP地址，然后响应给客户端。 此时报文的源IP为VIP，目标IP为CIP 2. LVS/DR很多时候，相应流是比请求流大的，如下图： 所以有第二种方案，响应流不走LB，这就是LVS/DR模式： 详细的数据包的流转图： (a) 当用户请求到达Director Server，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP(b) PREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链(c) IPVS比对数据包请求的服务是否为集群服务，若是，将请求报文中的源MAC地址修改为DIP的MAC地址，将目标MAC地址修改RIP的MAC地址，然后将数据包发至POSTROUTING链。 此时的源IP和目的IP均未修改，仅修改了源MAC地址为DIP的MAC地址，目标MAC地址为RIP的MAC地址(d) 由于DS和RS在同一个网络中，所以是通过二层来传输。POSTROUTING链检查目标MAC地址为RIP的MAC地址，那么此时数据包将会发至Real Server。(e) RS发现请求报文的MAC地址是自己的MAC地址，就接收此报文。处理完成之后，将响应报文通过lo接口传送给eth0网卡然后向外发出。 此时的源IP地址为VIP，目标IP为CIP(f) 响应报文最终送达至客户端 LVS-DR模型的特性 特点1：保证前端路由将目标地址为VIP报文统统发给Director Server，而不是RS。 RS可以使用私有地址；也可以是公网地址，如果使用公网地址，此时可以通过互联网对RIP进行直接访问。 RS跟Director Server必须在同一个物理网络中。 所有的请求报文经由Director Server，但响应报文必须不能进过Director Server。 不支持地址转换，也不支持端口映射。 RS可以是大多数常见的操作系统。 RS的网关绝不允许指向DIP(因为我们不允许他经过director)。 RS上的lo接口配置VIP的IP地址。 缺陷：RS和DS必须在同一机房中。 特点1的解决方案： 在前端路由器做静态地址路由绑定，将对于VIP的地址仅路由到Director Server。 存在问题：用户未必有路由操作权限，因为有可能是运营商提供的，所以这个方法未必实用。 arptables：在arp的层次上实现在ARP解析时做防火墙规则，过滤RS响应ARP请求。这是由iptables提供的 修改RS上内核参数（arp_ignore和arp_announce）将RS上的VIP配置在lo接口的别名上，并限制其不能响应对VIP地址解析请求。 3. LVS/TUN在原有的IP报文外再次封装多一层IP首部，内部IP首部(源地址为CIP，目标IIP为VIP)，外层IP首部(源地址为DIP，目标IP为RIP) (a) 当用户请求到达Director Server，此时请求的数据报文会先到内核空间的PREROUTING链。 此时报文的源IP为CIP，目标IP为VIP 。(b) PREROUTING检查发现数据包的目标IP是本机，将数据包送至INPUT链。(c) IPVS比对数据包请求的服务是否为集群服务，若是，在请求报文的首部再次封装一层IP报文，封装源IP为为DIP，目标IP为RIP。然后发至POSTROUTING链。 此时源IP为DIP，目标IP为RIP 。(d) POSTROUTING链根据最新封装的IP报文，将数据包发至RS（因为在外层封装多了一层IP首部，所以可以理解为此时通过隧道传输）。 此时源IP为DIP，目标IP为RIP。(e) RS接收到报文后发现是自己的IP地址，就将报文接收下来，拆除掉最外层的IP后，会发现里面还有一层IP首部，而且目标是自己的lo接口VIP，那么此时RS开始处理此请求，处理完成之后，通过lo接口送给eth0网卡，然后向外传递。 此时的源IP地址为VIP，目标IP为CIP。(f) 响应报文最终送达至客户端。 LVS-TUN模型特性 RIP、VIP、DIP全是公网地址。 RS的网关不会也不可能指向DIP。 所有的请求报文经由Director Server，但响应报文必须不能进过Director Server。 不支持端口映射。 RS的系统必须支持隧道。 4. LVS/FULLNATlvs-fullnat：通过同时修改请求报文的源IP地址和目标IP地址进行转发: CIP –&gt; DIP VIP –&gt; RIP (1) VIP是公网地址，RIP和DIP是私网地址，且通常不在同一IP网络；因此，RIP的网关一般不会指向DIP。 (2) RS收到的请求报文源地址是DIP，因此，只需响应给DIP；但Director还要将其发往Client。 (3) 请求和响应报文都经由Director。 (4) 支持端口映射。 注意：此类型kernel默认不支持。 LVS工作模式总结 VS/NAT VS/TUN VS/DR server any tunneling non-arp device server network private LAN/WAN LAN server number low (10~20) high high server gateway load balancer own router own router LVS/NAT, LVS/FULLNAT：请求和响应报文都经由VS LVS/NAT：RIP的网关要指向DIP LVS/FULLNAT：RIP和DIP未必在同一IP网络，但要能通信 LVS/DR, LVS/TUN：请求报文要经由VS，但响应报文由RS直接发往Client LVS/DR：通过封装新的MAC首部实现，通过MAC网络转发 LVS/TUN：通过在原IP报文之外封装新的IP报文实现转发，支持远距离通信 其实企业中最常用的是 DR 实现方式，而 NAT 配置上比较简单和方便，后边实践中会总结 DR 和 NAT 具体使用配置过程。 LVS调度算法根据其调度时是否考虑各RS当前的负载状态，分为静态方法和动态方法。 静态方法：仅根据算法本身进行调度 RR：roundrobin，轮询。 WRR：Weighted RR，加权轮询。 SH：Source Hashing，实现session sticky，源IP地址hash；将来自于同一个IP地址的请求始终发往第一次挑中的RS，从而实现会话绑定。 DH：Destination Hashing；目标地址哈希，将发往同一个目标地址的请求始终转发至第一次挑中的RS，典型使用场景是正向代理缓存场景中的负载均衡，如：宽带运营商。 动态方法：主要根据每RS当前的负载状态及调度算法进行调度，Overhead=value较小的RS将被调度。 LC：least connections，最少连接，适用于长连接应用。Overhead=activeconns*256+inactiveconns WLC：Weighted LC，加权最少连接，默认调度方法。Overhead=(activeconns*256+inactiveconns)/weight SED：Shortest Expection Delay,初始连接高权重优先。Overhead=(activeconns+1)*256/weight NQ：Never Queue，第一轮均匀分配，后续SED。 LBLC：Locality-Based LC，动态的DH算法，使用场景： 根据负载状态实现正向代理。 LBLCR：LBLC with Replication，带复制功能的LBLC，解决LBLC负载不均衡问题，从负载重的复制到负载轻的RS。 部分内容转自：使用LVS实现负载均衡原理及安装配置详解图片部分来自：章文嵩博士和他背后的负载均衡帝国]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>LVS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[防火墙Netfilter/iptables]]></title>
    <url>%2Flinux%2F20170815-iptables%2F</url>
    <content type="text"><![CDATA[Netfilter/iptables原理 Netfilter组件： 内核空间，集成在linux内核中 扩展各种网络服务的结构化底层框架 内核中选取五个位置放了五个hook function（勾子函数）(INPUT、OUTPUT、FORWARD、PREROUTING、POSTROUTING)， 而这五个hook function向用户开放，用户可以通过一个命令工 具（iptables）向其写入规则。 由信息过滤表（table）组成，包含控制IP包处理的规则集（ rules），规则被分组放在链（chain）上 三种报文流向： 流入本机：PREROUTING –&gt; INPUT–&gt;用户空间进程 流出本机：用户空间进程 –&gt;OUTPUT–&gt; POSTROUTING 转发：PREROUTING –&gt; FORWARD –&gt; POSTROUTING iptables由四个表和五个链以及一些规则组成: 四个表(table)： filter:过滤规则表，根据预定义的规则过滤符合条件的数据包 nat:network address translation 地址转换规则表 mangle:修改数据标记位规则表 Raw:关闭NAT表上启用的连接跟踪机制，加快封包穿越防火墙速度 优先级由高到低的顺序为:raw–&gt;mangle–&gt;nat–&gt;filter 五个内置链(chain)： INPUT OUTPUT FORWARD PREROUTING POSTROUTING Netfilter/iptables Packet Flow： 内核中数据包的传输过程 当一个数据包进入网卡时，数据包首先进入PREROUTING链，内核根据数据包目的IP判断是否需要转送出去 如果数据包就是进入本机的，数据包就会沿着图向下移动，到达INPUT链。数据包到达INPUT链后，任何进程都会收到它。本机上运行的程序可以发送数据包，这些数据包经过OUTPUT链 ，然后到达POSTROTING链输出 如果数据包是要转发出去的，且内核允许转发，数据包就会向右移动，经过FORWARD链，然后到达POSTROUTING链输出 防火墙工具 iptables firewalld工具集 firewall-cmd 命令行工具 firewall-config 图形工具 我们主要以实验的方式介绍iptables的使用,firewalld的工具集自行了解下。 上图加粗部分是重点实验内容，包括： filter表中的INPUT和OUTPUT filter表中的FORWARD nat表中的PREROUTING和POSTROUTING 实验前要关闭防火墙自带的服务，来自行配置防火墙，而不是系统定义的防火墙策略。 CentOS6: 12service iptables stopchkconfig iptables off CentOS7: 12systemctl stop firewalld.service systemctl disable firewalld. service iptables命令语法格式 -t table指的是chain表，默认不写为filter表，还可以指定nat、mangle、raw等表。 -COMMANDS1. 查看防火墙规则参数常用选项：-vnL -v：verbose,详细信息。如果要更详细信息，-vv。 -n：numberic，以数字格式显示地址和端口号 -L：List列出指定链上所有规则，不指定链的话，只输出filter表上的链。此参数要写在最后。 -x：exactly，显示计数器结果的精确值,而非单位转换后的易读值。 --line-numbers：显示规则的序号。 常用组合： 123-vnL-vnL --line-numbers-vvnxL --line-numbers 2. 规则管理的参数 -A chain：Append，添加某chain的规则。 -C chain：Check，检查某chain的规则。 -D chain rulenum：Delete，删除某chian指定序号位置的规则。 -I chain [rulenum]：Insert，在某chain的指定序号位置插入规则，不写默认插在开头（第一个）。 -R chain rulenum：Replace，替换指定链上的指定规则编号。 -F [chain]：Flush，清空指定的链的所有规则，如果不指定链，默认清空所有链的所有规则。 -Z：Zero，计数器置零。iptables的每条规则都有两个计数器： 匹配到的报文的个数。 匹配到的所有报文的大小之和。 这里的chain指的是以下几种链：PREROUTING，INPUT，FORWARD，OUTPUT， POSTROUTING 3. 链管理的参数 -N chain ：New,自定义一条新的规则链。 -X [chain]：delete，删除自定义的空的规则链。如果不指定链，默认删除所有非内置链（所有自定义的链）。 -P [chain] target：Policy，设置指定链的默认策略（target），不写链的话默认指定表中所有的链。对filter表中的链而言，其默认策略（target）有： ACCEPT：接受 DROP：丢弃 REJECT：拒绝 -E old-chain new-chain：重命名自定义链。引用计数不为0的自定义链不能够被重命名，也不能被删除 规则定义（rule-specification）123rule-specification = [matches...] [target]match = -m matchname [per-match-options]target = -j targetname [per-target-options] 先说匹配条件（match），分为两种： 一种是基本匹配条件，无需加载模块，由iptables/netfilter自行提供。CentOS6和7都是通过man iptables可以查看匹配条件使用说明。 扩展匹配条件：需要加载扩展模块（/usr/lib64/xtables/*.so） ，方可生效。CentOS6还是在man iptables里查看模块的匹配条件使用说明，CentOS7可以通过man iptables-extensions来查看。 处理动作(target)，target可以是ACCEPT、DROP、REJECT、RETURN等内置target，也可以是自定义的链。 1. 匹配条件（match）基本匹配条件 [!] -s：source，源IP地址或范围。 [!] -d：destination，目标IP地址或范围。 [!] -p：protocol，指定协议。 [!] -i name：in-interface， 报文流入的接口名字；只能应用于数据报文流入环节，只应用于INPUT、FORWARD、PREROUTING链。 [!] -o name：out-interface，报文流出的接口名字；只能应用于数据报文流出的环节，只应用于FORWARD、OUTPUT、POSTROUTING链。 扩展匹配条件 (1)隐式扩展： 隐式扩展，在使用-p选项指明了特定的协议时，无需再用-m选项 指明扩展模块的扩展机制，不需要手动加载扩展模块。例如-t tcp、-t udp、-t icmp等等。 tcp协议的扩展选项 [!] -sport PORT：匹配报文源端口,例如-sport 22 可为端口范围，例如20:23 [!] -dport PORT：匹配报文目标端口,例如-dport 22可为范围，例如22:23 [!] -tcp-flag mask comp：tcp标志位（flag），在mask列表中必须为1的标志位列表，无指定则必须为0。例子： --tcp-flags SYN,ACK,FIN,RST SYN： 表示要检查的标志位为SYN,ACK,FIN,RST四个，其中SYN必须为1，余下的必须为0。 --tcp-flags SYN,ACK,FIN,RST SYN,ACK --tcp-flags ALL ALL 表示要检查的标志位为所有，所有的都要为1。 --tcp_flags ALL NONE表示要检查的标志位为所有，所有的都要为0。 [!] --syn：用于匹配第一次握手。相当于：--tcp-flags SYN,ACK,FIN,RST SYN Wireshark抓到的TCP的flags： udp协议的扩展选项： [!] -sport：匹配报文源端口,可为端口范围 [!] -dport：匹配报文源端口,可为端口范围 Wireshark抓到的UDP的port： icmp协议的扩展选项： [!] --icmp-type type[/code] type/code为： 8/0：echo-request icmp请求 0/0：echo-reply icmp应答 Wireshark抓到的icmp的request/replay： (2)显式扩展： 必须使用-m选项指明要调用的扩展模块的扩展机制，要手动加载扩展模块： 1[-m matchname [per-match-options]] multiport扩展 iprange扩展 string扩展 time扩展 connlimit扩展 limit扩展 state扩展 2. 处理动作1-j targetname [per-target-options] 简单： ACCEPT DROP 扩展： REJECT：--reject-with:icmp-port-unreachable默认动作选项 RETURN：返回调用链 REDIRECT：端口重定向 LOG：记录日志，dmesg MARK：做防火墙标记 DNAT：目标地址转换 SNAT：源地址转换 MASQUERADE：地址伪装 … 自定义链]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Netfilter</tag>
        <tag>iptables</tag>
        <tag>firewalld</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vagrant--快速搭建实验环境利器]]></title>
    <url>%2Fcloud%2F20170813-vagrant%2F</url>
    <content type="text"><![CDATA[Vagrant是一个虚拟机管理软件，需要结合虚拟机软件来使用。使用Vagrant能迅速创建批量虚拟机环境。对于做实验的同学来说，可以说是利器。 官网地址：www.vagrantup.com 一、下载与安装 Vagrant不能单独使用，需要依赖虚拟机软件如Virtualbox和Vmware，这里推荐Virtualbox，Vmware是需要收费的才能使用（序列号破解的不能用的）。 box镜像是网上别人做好的模板，CentOS官方、Ubuntu官方、Debian官方都有专门去制作做好了的模板（而且一直更新迭代），直接安装就可以用（可以使用命令安装，也可以离线下载安装。） VirtualBox下载地址：https://www.virtualbox.org/wiki/Downloads Vagrant支持四大主流操作系统，可以根据自己的操作系统进行下载安装，安装过程不赘述。 Vagrant下载地址：https://www.vagrantup.com/downloads.html Vagrant Boxes镜像名称和介绍：https://app.vagrantup.com/boxes/search tips: CentOS下需要sudo yum install kernel-devel，要不然vagrant up 虚拟机启动不了；同样的Ubuntu下需要sudo apt install linux-headers-generic。 二、基础命令查看vagrant帮助命令： 1vagrant --help 比较常用的一些命令： 123456789101112131415# 模板（box）相关命令vagrant box list # 列出虚机模板vagrant box add USERNAME/BOX_NAME # 添加别人做好的虚机，在线下载。vagrant box add PATH/TO/BOX # 添加本地离线下载好的boxvagrant box remove # 移除虚机# 虚机（vm）相关命令vagrant init BOX # 初始化一个Vagrantfile文件。BOX为虚机模板名vagrant status [VM_NAME] # 虚拟机状态。不跟参数默认查看所有虚机，指定虚机名字（VM_NAME）查看指定的虚机状态vagrant destroy [VM_NAME] # 删除虚机。不跟参数默认删除所有，指定虚机名字（VM_NAME）删除指定的虚机vagrant up [VM_NAME] # 启动虚机。不跟参数默认启动所有，指定虚机名字（VM_NAME）启动指定的虚机vagrant down [VM_NAME] # 关闭虚机。不跟参数默认关闭所有，指定虚机名字（VM_NAME）关闭指定的虚机vagrant suspend [VM_NAME] # 挂起虚机。不跟参数默认关闭所有，指定虚机名字（VM_NAME）挂起指定的虚机vagrant resume [VM_NAME] # 从挂起状态恢复运行。不跟参数默认恢复所有，指定虚机名字（VM_NAME）恢复指定的虚机vagrant reload [VM_NAME] # 从挂起状态恢复运行。不跟参数默认恢复所有，指定虚机名字（VM_NAME）恢复指定的虚机 下面详细讲解box的命令使用，还有虚机相关命令的使用。 三、下载安装box模板安装box模板有两种途径。一种是在线下载安装；一种是离线下载，然后添加进去。 1. 在线下载安装 vagrant box add username/boxname 添加vagrant云仓库里别人做好的box模板。https://app.vagrantup.com/boxes/search在这里可以搜索到，还可以看到详情信息。 例如CentOS官方的两个： 12vagrant box add centos/7vagrant box add centos/6 vagrant box remove xxx/yyy可以移除你安装的box模板。 在线下载安装的时候，会让你选择下哪种虚拟机软件的box模板，我们选择virtualbox版本的。 用vagrant命令在线下载完成后，会有一行绿色的Sucessfully显示成功： 2. 离线下载安装因为主机在国外，有时候网络环境不是，下载慢。那么就可以复制上上个图那里圈起来的链接。复制链接到浏览器下载。（其实我发现，那个地址有的是地址转换，如下图，我们可以看到是从cloud.centos.org下载的，文件名都变成CentOS-x-Vagrant-xxxx.Virtualbox.box这种长格式。而且地址是centos.org的地址说明这个景象是CentOS官方提供的）： 这里贴出目前最新的，你可以自己去用上述方法找到最新的： CentOS 7 :1708_01CentOS 6 :1708_01 我在百度云也存了一份（不更新，如果你看这篇文章的时候在之后的很长时间的话，还是用上述方法离线自行下载最新版）： 链接: https://pan.baidu.com/s/1i5oqW5n密码: 7grf 离线下载完成后，就可以用命令添加下载好的box模板，例如： 1vagrant box add ~/Downloads/CentOS-6-x86_64-Vagrant-1708_01.VirtualBox.box --name centos/6 –name 参数，是指定本地离线添加的文件导入vagrant程序后的名字。 3. 关于版本列出添加的box模板： 1vagrant box list 在线下载安装的，括号里会跟其他人制作时所定义版本号。本地添加的box，括号里默认版本号是0。 4. 常用的几个模板几个常用的官方在线box模板： centos/7 CentOS 7 x64 centos/6 CentOS 6 x64 ubuntu/xenial64 Ubuntu 16.04 LTS x64 ubuntu/trusty64 Ubuntu 14.04 LTS x64 四、安装和使用单台虚机 我们先以一台为例，介绍常用的几个命令的使用方法。 1. 用指定模板来初始化配置文件Vagrantfile（init）123mkdir test # 创建一个test项目目录cd test # 进入test目录vagrant init centos/7 # 用centos/7这个box模板进行初始化 我们可以看到在项目目录test下面，创建了一个VagrantFile的文件，这个就是虚拟机的配置文件。后续配置复杂多台虚机的时候我们会讲到这个文件的使用。我们先用默认的配置启动虚拟机，默认配置只启动一台。通通过一台的命令，我们来熟悉一些命令的使用。 2. 启动(up)、关闭(halt)、暂停(suspend)、恢复(resume)、重载(reload)虚机1vagrant up 第一次启动会从模板复制一份虚机到当前目录，然后进行一系列配置工作。 vagrant status 可以查看虚拟机状态： 我们可以看到下面有一行提示几个命令的作用： vagrant halt 关闭(halt)虚机 vagrant up 启动(up)虚机 vagrant suspend 暂停(suspend)虚机 还有两条用的也很多： vagrant resume 从暂停中恢复（resume）虚机 vagrant reload 重载(reload)虚拟机，也就是重启虚拟机(在Vmware类的软件里也叫做重置，都是一个意思）。如果修改了配置文件，reload会按照配置文件来更改虚拟机的一些配置。 3. 登录虚机vagrant ssh可以登录虚拟机，是以vagrant用户登录的，我们可以看vagrant init那张图，可以看到有生成秘钥对并上传公钥到虚拟机了，所以vagrant ssh可以无秘钥登录。 4. 销毁(destory)虚机vagrant destory tips: 上述的命令都是基于一台虚拟机的操作，所以相对比较简单，多台是在上述命令基础上加一些参数而已。 五、制作自己习惯用的模板通常我们会自定义一个虚机，配置成自己习惯用的状态，然后打包，打包命令是： 1vagrant package 下面通过一个例子来说明如何打包：（下文中直接用了官方镜像，然后安装了一些自己需要的包，关闭了selinux策略。记得要删除网卡的信息。） 在项目目录下面，登录已经up的一台虚机： 123456789vagrant ssh #sudo su -setenforce 0 # 关闭selinuxsed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config # 永久禁止selinuxyum install -y vim tree wget bash-completion net-tools tcpdump ab expect# 安装一些需要的包，这里根据个人爱好。rm -rf /etc/udev/rules.d/70-persistent-net.rules # 删除网卡的信息，否则在其他地方启动会有问题mv /etc/yum.repos.d/CentOS-Base.repo&#123;,.bak&#125;curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repocurl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 退出虚机，运行下面命令： 12vagrant package # 关机并打包虚机，会在项目目录生成一个package.box模板文件vagrant box add longdream/centos7 package.box # 打包的虚机模板可以作为模板进行创建虚机。 六、Vagrantfile参数详解我们可以看到在项目目录下面有一个Vagrantfile文件，这个就是这个项目的配置文件，通过修改这个配置文件，我们可以来批量创建多个虚机并且有不同的配置，只要写好配置就可以实现。 config.xx.yyy指的是项目全局配置。NODE_NAME.xx.yyy指的是单独的节点的配置，需要比全局配置缩进一层。 下面来详解里面的配置选项，然后下一小节我们就实战来通过这个文件来搭一套测试环境。 1. config.vm.define定义多台主机123456config.vm.define "node1" do |node1|endconfig.vm.define "node2" do |node2|endconfig.vm.define "node3" do |node3|end 2. *.vm.network 虚拟机网络nat默认方式forwarded_port 端口转发private network 私有网络（仅主机模式）public network 公有网络（桥接模式） Vagrantfile自带的几个示例： 1234config.vm.network "forwarded_port", guest: 80, host: 8080config.vm.network "forwarded_port", guest: 80, host: 8080, host_ip: "127.0.0.1"config.vm.network "private_network", ip: "192.168.33.10"config.vm.network "public_network" 默认所有主机都会创建一个NAT网络作为eth0的网络，ip地址是自动分配的。 写了ip就是静态ip地址，没写ip就会dhcp自动分配一个地址。 如果写了多条网络配置，那么就是配置多个网络，服务器会自动多几个网卡。当然，默认的NAT网络的eth0还在（作为vagrant ssh命令登录使用的网络，还有yum更新的网络），自己写的配置文件，从上到下依次匹配eth1,eth2等。 例子： 123456789101112config.vm.define "node1" do |node1| node1.vm.network "private_network", ip: "192.168.33.11" node1.vm.network "public_network" end config.vm.define "node2" do |node2| node2.vm.network "private_network", ip: "192.168.33.12" end config.vm.define "node3" do |node3| node3.vm.network "private_network", ip: "192.168.33.13" end 3. *.vm.hostname 定义虚机主机名NODE_NAME.vm.hostname = HOSTNAME定义主机名，NODE_NAME为节点名，HOSTNAME为定义的主机名。 例子： 123456789101112131415config.vm.define "node1" do |node1| node1.vm.network "private_network", ip: "192.168.33.11" node1.vm.network "public_network" node1.vm.hostname = "node1.yulongjun.com" end config.vm.define "node2" do |node2| node2.vm.network "private_network", ip: "192.168.33.12" node2.vm.hostname = "node2.yulongjun.com" end config.vm.define "node3" do |node3| node3.vm.network "private_network", ip: "192.168.33.13" node3.vm.hostname = "node3.yulongjun.com" end 4. *.vm.synced_folder 宿主机与虚机共享同步目录默认宿主机项目目录和虚拟机下的/vagrant目录同步。 如果还想自定义共享目录，可以参照下面用法： 12config.vm.synced_folder "宿主机目录", "虚机机目录" create: true|false, owner: "用户", group: "用户组" 宿主机目录一般是写的相对路径。相对的路径是相对的项目根目录。例如项目目录为test，那么宿主机目录写&quot;www/&quot;指的就是就是test/www目录。 虚拟目录是虚拟机上目录，一般写绝对路径。 create: true|false指的是是否在虚拟机创建此目录。 owner: &quot;用户&quot;, group: &quot;用户组&quot;指的是指定目录的拥有者和拥有组。 12config.vm.synced_folder "www/", "/var/www/", create: true, owner: "root", group: "root" 七、插件插件安装方法： 1vagrant plugin install xxxx 1. 插件vagrant-hostmanager：用主机名访问（建议安装）可以实现虚机之间用主机名互相访问，也可以实现宿主机用主机名访问虚机。 安装： 1vagrant plugin install vagrant-hostmanager 修改Vagrantfile文件 123456789101112131415161718192021222324Vagrant.configure("2") do |config| config.vm.box = "centos/7" config.hostmanager.enabled = true # 启用hostmanager config.hostmanager.manage_guest = true # 允许更新虚拟机上的文件 config.hostmanager.manage_host = true # 允许更新主机上的文件 config.vm.define "node1" do |node1| node1.vm.network "private_network", ip: "192.168.33.11" node1.vm.network "public_network" node1.vm.hostname = "node1.yulongjun.com" end config.vm.define "node2" do |node2| node2.vm.network "private_network", ip: "192.168.33.12" node2.vm.hostname = "node2.yulongjun.com" end config.vm.define "node3" do |node3| node3.vm.network "private_network", ip: "192.168.33.13" node3.vm.hostname = "node3.yulongjun.com" endend 2. 插件vagrant-vbguest：安装VirtualBox 客户端（建议安装）有时候我们发现有些virtualbox无法使用自定义的共享目录，这时候就需要安装vbguest客户端（类似于VMware的client） 1vagrant plugin install vagrant-vbguest 这个是在虚拟机启动的时候实时添加的。其实如果不去添加，下次虚拟机启动的时候会默认去添加的。 每次启动虚机都会检查vbguest插件的更新，如果不想更新，修改Vgrantfilew文件，加上这样一条： 1config.vbguest.auto_update = false 3. 插件vagrant-bindfs 支持多种共享模式（可选）插件bindfs可以支持多种共享模式，如nfs，samba 命令行下输入： 1vagrant plugin install vagrant-bindfs 修改Vgrantfile文件： 123456config.vm.define "node1" do |node1| node1.vm.network "private_network", ip: "192.168.33.11" node1.vm.hostname="node1.yulongjun.com" node1.vm.synced_folder "./app" "/mnt/app-data", type: "nfs" node1.bindfs.bind_folder "/mnt/app-data" "/app" force_user: "root", force_group: "root", o: "noempty" 八、实验例子1. 搭建一个每台机器不同配置的实验环境。搭建一个LVS实验环境。（用自己做的镜像最好，首先是已经安装了一些常用的包；二是如果是初始镜像，一开始需要安装几个包（glibc、kernel -de），比较耽误时间，自己打包的镜像已经安装好了。） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure("2") do |config| # Vagrant Global Config # `longdream/centos7` is a custom centos7 box made by YuLongjun. config.vm.box = "longdream/centos7" # If this box is add online, set true will check update. # Also set `false` will not update it. # If this box is added locally, this setting is invalid. config.vm.box_check_update = false # you need `vagrant plugin install vagrant-vbguest` # You also need `vagrant plugin install vagrant-hostmanager` config.hostmanager.enabled = true # Allow update `/etc/hosts` file in VMs. config.hostmanager.manage_guest = true # Allow update `/etc/hosts` file in Hosts. config.hostmanager.manage_host = true # Define VM `client` config.vm.define "client" do |client| client.vm.network "private_network", ip: "172.16.111.123" client.vm.hostname = "client" end # Define VM `vs` config.vm.define "vs" do |vs| vs.vm.network "private_network", ip: "172.16.111.200" vs.vm.network "private_network", ip: "192.168.111.200" vs.vm.hostname = "vs" end # Define VM `rs1` config.vm.define "rs1" do |rs1| rs1.vm.network "private_network", ip: "192.168.111.101" rs1.vm.hostname = "rs1" rs1.vm.provision "shell", inline: " sudo echo 'GATEWAY=192.168.111.200' &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth1" end # Define VM `rs2` config.vm.define "rs2" do |rs2| rs2.vm.network "private_network", ip: "192.168.111.102" rs2.vm.hostname = "rs2" rs2.vm.provision "shell", inline: "sudo echo 'GATEWAY=192.168.111.200' &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth1" endend 2. 批量搭建一组服务器（20170829补充）1234567891011121314151617181920212223242526272829303132333435363738# -*- mode: ruby -*-# vi: set ft=ruby :Vagrant.configure("2") do |config| # Vagrant Global Config # `longdream/centos7` is a custom centos7 box made by YuLongjun. config.vm.box = "longdream/centos7" # If this box is add online, set true will check update. # Also set `false` will not update it. # If this box is added locally, this setting is invalid. config.vm.box_check_update = false # you need `vagrant plugin install vagrant-vbguest` # You also need `vagrant plugin install vagrant-hostmanager` config.hostmanager.enabled = true # Allow update `/etc/hosts` file in VMs. config.hostmanager.manage_guest = true # Allow update `/etc/hosts` file in Hosts. config.hostmanager.manage_host = true # define 10 VMs in batch. # you can up a VM:`vagrant up node1` # you can up all VMs:`vagrant up` # you can up some VMs:`vagrant up `node[1-4]` (1..9).each do |i| config.vm.define "node#&#123;i&#125;" do |node| node.vm.network "private_network", ip: "192.168.111.#&#123;i&#125;0" node.vm.hostname = "node#&#123;i&#125;" end end # Create a single VM. #config.vm.define "client" do |client| # client.vm.network "private_network", ip "192.168.0.123" # client.vm.hostname "client" # client.vm.provision "shell", inline: &lt;&lt;-SHELL # sudo yum install tcpdump ab # SHELLend]]></content>
      <categories>
        <category>cloud</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网站启用新logo和新头像]]></title>
    <url>%2Fessay%2F20170811-new-logo%2F</url>
    <content type="text"><![CDATA[新logo新气象，但是博客的笔记内容已经两周没更新了 -_—！ 红牛（强化型）已到货，鱼油在路上，看来得补充点能量顺便补下脑子了…… 懒筋谁给我抽一抽 ┐(ﾟ～ﾟ)┌ …… favicon favicon]]></content>
      <categories>
        <category>essay</category>
      </categories>
      <tags>
        <tag>logo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LAMP 一键安装脚本]]></title>
    <url>%2Flinux%2F20170804-lamp%2F</url>
    <content type="text"><![CDATA[项目地址： https://github.com/yulongjun/lamp LAMP各组件版本： L：CentOS 7.3 A：httpd 2.4.27 M：MariaDB 10.2.7 P：PHP 7.1.7 git方式安装(recommended)：123git clone https://github.com/yulongjun/lamp.gitcd lampbash install.sh curl方式安装：1curl -L http://ou5hkxl8l.bkt.clouddn.com/lamp-web-installer?attname= |bash 用的七牛云存储，域名备案中，暂时下载速度不快，只有100KB左右。（MariaDB除外，用的清华的yum源）]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>LAMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使MWeb外部模式支持Hexo]]></title>
    <url>%2Fmacos%2F20170728-mweb-hexo%2F</url>
    <content type="text"><![CDATA[Command + E 打开外部模式。 右下角引入外部文件夹，然后选择自己Hexo博客目录下的source文件夹。 在source文件夹上点右键，选择“编辑”，然后如下图修改。 这时候MWeb就支持hexo的格式了。贴图直接就帮你转化了格式为网络模式，而且支持预览： 详细的关于MWeb外部模式帮助文档可以去官网查看： http://zh.mweb.im/mweb-1.4-add-floder-octpress-support.html]]></content>
      <categories>
        <category>macos</category>
      </categories>
      <tags>
        <tag>MWeb</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-INSERT、DELETE、UPDATE语句]]></title>
    <url>%2Fdatabase%2F20170727-05-sql-insert-delete-update%2F</url>
    <content type="text"><![CDATA[INSERT1INSERT INTO &lt;表名&gt; (列名1, 列名2, ...) Values (value1, value2, ...); 省略列名不写，就得写每一列的数据： 12INSERT INTO &lt;表名&gt;Values (value1, value2, ...); 非空约束的列，不能插入Null值，否则报错。 有默认值约束的列，在列的列表里可以不写，这样就会插入默认值。如果要列表里有列名，想设置默认值需要写DEFAULT。 可以从其他表中取数据： 1234INSERT INTO &lt;表名1&gt; (列名1, 列名2, ...) SELECT 列名a, 列名b, ... from &lt;表名2&gt; ...; -- 子select可以接WGHO DELETE清空表： 1DELETE FROM &lt; 表名 &gt;; 加条件： 1DELETE FROM &lt; 表名 &gt; WHERE &lt; 条件 &gt;; 截断表，速度快，DELETE相当于一条条删除，截断一下子就截断了： 1TRUNCATE &lt; 表名 &gt;; UPDATEUPDATE 语句的基本语法 12UPDATE &lt; 表名 &gt; SET &lt; 列名 &gt; = &lt; 表达式 &gt;; 指定条件的 UPDATE 语句（搜索型 UPDATE ） 123UPDATE &lt; 表名 &gt;SET &lt; 列名 &gt; = &lt; 表达式 &gt; WHERE &lt; 条件 &gt;; 使用NULL进行更新 123UPDATE ProductSET regist_date = NULL WHERE product_id = '0008'; 多列更新 方法①：代码清单4-19 将代码清单4-18的处理合并为一条 UPDATE 语句 1234-- 使用逗号对列进行分隔排列 UPDATE ProductSET sale_price = sale_price * 10, purchase_price = purchase_price / 2 WHERE product_type = ' 厨房用具 '; 方法②：代码清单4-20 将代码清单4-18的处理合并为一条 UPDATE 语句 方法②只可以在PostgreSQL和DB2中使用，MySQL和Oracle不支持。 1234-- 将列用 () 括起来的清单形式 UPDATE ProductSET (sale_price, purchase_price) = (sale_price * 10, purchase_price / 2) WHERE product_type = ' 厨房用具 ';]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>SELECT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-SELECT语句]]></title>
    <url>%2Fdatabase%2F20170727-04-sql-select%2F</url>
    <content type="text"><![CDATA[可以看《SQL基础教程（第2版）》第二章，这本书是2017年5月出的最新版。 第二章内容： （下面内容请忽略，笔者已会SQL语句，下面内容是查漏补缺写的笔记） 聚合函数(Group Function) count() sum() max() min() GROUP BY使用 GROUP BY 子句可以像切蛋糕那样将表分割。通过使用聚合函数和 GROUP BY 子句，可以根据“商品种类”或者“登记日期”等将表分割后再 进行汇总。 聚合键中包含 NULL 时，在结果中会以“不确定”行（空行）的形式表现出来。 使用聚合函数和 GROUP BY 子句时需要注意以下4点。 ① 只能写在 SELECT 子句之中 ② GROUP BY 子句中不能使用 SELECT 子句中列的别名 ③ GROUP BY 子句的聚合结果是无序的 ④ WHERE 子句中不能使用聚合函数 GROUP BY 和 WHERE 并用时 SELECT 语句的执行顺：FROM → WHERE → GROUP BY → SELECT 所以这样使用会报错： 123SELECT product_type AS pt, COUNT(*) FROM Product GROUP BY pt; 在WHERE子句里不能使用聚合函数： 1234SELECT product_type, COUNT(*)FROM Product WHERE COUNT(*) = 2GROUP BY product_type; 上面这个会报错： 12ERROR 1111 (HY000): Invalid use of group function 想要在子句里使用聚合函数，可以用having子句。 GROUP BY和DISTINCT HAVING说到指定条件，估计大家都会首先想到 WHERE 子句。但是， WHERE 子句只能指定记录（行）的条件，而不能用来指定组的条件（例如，“聚合结果正好为2行的组”或者“平均值为500的组”等）。 HAVING 子句必须写在 GROUP BY 子句之后，其在 DBMS 内部的执行顺序也排在 GROUP BY 子句之后。 使用 HAVING 子句时 SELECT 语句的顺序： SELECT → FROM → WHERE → GROUP BY → HAVING sale_pricegroup by 后，count(*)大于2的显示出来： 1234SELECT product_type, COUNT(*) FROM Product GROUP BY product_type HAVING COUNT(*) = 2; sale_price group by 后，平均值大于2的显示出来： 1234SELECT product_type, AVG(sale_price) FROM Product GROUP BY product_type HAVING AVG(sale_price) &gt;= 2500; ORDER BY 子句的书写顺序：1. SELECT 子句 → 2. FROM 子句 → 3. WHERE 子句 → 4. GROUP BY 子句 → 5. HAVING 子句 → 6. ORDER BY 子句 默认是升序ASC（Ascendent），可以加DESC(Descendent)就变成降序。 可以指定多个排序键，那么就先排序第一个排序键，然后再第一个排序键有相同的值的情况下，第二个再排序。 NULL在ASC排序里是排在最后，DESC排序里排在最前面。 在 ORDER BY 子句中可以使用 SELECT 子句中未使用的列和聚合函数。 总的顺序：WGHO原则： 1234WHEREGROUP BYHAVINGORDER BY]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>SELECT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-SQL语句简单示例]]></title>
    <url>%2Fdatabase%2F20170727-03-sql-language%2F</url>
    <content type="text"><![CDATA[简单的几个示例演示一下。 创建1. 数据库的创建（ CREATE DATABASE 语句）1CREATE DATABASE &lt;数据库名称&gt; ; 2. 表的创建（CREATE TABLE 语句）12345678CREATE TABLE &lt;表名&gt;(&lt;列名1&gt; &lt;数据类型&gt; &lt;该列所需约束&gt; ，&lt;列名2&gt; &lt;数据类型&gt; &lt;该列所需约束&gt; ， &lt;列名3&gt; &lt;数据类型&gt; &lt;该列所需约束&gt; ，&lt; 列名4&gt; &lt;数据类型&gt; &lt;该列所需约束&gt; ，...&lt;该表的约束1&gt; ， &lt;该表的约束2&gt; ，……）； 例子： 12345678CREATE TABLE Product(product_id CHAR(4) NOT NULL, -- 商品编号 product_name VARCHAR(100) NOT NULL, -- 商品名称 product_type VARCHAR(32) NOT NULL, -- 商品种类 sale_price INTEGER , -- 销售单价 purchase_price INTEGER , -- 进货单价 regist_date DATE , -- 登记日期 PRIMARY KEY (product_id)); 下面对上面内容进行分解： 3. 命名规则标准SQL命名规则： 标准SQL，只能使用半角英文字母、数字、下划线（_）作为数据库、表和列的名称。 标准SQL名称必须以半角英文字母开头。 名称不能重复。在同一个数据库中不能创建两个相同名称的表，在同一个表中也不能创建两个名称相同的列。 4. 列的数据类型数据类型表示数据的种类，包括数字型、字符型和日期型等。 上文中的几种数据类型： INTEGER：整数型 CHAR：定长字符型 VARCHAR：可变字符型 DATE：日期型 5. 约束设置 NOT NULL 非空约束：字段不能为空。 UNIQUE 唯一性约束：字段全列唯一，可以为空。 PRIMARY KEY 主键约束：NOT NULL和UNIQUE结合起来，即字段不能为空，而且唯一。 表的删除和更新1. 表的删除（ DROP TABLE 语句）1DROP TABLE &lt;表名&gt;; 2. 表定义的更新（ ALTER TABLE 语句）添加列的 ALTER TABLE 语句： 1ALTER TABLE &lt;表名&gt; ADD COLUMN &lt;列的定义&gt;； 删除列的 ALTER TABLE 语句 1ALTER TABLE &lt;表名&gt; DROP COLUMN &lt;列名&gt;； 插入数据 向 Product 表中插入数据的SQL语句 12345678910INSERT INTO Product VALUES (&apos;0001&apos;, &apos;T恤衫&apos;, &apos;衣服&apos;, 1000, 500, &apos;2009-09-20&apos;); INSERT INTO Product VALUES (&apos;0002&apos;, &apos;打孔器&apos;, &apos;办公用品&apos;, 500, 320, &apos;2009-09-11&apos;); INSERT INTO Product VALUES (&apos;0003&apos;, &apos;运动T恤&apos;, &apos;衣服&apos;, 4000, 2800, NULL); INSERT INTO Product VALUES (&apos;0004&apos;, &apos;菜刀&apos;, &apos;厨房用具&apos;, 3000, 2800, &apos;2009-09-20&apos;); INSERT INTO Product VALUES (&apos;0005&apos;, &apos;高压锅&apos;, &apos;厨房用具&apos;, 6800, 5000, &apos;2009-01-15&apos;); INSERT INTO Product VALUES (&apos;0006&apos;, &apos;叉子&apos;, &apos;厨房用具&apos;, 500, NULL, &apos;2009-09-20&apos;); INSERT INTO Product VALUES (&apos;0007&apos;, &apos;擦菜板&apos;, &apos;厨房用具&apos;, 880, 790, &apos;2008-04-28&apos;); INSERT INTO Product VALUES (&apos;0008&apos;, &apos;圆珠笔&apos;, &apos;办公用品&apos;, 100, NULL,&apos;2009-11-11&apos;);COMMIT;]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>CREATE</tag>
        <tag>ALTER</tag>
        <tag>DROP</tag>
        <tag>INSERT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-MariaDB安装]]></title>
    <url>%2Fdatabase%2F20170727-02-mariadb-install%2F</url>
    <content type="text"><![CDATA[CentOS7中，iso镜像里就带有MariaDB，因此通过yum安装mariaDB就可以了，但是版本不是最新的。 如果想要在6或7上安装最新的MariaDB，可以设置yum generator来找到yum repo地址，来安装： yum generator：https://downloads.mariadb.org/mariadb/repositories/ 选择要安装的版本，这里我选的是目前最新的稳定版本10.2。 在/etc/yum.repos.d/下创建MariaDB.repo文件。 CentOS 7写入： 123456# MariaDB 10.2 CentOS repository list - created 2017-07-28 08:01 UTC# http://downloads.mariadb.org/mariadb/repositories/[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.2/centos7-amd64gpgcheck=0 CentOS 6写入： 123456# MariaDB 10.2 CentOS repository list - created 2017-07-28 08:04 UTC# http://downloads.mariadb.org/mariadb/repositories/[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.2/centos6-amd64gpgcheck=0 然后（6和7通用）： 1sudo yum install MariaDB-server MariaDB-client]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>MariaDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-数据库和SQL简介]]></title>
    <url>%2Fdatabase%2F20170727-01-db-and-sql-introduction%2F</url>
    <content type="text"><![CDATA[上图为DB-ENGINES的数据库排行榜。 现在比较流行的数据库分为以下几种： Relational DBMS：如Oracle，MySQL/MariaDB，SQL Server，DB2。Document Store：如MongoDB，Amazon DynamoDB。Key-Value Store：如Redis。Search Engine：如Elasticsearch。 行（记录）、列（字段） 表的列（column）（垂直方向）称为字段(field)，它代表了保存在表中的数据项目。表的行（row）（水平方向）称为记录(record)，它相当于一条数据。关系数据库必须以行为单位进行数据读写。 SQL语句种类：DDLDDL（Data Definition Language，数据定义语言） 用来创建或者删除存储数据用的数据库以及数据库中的表等对象。DDL 包含以下几种指令。 CREATE ：创建数据库和表等对象DROP ： 删除数据库和表等对象ALTER ： 修改数据库和表等对象的结构 DMLDML（Data Manipulation Language，数据操纵语言） 用来查询或者变更 表中的记录。DML 包含以下几种指令。 SELECT ：查询表中的数据（Oracle算在DML里面，MySQL算在DQL（Data Query Language）里面）INSERT ：向表中插入新数据UPDATE ：更新表中的数据DELETE ：删除表中的数据 DCLDCL（Data Control Language，数据控制语言） 用来确认或者取消对数据库中的数据进行的变更。除此之外，还可以对 RDBMS 的用户是否有权限 操作数据库中的对象（数据库表等）进行设定。DCL 包含以下几种指令。 COMMIT ： 确认对数据库中的数据进行的变更ROLLBACK ：取消对数据库中的数据进行的变更GRANT ： 赋予用户操作权限 REVOKE ： 取消用户的操作权限 DDL和DCL是自带commit的，一旦使用，无法rollback。 实际使用的 SQL 语句当中有 90% 属于 DML。]]></content>
      <categories>
        <category>database</category>
      </categories>
      <tags>
        <tag>DB</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-DNS主从复制]]></title>
    <url>%2Flinux%2F20170725-06-dns-slave%2F</url>
    <content type="text"><![CDATA[主DNS正向解析、从DNS正向解析，主DNS反向解析，从DNS反向解析。 正向解析和反向解析是两个完全不同的系统，可以分别来设置。 只有一个主，但是可以有多个从服务器，从服务器可以接从服务器（不一定接主服务器)。 一个从服务器，可以从多个主服务器同步数据。 定义从区域的方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748zone &quot;ZONE_NAME&quot; IN &#123; type slave; masters &#123; MASTER_IP; &#125;; file &quot;slaves/ZONE_NAME.zone&quot;&#125;``` &gt; 前提：无论正向解析反向解析，主服务器的zone文件里，要有从的NS服务器的定义。我们可以查看要被被服务器同步的主服务器的正向解析和反向解析的zone文件，来确认下：![](/images/15020237612906.jpg)![](/images/15086450357684.jpg)NS记录有从服务器的记录才可以配置从服务器。## 实验1:增加从服务器正向解析我们把之前设置的`192.168.111.254`设为主DNS服务器我们在设置一个`192.168.111.253`作为从dns服务器，从主服务器同步dns信息。同步的目录默认为`/var/named/slaves`，系统已经默认在这个目录下面创建一个`slaves`的文件夹(属主属组都是named，`/var/named`目录只有属组是named，而属组没有写权限）### 1. 更改`/etc/named.conf````bashoptions &#123; listen-on port 53 &#123; 192.168.111.253; &#125;; // 监听端口配置为本机的一个网卡的地址 listen-on-v6 port 53 &#123; ::1; &#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; allow-query &#123; 192.168.111.0/24; &#125;; // 只允许192.168.11.0/24网段查询recursion yes; dnssec-enable no; // 关闭安全 dnssec-validation no; // 关闭安全 // 下面保持不变，就不贴了 2. 更改/etc/named.rfc1912.zones/etc/named.rfc1912.zones填加下面内容： 12345zone "yulongjun.com" IN &#123; type slave; masters &#123; 192.168.111.254; &#125;; file "slaves/yulongjun.com.zone";&#125; type 和主不同的是，要改为slave 然后需要写masters是谁，可以有多个master，即使就一个，也要写masters，后面跟大括号 file指向的从主同步下来的文件。写slaves/yulongjun.com.zone，即/var/named/slaves/yulongjun.com.zone(自动加了参数里的默认directory/var/named) 3. 重启服务改完了后 ： 1rndc reload tail -20 /var/log/messages查看日志可以看到成功了 ls /var/named/slaves/可以看到同步过来的zone文件。 4. 主服务器发生更改主服务器发生更改，然后rndc reload，会通知所有的从服务器立即进行更改。 实验2:增加从服务器反向解析1. 更改/etc/named.rfc1912.zones/etc/named.rfc1912.zones填加下面内容： 12345zone "111.168.192.in-addr.arpa" IN &#123; type slave; masters &#123; 192.168.111.254; &#125;; file "slaves/192.168.111.zone";&#125; 2. 重启从服务器的服务1rndc reload]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-bind各种辅助工具]]></title>
    <url>%2Flinux%2F20170725-05-bind-utils%2F</url>
    <content type="text"><![CDATA[named-checkconf检查/etc/named.conf主配置文件语法是否有错误。 有错误会提示那一行有错误： 如果没有错误，不会提示任何信息。 tips: 因为/etc/named.conf里面定义也include/etc/named.rfc1912.zones文件，所以也会检查/etc/named.rfc1912.zones这个文件。所以有时候提示include &quot;/etc/named.rfc1912.zones&quot;;那行有语法错误，看半天没错误，那就应该去/etc/named.rfc1912.zones取查看语法错误。 named-checkzone检查/var/named/下的区域文件，是否有语法错误。 OK表示没有语法错误。 rndc`rndc status|reload|querylog|trace rndc status rndc reload 重载全局配置文件和区域配置文件。相当于systemctl restart named rndc querylog 开启查询日志，即有走此dns的查询，都会记录下来，默认记录在/var/log/messages，频繁查询对io影响比较大，一般不开启，除非测试查看用。 运行一次rndc querylog打开此功能，在运行一次关闭。 rndc trace rndc trace [LEVEL] 开启debug模式，日志比较详尽，一般不开启，再次运行会增加debug等级，也可以直接指定debug级别： 123456789rndc satatus # 初始状态，无debug，返回debug level: 0rndc tracerndc satatus # 返回debug level: 1rndc tracerndc status # 返回debug level: 2rndc trace 5rndc status # 返回debug level: 5rndc trace 0rndc status # 返回初始状态，debug level: 0 digdig用于测试dns系统，因此，不会查询hosts文件进行解析。 1. dig -t正向解析dig [-t type] name [@server] [query options] -t 指定类型，如SOA、A、MX、NS之类的类型 name指的是要解析的域名 @后接dns服务器的地址,表示使用哪个dns服务器进行解析。如果不写，会使用本机设置的dns服务器地址，即/etc/resolv.conf下面设定的地址。 如果解析地址处，定义了同一个地址到两个IP，会去做轮询。 查询选项： +[no]trace：是否追踪解析过程 +[no]recurse：是否递归解析 12345dig -t SOA yulongjun.com @192.168.111.254dig -t MX yulongjun.com @192.168.111.254dig -t NS yulongjun.com @192.168.111.254dig -t A yulongjun.com @192.168.111.254dig -t A www.yulongjun.com @192.168.111.254 如上，我们查询的如果是一个CNAME地址，即使是type写的A也会查出CNAME地址。 1dig -t A www.baidu.com 1dig -t A www.baidu.com +trace 2. dig -x反向解析dig -x IP_ADDRESS @SERVER 例子：1dig -x 192.168.111.100 @192.168.111.254 dig -t axfr全量区域传送可以查询到dns上的全量数据（好多网络上的dns是禁止使用此选项的） 12dig -t axfr yulongjun.com @192.168.111.254dig -t axfr 111.168.192.in-addr.arpa @192.168.111.254 这是相当危险的，可以去掉全量传送。 hosthost [-t type] [Server] 比dig少@，输出的信息也简洁很多。12host -t A www.yulongjun.com 192.168.111.254host -t NS yulongjun.com 192.168.111.254 nslookup主要用nslookup的交互式模式。 了解下就行，一般不使用。 以上命令，dig用的比较多，功能比较全，建议记牢。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-bind（named）配置文件]]></title>
    <url>%2Flinux%2F20170725-04-bind-config%2F</url>
    <content type="text"><![CDATA[CentOS下，yum install bind安装bind软件来实现DNS服务,yum info bind可以查看到描述： BIND是DNS协议的一种实现。BIND包含了一个DNS Server（服务名叫named）,用来解析主机名到ip地址；一个解析库；一些辅助工具，还有一个安全目录工具，分别属于下面几个包： bind：包里主要包含： named DNS服务 named-chkconfig（named.conf文件检查工具） named-checkzone(zone文件检车工具） rndc（本地和远程dns控制工具） bind-libs：named DNS服务的库 bind-utils：包含一系列辅助工具来测试 host dig nslookup nsupdate bind-chroot：切根程序，用来切换默认目录到另外一个深层的安全的目录/var/named/chroot，类似于前面光盘进入救援模式的那种情况。 named 涉及的文件rpm -ql named: 1234567891011/etc/named.conf # bind主配置文件/etc/named.rfc1912.zones # 定义zone的文件/etc/rc.d/init.d/named # bind脚本文件/etc/rndc.conf # rndc配置文件/usr/sbin/named-checkconf # 检测/etc/named.conf文件语法/usr/sbin/named-checkzone # 检测zone和对应zone文件的语法/usr/sbin/rndc # 远程dns管理工具/usr/sbin/rndc-confgen # 生成rndc密钥/var/named/named.ca # 根解析库/var/named/named.localhost # 本地主机解析库/var/named/slaves # 从ns服务器文件夹 可以查看/usr/share/doc/bind-*/sample/下有各种例子可以参考： 根据参考来配置各个文件。 named主配置文件/etc/named.conf这里主要配置named服务的配置，包括： 监听端口(listen-on port)和ip地址 服务作用范围（本机还是指定网段还是全网）（allow-query） 递归还是迭代查询(recursion) 根区域解析文件（zone），其他区域文件可以看到有个include &quot;/etc/named.rfc1912.zones&quot;;，这下面保存了localhost的区域文件，如果新添加的，卸载这个zones文件里，里面指向了zone文件地址。然后每一个zone文件，是在/var/named下面。 下面是原配置文件的部分： 123456789101112131415161718192021222324252627282930313233343536options &#123; listen-on port 53 &#123; 127.0.0.1; &#125;; // ipv4监听端口和ip地址，默认只有本地的 listen-on-v6 port 53 &#123; ::1; &#125;; // ipv6的监听端口和ip地址 directory "/var/named"; dump-file "/var/named/data/cache_dump.db"; statistics-file "/var/named/data/named_stats.txt"; memstatistics-file "/var/named/data/named_mem_stats.txt"; allow-query &#123; localhost; &#125;; recursion yes; // 递归还是迭代查询 dnssec-enable yes; // dns安全扩展,可以改为no关闭 dnssec-validation yes; //可以改为no关闭 /* Path to ISC DLV key */ bindkeys-file "/etc/named.iscdlv.key"; managed-keys-directory "/var/named/dynamic"; pid-file "/run/named/named.pid"; session-keyfile "/run/named/session.key";&#125;;logging &#123; channel default_debug &#123; file "data/named.run"; severity dynamic; &#125;;&#125;;zone "." IN &#123; // 定义zone文件，这里是定义的根域的文件位置 type hint; file "named.ca";&#125;;include "/etc/named.rfc1912.zones"; // 把named.rfc1912.zones文件包含进来include "/etc/named.root.key"; // 把/etc/named.root.key文件包含进来 下面贴出一个修改后的区域文件： 123456789101112131415161718192021222324252627282930313233343536options &#123; listen-on port 53 &#123; 192.168.111.254; 127.0.0.1; &#125;; //监听端口修改为某一网卡地址，和回环地址。 listen-on-v6 port 53 &#123; ::1; &#125;; directory "/var/named"; dump-file "/var/named/data/cache_dump.db"; statistics-file "/var/named/data/named_stats.txt"; memstatistics-file "/var/named/data/named_mem_stats.txt"; allow-query &#123; 192.168.111.0/24; &#125;; //查询范围只允许192.168.111.0网段查询。 recursion yes; dnssec-enable no; //关闭dnssec dnssec-validation no; //关闭dnssec bindkeys-file "/etc/named.iscdlv.key"; managed-keys-directory "/var/named/dynamic"; pid-file "/run/named/named.pid"; session-keyfile "/run/named/session.key";&#125;;logging &#123; channel default_debug &#123; file "data/named.run"; severity dynamic; &#125;;&#125;;zone "." IN &#123; type hint; file "named.ca";&#125;;include "/etc/named.rfc1912.zones";include "/etc/named.root.key"; 配置解析库文件（Zone files）,一般是在/var/named下写，文件名格式一般写为ZONE_NAME.zone named.conf配置文件所有的配置语句 123456789101112acl 定义一个主机匹配列表，用户访问控制权限controls 定义rndc工具与bind服务进程的通信include 把其他文件的内容包含进来key 定义加密秘钥logging 定义系统日志信息lwres 把named配置为轻量级解析器masters 定义主域列表options 设置全局选项server 定义服务器属性trusted-keys 定义信任的dnssec秘钥view 定义视图zone 定义区域 named.rfc1912.zones主配置文件1234zone "ZONE_NAME" IN &#123; type &#123;master|slave|hint|forward&#125;; file "ZONE_NAME.zone";&#125;; zone &quot;ZONE_NAME“：定义解析库名字，通常和解析库文件前缀对应起来。 type：master指的是主dns解析，slave指的是从dns解析，hint指的是根域名解析（根提示域），forward指的是转发，转发不使用file file ：定义区域解析库文件名字（位置默认在/var/named下面）,file的前缀通常和zone的名字通常对应起来，然后加一个.zone的后缀。 这里给出一个自定义的总的区域定义文件，新加一个区域文件的定义： 1234zone "yulongjun.com" IN &#123; type master; file "yulongjun.com.zone";&#125;; /var/named/ZONE_NAME.zone区域配置文件这里给出我一个自定义的区域文件： yulongjun.com.zone 123456789101112131415$TTL 1D@ IN SOA ns1.yulongjun.com. me.yulongjun.com ( 0 1H 10M 1D 3H) IN NS ns1.yulongjun.com IN NS ns2 MX 10 mail1 MX 20 mail2ns1.yulongjun.com IN A 192.168.111.254ns2 IN A 192.168.111.253 db1 A 192.168.111.100db2 A 192.168.111.111web1 A 192.168.111.200web2 A 192.168.111.222mail1 A 192.168.111.10mail2 A 192.168.111.20www CNAME web1 具体配置选项意义，见前一节DNS记录类型。 @指的就是本域yulongjun.com IN ns2.yulongjun.com可以省略写成IN ns2 IN都可以省略不写,比如直接写MX mail 设置.zone文件权限（参照/var/named/name.xxxx的权限来设置，这里xxx为任意字符） 12chmod 640 yulongjun.com.zonechown :named yulongjun.com.zone 配置完毕后，启动或重启(已启动的话）服务。 1service named start|restart 反向区域区域名称：是网络地址的犯些.in-addr.arpa. 192.168.111. –&gt; 111.168.192.in-addr.arpa. 配置方法： 先在/etc/named.rfc1512.zones文件下插入下面内容： 123zone "Reverse_Net_Addr.in-addr.arpa" IN &#123; type &#123;master|slave|forward&#125;; file "Net_Addr.zone" 例子： 1234zone "111.168.192.in-addr.arpa" IN &#123; type master; file "192.168.111.zone";&#125;; 配置/var/named/ZONE_NAME.zone不需要MX、A、AAAA，要有NS记录，以PTR记录为主。 例子： 配置192.168.111.zone： 1234567891011121314151617$TTL 1D@ IN SOA ns1.magedu.com. me.yulongjun.com ( 20170001 1H 5M 7D 1D) IN NS ns1.yulongjun.com. IN NS ns2.yulongjun.com.254 IN PTR ns1.yulongjun.com.253 IN PTR ns2.yulongjun.com.100 IN PTR db1.magedu.com.111 IN PTR db2.magedu.com.200 IN PTR web1.magedu.com.222 IN PTR web2.magedu.com.10 IN PTR mail1.magedu.com.20 IN PTR mail2.magedu.com.]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-DNS记录类型]]></title>
    <url>%2Flinux%2F20170725-03-dns-record-types%2F</url>
    <content type="text"><![CDATA[在之前的文章中，我们了解了什么是DNS以及DNS如何工作，现在让我们来看看 DNS 记录有哪些种类，以及它们有什么作用。 要理解不同的 DNS 记录，首先必须了解区域文件是什么? 区域文件(Zone files)区域文件是名称服务器存储其所知道的域名的信息的方式。名称服务器知道的每个域名都存储在区域文件中。对于名称服务器来说，大多数请求都不能在它自己服务器中找到区域文件。 如果它被配置成可以递归查询，如解析名称服务器，那它会递归找到结果并返回。否则，它会告诉请求者方下一步到哪里查询。 名称服务器具有的区域文件越多，它能够权威回答的请求越多。 区域文件描述 DNS “区域”，其基本上是整个 DNS 命名系统的子集。它通常只配置一个域名。它可以包含多个记录，定义了该域名下的资源位置。 区域文件的 $ORIGIN 表示该区域最高等级的权威。 所以如果一个区域文件被配置为 “example.com” 域，$ORIGIN 会被设置为 example.com。 它配置在区域文件的顶部，或者可以在引用区域文件的 DNS 服务器的配置文件中定义。无论哪种方式，此参数描述区域将是什么等级的权威。 类似地，$TTL 配置它提供的信息的 “生存时间”。它基本上是一个计时器。高速缓存名称服务器可以使用先前查询的结果来回答问题，直到 TTL 值用完。 ZONE file的资源记录（RR:Resource Record)的格式： 1name [TTL] IN rr_type value TTL可以从全局继承，即在文件首行放一个类似这样的$TTL 1D的全局字段。 @可以用于表明当前区域的名字，属于省略的写法，如果name后面没有跟区域名，如ns1的话，默认就会补全区域名。 rr_type,资源记录类型（RR:Resource Record)主要分为以下几种： SOA记录（Start Of Authority record）：起始授权记录 A 和 AAAA 记录（Adress record） CNAME：Canonical Name record MX 记录（Mail eXchange record） NS 记录(Name Server record) PTR 记录（PoinTer Record） 下面详解一下各种记录类型： SOA记录（Start Of Authority record）起始授权记录，或者说是 SOA，这种记录是所有区域性文件中的强制性记录。它必须是一个文件中的第一个记录（$ORIGIN 和 $TTL 会在它之前指定）。它还是最难理解的一种记录。 开始权限记录的看起来像这样： 我们来解释一下各部分分别表示什么： domain.com.：这是区域的根。这表明该区域文件用于 domain.com 域名。通常，你会看到这个用 @ 代替，它只是一个占位符，表示我们之前学到的 $ORIGIN 变量的内容。 IN SOA：”IN” 部分表示互联网（它会出现在许多记录中）, IN 可以省略不写。 SOA 是表示这是开始权限记录。 ns1.domain.com.：这定义了该域的主名称服务器。名称服务器可以是主服务器或从服务器，如果配置了动态 DNS，就像这里，则一个服务器需要是 “主服务器”。如果你未配置动态 DNS，那么这只是你其中一个主名称服务器。 admin.domain.com.：这是这个区域文件管理员的邮箱地址。邮箱地址的 @ 这里用一个 . 代替。如果你的名字中也有 . 它会用 \ 代替。（比如 your.name@domain.com 变成 your\name.domain.com） 12083：这是区域文件的序列号。每次编辑区域文件时，必须增加此序列号以使区域文件能够正确传播。从服务器将检查主服务器的区域序列号是否大于它们在系统上的序列号。如果是，它请求新的区域文件，如果不是，它继续服务原始文件。 3h：这是区域的刷新间隔。这是从服务器向主服务器轮询检查区域文件是否变更之间等待的时间量。 30m：这是此区域的重试间隔。如果slave从机在刷新周期结束时无法连接到master主机，则它将等待此时间并重试轮询主机。 3w：这是到期时间。如果slave从服务器在此时间内无法与master主服务器联系，则它不再作为此区域的权威来源的返回响应，并停止对外提供服务。 1h：这是名称服务器在此文件中找不到所请求的名称时缓存找不到结果的时间量。 A 和 AAAA 记录（Adress record）这两个记录都将主机映射到 IP 地址。 “A” 记录用于将主机映射到 IPv4 IP 地址，而 “AAAA” 记录用于将主机映射到 IPv6 地址。 这些记录的一般格式是： 12host IN A IPv4_addresshost IN AAAA IPv6_address 因为我们的 SOA 记录指出了我们的主服务器是 “ns1.domain.com”，而 “ns1.domain.com” 也是 “domain.com” 区域文件定义的，所以我们需要把它指向一个 IP 地址。 这条记录可能看起来像这样： 1ns1 IN A 111.222.111.222 请注意，我们不必提供全名。我们可以只给主机名，不需要 FQDN，然后 DNS 服务器会通过 $ORIGIN 补足其余部分。但是，我们也可以使用完整的 FQDN ： 1ns1.domain.com. IN A 111.222.111.222 大多数情况下，在这里你可以将你的 web 服务器定义为 “www”： 1www IN A 222.222.222.222 我们还可以说明基本域解析到哪里。如下表示： 1domain.com. IN A 222.222.222.222 我们还可以使用 “@” 来表示基本域名： 1@ IN A 222.222.222.222 我们还可以解析此域下未明确定义的任何内容到此服务器。可以使用 “*” 通配符： 1* IN A 222.222.222.222 以上这些对于 AAAA 记录同样适用。 CNAME 记录（Canonical Name record）CNAME 记录为您的服务器（由A或AAAA记录定义的名称）定义规范名称的别名。 例如，我们可以有一个 A 记录定义 “server1” 主机，然后使用 “www” 作为此主机的别名： server1 IN A 111.111.111.111 www IN CNAME server1 请注意，这些别名会带来一些性能损失，因为它们需要对服务器进行额外的查询。大多数时候，通过使用附加的 A 或 AAAA 记录可以实现相同的结果。 推荐使用 CNAME 的一种情况是为当前区域之外的资源提供别名。 MX 记录（Mail eXchange record）MX 记录用来定义用于域的邮件交换。这有助于电子邮件正确到达您的邮件服务器。 与许多其他记录类型不同的是，邮件记录通常不会将主机映射到某些内容，因为它们适用于整个区域。因此，他们通常看起来像这样： IN MX 10 mail.domain.com. 请注意，开头没有主机名。 还要注意，它还有一个额外的数字。如果定义了多个邮件服务器，这是帮助计算机决定发送邮件的服务器的首选项号。较低的数字具有较高的优先级。 MX 记录通常应指向由 A 或 AAAA 记录定义的主机，而不是由 CNAME 定义的主机。 所以，假设我们有两个邮件服务器。可以这样表示： IN MX 10 mail1.domain.com. IN MX 50 mail2.domain.com. mail1 IN A 111.111.111.111 mail2 IN A 222.222.222.222 在这个例子中，”mail1” 主机是首选电子邮件交换服务器。 我们也可以这样写： IN MX 10 mail1 IN MX 50 mail2 mail1 IN A 111.111.111.111 mail2 IN A 222.222.222.222 NS 记录(Name Server record)此记录类型定义用于此区域的名称服务器。 你可能想知道，”如果区域文件放在名称服务器上，为什么它需要引用自身？”。DNS 如此成功的其中一个原因是它的多级缓存。在区域文件中定义名称服务器的一个原因是区域文件可能是另外一个名称服务器的缓存副本。至于其他原因，我们不在这里讨论。 与 MX 记录一样，它也有一些区域范围的参数，因此它们也不使用主机。一般来说，它们看起来像这样： IN NS ns1.domain.com. IN NS ns2.domain.com. 你应该在每个区域文件中至少定义两个名称服务器，以便在一个服务器出现问题时还能正确运行。如果只有一个名称服务器，大多数 DNS 服务器软件都会认为区域文件无效。 同样，把主机的 A 或者 AAAA 映射也包含记录中： IN NS ns1.domain.com. IN NS ns2.domain.com. ns1 IN A 111.222.111.111 ns2 IN A 123.211.111.233 你还可以使用很多其他记录类型，但这些可能是你会遇到的最常见的类型。 PTR 记录（PoinTer Record）PTR 记录用于定义与 IP 地址相关联的名称。 PTR 记录是 A 或 AAAA 记录的逆。 PTR 记录是唯一的，因为它们以 .arpa 根开始并被委派给 IP 地址的所有者。区域互联网注册管理机构（RIRs）管理 IP 地址到组织和服务提供商的指派。区域互联网注册管理机构包括 APNIC，ARIN，RIPE NCC，LACNIC 和 AFRINIC。 这里是一个 111.222.333.444 的 PTR 记录的示例： 444.333.222.111.in-addr.arpa. 33692 IN PTR host.example.com. 我们可以看到地址是反的，而且ip后面加了一个特定的后缀.in-addr.arpa.。 IPv6 地址的这个 PTR 记录示例使用了 Google IPv6 DNS 服务器(2001:4860:4860::8888)的逆转形式的半字节格式。 8.8.8.8.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.6.8.4.0.6.8.4.1.0.0.2.ip6.arpa. 86400IN PTR google-public-dns-a.google.com. dig 命令的 -x 参数可用于查找 IP 地址的反向 DNS 名称。 下面是一个 dig 命令的例子。+short 确保只输出反向 DNS 名称。 dig -x 8.8.4.4 +short 上述 dig 命令的输出将是该 IP 地址的 PTR 记录中的域名： google-public-dns-b.google.com. 互联网上的服务器在日志中使用 PTR 记录，来做出明智的垃圾邮件处理决策，并显示​​其他设备上的易于阅读的详细信息。 最常用的电子邮件服务器将查找从其接收电子邮件的 IP 地址的 PTR 记录。如果源 IP 地址没有与其相关联的 PTR 记录，则发送的电子邮件可被视为垃圾邮件并被拒绝。PTR 中的 FQDN 与要发送的电子邮件的域名是否匹配并不重要，重要的是存在有效的 PTR 记录，具有对应的和匹配的前向 A 记录。 通常，互联网上的网络路由器会被赋予与其物理位置相对应的 PTR 记录。例如，你可能会在纽约市或芝加哥看到使用 “NYC” 或 “CHI”。这对于运行 traceroute 或者 MTR 来检查网络流量经过的路径很有用。 大多数提供专用服务器或 VPS 服务的提供商允许客户为其 IP 地址设置 PTR 记录。 注意: PTR 记录中的 FQDN 具有对应的和匹配的正向 A 记录是非常重要的。示例：111.222.333.444 有一条指向 server.example.com 的 PTR 记录，那 server.example.com 需要具有一条指向 111.222.333.444 的 A 记录。 总结了解了不同类型的 DNS 记录以及它们的作用之后，你就可以根据需要选择不同的 DNS 记录。 不同的域名服务商提供了不一样的 DNS 记录的的配置方法，但是你理解了 DNS 记录的作用之后，配置对你来说就不是难事了。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>DNS</tag>
        <tag>DNS Record</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-DNS如何工作]]></title>
    <url>%2Flinux%2F20170725-02-how-dns-work-together%2F</url>
    <content type="text"><![CDATA[上一篇文章(什么是DNS)中，我们解释了 DNS 所涉及到的一些术语，接下来我们来看看 DNS 这个系统是如何工作的？ 从高层次上看，这个系统非常简单，但是当你关注细节时，它又非常复杂。总的来说，它是一个非常可靠的基础设施，对于构建我们当今的互联网，是至关重要的。 根服务器如前所述，DNS 的核心是一个分层系统。在这个系统的顶部是所谓的 “根服务器”。这些服务器由各种组织控制，并由 ICANN（互联网名称和数字地址分配公司）授权。 目前正在使用的根服务器有 13 个。但是，由于每分钟都要解析的名称数量多得令人难以置信，所以实际上每个根服务器都有镜像服务器。有关这个一个有趣的事情是，每个根服务器与它的镜像服务器共享同一个 IP 地址。当你对某个根服务器发出请求时，请求会被路由到该根服务器离你最近的镜像服务器。 这些根服务器做什么的？根服务器处理有关顶级域名信息的请求。因此，如果某个请求低级别名称服务器无法解析，则会向该域的根服务器进行查询。 根服务器不知道实际托管域名的位置。然而，他们会将请求引导到处理特定请求的顶级域名的名称服务器。 因此，如果向根服务器发出对 “www.wikipedia.org” 的请求，则根服务器不能在它的记录文件中找到与 “www.wikipedia.org” 匹配的记录。 但是它会找到 “org” TLD 的记录，并把负责 “org” 地址的名称服务器的地址发回给请求者。 定义根服务器的文件地址：/var/named/named.ca 123456789101112131415161718192021222324252627282930313233343536373839404142;; ANSWER SECTION:. 518400 IN NS a.root-servers.net.. 518400 IN NS b.root-servers.net.. 518400 IN NS c.root-servers.net.. 518400 IN NS d.root-servers.net.. 518400 IN NS e.root-servers.net.. 518400 IN NS f.root-servers.net.. 518400 IN NS g.root-servers.net.. 518400 IN NS h.root-servers.net.. 518400 IN NS i.root-servers.net.. 518400 IN NS j.root-servers.net.. 518400 IN NS k.root-servers.net.. 518400 IN NS l.root-servers.net.. 518400 IN NS m.root-servers.net.;; ADDITIONAL SECTION:a.root-servers.net. 3600000 IN A 198.41.0.4a.root-servers.net. 3600000 IN AAAA 2001:503:ba3e::2:30b.root-servers.net. 3600000 IN A 192.228.79.201b.root-servers.net. 3600000 IN AAAA 2001:500:84::bc.root-servers.net. 3600000 IN A 192.33.4.12c.root-servers.net. 3600000 IN AAAA 2001:500:2::cd.root-servers.net. 3600000 IN A 199.7.91.13d.root-servers.net. 3600000 IN AAAA 2001:500:2d::de.root-servers.net. 3600000 IN A 192.203.230.10e.root-servers.net. 3600000 IN AAAA 2001:500:a8::ef.root-servers.net. 3600000 IN A 192.5.5.241f.root-servers.net. 3600000 IN AAAA 2001:500:2f::fg.root-servers.net. 3600000 IN A 192.112.36.4g.root-servers.net. 3600000 IN AAAA 2001:500:12::d0dh.root-servers.net. 3600000 IN A 198.97.190.53h.root-servers.net. 3600000 IN AAAA 2001:500:1::53i.root-servers.net. 3600000 IN A 192.36.148.17i.root-servers.net. 3600000 IN AAAA 2001:7fe::53j.root-servers.net. 3600000 IN A 192.58.128.30j.root-servers.net. 3600000 IN AAAA 2001:503:c27::2:30k.root-servers.net. 3600000 IN A 193.0.14.129k.root-servers.net. 3600000 IN AAAA 2001:7fd::1l.root-servers.net. 3600000 IN A 199.7.83.42l.root-servers.net. 3600000 IN AAAA 2001:500:9f::42m.root-servers.net. 3600000 IN A 202.12.27.33m.root-servers.net. 3600000 IN AAAA 2001:dc3::35 TLD 服务器请求者然后向负责该请求的顶级域名的 IP 地址（由根服务器给予）发送新请求。 对于我们的例子，它会发送想负责 “org” 域名的名称服务器发送一个请求，看看它是否知道 “www.wikipedia.org” 在哪里。 同样，该名称服务器也不会在记录文件中找到 “www.wikipdia.org” 记录。 但是，它会找到负责 “wikipedia.org” 的名称服务器的 IP 地址。这样就越来越接近我们想要的答案了。 域名级别名称服务器此时，请求者知道了具体负责该资源的实际 IP 地址的名称服务器的 IP 地址。它向该名称服务器发送一个新的请求，再次询问它是否可以解析 “www.wikipedia.org”。 名称服务器检查其区域文件，并发现它有与 “wikipedia.org” 相关联的区域文件。在此文件的内部，有一个 “www” 主机的记录。此记录说明此主机所在的 IP 地址，并向请求者返回最终答案。 请求者是什么？在上面的场景中，我们引用了 “请求者”。在这种情况下请求者指的是什么？ 在几乎所有情况下，请求者都是我们所谓的 “解析名称服务器”。解析名称服务器是配置着为询问其他服务器的问题的。它基本上是用户的中介，它缓存着先前的查询结果来提高速度，并且知道根服务器的地址，以便能够 “解析” 它还不知道的域名。 基本上，用户通常会在其计算机系统上配置多个解析名称服务器。解析名称服务器通常由 ISP 或其他组织提供。例如，Google 提供了你可以使用的 DNS 解析服务器。这些可以在计算机中自动或手动配置。 当你在浏览器的地址栏中键入网址时，你的计算机将首先查看是否可以在本地找到资源所在的位置。它检查计算机上的 “hosts” 文件和其他几个位置。然后它将请求发送到解析名称服务器，并等待接收资源的 IP 地址。 解析名称服务器首先检查其缓存。如果没有，它将通过上述步骤找到答案。 解析名称服务器基本上压缩了最终用户的请求过程。客户端只需要知道请求资源所在的解析名称服务器，并且确信他们会查询并返回最终答案。 总结你现在知道了 DNS 的工作原理。但是要实际操作，你依然需要了解有哪些常见的 DNS记录以及它们的作用。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-什么是DNS]]></title>
    <url>%2Flinux%2F20170725-01-what-is-dns%2F</url>
    <content type="text"><![CDATA[DNS，或者说域名系统，通常是学习如何配置网站和服务器的一个非常困难的部分。了解 DNS 的工作原理将有助于诊断网络访问的问题，也有助于理解 DNS 系统背后的工作原理。 这篇文章中，我们会讨论一些基本的 DNS 概念，这些概念将有助于你配置并使用 DNS。 在我们开始配置你自己的服务器域名解析之前，让我们先来看一些关于这些是如何实现的的基本概念。 我们应该先从术语定义开始。虽然有一些术语在谈论计算领域中其它内容时经常出现，但是有许多术语不常使用。 先从简单的开始： 域名系统（Domain Name System）域名系统（通常被称为“DNS”）是一个网络系统，允许我们把对人类友好的名称解析为唯一的地址。 域名（Domain Name）域名是我们习惯于与互联网资源关联的人性化名称。例如，”google.com” 是一个域名。有些人会说 “google” 部分是域名部分，但我们通常可以将组合形式称为域名。 网址 “google.com” 与 Google Inc. 拥有的服务器相关联。当我们在浏览器中键入 “google.com” 时，域名系统允许我们访问其相关联的 Google 服务器。 IP 地址（IP Address）IP 地址是我们所说的网络可寻址位置。每个 IP 地址在其网络中必须是唯一的。我们这里谈论的网络就是指整个互联网。 IPv4，目前最常见的地址形式，由四组数字组成，每组最多有三位数字，每一组用一个点分隔。例如，111.222.111.222 是有效的 IPv4 IP 地址。使用 DNS，我们可以将名称映射到该地址，这样，你就不必记住一组复杂的数字，来访问你需要的网站。 顶级域名（Top Level Domain）顶级域名，或者说 TLD，是域名的最基本部分。顶级域名是右侧的最远部分（由点分隔）。常见的顶级域名是 com、net、org、gov、edu 和 io。 域名 代表含义 域名 代表含义 .com 表示商业机构 .cn 中国 .net 表示网络服务机构 .hk 中国香港 .org 表示非营利性组织 .tw 中国台湾 .gov 表示政府机构 .us 美国 .edu 表示教育机构 .jp 日本 .mil 表示军事机构 顶级域名在域名术语层次结构的最上层。由 ICANN（互联网名称和号码分配公司）对顶级域名进行管理控制。然后，通过域名注册商来分发 TLD 下面的域名。 主机（Host）域名所有者可以定义多个单独的主机，指向可以通过该域名访问的不同的计算机或者服务。例如，大多数域名所有者会让他们的 web 服务器可以通过裸域（example.com）以及 www 主机（www.example.com）访问。 你可以在一个域名下面定义其它主机。比如说，通过 api 主机(api.example.com) 允许 API 访问，通过 ftp 主机或者 files 主机(ftp.example.com 或者 files.example.com）允许 ftp 访问。主机名可以任意指定，只要它们在该域名下是唯一的。 子域名（Sub Domain Name）一个和主机相关的主题就是子域名。 DNS 有层次结构，TLD 下面可以有多个域名。例如，com 下面有 google.com 和 ubuntu.com。”子域名” 是指作为较高层级域名的一部分。所以说，ubuntu.com 可以说是 com 的子域名，但是通常这被称为域名，或者 “ubuntu” 部分是 SLD(Second Level Domain)，所以这是一个二级域名。 同样，每个域名可以控制它下面的子域名。这通常就是我们所指的子域名。例如，你可以把 “www.history.school.edu” 作为你学校的历史部门的域名。 “history” 部分是一个子域名。 主机名和子域名之间的区别是主机定义计算机或资源，而子域名扩展父域。它是一种把域名本身细分的方法。 无论谈论子域名还是主机，你都可以开始看到域名的最左边部分是最具体的。这也是 DNS 的工作原理：从左到右阅读时，从最具体到最不具体。 完全限定域名（Fully Qualified Domain Name）完全限定的域名，通常称为 FQDN，也就是我们所说的绝对域名。DNS 系统中的域名可以是相对的，所以可能是模糊的。FQDN 是一个绝对名称，表示了它相对于域名系统中绝对根目录的位置。 这表明它表示的每个域名都包括 TLD 部分。正确的 FQDN 以点结束，表示 DNS 层次结构的根。“mail.google.com.” 就是一个标准的 FQDN 的例子。有时候，一些软件使用的 FQDN 不需要末尾的点，但是要符合 ICANN 标准的话一定要加上末尾的点。 名称服务器（Name Server）名称服务器(NS)是一种将域名翻译成 IP 地址的计算机。这些服务器完成了 DNS 系统中的大部分工作。由于域名翻译的数量对于任何一台服务器来说都太多了，因此每台服务器可以将请求转发给其他名称服务器或把它们负责的子域名的子集委派给其他名称服务器。 名称服务器可以是 “权威的”，表示它们自己可以提供所负责的域名的查询结果。否则，它们可能会转发到其他服务器，或者提供其他名称服务器数据的缓存副本。 区域文件（Zone Files）区域文件是一个简单的文本文件，包含域名和 IP 地址之间的映射。这是当用户请求某个域名时，DNS 系统最终找出 IP 关联记录的地方。 区域文件放置在名称服务器中，通常定义了特定域名下可用的资源，或者可以去获取该信息的位置。 记录（Record）在区域文件中，保存着记录。其中最简单的记录形式是，是资源和名称之间的单独映射。它们可以将域名映射到 IP 地址，定义域名的名称服务器，定义域名的邮件服务器等。 总结现在你已经了解了 DNS 所涉及到的一些术语，接下来你可以想了解 DNS如何工作。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cobbler 同步网络yum仓库到本地（Reposync用法)]]></title>
    <url>%2Flinux%2F20170722-cobbler-reposync%2F</url>
    <content type="text"><![CDATA[我们在图形界面进行操作： Arch选择x86_64，Breed选择yum或者wget方式都可以，Keep Updated不要勾选（以后你可以设置计划任务，每天凌晨时间更新）。Mirror和Name根据添加的repo不同而不同，列出几个用到的repo：如需还有需要，可以自行添加： Name Mirror centos7.4-base https://mirrors.tuna.tsinghua.edu.cn/centos/7.4.1708/os/x86_64/ centos7.4-updates https://mirrors.tuna.tsinghua.edu.cn/centos/7.4.1708/updates/x86_64/ centos7.4-extras https://mirrors.tuna.tsinghua.edu.cn/centos/7.4.1708/extras/x86_64/ centos7-epel https://mirrors.tuna.tsinghua.edu.cn/epel/7/x86_64/ centos7-mariadb http://yum.mariadb.org/10.2/centos7-amd64/ centos7-nginx http://nginx.org/packages/centos/7/x86_64/ centos7-zabbix-3.4 https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/3.4/rhel/7/x86_64/ centos7-docker-ce-stable https://mirror.tuna.tsinghua.edu.cn/docker-ce/linux/centos/7/x86_64/stable/ centos7-gitlab-ce https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ centos7-saltstack-latest https://mirror.tuna.tsinghua.edu.cn/saltstack/yum/redhat/7/x86_64/latest/ elastic-stack-5.x https://mirrors.tuna.tsinghua.edu.cn/elasticstack/5.x/yum/ jenkins-stable http://pkg.jenkins.io/redhat-stable/ 可以点图形界面 Reposync，但是没有输出，看不到同步过程，不推荐使用图形界面的Reposync按钮，看不到详情： 可以在CLI下运行： 1cobbler reposync 可以看到结果如下（省略部分内容）： 1234567891011121314151617181920212223242526272829303132333435363738394041[root@localhost ~]# cobbler reposynctask started: 2017-10-03_082140_reposynctask started (id=Reposync, time=Tue Oct 3 08:21:40 2017)hello, reposyncrun, reposync, run!creating: /var/www/cobbler/repo_mirror/centos7-nginx/config.repocreating: /var/www/cobbler/repo_mirror/centos7-nginx/.origin/centos7-nginx.reporunning: /usr/bin/reposync -l -n -d --config=/var/www/cobbler/repo_mirror/centos7-nginx/.origin/centos7-nginx.repo --repoid=centos7-nginx --download_path=/var/www/cobbler/repo_mirror -a x86_64received on stdout: Repository rdo-trunk-pike-tested is listed more than once in the configuration3.6 kB 00:00 ... 3.4 kB 00:00 received on stderr: running: createrepo -c cache -s sha /var/www/cobbler/repo_mirror/centos7-nginxreceived on stdout: Spawning worker 0 with 3 pkgsSpawning worker 1 with 3 pkgsSpawning worker 2 with 2 pkgsSpawning worker 3 with 2 pkgsSpawning worker 4 with 2 pkgsSpawning worker 5 with 2 pkgsWorkers FinishedSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs completereceived on stderr: running: chown -R root:apache /var/www/cobbler/repo_mirror/centos7-nginxreceived on stdout: received on stderr: running: chmod -R 755 /var/www/cobbler/repo_mirror/centos7-nginxreceived on stdout: received on stderr: creating: /var/www/cobbler/repo_mirror/epel-7/config.repocreating: /var/www/cobbler/repo_mirror/epel-7/.origin/epel-7.repo...*** TASK COMPLETE *** 如果想单独只同步某一个仓库，可以使用cobbler reposync --only=REPO_NAME，例如： 1cobbler reposync --only=centos7-mariadb-10.2 我们还可以通过浏览器键入http://172.30.0.222/cobbler/repo_mirror/查看同步下来的仓库： 提示： 由于这篇文章2017年10月3号写的，和上一篇不是一个时间段写的，ip地址已更换，镜像也换成新版的CentOS7.4的地址，不是上一节的地址和系统版本，特此提示。 cat /var/www/cobbler/repo_mirror/xxx/conf.repo，xxx为每一个repo的目录，然后我们可以做一个all.repo文件,例如： 12cd /var/www/cobbler/repo_mirrorcat */config.repo &gt; all.repo 刚出来的all.repo文件是这样的： 12345678910111213[centos7.4-base]name=centos7.4-basebaseurl=http://$&#123;http_server&#125;/cobbler/repo_mirror/centos7.4-baseenabled=1priority=99gpgcheck=0[centos7.4-updates]name=centos7.4-updatesbaseurl=http://$&#123;http_server&#125;/cobbler/repo_mirror/centos7.4-updatesenabled=1priority=99gpgcheck=0# ... 还有其他的仓库 我们可以用vim的替换功能，把${http_server}替换掉： 1:%s#$&#123;http_server&#125;#172.16.0.222#g 或者利用sed命令也可以替换： 1sed -i.bak &apos;s#$&#123;http_server&#125;#base.yulongjun.com#g&apos; all-sed.repo 然后就可以把all.repo copy到各个服务器的/etc/yum.repos.d/下使用。 可以用Ansible或者Salstack推送all.repo配置文件到所有的服务器，这样所有服务器都可以从内网下载安装包了。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Cobbler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cobbler——无人值守安装多种版本多种配置操作系统]]></title>
    <url>%2Flinux%2F20170722-cobbler%2F</url>
    <content type="text"><![CDATA[本文未经允许，不得转载，否则追究到底。 Cobbler官网地址：cobbler.github.io Cobbler是一个Linux安装服务器，它能实现网络安装环境下的快速安装。 Cobbler是基于Python研发的。 我们可能经常遇到这种情况，需要大规模的安装Linux操作系统（几百上千台），不同版本的操作系统，同一版本又有不同配置，用Cobbler就可以搞定。 Cobber基于DHCP、PXE、TFTP、HTTP、Kickstart的技术，来提供统一的对外服务，另外还有Cobbler Web界面，不过Cobbler CLI已经满足大部分人需求，本节Cobbler Web不做研究，下节有个简单的Web实际用法。 另外Cobbler还可以作为一个本地yum仓库，利用reposync 一、配置Cobbler1. 主机信息虚拟机安装CentOS7，配置两个网卡。一个仅主机模式，关闭仅主机模式的dhcp服务（仅主机模式不影响其他网段的机器，做实验比较安全）；一个桥接模式，主要用来连外网下载Cobbler和Cobbler-Web安装包。 仅主机模式配置静态ip为192.168.111.1；桥接模式的ip用dhcp获取即可。 2. 配置阿里云的CentOS7和epel的yum源（比较快）12345cd /etc/yum.repos.d/mkdir backupmv *.repo backupwget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repowget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo 3. 关闭SELinux和iptables1234setenforce 0sed -i.bak 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/configsystemctl stop firewalldsystemctl disable firewalld 4. 安装dhcp、tftp、cobbler、cobbler-web, 启动和启用服务yum安装cobbler的时候，除了dhcp和，其他依赖如tftp-server、httpd、syslinux(里面有pxelinux.0文件）等会自动安装，所以只要装dhcp、cobbler、cobbler-web就好。 123yum install cobbler cobbler-web tftp-server dhcpsystemctl enable httpd cobblerd dhcpd systemctl start httpd cobblerd dhcpd 这里启动dhcpd会报错，是因为配置还没配置，不用管，忽略就行。 5. 更改配置文件配置文件路径：/etc/cobbler/settings 12cp /etc/cobbler/settings&#123;,.bak&#125;vim /etc/cobbler/settings 要更改的内容： (1) Default Encrypted Password 默认为 1default_password_crypted: "$1$bfI7WLZz$PxXetL97LkScqJFxnW7KS1" 但这个密码必须要更改，否则cobbler check会报错。 用openssl passwd -1命令来生成新的sha1sum加密密码，这里设置的密码是cobbler1024，加密后变成$1$mq2YqFBn$um6u0ScNqnDHltdWGQSIj0，把生成的密码覆盖原来的密码： 12345# openssl passwd -1Password: Verifying - Password: $1$mq2YqFBn$um6u0ScNqnDHltdWGQSIj0 (2) Server 和 Next_Server server选项是指cobbler server的ip地址，这里要指定一个网卡的ip，实验环境里指的是仅主机模式的192.168.111.1。 next_server选项被用在DHCP/PXE上，用来作为DHCP Server和TFTP Server的IP地址，一般和Cobbler服务地址使用一个IP。 1234# default, localhostserver: 192.168.111.1# default, localhostnext_server: 192.168.111.1 (3) DHCP Management 和 DHCP Server Template（模板） manage_dhcp，0更改为1 12# default, don't managemanage_dhcp: 1 6. 更改dhcp模板由于上一节更改settings里的manage_dhcp为1了，所以，dhcpd变成cobblerd来管理了。我们通过更改/etc/cobbler/dhcp.template，然后sync同步到/etc/dhcp/dhcpd.conf。 12cp /etc/cobbler/dhcp.template&#123;,.bak&#125;vim /etc/cobbler/dhcp.template 修改模板里的内容： 123456subnet 192.168.111.0 netmask 255.255.255.0 &#123; option subnet-mask 255.255.255.0; range dynamic-bootp 192.168.111.100 192.168.111.200; default-lease-time 21600; max-lease-time 43200; next-server $next_server; 备份原来的dhcpd.config,然后用cobbler sync一下： 123cp /etc/dhcp/dhcpd.conf&#123;,.bak&#125;systemctl restart cobblerdcobbler sync 同步完后，这时候去查看一下/etc/dhcp/dhcpd.conf，内容已经被覆盖了,我们也可以看到注释里写的是被Cobbler管理的dhcpd.conf：Cobbler managed dhcpd.conf file 警告：千万不要修改next-server $next_server;这行，不用改。配置是写在#for dhcp_tag in $dhcp_tags.keys():这行之前，不要写在最后。如果想了解更多关于dhcpd.conf的信息，可以man dhcpd.conf 7. 同步配置，重启相关服务1cobbler sync 8. 下载bootloader的加载程序12cobbler get-loaderscobbler sync 9. 配置cobbler-web配置文件是/etc/cobbler/modules.conf。 修改cobbler web登录时候的的用户名和密码： 1htdigest /etc/cobbler/users.digest "Cobbler" cobbler 123systemctl restart cobblerdcobbler syncsystemctl restart httpd 用修改过用户名密码登录测试： https://192.168.111.1/cobbler_web 二、cobbler使用指南1. 导入光盘(1)挂载光盘给虚拟机配置两个光盘，分别挂载CentOS6和CentOS7的光盘。挂载光盘到目录： 1234mkdir /mnt/centos6mkdir /mnt/centos7mount /dev/sr0 /mnt/centos6mount /dev/sr1 /mnt/centos7 如果是拷贝的iso文件到服务器，可以mount iso到目录： 1234mkdir /mnt/centos6mkdir /mnt/centos7mount CentOS-6.9-x86_64-bin-DVD1.iso /mnt/centos6mount CentOS-7-x86_64-Everything-1611.iso /mnt/centos7 (2)cobbler import导入光盘 1cobbler import --name=CentOS6.9 --path=/mnt/centos6 &amp;&amp; cobbler import --name=CentOS7.3 --path=/mnt/centos7 如图我们可以看到，我们添加了两个发行版本到distros,也创建了两个profile(使用的是sample的ks文件），名字都是CentOSx.x-x86_64，是cobblerd自动侦测了是x86_64的版本，自动添加到上面import命令的name后面。 查看distro/profile对，这两个list目前显示应该是一样的。（后期增加不同配置的ks文件，生成不同的profile，就不一样了，还需要把profile默认的两个例子删掉） 123cobbler distro listcobbler profile list 查看导入的发行版操作系统信息(distro)： 123# 导入的发行版操作系统信息cobbler distro report # 全部cobbler distro report --name=CentOS6.9-x86_64 # 单独一个 我们看到有两行信息： 123Kickstart Metadata : &#123;'tree': 'http://@@http_server@@/cblr/links/CentOS7.3-x86_64'&#125;Kickstart Metadata : &#123;'tree': 'http://@@http_server@@/cblr/links/CentOS6.9-x86_64'&#125; 这两行就是你将来ks文件里写的安装url，后续在ks文件就采用这个地址作为源的地址。 ks文件如果原来是cdrom安装方式，把cdrom换成url --url=http://xxxx： CentOS 6: 1url --url=http://192.168.111.1/cblr/links/CentOS6.9-x86_64/ CentOS 7: 1url --url=http://192.168.111.1/cblr/links/CentOS7.3-x86_64/ 导入ks文件如果要自己制作，详见博文自制kickstart光盘 。 然后记得修改cdrom安装方式为url --url=http://192.168.111.1/cblr/links/CentOSx.x-x86_64/，x.x为你的上面写的系统版本。 本人自制的cobbler的ks文件链接: http://pan.baidu.com/s/1i4BGeUh密码:0kfe 如果有网友拿过来，记得更改url地址为自己cobbler的地址。 cobbler文件夹就是存放cobbler用的ks文件,分别对应6和7的最小化安装和开发包安装（其中手选了一些必要的包），包的话可以根据个人喜好自行更改: 把这四个文件copy到cobbler服务器的/var/lib/cobbler/kickstarts/目录下 这时候我们只是拷贝到目录里了，但是cobbler并没有对系统和ks文件做对应关系，我们用cobbler profile report（和distro命令一样，加--name=xxx可以查看具体单个的属性信息）可以看到kickstart的那一条属性只是一个例子而已： 我们删掉例子，来重新添加新的kickstart文件对应关系： 123456cobbler profile remove --name="CentOS6.9-x86_64"cobbler profile remove --name="CentOS7.3-x86_64"cobbler profile add --name=CentOS6.9-Devel--x86_64 --kickstart=/var/lib/cobbler/kickstarts/ks-centos6-devel.cfg --distro=CentOS6.9-x86_64cobbler profile add --name=CentOS6.9-Mini-x86_64 --kickstart=/var/lib/cobbler/kickstarts/ks-centos6-mini.cfg --distro=CentOS6.9-x86_64cobbler profile add --name=CentOS7.3-Devel-x86_64 --kickstart=/var/lib/cobbler/kickstarts/ks-centos7-devel.cfg --distro=CentOS7.3-x86_64cobbler profile add --name=CentOS7.3-Mini-x86_64 --kickstart=/var/lib/cobbler/kickstarts/ks-centos7-mini.cfg --distro=CentOS7.3-x86_64 这时候我们去查看pxelinux.cfg/default文件就能看到菜单选项也跟着变了： 123[root@center tftpboot]# locate pxelinux.cfg/default/var/lib/tftpboot/pxelinux.cfg/default[root@center tftpboot]# cat /var/lib/tftpboot/pxelinux.cfg/default 重启cobblerd 12systemctl restart cobblerdcobblerd sync Cobbler自动化安装图示新增加一台虚机，和cobbler服务器在一个网段，打开运行，就会出现如下界面： 好了，可以尽情的使用了，如果要添加新的版本，按照上面步骤添加就可以了。 Cobbler Web图形界面也能实现类似的导入功能，可以参见下一节举了一个Cobbler Web的用法： cobbler 添加网络同步仓库（Reposync用法)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Cobbler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TFTP和PXE]]></title>
    <url>%2Flinux%2F20170722-02-tftp-pxe%2F</url>
    <content type="text"><![CDATA[TFTPTFTP（Trivial File Transfer Protocol,简单文件传输协议）是TCP/IP协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务。端口号为69，基于UDP协议。 yum info tftp-server或rpm -qi tftp-server可以查看描述： 看到描述我们可以知道，TFTP通常用来作为无盘工作站的启动。允许用户从远程机器上传输文件。 tftp服务可以用来做pxe的启动拉取pxelinux.0文件和启动菜单文件。 一般存放pxe相关文件的位置为/var/lib/tftpboot/下面。 PXEPXE：Preboot Excution Environment, Intel公司研发，没有任何操作系统的主机，能够基于网络完成系统的安装工作。 PXE工作原理： Client向PXE Server上的DHCP发送IP地址请求消息，DHCP检测Client是否合法（主要是检测Client的网卡MAC 地址），如果合法则返回Client的IP地址，同时将启动文件 pxelinux.0的位置信息一并传送给Client Client向PXE Server上的TFTP发送获取pxelinux.0请求消息，TFTP接收到消息之后再向Client发送pxelinux.0大小信息，试探Client是否满意，当TFTP收到Client发回的同意大小信息之后，正式向Client发送pxelinux.0 Client执行接收到的pxelinux.0文件 Client向TFTP Server发送针对本机的配置信息文件（在 TFTP 服务的pxelinux.cfg目录下），TFTP将配置文件发回Client，继而Client根据配置文件执行后续操作。 Client向TFTP发送Linux内核请求信息，TFTP接收到消息之后将内核文件发送给Client Client向TFTP发送根文件请求信息，TFTP接收到消息之后返回Linux根文件系统 Client启动Linux内核 Client下载安装源文件，读取自动化安装脚本。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>tftp</tag>
        <tag>tftpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DHCP详解]]></title>
    <url>%2Flinux%2F20170722-01-dhcp%2F</url>
    <content type="text"><![CDATA[动态主机配置协议（Dynamic Host Configuration Protocol，DHCP）是一个局域网的网络协议，使用UDP协议工作，主要有两个用途： 用于内部网络或网络服务供应商自动分配IP地址给用户。 用于内部网络管理员作为对所有电脑作中央管理的手段。 DHCP使用UDP协议，服务端的端口为67，客户端的端口为68。 DHCP的服务过程DHCP相关消息： DHCP DISCOVER：客户端到服务器 DHCP OFFER ：服务器到客户端 DHCP REQUES：客户端到服务器 DHCP ACK ：服务器到客户端 DHCP NACK：服务器到客户端指示客户 端的网络地址的概念是不正确的 DHCP DECLINE ：客户端到服务器，指 示地址已被使用 DHCP RELEASE：客户端到服务器，放弃 网络地址和取消剩余的租约时间 DHCP INFORM：客户端到服务器,只要 求本地配置参数，客户端已经具有外部配 置的网络地址 主要用到前四个： DHCP正常的运行分为四个基本过程，分别为： DHCP发现（DISCOVER） DHCP提供（OFFER） DHCP请求（REQUEST） DHCP确认（Acknowledge，ACK） 租期DHCP使用了租约的概念，或称为计算机IP地址的有效期。租用时间是不定的，主要取决于用户在某地连接Internet需要多久。通过较短的租期，DHCP能够在一个计算机比可用IP地址多的环境中动态地重新配置网络。DHCP支持为计算机分配静态地址，可以自行手动指定。 客户在获得了一个IP地址以后，就可以发送一个ARP请求来避免由于DHCP服务器地址池重叠而引发的IP冲突。 租期一般为10分钟，在配置文件里可以更改。 50%：租赁时间达到50%时来续租，刚向DHCP服务器发向新的DHCPREQUEST请求。如果DHCP服务没有拒绝的理 由，则回应DHCPACK信息。当DHCP客户端收到该应答信 息后，就重新开始新的租用周期。 87.5%：如果之前DHCP Server没有回应续租请求，等到 租约期的7/8时，主机会再发送一次广播请求 DHCP实现程序dhcpd服务，主要用到dhcp这个包(也有轻量级的的DNS+DHCP的包，叫dnsmasq。） Client方面，系统默认安装了dhclient。 yum info dhcp我们可以看到dhcp这个包是isc.org这个组织开发的。 这个组织是开发DHCP和BIND（DNS软件）开源项目的组织。 dhcpd主要配置虚机配置仅主机模式，不对外服务，同时在网络编辑器里设置仅主机网络不提供dhcp服务（会跟配置的dhcp服务器冲突）。 安装dhcpd服务 1yum install dhcp 我们需要先行配置dhcpd的配置文件，如果不配置，服务使起不来的。 dhcpd服务的配置文件/etc/dhcp/dhcpd.conf 我们查看dhcpd.conf文件，发现只有这个： 12345## DHCP Server Configuration file.# see /usr/share/doc/dhcp*/dhcpd.conf.example# see dhcpd.conf(5) man page# 让我们去查看示例：/usr/share/doc/dhcp*/dhcpd.conf.example或者man 5 dhcpd.conf。 我们复制一份示例到配置下： 1cp /usr/share/doc/dhcp*/dhcpd.conf.example /etc/dhcp/dhcpd.conf 我们来看下面几个配置选项(更改过的），分别注释下意义： 123456789## 全局网络的选项定义option domain-name "yulongjun.com"; option domain-name-servers 223.5.5.5, 223.6.6.6;option routers 192.168.111.1;default-lease-time 600;max-lease-time 7200;## 局部网络的选项定义subnet 192.168.111.0 netmask 255.255.255.0 &#123; range 192.168.111.100 192.168.111.200; `option domain-name “yulongjun.com”：搜索域，即search domain,网络中获取dhcp的服务器默认被设置的搜索域名。有什么作用呢？在我们ping的时候，ping www即是ping www.yulongjun.com，ping blog即是ping blog.yulongjun.com`。 option routers 192.168.111.1：网关option domain-name-servers 223.5.5.5, 223.6.6.6;：DNS服务器，这里设置的223.5.5.5，223.6.6.6，阿里的dns服务器。 default-lease-time 600;：默认租期时间。单位为秒。 max-lease-time 7200;：最大租期时间。指的是客户端有请求长租期的时候，默认分配的时间。单位为秒。 subnet 写网段的参数，包括range范围之类的，也可以自定义上述全局的那些参数（如domain-name、domain-name-server，router等），就变成这个网段的局部参数，只对这个网段生效。 启动dhcpd服务： 1systemctl start dhcpd 另一台在同一个网段的服务器，如果网络是dhcp获取的，就可以获取range范围内的ip地址。 可以通过查看/var/lib/dhclient/xxx-网卡名.lease看到租期信息。 123456789101112lease &#123; interface "ens192"; fixed-address 192.168.111.100; option subnet-mask 255.255.255.0; option dhcp-lease-time 600; option dhcp-message-type 5; option dhcp-servers 223.5.5.5,223.6.6.6; option dhcp-server-identifier 192.168.111.1; renew 6 2017/07/22 01:46:30; rebind 6 2017/07/22 01:50:46; expire 6 2017/07/22 01:52:01;&#125; renew：续租时间（50%）rebind：7/8时间，再询问的时间（87.5%）expire：过期时间（100%） dhclient -d可以重新发个广播，来请求dhcpd服务。 我们发现还会使用旧的地址，除非你关机了，地址被回收了，那么就会使用新地址。 如果给多个网段提供服务，网关可以写到各个子网段里，而不是写到全局配置里。还有domain-name、domain-name-server，router等参数，也可以写到子网段里。如果全局和局部都有，优先使用局部的，如果没有定义，则使用全局的。如下： 1234567891011121314## 全局网络的选项定义option domain-name "yulongjun.com"; option domain-name-servers 223.5.5.5, 223.6.6.6;option routers 192.168.111.1;default-lease-time 600;max-lease-time 7200;## 局部网络的选项定义subnet 192.168.111.0 netmask 255.255.255.0 &#123; range 192.168.111.100 192.168.111.200;&#125;subnet 192.168.222.0 netmask 255.255.255.0 &#123; range 192.168.222.100 192.168.222.200; option routers 192.168.222.1;&#125; 111网段没有定义局部的routers，所以使用全局的routers ，222网段由于定义了局部的routers，就使用局部的routers dhcpd其他配置filename: 指明引导文件名称next-server: 指明引导文件的服务器ip地址 例如： 12345subnet 192.168.111.0 netmask 255.255.255.0 &#123; range 192.168.111.100 192.168.111.200; filename &quot;pxelinux.0&quot; next-server 192.168.111.1;&#125; 如果我们制定了这两个文件，一个网络引导的有pxe服务的dhcp客户端，就可以通过tftp去拉取next-server上的filename指定的pxelinux.0文件。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>dhcp</tag>
        <tag>dhcpd</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第二阶段考试]]></title>
    <url>%2Flinux%2F20170721-exam%2F</url>
    <content type="text"><![CDATA[第一题 (1)、 简述raid0、raid1、raid5三种工作模式的工作原理及特点(2)描述linux系统下创建软RAID5的命令和步骤 (1) RAID 0 RAID 0亦称为带区集。它将两个以上的磁盘并联起来，成为一个大容量的磁盘。在存放数据时，分段后分散存储在这些磁盘中，因为读写时都可以并行处理，所以在所有的级别中，RAID 0的速度是最快的。但是RAID 0既没有冗余功能，也不具备容错能力，如果一个磁盘（物理）损坏，所有数据都会丢失。 RAID 1 两组以上的N个磁盘相互作镜像，在一些多线程操作系统中能有很好的读取速度，理论上读取速度等于硬盘数量的倍数，与RAID 0相同。另外写入速度有微小的降低。只要一个磁盘正常即可维持运作，可靠性最高。其原理为在主硬盘上存放数据的同时也在镜像硬盘上写一样的数据。当主硬盘（物理）损坏时，镜像硬盘则代替主硬盘的工作。因为有镜像硬盘做数据备份，所以RAID 1的数据安全性在所有的RAID级别上来说是最好的。但无论用多少磁盘做RAID 1，仅算一个磁盘的容量，是所有RAID中磁盘利用率最低的一个级别。 如果用两个不同大小的磁盘建RAID 1，可用空间为较小的那个磁盘，较大的磁盘多出来的空间也可以分区成一个区来使用，不会造成浪费。 \begin{aligned}Size&amp;=\min \left(S{1},S{2},S_{3}\dots \right)\end{aligned} RAID 5 RAID Level 5是一种储存性能、数据安全和存储成本兼顾的存储解决方案。它使用的是Disk Striping（硬盘分区）技术。RAID 5至少需要三块硬盘，RAID 5不是对存储的数据进行备份，而是把数据和相对应的奇偶校验信息存储到组成RAID5的各个磁盘上，并且奇偶校验信息和相对应的数据分别存储于不同的磁盘上。当RAID5的一个磁盘数据发生损坏后，可以利用剩下的数据和相应的奇偶校验信息去恢复被损坏的数据。RAID 5可以理解为是RAID 0和RAID 1的折衷方案。RAID 5可以为系统提供数据安全保障，但保障程度要比镜像低而磁盘空间利用率要比镜像高。RAID 5具有和RAID 0相近似的数据读取速度，只是因为多了一个奇偶校验信息，写入数据的速度相对单独写入一块硬盘的速度略慢，若使用“回写缓存”可以让性能改善不少。同时由于多个数据对应一个奇偶校验信息，RAID 5的磁盘空间利用率要比RAID 1高，便宜。 \begin{aligned}Size&amp;=(N-1)\times \min \left(S{1},S{2},\dots ,S_{N}\right)\end{aligned} (2) 新增加一块硬盘sdb，200G 划分4个分区，分别为20Gfdisk /dev/sdb，具体过程不贴了，最后的效果是这样： 123456 Device Boot Start End Blocks Id System/dev/sdb1 2048 41945087 20971520 83 Linux/dev/sdb2 41945088 83888127 20971520 83 Linux/dev/sdb3 83888128 125831167 20971520 83 Linux/dev/sdb4 125831168 419430399 146799616 5 Extended/dev/sdb5 125833216 167776255 20971520 83 Linux sdb1,sdb2,sdb3,sdb5各20G 123456789# 创建md0的软raid5阵列mdadm -C /dev/md0 -a yes -l 5 -n 3 -x 1 /dev/sdb1 /dev/sdb2 /dev/sdb3 /dev/sdb5# 格式化mkfs.xfs /dev/md0# 挂载mkdir /mnt/raid5mount /dev/md0 /mnt/raid5 第二题 每天的2 点和12 点整，将/etc 备份至/testdir/backup目录中，保存的文件名称格式为etcbak-yyyy-mm-dd-HH.tar.xz crontab -e 10 2,14 * * * /usr/bin/tar -Jcvf etcbak-`data +%F-%H`.tar.xz /etc 第三题 列出三个私有地址网络，用 CIDR 表示，并将 10.100.208.0/20 网络划分成 8 个子网，写出最大子网络的 IP 范围。 三个私有地址网络: 10.0.0.0/8172.16.0.0/16 ~ 172.31.0.0/16192.168.0.0/24 ~ 192.168.255.0/24划分后的最大子网络的IP范围为： 10.100.222.1-10.100.223.255 第三题 给CentOS6 eth0 网 卡 ， 分 别 设 置 三 个 IP 地 址 ：10.0.0.200/8,172.18.0.200/16,192.168.0.200/24，请写出步骤 临时配置： 123ifconfig eth0:0 10.0.0.200 netmask 255.0.0.0 upifconfig eth0:1 172.18.0.200 netmask 255.255.255.0 upifconfig eth0:2 192.168.0.200 netmask 255.255.255.0 up 永久生效： vim /etc/sysconfig/network-scripts/ifcfg-eth0:0 12345DEVICE=eth0:0ONBOOT=yesBOOTPROTO=staticIPADDR=10.0.0.200PREFIX=8 vim /etc/sysconfig/network-scripts/ifcfg-eth0:1 12345DEVICE=eth0:1ONBOOT=yesBOOTPROTO=staticIPADDR=172.18.0.200PREFIX=16 vim /etc/sysconfig/network-scripts/ifcfg-eth0:2 12345DEVICE=eth0:2ONBOOT=yesBOOTPROTO=staticIPADDR=192.168.0.200PREFIX=24 第五题 在 CentOS6 中，误删除/boot 下所有文件后无法启动，写出恢复的详细步骤。 救援模式进入系统 1234chroot /mnt/sysimagemount /dev/sr0 /mntrpm -ivh /mnt/Packages/kernel-2.6.32-696.el6.x86_64.rpm --forcegrub-install /dev/sda 手写/boot/grub/grub.conf 123456default =0timeout=5root (hd0,0)title CentOS 6.9 kernel /vmlinuz-2.6.32-696.el6.x86_64 root=/dev/sda2 init initramfs-2.6.32-696.el6.x86_64.img 退出重启 第六题 快速查找/root目录中大于2M的文本，并将文件中的magedu，换成www.magedu.com 1find /root --size +2M -type f -exec sed -i 's/magedu/www\.magedu\.com/g' &#123;&#125; \; 第七题 若系统检测到黑客用root用户登录了系统，如何将黑客所登录的终端杀死，并立即对root用户修改密码。 root用户执行who查看登录的终端信息（TERMINAL）ps -t |grep TERMINAL查看终端的进程号kill -9 PID按终端的进程号号杀掉异常的终端进程echo xxxx |passwd --stdin rootxxxx为新的密码 第八题 简述CentOS6开机启动流程 post–mbr grub 1stage–stage1.5–stage 2 /boot/grub—/boot/grub/grub.conf —kernel /vmlinuz.XXX root=— /boot/initramfs |/boot/initrd.XX.img —/sbin/init –/etc/inittab —/etc/rc.d/rc.sysinit（/etc/fstab） —/etc/rc5.d/K,S —/etc/rc.d/rc.local –login POST加电自检 引导加载器bootloader bootloader的引导程序GRUB的一部分放在MBR中 引导加载器程序GRUB grub 1.5阶段和2阶段 加载内核模块 先加载vmliuz内核，然后加载initramfs文件initd.img(里面都是预加载用到的的模块） 运行init,挂载硬盘和启动程序 运行init程序，init去读inittab启动模式,读取/etc/rc.d/rc.sysinit（里面有硬盘的挂载），找到相应模式对应的程序启动脚本，比如在5模式，去/etc/rc5.d里按顺序启动程序sbin/init –/etc/inittab —/etc/rc.d/rc.sysinit（/etc/fstab） —/etc/rc5.d —/etc/rc.d/rc.local 登录 第九题 Linux现连接一个新的存储(如/dev/sdb,容量为10T)一人应用程序需要在/data目录使用此存储的100G的存储空间，若做成LVM需要哪些步骤，请描述 12345# 让服务器识别硬盘echo '- - -' /sys/class/scsi_host/host2/scan# lsblk可以看到硬盘lsblk lvm创建过程： 123456pvcreate /dev/sdbvgcreate vg1 /dev/sdblvcreate -L 100G -n lv1 vg1mkfs.ext4 /dev/vg1/lv1mkdir /datamount /dev/vg1/lv1 /data 如果要加到fstab里设成开机启动： 1echo "/dev/vg1/lv1 /data ext4 defaults 0 0" &gt;&gt; /etc/fstab 第十题 修改上述网站的http 端口为9527 ，并为之增加SELinux 端口标签。 1234sed -i.bak &apos;s/Listen\ 80/Listen\ 9527/g&apos; /etc/httpd/conf/httpd.conf # 修改端口为9527semanage port -l| grep http_port_t # SELinux策略里没有9527端口semanage port -a -t http_port_t -p tcp 9527 # 添加9527端口semanage port -l| grep http_port_t # 再次查看有了 第十一题 查看crond进程打开了哪些文件 1lsof -c crond|grep REG|tr -s " " |cut -d" " -f9 第十二题 请完成以下操作1）查询file.txt文件里第一列数据数值之和(字段以&amp;符号分隔)2）查询Hie.txt第7行之前添加一行，内容为”#注释”3）打印出file.txt文件第6到第10行 1) 1awk -F'&amp;' 'NR==1 &#123;for (i=1;i&lt;=NF;i++)&#123;sum+=$i&#125;;print sum&#125;' file.txt 2) 1sed -i.bak '7i/#注释' Hie.txt 3) 1awk 'NR &gt;=6 &amp;&amp; NR&lt;=10' file.txt 第十三题 编写脚本，利用变量RANDOM生成10个随机数字，输出这10个数字，并显示其中的最大值和最小值，用两种方法实现 第一种： 12345678for i in `seq 0 9`;do array[$i]=$RANDOM echo $&#123;array[$i]&#125;doneecho "the min number:"echo $&#123;array[*]&#125;|awk -v RS=' ' -v ORS="\n" '&#123;print $0&#125;'|sort -n|grep -v '^$'|head -1echo "the max number:"echo $&#123;array[*]&#125;|awk -v RS=' ' -v ORS="\n" '&#123;print $0&#125;'|sort -n|grep -v '^$'|tail -1 第二种： 123456789101112131415161718for i in `seq 10`;do j=$RANDOM echo $j if [ $i -eq 1 ];then max=$j min=$j else if [ $j -ge $max ];then max=$j fi if [ $j -le $min ];then min=$j fi fidoneecho max number is $maxecho min number is $min 第14题 编写脚本，提示请输入网络地址，如192.168.0.0，判断输入的网段中主机在线状态，并统计在线主机和离线主机各多少 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#!/bin/bash# ┌───────────────────────────────────────────────────────┐# │Script Name | scan_ip.sh ▮▮▮▮▮▮▮▮ │# │Date | 2017-07-04 ▮▮ │# │Author | Yu Longjun ▮▮▮▮▮▮▮▮▮▮▮▮ │# │Blog | http://www.yulongnjun.com ▮▮ │# │Version | 1.0 ▮▮▮▮▮ │# │Description | This is description. ▮▮ │# └───────────────────────────────────────────────────────┘# 读入网段read -p "Please input network segment(like '172.17.0.0'):" net_seg# 判断是否是ip地址（这里暂时没写是否是网段的校验）echo $net_seg | egrep -o "\&lt;(([0-9]|[1-9][0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])\.)&#123;3&#125;([0-9]|[1-9][0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])"# 判断是否是ip地址，不是报错退出if [ $? != "0" ]; then echo "FormatError, exit." exitfi# 判断网络位sub1=`echo $net_seg| cut -d. -f1`sub2=`echo $net_seg| cut -d. -f2`sub3=`echo $net_seg| cut -d. -f3`sub4=`echo $net_seg| cut -d. -f4`# func_ping函数，ping主机的函数func_ping() &#123;# 网络为3位if [ $5 == "3" ];then for i in `seq 0 255`;do for j in `seq 0 255`;do for k in `seq 0 255`;do address=$1.$i.$j.$k if [ $address == "$1.0.0.0" -o $address == "$1.255.255.255" ];then continue else ping -c1 -w1 $address &amp;&gt;/dev/null &amp;&amp; echo "host $address is up." | tee -a host_up.log || echo "host $address is down." | tee -a host_down.log &amp; fi done done done# 网络位为2位elif [ $5 == "2" ];then for i in `seq 0 255`;do for j in `seq 0 255`;do address=$1.$2.$i.$j if [ $address == "$1.$2.0.0" -o $address == "$1.$2.255.255" ];then continue else ping -c1 -w1 $address &amp;&gt;/dev/null &amp;&amp; echo "host $address is up." | tee -a host_up.log || echo "host $address is down." | tee -a host_down.log &amp; fi done done# 网络位为1位elif [ $5 == "1" ];then for i in `seq 0 254`;do address=$1.$2.$3.$i ping -c1 -w1 $address &amp;&gt;/dev/null &amp;&amp; echo "host $address is up."| tee -a host_up.log || echo "host $address is down."| tee -a host_down.log &amp; donefi&#125;# 判断网络位是几位if [ $sub1 == "0" -o $sub4 != "0" ];then # 第一位不能是0，最后一位要是0，否则格式错误，报错退出 echo "FormatError, exit." exitelif [ $sub2 == "0" ];then if [ $sub3 == "0" ];then func_ping $sub1 255 255 255 3 else func_ping $sub1 0 $sub3 255 1 fielse if [ $sub3 == "0" ];then func_ping $sub1 $sub2 255 255 2 else func_ping $sub1 $sub2 $sub3 255 1 fifisleep 5if [ -e host_up.log ];then sum_up=`cat host_up.log| wc -l` echo "Up host's number is $sum_up" rm -rf host_up.logelse echo "Up host's number is 0"fiif [ -e host_down.log ];then sum_down=`cat host_down.log|wc -l` echo "Down host's number is $sum_down" rm -rf host_down.logelse echo "Down host's number is 0"fi 第15题 编写服务 脚本/root/bin/testsrv.sh ，完成如下要求(1) 脚本可接受参数：start, stop, restart, status(2) 如果参数非此四者之一，提示使用格式后报错退出(3) 如是start则创建/var/lock/subsys/ SCRIPT_NAME , 并显示“启动成功”考虑：如果事先已经启动过一次，该如何处理？(4) 如是stop则删除/var/lock/subsys/ SCRIPT_NAME, 并显示“停止完成”考虑：如果事先已然停止过了，该如何处理？(5) 如是restart ，则先stop, 再start考虑：如果本来没有start ，如何处理？(6) 如是status, 则如果/var/lock/subsys/ SCRIPT_NAME 文件存在，则显示SCRIPT_NAME is running...如果/var/lock/subsys/ SCRIPT_NAME 文件不存在，则显示 SCRIPT_NAME is stopped...其中： SCRIPT_NAME为当前脚本名 (7) 在所有模式下禁止启动该服务，可用chkconfig 和 service 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/env bash#chkconfig: 345 99 0# ┌───────────────────────────────────────────────────────┐# │Script Name | testsrv.sh ▮▮▮▮▮▮▮▮ │# │Date | 2017-07-04 ▮▮ │# │Author | Yu Longjun ▮▮▮▮▮▮▮▮▮▮▮▮ │# │Blog | http://www.yulongnjun.com ▮▮ │# │Version | 1.0 ▮▮▮▮▮ │# │Description | This is description. ▮▮ │# └───────────────────────────────────────────────────────┘srvfile="/var/lock/subsys/`basename $0`"func_start()&#123; if [ -e $srvfile ];then echo "服务已启动,无需再次启动" else touch $srvfile echo "启动成功" fi&#125;func_stop()&#123; if [ -e $srvfile ];then rm -f $srvfile echo "停止完成" else echo "服务未启动，无需停止" fi&#125;func_restart()&#123; func_stop func_start&#125;func_status()&#123; if [ -e $srvfile ];then echo "`basename $0` is running..." else echo "`basename $0` has stoped..." fi&#125;case $1 in "start") func_start ;; "stop") func_stop ;; "restart") func_restart ;; "status") func_status ;; *) "输入参数错误, Usage: `basename $0` start|stop|restart|status" ;;esac 第16题 编写脚本/root/bin/copycmd.sh(1) 提示用户输入一个可执行命令名称(2) 获取此命令所依赖到的所有库文件列表(3) 复制命令至某目标目录( 例如/mnt/sysroot) 下的对应路径下；如：/bin/bash ==&gt; /mnt/sysroot/bin/bash /usr/bin/passwd ==&gt; /mnt/sysroot/usr/bin/passwd(4) 复制此命令依赖到的所有库文件至目标目录下的对应路径下：/lib64/ld-linux-x86-64.so.2 ==&gt; /mnt/sysroot/lib64/ld-linux-x86-64.so.2(5) 每次复制完成一个命令后，不要退出，而是提示用户键入新的要复制的命令，并重复完成上述功能；直到用户输入quit 12345678910111213141516171819202122232425262728293031#!/bin/bash# ┌───────────────────────────────────────────────────────┐# │Script Name | copycmd.sh ▮▮▮▮▮▮▮▮ │# │Date | 2017-07-04 ▮▮ │# │Author | Yu Longjun ▮▮▮▮▮▮▮▮▮▮▮▮ │# │Blog | http://www.yulongnjun.com ▮▮ │# │Version | 1.0 ▮▮▮▮▮ │# │Description | This is description. ▮▮ │# └───────────────────────────────────────────────────────┘read -p "请输入复制路径:" copy_pathwhile true;do read -p "请输入一个可执行的命令的名称(quit 退出):" command if [ "$command" == "quit" ]; then exit else cmd=`which $command` cmd_path=`dirname $cmd` mkdir -p $copy_path$cmd_path cp -f $cmd $copy_path$cmd_path printf "复制 %-30s ====&gt; $copy_path%-30s" $cmd $cmd_path echo list=`ldd $cmd|grep -o "/lib.* "|tr -d " "` [ -e $copy_path/lib64 -a -e $copy_path/lib ] || mkdir $copy_path/&#123;lib64,lib&#125; for i in $list;do cp -f $i $copy_path$i printf "复制 %-30s ====&gt; $copy_path%-30s" $i $i echo done echo "复制 $cmd 命令完成" fidone]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>exam</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PAM认证模块]]></title>
    <url>%2Flinux%2F20170720-02-pam%2F</url>
    <content type="text"><![CDATA[PAM:Pluggable Authentication Modules（可插拔的认证模块） PAM 是关注如何为服务验证用户的 API，通过提供一些动 态链接库和一套统一的API，将系统提供的服务和该服务的认证方式分开。PAM模块使得系统管理员可以灵活地根据需要给不同的服务配置不同的认证方式而无需更改服务程序。PAM是一种认证框架，自身不做认证。 它提供了对所有服务进行认证的中央机制，适用于login，远程登录（telnet,rlogin,fsh,ftp,点对点协议（PPP）），su等应用程序中。系统管理员通过PAM配置文件来制定不同应用程序的不同认证策略；应用程序开发者通过在服务程序中使用PAM API(pam_xxxx( ))来实现对认证方法的调用；而PAM服务模块的开发者则利用PAMSPI来编写模块（主要是 引出一些函数pam_sm_xxxx( )供PAM接口库调用），将不同的认证机制加入到系统中；PAM接口库（libpam）则读取配置文件，将应用程序和相应的PAM服务模块联系起来。 PAM一般的认证顺序：Service（服务）-&gt;PAM（配置文件）-&gt;pam_*.so（相应的PAM库） PAM认证首先要确定那一项服务，然后加载相应的PAM的配置文件(位于/etc/pam.d下)，最后调用认证库文件(位于 /lib/security下)进行安全认证。 服务对应的pam模块的库的定义的配置，全在/etc/pam.d/目录下面 pam模块相关的库，都存放在/lib64/security下面 PAM模块配置格式PAM模块配置文件：/etc/pam.d/APP_NAME 配置名一般就是程序名，如login、sshd、sudo等等。第一行是PAM模块版本#%PAM-1.0。 下面的内容的格式： module-type control module-path arguments module-type： auth：账号的认证和授权 account：与账号管理相关的非认证类的功能，如：用来限制/允许用户对某个服务的访问时间，当前有效的系统资源(最多可以有多少个用户)，限制用户的位置(例如：root用户只能从控制台登录) passwd：用户修改密码时密码复杂度检查机制等功能 session：用户获取到服务之前或使用服务完成之后需要进行一些附加的操作，如：记录打开/关闭数据的信息，监视目录等 -type： 表示因为缺失而不能加载的模块将不记录到系统日 志,对于那些不总是安装在系统上的模块有用 control：control规定了PAM库如何处理与该服务相关的PAM模块成功或失败情况。 分为两种方式实现：简单和复杂。 简单方式实现：一个关健词实现 required： 一票否决，表示本模块必须返回成功才能通过 认证，但是如果该模块返回失败，失败结果也不会立即通知用户，而是要等到同一type中的所有模块全部执行完毕，再将失败结果返回给应用程序。即为必要条件 requisite ：一票否决，该模块必须返回成功才能通过认证， 但是一旦该模块返回失败，将不再执行同一type内的任何模块，而是直接将控制权返回给应用程序。是一个必要条件 sufficient： 一票通过，表明本模块认证的要求，不必再执行同一type内的其它模块，但如果本模块返回失败可忽略，即为充分条件 optional：表明本模块是可选的，它的成功与否不会对身份认证起关键作用，其返回值一般被忽略 include： 调用其他的配置文件中定义的配置信息 复杂详细实现（用的很少，偶尔有几个）：使用一个或多个“status=action”，[status1=action1 status2=action …] (1)Status:检查结果的返回状态 (2)Action:采取行为。ok，done，die，bad，ignore，reset ok 模块通过，继续检查 done 模块通过，返回最后结果给应用 bad 结果失败，继续检查 die 结果失败，返回失败结果给应用 ignore 结果忽略，不影响最后结果 reset 忽略已经得到的结果 module-path： 模块路径 相对路径：/lib64/security目录下的模块可使用相对路径。如：pam_shells.so、pam_limits.so 绝对路径 Arguments： 用来传递给该模块的参数 tips：修改PAM配置文件将马上生效。所以建议，编辑pam规则时，保持至少打开一个root会话，以防止 root身份验证错误。 PAM模块功能说明：每个pam模块的使用方法，可以用man 模块名来查，比如man pam_shell，man pam_env 也可以去官网下载pdf文档http://www.linux-pam.org/ 日志认证相关的日志，默认写在/var/log/secure pam模块示例示例1：pam_shells.so 示例2：pam_securetty.so 示例3：pam_nologin.so 示例4：pam_limits.so（重要） 模块通过读取配置文件/etc/security/*.conf完成用户对系统资源的使用控制。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>PAM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自制kickstart光盘]]></title>
    <url>%2Flinux%2F20170720-kickstart-iso%2F</url>
    <content type="text"><![CDATA[1. 制作好ks文件。最开始安装的CentOS6里选的开发者模式安装，安装了很多包，还选了中文包，还有一些工具，做为开发机用，故用这台机器生成anaconda.cfg文件进行配置。打开图形工具system-config-kickstart(默认没有安装，yum安装一下），打开系统安装完在root家目录生成的/root/anaconda.cfg文件，进行配置。 File –&gt; Open File 更改每一步的选项，使其适用于kickstart安装，这里给出每页的设置： 这里选新安装，安装源选cd-rom。 这里保持不变。 这里改为clear Master Boot Record清除主引导记录（MBR）,然后按照自己喜好分区。 网卡不变，如果你要装的服务器有多块网卡，可以自行添加。 不变 关闭SELinux和防火墙。 不安装图形环境 包用你安装时候选的就可以，也可以自行添加，也可以后期改安装文件添加。 写安装后脚本，比如添加用户，配置本地yum，拷贝操作机的key到机器上（之后操作机可以免密登录） File –&gt; Save,另存为另外的文件，这里我保存为centos6-devel-ks.cfg(因为我安装时候选了开发组件，故起名字为devel） ！！！注意：记得删除repo --name=&quot;CentOS&quot; --baseurl=cdrom:sr0 --cost=100这行，要不后面光盘制作好了找不到repodata！！！（坑死我了，好几个小时才找到原因） 复制centos6-devel-ks.cfg文件为centos6-mini-ks.cfg，修改里面的%packages到%end的内容为： 12345%packages@core@server-policy@workstation-policy%end 把他们放在一台机器上为之后做iso文件做准备。如果还有想安装的程序，可以手动写到ks文件里面。 修改完ks文件后，怕格式有问题，可以用下面的命令检测一下(此命令来自于pykickstart包，没有的话可以yum安装一下）： 1ksvalidator xxx.cfg 2. 制作光盘前配置先把光盘里内容复制一份: 12mount /dev/sr0 /mediacp -rv /media /app/centos6 在/app/centos6目录下创建ks目录，把两个ks的cfg文件复制过去: 12mkdir /app/centos6/kscp centos6-devel-ks.cfg centos6-mini-ks.cfg /app/centos6/ks 修改启动界面的配置文件： vim /app/centos6/isolinux/isolinux.cfg 把最后面的启动菜单更改为下面格式： 12345678910111213141516label local menu label Boot From ^Local Drive menu default localboot 0xfffflabel manual menu label ^Manual Install Linux kernel vmlinuz append initrd=initrd.imglabel minix menu label ^Automatic Minimal Install kernel vmlinuz append initrd=initrd.img ks=cdrom:/ks/centos6-mini-ks.cfglabel development menu label Automatic ^Development Install kernel vmlinuz append initrd=initrd.img ks=cdrom:/ks/centos6-devel-ks.cfg 解析：第一个菜单为从本地硬盘启动，这样即使超时，也不会误覆盖已有的操作系统；第二个菜单为手动安装，也就是一步步的安装；第3个菜单用的最小化安装的ks；第四个用的是开发机的ks，选了很多包。还可以改这个文件里的一行menu title ...为menu title CentOS 6.9 Kickstart by Yu Longjun，毕竟是自己做的嘛，打个标签的。 3. 删除TRANS.TBL，重做repodata1234find /app/centos6/ -name TRANS.TBL -exec rm -rf &#123;&#125; \;cp /app/centos6/repodata/43d8fd068164b0f042845474d6a22262798b9f0d1f49ad1bf9f95b953089777d-c6-x86_64-comps.xml /root rm -rf /app/centos6/repodata createrepo -g /root/43d8fd068164b0f042845474d6a22262798b9f0d1f49ad1bf9f95b953089777d-c6-x86_64-comps.xml /app/centos6 4. iso镜像制作1mkisofs -R -J -T -v --no-emul-boot --boot-load-size 4 --boot-info-table -V "CentOS 6.9 by Yu Longjun" -b isolinux/isolinux.bin -c isolinux/boot.cat -o /root/CentOS-6.9-x86_64-Kickstart-by-YuLongjun-origin.iso /app/centos6 5. iso镜像写入u盘作为安装盘上面的iso镜像刻成光盘，是可以启动安装的，但是直接dd写入到u盘，是无法工作的，需要用isohybird添加一些启动信息到iso头部： 123cp /root/CentOS-6.9-x86_64-by-YuLongjun-origin.iso /root/CentOS-6.9-x86_64-by-YuLongjun-usb.isoisohybrid /root/CentOS-6.9-x86_64-by-YuLongjun-usb.iso 我们可以ll看到这两个文件大小不一样： 1ll /root/CentOS-6.9-x86_64-by-YuLongjun*]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>kickstart光盘</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP_Wrappers应用层防火墙]]></title>
    <url>%2Flinux%2F20170720-01-tcp_wrappers%2F</url>
    <content type="text"><![CDATA[TCP_Wrappers是一个工作在应用层的安全工具，它只能针对某些具体的应用或者服务起到一定的防护作用。比如说ssh、telnet、FTP等服务的请求，都会先受到TCP_Wrappers的拦截。 TCP_Wrappers是基于库调用实现的。 包名为tcp_wrappers-libs 1234567[root@blog ~]# rpm -ql tcp_wrappers-libs/usr/lib64/libwrap.so.0/usr/lib64/libwrap.so.0.7.6[root@blog ~]# ll /usr/lib64/libwrap.so.0*lrwxrwxrwx. 1 root root 16 Jul 10 20:22 /usr/lib64/libwrap.so.0 -&gt; libwrap.so.0.7.6-rwxr-xr-x. 1 root root 42520 Jun 10 2014 /usr/lib64/libwrap.so.0.7.6 我们可以查询到好多服务都是调用的这个库文件,如sshd、vsftpd、xinetd 123456[root@blog ~]# ldd `which sshd` |grep libwrap.so.0 libwrap.so.0 =&gt; /lib64/libwrap.so.0 (0x00007fc777586000)[root@blog ~]# ldd `which vsftpd` |grep libwrap.so.0 libwrap.so.0 =&gt; /lib64/libwrap.so.0 (0x00007fdb8d276000)[root@blog ~]# ldd `which xinetd` |grep libwrap.so.0 libwrap.so.0 =&gt; /lib64/libwrap.so.0 (0x00007f4ae8036000) TCP_Wrappers访问控制实现TCP_Wrappers访问控制实现，是靠两个文件实现的：/etc/hosts.allow和/etc/hosts.deny .man hosts.allow或者 man hosts.deny可以查看规则和用法。 /usr/sbin/tcpd进程会根据这两个文件判断是否对访问请求提供服务。 /usr/sbin/tcpd进程先检查文件/etc/hosts.allow，如果请求访问的主机名或IP包含在此文件中，则允许访问。如果请求访问的主机名或IP不包含在/etc/hosts.allow中，那么tcpd进程就检查/etc/hosts.deny。看请求访问的主机名或IP有没有包含在hosts.deny文件中。如果包含，那么访问就被拒绝；如果既不包含在/etc/hosts.allow中，又不包含在/etc/hosts.deny中，那么此访问也被允许。 语法： daemon_list: client_list1 [EXCEPT client_list2][:options] deamon_list 应用程序文件名称 而非服务名 应用程序文件名称列表 彼此间使用逗号或空格分隔 All 表示所有服务 client_list1 IP地址 主机名 域名段 ,例如.yulongjun.com,即可以匹配db1.yulongjun.com node1.web.yulongjun.com等。 网络段CentOS6必须使用完整格式的掩码net/mask，如127.16.111.0/255.255.255.0，不能使用net/prefix,如127.16.111.0/24,从CentOS7开始，支持使用net/prefix 短格式的网络段（如 172.16.，相当于172.16.0.0/255.255.0.0） ALL 所有来源主机 KNOW 所有能解析到的主机 UNKNOW 所有未解析到的主机 LOCAL 主机名中不带.的 PARANOID 正反解析不匹配的地址 EXCEPT client_list2 除了client_list2的主机之外，client_list1的都匹配。 options deny 拒绝，主要用于hosts.allow文件中，实现deny功能 allow 允许，主要用在hosts.deny文件中，实现allow功能 spawn 启动额外应用程序,用的不多 twist 无论写在hosts.allow还是hosts.deny都是拒绝，并提示例子1：不允许172.16.111.100的ftp访问 vim /etc/hosts.deny 1vsftpd : 172.16.111.100 例子2：仅允许172.16.111.网段的ftp访问 vim /etc/hosts.allow 1vsftpd : 172.16.111. vim /etc/hosts.deny 1vsftpd : ALL 例子3：仅允许172.16.111.网段的ftp访问，除了172.16.111.100外 vim /etc/hosts.allow 1vsftpd : 172.16.111. EXCEPT 172.16。111.100 vim /etc/hosts.deny 1vsftpd : ALL 例子3：不允许172.16.111.网段访问ftp，写在hosts.allow里 vim /etc/hosts.allow 1vsftpd : 172.16.111. : deny 例子4，不允许192.168.111.网段ssh来访问，尝试的访问记录在/var/log/sshd.deny.log里 vim /etc/hosts.deny 1sshd: 192.168.111. :spawn echo &quot;`date` login attempt from %c to %s,%d&quot; &gt;&gt;/var/log/sshd.deny.log 我们用一台192.168.111.网段的机器ssh访问，可以看到下面结果：12[root@center ~]# cat /var/log/sshd.deny.log Mon Aug 14 17:25:08 CST 2017 login attempt from 192.168.111.100 to sshd@192.168.111.254,sshd 拒绝192.168.111.网段的ftp访问，并输出提示符。vim /etc/hosts.deny 或者 vim /etc/hosts.allow均可： 1vsftpd: 192.168.111. :twist /bin/echo &quot;connection refused!!!&quot;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>TCP_Wrappers</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sudo]]></title>
    <url>%2Flinux%2F20170718-04-sudo%2F</url>
    <content type="text"><![CDATA[我们之前接触过su（Switch User）命令 我们用过曾经用过这种用法：su - LongDream -c &#39;ll -a ~/&#39;，即临时的切换呢用户并执行命令然后回来。 我们通常用某个用户来执行一些其他用户才能运行的命令，这时候我们用su的话，会用`su - root -c ‘COMMAND’”。这样有个缺点，就是权限不好控制，root的权限太大，有时候不想给root权限和密码给某些用户。 Sudo机制：可以让某些用户像root用户一样运行一些命令，而不需要root密码。 命令格式：sudo COMMAND sudo权限配置文件：/etc/sudoers 默认有这两项： 12root ALL=(ALL) ALL%wheel ALL=(ALL) ALL 一个是root用户可以执行所有命令，一个是wheel用户可以执行所有命令 如果不知道管理员密码，一个有wheel组的用户，也可以切换到root，可以这样：sudo su - root（只要输入用户自己的密码就可以了，可以简写为sudo su -）。 我们看到/etc/sudoers文件有这样一句话：”This file must be edited with the ‘visudo’ command.”(这个文件最好是用visudo命令来编辑）。所以我们尽量用visudo命令来配置，有语法错误会报错： 我们继续来分析参数： 12root ALL=(ALL) ALL%wheel ALL=(ALL) ALL 参数分为几块： Users Hosts=(Runas_users) Commands 也就是运行命令的用户或用户组，在哪些主机上，以哪些用户的身份，来执行哪些命令。 详解： Users username ：用户名 #uid：用户UID %groupname：用户组 %#gid：用户组GID User_Alias：用户别名(sudoers文件中单独定义的) User_Alias用法：User_Alias ADMINS = jsmith, mikem，ADMINS为别名，必须为大写；jsmith, mikem为用户名。 Hosts hostname：主机名 ip_addr：ip地址 network(/netmask)：网段 Host_Alias：主机别名(sudoers文件中单独定义的) Host_Alias定义方法：Host_Alias FILESERVERS = fs1, fs2 Runas_users username ：用户名 #uid：用户UID %groupname：用户组 %#gid：用户组GID Runas_Alias：用户别名(sudoers文件中单独定义的) Runas_Alias定义方法：同Users里User_Alias用法。 Commands command_name：实际的命令，写绝对路径 directory：目录，目录下所有命令 sudoedit：有编辑sudoers文件的权限，相当于有授权权限。 Cmnd_Alias：一组命令的别名 Cmnd_Alias定义方法：Cmnd_Alias SOFTWARE = /bin/rpm, /usr/bin/up2date, /usr/bin/yum SOFTWARE为别名，必须为大写；j/bin/rpm, /usr/bin/up2date, /usr/bin/yum为命令名。 可以不输入用户密码,在Commands前可以加NOPASSWD：(如果有同学接触过Vagrant，就知道默认创建的vagrant用户执行sudo命令不用输入密码)，例如： root ALL=(ALL) NOPASSWD: ALL tips:通常，我们一般也不编辑sudoers文件，一般编辑/etc/sudoers.d/xxxxxx为你自定义的文件，一般以用户名或者组名来命名文件。例如：vim /etc/sudoers.d/vagrant%vagrant ALL=(ALL) NOPASSWD: ALL这样一个用户或一个用户组一个文件，便于后期管理。 Sudo示例示例1：(ALL全权限） 12Student ALL=(ALL) ALL%wheel ALL=(ALL) ALL 示例2：（单独命令和NOPASSWD） 12student ALL=(root) /sbin/pidof,/sbin/ifconfig%wheel ALL=(ALL) NOPASSWD: ALL 示例3：（使用Alias的示例） 123User_Alias NETADMIN= netuser1,netuser2Cmnd_Alias NETCMD = /usr/sbin/ipNETADMIN ALL=（root） NETCMD 示例4：（使用多Alias的示例，其中SERS没写表示任何人） 12345678User_Alias SYSADER=wang,mage,%adminsUser_Alias DISKADER=tomHost_Alias SERS=www.magedu.com,172.16.0.0/24Runas_Alias OP=rootCmnd_Alias SYDCMD=/bin/chown,/bin/chmodCmnd_Alias DSKCMD=/sbin/parted,/sbin/fdiskSYSADER SERS= SYDCMD,DSKCMDDISKADER ALL=(OP) DSKCMD 示例5：（使用正则表达式、NOPASSWD和PASSWD并存的示例，！排除） 123User_Alias ADMINUSER = adminuser1,adminuser2Cmnd_Alias ADMINCMD = /usr/sbin/useradd, /usr/sbin/usermod, /usr/bin/passwd [a-zA-Z]*, !/usr/bin/passwd rootADMINUSER ALL=(root) NOPASSWD:ADMINCMD， PASSWD:/usr/sbin/userdel 示例6：（Defaults表示默认用户，表示如果sudo不指定用户名时，默认使用的用户，在下面，指的是如果wang使用sudo不-u指定用户名，默认是tom） 12Defaults:wang runas_default=tomwang ALL=(tom,jerry) ALL 示例7：（指定了文件夹，但是排除了文件夹下面的useradd命令） 1wang 192.168.175.136,192.168.175.138=(root) /usr/sbin/,!/usr/sbin/useradd 示例8：（命令限定哪些文件，但是用*很不安全，比如我用wang用户 sudo cat /var/log/messages /etc/shadow也是可以的,这里不安全，不要这么使用。） 1wang ALL=(ALL) /bin/cat /var/log/messages* sudo 常用命令参数sudo -l 列出（list）当前用户可以运行的sudo命令。 sudo -b 将要执行的命令放在后台(background)执行。 sudo -u username/#uid 指定以另外一个用户(user)的身份运行命令，不加-u默认是root 默认5分钟内不用输入密码，如果sudo -k，则下次需要输入密码 sudo的好处不给用户root权限，便于日志审计，只要给他sudo权限就好。 如果大家都用root登录，就不知道是谁操作的，用sudo权限来分配，可以直接审计单个用户的操作。 还有一个好处是，便于权限控制。有些用户不能给他太多的权限，就可以用sudo来控制他的权限范围。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Sudo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH服务端]]></title>
    <url>%2Flinux%2F20170718-03-sshd%2F</url>
    <content type="text"><![CDATA[系统自带的ssh服务端配置文件：/etc/ssh/sshd_config 常用配置（包含默认配置）： Port 22：端口号，可以改为其他的端口号ListenAddress 0.0.0.0：可以设置监听的网段，默认为全网段限制登录(遵循最严权限，如果同时在禁止和允许的名单里，就禁止）： AllowUsers DenyUsers AllowGroups DenyGroups dropbear 第三方ssh服务端安装准备： • 1、安装开发包组: • 2、https://matt.ucc.asn.au/dropbear/下载dropbear-xxx.tar.bz2 安装： • 3、tar xf dropbear-xxx.tar.bz2 • 4、less INSTALL 查看安装过程，按照安装过程安装 • 5、./configure • 6、make PROGRAMS=&quot;dropbear dbclient dropbearkey dropbearconvert scp&quot; • 7、make PROGRAMS=&quot;dropbear dbclient dropbearkey dropbearconvert scp&quot; install 查看安装的程序： 查看使用帮助dropbear -h 默认打开需要key，如果没有指定的话，会去默认位置找，所以要在默认位置创建key 查看安装包里的README和dropbearkey -h里写的，来创建key： 123dropbearkey -t rsa -f /etc/dropbear/dropbear_rsa_host_key -s 2048 # -s 2048可以不写，默认就是2048，可以指定更大的数,最大支持4096或者是dropbearkey -t dss -f /etc/dropbear/dropbear_dsa_host_key # 固定值，1024 启用dropbear的ssh服务 12dropbear -p :2222 -F -E # 前台运行dropbear -p :2222 # 后台运行 客户端访问，可以用系统的ssh，也可以用dropbear的客户端dbclient，使用方法是一样的 12ssh -p 2222 root@127.0.0.1dbclient -p 2222 root@127.0.0.1]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH转发]]></title>
    <url>%2Flinux%2F20170718-02-ssh-port-forward%2F</url>
    <content type="text"><![CDATA[SSH端口转发SSH的端口转发（port forwarding via SSH),又叫做SSH隧道（SSH tunneling) ，是一种在本地计算机和远程计算机之间建立一个安全的连接，使远程计算机还可以作为中继服务。因为连接是加密的，所以SSH隧道对于传输使用未加密协议的信息（如IMAP，VNC或IRC）很有用。 SSH有3种类型的端口转发： 本地端口转发（Local port forwarding）: 连接是从SSH客户端经由SSH服务端转发，然后传递到目标服务器 远程端口转发（Remote port forwarding）: 连接是从SSH服务器经由ssh client转发，然后传递到目标服务器。 动态端口转发（Dynamic port forwarding）: 连接是从各种应用经由ssh客户端转发，然后经过SSH服务器，最终到达目标服务器。 1. 本地端口转发：1ssh -L [bind_local_address:]local_port:remote_host_address:remote_host_port remote_sshd_jumpserver bind_local_address指的是本机有多个ip地址，可以指定一个ip作为出口,可选项，可以不写，不写默认就是 local_port是指指定一个本地端口，作为连接sshd跳板机的端口 remote_host_address指的是要登录的远程主机地址 remote_host_port指的是要登录的远程主机的端口 remote_ssh_jumpserver 是指开启了sshd服务的跳板机(跟remote_host网络是通的） 选项： -f 后台启用 -N 不打开远程shell，处于等待状态 -g 启用网关功能，跟本地这台机器在同一个网段的，都可以通过这台机器进行转发。 示例1： 1ssh –L 9527:telnetsrv:23 -fN sshsrv 当访问本机的9527的端口时，被加密后转发到sshsrv的ssh服务， 再解密被转发到telnetsrv:23 data &lt;–&gt; localhost:9527 &lt;–&gt; localhost:XXXXX &lt;–&gt; sshsrv:22 &lt;–&gt; sshsrv:YYYYY &lt;–&gt; telnetsrv:23 示例2： 1ssh -fNL 8080:www.ubuntuforums.org:443 -L 12345:www.ubuntu.com:443 172.16.111.200 2. 远程端口转发:1ssh -R [bind_local_address:]local_sshd_server_port:remotehost:remote_host_port sshserver 示例1： 1ssh –R 9527:telnetsrv:23 –fN sshssrv 让sshsrv侦听9527端口的访问，如有访问，就加密后通过ssh 服务转发请求到本机ssh客户端，再由本机解密后转发到telnetsrv:23 Data &lt;–&gt; sshsrv:9527 &lt;–&gt; sshsrv:22 &lt;–&gt; localhost:XXXXX &lt;–&gt; localhost:YYYYY &lt;–&gt; telnetsrv:23 3. 动态端口转发：当用firefox访问internet时，本机的1080端口做为代理服务 器，firefox的访问请求被转发到sshserver上，由sshserver 替之访问internet 在本机firefox设置代理socket proxy:127.0.0.1:1080 1ssh -C -D 1080 root@sshserver X协议转发1ssh -X user@remotehost gedit]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSH客户端命令]]></title>
    <url>%2Flinux%2F20170718-01-ssh-commands%2F</url>
    <content type="text"><![CDATA[相关包 1234[root@centos73 ~]# rpm -qa "openssh*"openssh-6.6.1p1-31.el7.x86_64 # 两者通用设置openssh-server-6.6.1p1-31.el7.x86_64 # 服务端openssh-clients-6.6.1p1-31.el7.x86_64 # 客户端 server的服务都是sshd*,client的服务都是ssh*ls ssh命令ssh命令参数：-p PORT：指定对端端口-X: 支持x11转发-t: 强制伪tty分配 ssh -t remoteserver1 ssh remoteserver2 可以以remoteserver1作为跳板，然后登陆remoteserver2 ssh-keygen、ssh-copy-id命令–基于key的验证(免密登录）ssh-keygen 生成密钥对儿。 ssh-copy-id REMOTESERVER 拷贝公钥到对端服务器（实际是拷贝到.ssh/authorized_keys文件里）。 ssh-keygen -p 更改秘钥的密码（passphrase），如果有旧密码，需要输入旧密码。 ssh root@ADDRESS &#39;cat /etc/shadow&#39; 登录远程机器执行命令，然后退出。 实际应用： 123ssh-keygen # 生成秘钥ssh-copy-id root@172.16.111.200 # 把秘钥copy到远程服务器上ssh root@172.16.111.200 # 此时就可以基于key登录了 在Ansible权威指南有见过一个例子，用了一些参数： 12ssh-keygen -N "" -t rsa -b 4096 -C "me@yulongjun.com" -f /root/.ssh/yulongjun.rsassh-copy-id -i /root/.ssh/yulongjun.rsa.pub root@romate_host 这个例子里，第一个ssh-keygen命令里，-N参数指的是秘钥的密码，这里是空，-t是指的秘钥类型，-b指的是位数，默认2048，-C可以自定义一个邮箱 -f是指定一个自定义的秘钥文件（包括名字）。由于是自定义的秘钥，所以ssh-copy-id需要手动-i指定公钥路径（私钥路径也可以，会自动去匹配公钥发送） ssh-agent bash、ssh-add命令–免重复输入key的密码（passphrase）如果密钥设置了密码，每次输入都很麻烦，这时候可以用ssh-agent代理，只要输入一次口令，之后都可以用。退出会话之后就失效 12ssh-agent bashssh-add ~/.ssh/id_rsa # 此时只后，需要id_rsa的passphrase密码的话，只要输入一次就够了。 scp命令–走ssh端口的远程复制命令scp [options] SRC... DEST/源可以有多个。 主要两种方式： scp [options] [user@]host:/sourcefile /destpath scp [options] /sourcefile [user@]host:/destpath 常用选项： -C: 压缩数据流(compress) -r: 递归复制(recursive) -p: 保持原文件的属性信息(preserve) -q: 静默模式(quiet) -P PORT: 指明remote host的监听的端口(port) rsync命令–更聪明的复制基于ssh和rsh服务实现高效率的远程系统之间复制文件，使用安全的shell连接做为传输方式。 如果重复的文件可以不copy,比scp更快。 选项： -n 模拟复制过程 -v 显示详细过程 -r 递归复制目录树 -p 保留权限 -t 保留时间戳 -g 保留组信息 -o 保留所有者信息 -l 将软链接文件本身进行复制（默认） -L 将软链接文件指向的文件复制 -a 存档，相当于–rlptgoD，但不保留ACL（-A）和SELinux属性（-X） 例子： 12rsync –av /etc server1:/tmp # 复制目录和目录下文件rsync –av /etc/ server1:/tmp # 只复制目录下文件]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CA认证和证书]]></title>
    <url>%2Flinux%2F20170715-ca%2F</url>
    <content type="text"><![CDATA[一些概念： PKI：Public Key Infrastructure 签证机构：CA（Certificate Authority） 注册机构：RA（Register Authority） 证书吊销列表：CRL（Certificate Revoke Lists） 证书存取库 X.509：定义了证书的结构和认证协议的标准。包括版本号、序列号、签名算法、颁发者、有效期限、主体名称、主体公钥、CRL分发点、扩展信息、发行者签名等 获取证书的两种方法： 使用证书授权机构 生成签名请求（csr） 将csr发送给CA 从CA处接收签名 自签名的证书 自已签发自己的公钥 重点介绍一下自建CA颁发机构和自签名。 自建CA颁发机构和自签名实验用两台服务器，一台做ca颁发证书，一台去请求签名证书。 证书申请及签署步骤： 生成申请请求 CA核验 CA签署 获取证书 我们先看一下openssl的配置文件：/etc/pki/tls/openssl.cnf 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768####################################################################[ ca ]default_ca = CA_default # The default ca section(默认的CA配置，是CA_default,下面第一个小节就是)####################################################################[ CA_default ]dir = /etc/pki/CA # Where everything is kept （dir变量）certs = $dir/certs # Where the issued certs are kept（认证证书目录）crl_dir = $dir/crl # Where the issued crl are kept（注销证书目录）database = $dir/index.txt # database index file.（数据库索引文件）new_certs_dir = $dir/newcerts # default place for new certs.（新证书的默认位置）certificate = $dir/cacert.pem # The CA certificate（CA机构证书）serial = $dir/serial # The current serial number（当前序号，默认为空，可以指定从01开始）crlnumber = $dir/crlnumber # the current crl number（下一个吊销证书序号） # must be commented out to leave a V1 CRLcrl = $dir/crl.pem # The current CRL（下一个吊销证书）private_key = $dir/private/cakey.pem# The private key（CA机构的私钥）RANDFILE = $dir/private/.rand # private random number file（随机数文件）x509_extensions = usr_cert # The extentions to add to the cert# Comment out the following two lines for the "traditional"# (and highly broken) format.name_opt = ca_default # Subject Name options（被颁发者，订阅者选项）cert_opt = ca_default # Certificate field options（认证字段参数）# Extension copying option: use with caution.# copy_extensions = copy# Extensions to add to a CRL. Note: Netscape communicator chokes on V2 CRLs# so this is commented out by default to leave a V1 CRL.# crlnumber must also be commented out to leave a V1 CRL.# crl_extensions = crl_extdefault_days = 365 # how long to certify for （默认的有效期天数是365）default_crl_days= 30 # how long before next CRLdefault_md = sha256 # use SHA-256 by defaultpreserve = no # keep passed DN ordering# A few difference way of specifying how similar the request should look# For type CA, the listed attributes must be the same, and the optional# and supplied fields are just that :-)policy = policy_match # 是否匹配规则# For the CA policy[ policy_match ]countryName = match # 国家名是否匹配，match为匹配stateOrProvinceName = match # 州或省名是否需要匹配organizationName = match # 组织名是否需要匹配organizationalUnitName = optional # 组织的部门名字是否需要匹配commonName = supplied # 注释emailAddress = optional # 邮箱地址# For the 'anything' policy# At this point in time, you must list all acceptable 'object'# types.[ policy_anything ]countryName = optionalstateOrProvinceName = optionallocalityName = optionalorganizationName = optionalorganizationalUnitName = optionalcommonName = suppliedemailAddress = optional#################################################################### 重点关注下面的几个参数： 12345678910dir = /etc/pki/CA # Where everything is keptcerts = $dir/certs # Where the issued certs are keptdatabase = $dir/index.txt # database index file.new_certs_dir = $dir/newcerts # default place for new certs.certificate = $dir/cacert.pem # The CA certificateserial = $dir/serial # The current serial numberprivate_key = $dir/private/cakey.pem# The private key 1、创建所需要的文件touch /etc/pki/CA/index.txt 生成证书索引数据库文件 echo 01 &gt; /etc/pki/CA/serial 指定第一个颁发证书的序列号,16进制数，比如可以从1a开始，一般从01开始。 2、CA自签证书在作为CA的服务器上操作： 生成私钥 1(umask 066;openssl genrsa -out /etc/pki/CA/private/cakey.pem 4096) 生成自签名证书 1openssl req -new -x509 -key /etc/pki/CA/private/cakey.pem -days 7300 -out /etc/pki/CA/cacert.pem 参数解析： -new: 生成新证书签署请求 -x509: 专用于CA生成自签证书 -key: 生成请求时用到的私钥文件 -days n：证书的有效期限 -out /PATH/TO/SOMECERTFILE: 证书的保存路径 3、颁发证书 在需要使用证书的主机生成证书请求。 比如给一台作为博客web服务的服务器生成私钥： 1(umask 066; openssl genrsa -out /etc/pki/tls/private/blog.key 4096) 生成证书申请文件 1openssl req -new -key /etc/pki/tls/private/blog.key -days 3560 -out /etc/pki/tls/blog.csr 和CA生成证书的区别是没有-x509参数，加了-x509就是CA自签名证书 将证书请求文件传输给CA 1scp /etc/pki/tls/blog.csr root@172.16.111.100:/tmp/ CA签署证书，并将证书颁发给请求者 1openssl ca -in /tmp/blog.csr –out /etc/pki/CA/certs/blog.crt -days 365 注意：默认国家，省，公司名称三项必须和CA一致 把blog.crt证书回传给申请者，申请者可以使用此证书。 证书可以放在网站里，比如tomacat服务有专门存放证书的地方，还有可能需要转化格式，此处使用方法暂略 4、吊销证书 在客户端获取要吊销的证书的serial 1openssl x509 -in /PATH/FROM/CERT_FILE -noout -serial -subject 在CA上，根据客户提交的serial与subject信息，对比检验是否与index.txt文件中的信息一致，吊销证书： 1openssl ca -revoke /etc/pki/CA/newcerts/SERIAL.pem 指定第一个吊销证书的编号 注意：这里只有在第一次更新证书吊销列表前，才需要执行指定编号。 1echo 01 &gt; /etc/pki/CA/crlnumber 更新证书吊销列表 1openssl ca -gencrl -out /etc/pki/CA/crl/crl.pem 查看crl文件： 1openssl crl -in /etc/pki/CA/crl/crl.pem -noout -text]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>CA</tag>
        <tag>CRL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scratch 8 之三--构建临时系统]]></title>
    <url>%2Flinux%2F20170709-lfs8-3%2F</url>
    <content type="text"><![CDATA[这一节来构造一个最小的Linux系统，这个小系统包含了用来构建的LFS系统所需要的工具，还有工作环境。 构建分为两步： 构建一个跟宿主系统（Fedora）无关的新工具链（编译器（compiler）, 汇编器（assembler）, 链接器（linker）, 库（libraries）, 一些有用的其他工具（utilities）） 用这个新工具链来构建其他的必要工具。 这些新编译的工具将被安装在$LFS/tools里，这是一个临时的文件夹，后期完全构建完系统后会删掉。 构建前检查12echo $LFSls -l `which sh` `which awk` `which yacc` 确保当前用户是lfs 每个包的构建过程要点： 重要下面所有的构建，都是基于下面的步骤，请牢记： 把所有源文件和补丁放到 chroot 环境可访问的目录，例如 /mnt/lfs/sources/。但是千万不能把源文件放在 /mnt/lfs/tools/中。 进入到源文件目录。 对于每个软件包: a. 用 tar 程序解压要编译的软件包。在第五章中，确保解压软件包时你使用的是 lfs 用户。 b. 进入到解压后创建的目录中。(cd xxx-x.x.x) c. 根据指南说明编译软件包(见每个包的要求） d. 回退到源文件目录($LFS/sources) e. 除非特别说明，删除解压出来的目录和所有编译过程中生成的 &lt;package&gt;-build 目录。 开始构建按照下面的顺序依次构建 安装 Binutils 第一部分Binutils 软件包包括了一个链接器、汇编器和其它处理目标文件的工具。 用lfs用户做下面操作： 解压并创建build编译目录 12345cd $LFS/sourcestar -xvf binutils-2.28.tar.bz2cd binutils-2.28mkdir buildcd build/ 配置： 123456../configure --prefix=/tools \ --with-sysroot=$LFS \ --with-lib-path=/tools/lib \ --target=$LFS_TGT \ --disable-nls \ --disable-werror 编译： 1time make -j12 这里是为了统计make编译时间，命令前面加个time，同时虚拟机是在一台 6核cpu * 2颗 的服务器上分配了12颗的cpu，所以使用12个核心同事编译：time make -j12 创建lib64和lib文件夹： 123case $(uname -m) in x86_64) mkdir -v /tools/lib &amp;&amp; ln -sv lib /tools/lib64 ;;esac 安装： 1make install 安装 Gcc安装Gcc需要gmp、mpfr、mpc包，把这3个包解压到gcc的解压目录，记得把目录名更名为无版本号的目录名 123456789cd $LFS/sourcestar -xvf gcc-7.1.0.tar.bz2cd gcc-7.1.0.tar.bz2tar -xf ../mpfr-3.1.5.tar.xzmv -v mpfr-3.1.5 mpfrtar -xf ../gmp-6.1.2.tar.xzmv -v gmp-6.1.2 gmptar -xf ../mpc-1.0.3.tar.gzmv -v mpc-1.0.3 mpc 以下命令将更改GCC的默认动态链接器的位置以使用安装的动态链接器 /tools。它也/usr/include从GCC的包含搜索路径中删除 ： 123456789101112for file in gcc/config/&#123;linux,i386/linux&#123;,64&#125;&#125;.hdo cp -uv $file&#123;,.orig&#125; sed -e 's@/lib\(64\)\?\(32\)\?/ld@/tools&amp;@g' \ -e 's@/usr@/tools@g' $file.orig &gt; $file echo '#undef STANDARD_STARTFILE_PREFIX_1#undef STANDARD_STARTFILE_PREFIX_2#define STANDARD_STARTFILE_PREFIX_1 "/tools/lib/"#define STANDARD_STARTFILE_PREFIX_2 ""' &gt;&gt; $file touch $file.origdone 最后，在x86_64主机上，将64位库的默认目录名设置为lib： 123456case $(uname -m) in x86_64) sed -e '/m64=/s/lib64/lib/' \ -i.orig gcc/config/i386/t-linux64 ;;esac GCC文件建议在专用的构建目录中构建GCC： 12mkdir -v buildcd build 准备GCC编译： 12345678910111213141516171819202122../configure \ --target=$LFS_TGT \ --prefix=/tools \ --with-glibc-version=2.11 \ --with-sysroot=$LFS \ --with-newlib \ --without-headers \ --with-local-prefix=/tools \ --with-native-system-header-dir=/tools/include \ --disable-nls \ --disable-shared \ --disable-multilib \ --disable-decimal-float \ --disable-threads \ --disable-libatomic \ --disable-libgomp \ --disable-libmpx \ --disable-libquadmath \ --disable-libssp \ --disable-libvtv \ --disable-libstdcxx \ --enable-languages=c,c++ 编译GCC： 1time make -j12 安装：1make install 安装 Linux-4.12 API Headers12tar -xvf linux-4.12.tar.xzcd linux-4.12 确保包中不存在过时的文件： 1make mrproper 从源代码中提取用户可见的内核头文件 make -j12 INSTALL_HDR_PATH=dest headers_install1cp -rv dest/include/* /tools/include 安装 glibc-2.25经过前面的几个安装，步骤应该已经熟悉了，下面全部略过安装步骤，只显示时间。 安装listdc++-7.1.0 安装 Binutils-2.28 第二部分 安装 GCC-7.1.0 - 第2部分 安装 tcl-core-8.6.6 安装expect 剩下的略过，同上，参考文档来做改变属主当前，$LFS/tools 目录属于 lfs 用户，这是一个只存在于宿主系统上的帐号。如果继续保持 $LFS/tools 目录的现状，其中的文件将属于一个没有相关联帐号的用户ID。这很危险，因为随后创建的用户有可能会分配到相同的用户ID，从而变成 $LFS/tools 目录及其中所有文件的属主，以致留下恶意操作这些文件的可能。 为了解决这个问题，你可以在随后新的 LFS 系统里创建 /etc/passwd 文件时增加一个 lfs 用户，并注意给它分配和宿主系统里相同的用户和组ID。不过更好的方式是，通过下面的命令将 $LFS/tools 目录的属主改为 root 用户： chown -R root:root $LFS/tools 打包备份 $LFS/tools用root用户打包这个文件保存下来，便于以后使用: 1tar -Jcvf lfs8-tools.tar.xz $LFS/tools]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>LFS 8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scratch 8 之二--创建LFS的分区，下载源码包]]></title>
    <url>%2Flinux%2F20170709-lfs8-2%2F</url>
    <content type="text"><![CDATA[添加新硬盘并格式化笔者用的虚拟机，直接设置里面添加一块20G硬盘即可,系统识别为sdb 划分分区，并格式化。 分区 用途 文件系统大小 sdb1 /boot ext4 512 MiB sdb2 / ext4 10 GiB sdb3 swap swap 2 GiB fdisk /dev/sdb来分区。记得t来更改sdb3的类型为swap（82）: 对分区进行格式化123mkfs.ext4 /dev/sdb1mkfs.ext4 /dev/sdb2mkswap /dev/sdb3 设置环境变量vim ~/.bashrc或者vim ~/.bash_profile都可以，添加下面环境变量，方便以后使用。 1export LFS=/mnt/lfs . ~/.bashrc使之实时生效。 挂载新分区先挂载跟分区，然后在根分区上创建boot文件夹，然后挂载boot分区到$LFS/boot下。 123456mkdir -pv $LFSmount /dev/sdb2 $LFSmkdir $LFS/bootmount /dev/sdb1 $LFS/bootswapon /dev/sdb3 # 测试是否能挂载上swap 下载源码包下载源码包的wget列表文件wget-list到Fedora上，这里我是下载保存为~/Downloads/wget-list，然后wget这个列表，下载源码包。 123456mkdir -v $LFS/sourceschmod a+wt $LFS/sourceswget -i ~/Downloads/wget-list-c -P $LFS/sources# 或者使用长格式：`wget --input-file=~/Downloads/wget-list --continue --directory-prefix=$LFS/sources` 国内网络环境不好，如果下的很慢，直接把这个列表的里地址复制一下，黏贴到迅雷批量下载里面，直接就全下下来了，下载完了后，再复制到$LFS/sources里 但是还是有坑，有的源文件都没了……还有的是sourceforge.net网站的文件，无法迅雷下载（下载下来文件大小也不对），需要手工去网站下载。笔者整理了完整的一份，连md5sums文件一起打了个包，放到百度云上，有需要的可以去下载。下载链接:http://pan.baidu.com/s/1geDLprd 密码:k5du 下载MD5校验文件进行校验下载md5校验文件md5sums，复制到$/LFS/sources下，运行下面命令： 1234cp ~/Downloads/md5sums $LFS/sourcespushd $LFS/sourcesmd5sum -c md5sumspopd 创建$LFS/tools目录这个目录是一个临时工具目录。临时使用Fedora主机的某些软件而不对原主机构成影响。 12mkdir -v $LFS/toolsln -sv $LFS/tools / 添加lfs用户用root容易损坏原系统，故创建一个lfs用户。 12345useradd -s /bin/bash -m -k /dev/null lfspasswd lfschown -v lfs $LFS/toolschown -v lfs $LFS/sourcessu - lfs 设置lfs的环境切换到lfs用户后，设置lfs用户的环境文件 123cat &gt; ~/.bash_profile &lt;&lt; "EOF"exec env -i HOME=$HOME TERM=$TERM PS1='\u:\w\$ ' /bin/bash EOF 确保构建的shell环境完全为新的空环境，即确保主机系统中的危险环境变量不进入构建的环境中，目的是为了有一个干净的环境。 123456789cat &gt; ~/.bashrc &lt;&lt; "EOF"set +humask 022LFS=/mnt/lfsLC_ALL=POSIXLFS_TGT=$(uname -m)-lfs-linux-gnu PATH=/tools/bin:/bin:/usr/binexport LFS LC_ALL LFS_TGT PATHEOF set +h hash表，可以记住命令的完整路径，不用重复去找，提高效率。 umask 022确保新创建的文件和目录只能由owner写入，但是任何人可以读取和执行。 LFS=/mnt/lfs目录环境变量 LC_ALL=POSIX遵循POSIX规则，确保chroot环境中的的正常工作 LFS_TGT=$(uname -m)-lfs-linux-gnu当编译我们的交叉编译器和链接器以及交叉编译我们的临时工具链时，LFS_TGT变量设置了一个非默认，但兼容的机器说明。 PATH=/tools/bin:/bin:/usr/bin把/tools/bin放到标准的PATH变量前面，这样使后续安装的新版本的命令后，去使用新版本的命令，而不会使用旧版本命令。 export LFS LC_ALL LFS_TGT PAT这里把上述变量变为环境变量 12source ~/.bash_profilesource ~/.bashrc]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>LFS 8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux From Scratch 8 之一--构建前准备主机系统]]></title>
    <url>%2Flinux%2F20170709-lfs8-1%2F</url>
    <content type="text"><![CDATA[基于20170703的LFS文档，Linux Kernel 采用 4.12(20170702发布) 采用Fedora 25(点击下载)进行构建，已经dnf update到最新包。 介绍本节检查构建LFS所需的工具，然后准备一个LFS系统的分区，并在上面创建一个文件系统，然后挂载 主机准备工作fedora系统需要安装构建系统所需要的包（版本号要求在下面列出的版本之上，但是不要使用不推荐的版本） 列出要安装的包，都有，有的已经安装了，fedora 25里包的版本，正好符合FHS的要求。 1dnf list bash binutils bison bzip2 coreutils diffutils findutils gawk gcc glibc grep gzip kernel m4 make patch perl sed tar texinfo xz 2 无论装没装过，全部装一遍（装过的会skip跳过） 1dnf -y install bash binutils bison bzip2 coreutils diffutils findutils gawk gcc glibc grep gzip kernel m4 make patch perl sed tar texinfo xz 运行脚本检查：直接复制到命令行就可以了。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758cat &gt; version-check.sh &lt;&lt; "EOF"#!/bin/bash# Simple script to list version numbers of critical development toolsexport LC_ALL=Cbash --version | head -n1 | cut -d" " -f2-4MYSH=$(readlink -f /bin/sh)echo "/bin/sh -&gt; $MYSH"echo $MYSH | grep -q bash || echo "ERROR: /bin/sh does not point to bash"unset MYSHecho -n "Binutils: "; ld --version | head -n1 | cut -d" " -f3-bison --version | head -n1if [ -h /usr/bin/yacc ]; then echo "/usr/bin/yacc -&gt; `readlink -f /usr/bin/yacc`";elif [ -x /usr/bin/yacc ]; then echo yacc is `/usr/bin/yacc --version | head -n1`else echo "yacc not found" fibzip2 --version 2&gt;&amp;1 &lt; /dev/null | head -n1 | cut -d" " -f1,6-echo -n "Coreutils: "; chown --version | head -n1 | cut -d")" -f2diff --version | head -n1find --version | head -n1gawk --version | head -n1if [ -h /usr/bin/awk ]; then echo "/usr/bin/awk -&gt; `readlink -f /usr/bin/awk`";elif [ -x /usr/bin/awk ]; then echo awk is `/usr/bin/awk --version | head -n1`else echo "awk not found" figcc --version | head -n1g++ --version | head -n1ldd --version | head -n1 | cut -d" " -f2- # glibc versiongrep --version | head -n1gzip --version | head -n1cat /proc/versionm4 --version | head -n1make --version | head -n1patch --version | head -n1echo Perl `perl -V:version`sed --version | head -n1tar --version | head -n1makeinfo --version | head -n1xz --version | head -n1echo 'int main()&#123;&#125;' &gt; dummy.c &amp;&amp; g++ -o dummy dummy.cif [ -x dummy ] then echo "g++ compilation OK"; else echo "g++ compilation failed"; firm -f dummy.c dummyEOFbash version-check.sh 报了三个错误： yacc not found version-check.sh: line 36: g++: command not found version-check.sh: line 50: g++: command not foundg++ completion failed 我们看到FHS官方文档里写到： yacc可以直接链接到bison： 123cd /usr/binln -s bison yaccll bison yacc g++这里描述的不清楚，fedora装gcc-c++包就可以了： 1dnf install gcc-c++ 再次运行bash version-check.sh脚本，完美：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>LFS 8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本处理工具-sed]]></title>
    <url>%2Flinux%2F20170706-sed%2F</url>
    <content type="text"><![CDATA[sed：Stream EDitor, 行编辑器 usage： sed [option]... &#39;script&#39; inputfile... 常用option： -n：不输出模式空间内容到屏幕，即不自动打印 -e： 多点编辑 -f：/PATH/SCRIPT_FILE: 从指定文件中读取编辑脚本 -r： 支持使用扩展正则表达式 -i.bak： 备份文件并原处编辑 script： ‘地址+命令’ 地址定界： (1) 不给地址：对全文进行处理 (2) 单地址： #: 指定的行 /pattern/：被此处模式所能够匹配到的每一行 (3) 地址范围： #,# #,+# /pat1/,/pat2/ #,/pat1/ (4) ~：步进 1~2 奇数行 2~2 偶数行 编辑命令： d: 删除模式空间匹配的行 p: 显示模式空间中的内容 a [\]text：在指定行后面追加文本 支持使用\n实现多行追加 i [\]text：在行前面插入文本(注意：可以不加斜线，在有多个空白符的时候，不加斜线不显示空白符，加了才会显示） c [\]text：替换行为单行或多行文本 w /path/somefile: 保存模式匹配的行至指定文件 r /path/somefile：读取指定文件的文本至模式空间中 匹配到的行后 =: 为模式空间中的行打印行号 !:模式空间中匹配行取反处理 sed -e &quot;/^[[:space:]]*$/d&quot; -e &quot;/^#/d&quot; /etc/fstab]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串处理]]></title>
    <url>%2Flinux%2F20170705-02-string%2F</url>
    <content type="text"><![CDATA[字符串字符切片 基于模式取子串 字符串查找并替换 字符串查找并删除]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>string</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数组和关联数组]]></title>
    <url>%2Flinux%2F20170705-01-array%2F</url>
    <content type="text"><![CDATA[声明数组：声明数组：declare -a array （数组可以不声明，直接用）声明关联数组：declare -A ass_array（关联数组必须declare声明） 数组赋值数组是带索引的一系列值。 定义数组weeks=(Sun Mon Tue Wed Thu Fri Sat)fruits[0]=&quot;Apple&quot; 遍历数组 *可以换成@ 数组长度 *数组切片 添加元素到末尾]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>array</tag>
        <tag>associate array</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设置Fedora合上盖子不休眠]]></title>
    <url>%2Flinux%2F20170705-fedora25-dont-sleep%2F</url>
    <content type="text"><![CDATA[最近做实验，在另一台笔记本上装了一个Fedora 25，发现合上盖子就休眠了，而且桌面环境的电源设置里也没有针对合上盖子的设置 +_+ 从网上找了一下，找到一篇文章：http://www.tuicool.com/articles/ZvQRF3 下面记录下来，作为参考： Fedora的电源管理是在systemd的管理之下，参数文件为：/etc/systemd/logind.conf。 里面有几行被注释掉的配置，就是默认的电源配置： 1234#HandlePowerKey=poweroff#HandleSuspendKey=suspend#HandleHibernateKey=hibernate#HandleLidSwitch=suspend 前面是动作，后面是系统响应的操作。HandlePowerKey：按下电源后的动作HandleSleepKey：按下挂起键的动作HandleHibernateKey: 按下休眠建的动作HandleLidSwitch：合上笔记本盖子后的动作。 对应的操作有ignore、poweroff、reboot、halt、suspend、hibernate、hybrid-sleep、lock 或 kexec。 这里合上盖子的默认参数是suspend挂起。 复制这条注释，然后去掉#好，把suspend改为lock或者ignore都可以。 12345#HandlePowerKey=poweroff#HandleSuspendKey=suspend#HandleHibernateKey=hibernate#HandleLidSwitch=suspendHandleLidSwitch=lock 重启服务：systemctl restart systemd-logind]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Fedora 25</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[破坏实验--破坏5的initrd、6和7的initramfs文件，然后恢复]]></title>
    <url>%2Flinux%2F20170704-03-rescure-initrd-initramfs%2F</url>
    <content type="text"><![CDATA[删掉5的/boot/initrdxxxxx.img 恢复：救援模式进入系统： mkinitrd /boot/initramfs-$(uname -r).img $(uname -r) 删掉6的/boot/initramfsxxx.img mkinitrd /boot/initramfs-$(uname -r).img $(uname -r) 删掉7的/boot/initramfsxxx.img mkinitrd /boot/initramfs-$(uname -r).img $(uname -r)或者dracut /boot/initramfs-$(uname -r).img $(uname -r)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>initrd</tag>
        <tag>initramfs</tag>
        <tag>mkinitrd</tag>
        <tag>dracut</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell进阶之函数练习题]]></title>
    <url>%2Flinux%2F20170704-02-function-practice%2F</url>
    <content type="text"><![CDATA[1 打印国际象棋 1234567891011121314151617181920212223242526#!/bin/bash# ┌───────────────────────────────────────────────────────┐# │Script Name | cheer.sh ▮▮▮▮▮▮▮▮ │# │Date | 2017-07-04 ▮▮ │# │Author | Yu Longjun ▮▮▮▮▮▮▮▮▮▮▮▮ │# │Blog | http://www.yulongnjun.com ▮▮ │# │Version | 1.0 ▮▮▮▮▮ │# │Description | This is description. ▮▮ │# └───────────────────────────────────────────────────────┘red()&#123; echo -ne "\033[41m \033[0m"; &#125;yellow()&#123; echo -ne "\033[43m \033[0m"; &#125;for i in `seq 9`; do for k in `seq 3`;do for j in `seq 9`; do let sum=i+j let mod=sum%2 if [ $mod -eq 1 ]; then red else yellow fi done echo donedone 2 斐波那契数列又称黄金分割数列，因数学家列昂纳多·斐波那契以兔子繁殖为例子而引入，故又称为“兔子数列”，指的是这样一个数列：0、1、1、2、3、5、8、13、21、34、……，斐波纳契数列以如下被以递归的方法定义：F（0）=0，F（1）=1，F（n）=F(n-1)+F(n-2)（n≥2）利用函数，求n阶斐波那契数列 12345678910111213141516171819202122232425262728#!/bin/bash# ┌───────────────────────────────────────────────────────┐# │Script Name | fib.sh ▮▮▮▮▮▮▮▮ │# │Date | 2017-07-04 ▮▮ │# │Author | Yu Longjun ▮▮▮▮▮▮▮▮▮▮▮▮ │# │Blog | http://www.yulongnjun.com ▮▮ │# │Version | 1.0 ▮▮▮▮▮ │# │Description | This is description. ▮▮ │# └───────────────────────────────────────────────────────┘read -p "请输入数列个数：" nfib()&#123; if [ $1 -eq 0 ] ; then ret=0 echo $ret elif [ $1 -eq 1 ]; then ret=1 echo $ret elif [ $1 -gt 1 ]; then let ret=$(fib $[$1-1])+$(fib $[$1-2]) echo $ret fi&#125;for i in `seq $n`; do fib $idone#!/bin/bash 3 编写服务脚本/root/bin/testsrv.sh，完成如下要求(1) 脚本可接受参数：start, stop, restart, status(2) 如果参数非此四者之一，提示使用格式后报错退出(3) 如是start:则创建/var/lock/subsys/SCRIPT_NAME, 并显示“启动成功”考虑：如果事先已经启动过一次，该如何处理？(4) 如是stop:则删除/var/lock/subsys/SCRIPT_NAME, 并显示“停止完成”考虑：如果事先已然停止过了，该如何处理？(5) 如是restart，则先stop, 再start考虑：如果本来没有start，如何处理？(6) 如是status, 则如果/var/lock/subsys/SCRIPT_NAME文件存在，则显示“SCRIPT_NAMEis running…”如果/var/lock/subsys/SCRIPT_NAME文件不存在，则显示“SCRIPT_NAME is stopped…”其中：SCRIPT_NAME为当前脚本名(7)在所有模式下禁止启动该服务，可用chkconfig和service命令管理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/bin/env bash#chkconfig: 345 99 0# ┌───────────────────────────────────────────────────────┐# │Script Name | testsrv.sh ▮▮▮▮▮▮▮▮ │# │Date | 2017-07-04 ▮▮ │# │Author | Yu Longjun ▮▮▮▮▮▮▮▮▮▮▮▮ │# │Blog | http://www.yulongnjun.com ▮▮ │# │Version | 1.0 ▮▮▮▮▮ │# │Description | This is description. ▮▮ │# └───────────────────────────────────────────────────────┘srvfile=&quot;/var/lock/subsys/`basename $0`&quot;func_start()&#123; if [ -e $srvfile ];then echo &quot;服务已启动,无需再次启动&quot; else touch $srvfile echo &quot;启动成功&quot; fi&#125;func_stop()&#123; if [ -e $srvfile ];then rm -f $srvfile echo &quot;停止完成&quot; else echo &quot;服务未启动，无需停止&quot; fi&#125;func_restart()&#123; func_stop func_start&#125;func_status()&#123; if [ -e $srvfile ];then echo &quot;`basename $0` is running...&quot; else echo &quot;`basename $0` has stoped...&quot; fi&#125;case $1 in &quot;start&quot;) func_start ;; &quot;stop&quot;) func_stop ;; &quot;restart&quot;) func_restart ;; &quot;status&quot;) func_status ;; *) &quot;输入参数错误, Usage: `basename $0` start|stop|restart|status&quot; ;;esac]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>practice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell进阶之流程练习题]]></title>
    <url>%2Flinux%2F20170704-01-flow-practice%2F</url>
    <content type="text"><![CDATA[1) 添加10个用户user1-user10，密码为8位随机字符 2) 打印九九乘法表 Shell版本： 12345678#!/bin/env bashfor i in `seq 9`; do for j in `seq $i`; do let x=i*j echo -ne "$&#123;i&#125;x$&#123;j&#125;=$&#123;x&#125;\t" done echodone Python版本： 1234for i in range(1, 10): for j in range(1, i+1): print('&#123;&#125;x&#123;&#125;=&#123;&#125;\t'.format(j, i, i*j), end='') print() 3) 在/testdir目录下创建10个html文件,文件名格式为数字N（从1到10）加随机8个字母，如：1AbCdeFgH.html 1234mkdir /testdirfor i in &#123;1..10&#125;;do alpha=`cat /dev/urandom |tr -dc "a-zA-Z" |head -c 8` touch /testdir/$i$alpha.html 4) 打印等腰三角形 shell版本： 12345678910111213141516171819202122232425#!/bin/bashread -p &quot;请输入等腰三角形行数:&quot; line# 判断输入的是否是数字expr $line + 1 &amp;&gt;/dev/nullif [ $? != &quot;0&quot; ];then echo 输入错误，退出。 exitfi# 循环打印for cur_line in `seq $line`;do let space=line-cur_line let filler=2*cur_line-1 if [ $space == 0 ]; then printf %$&#123;filler&#125;s &quot; &quot; |tr &quot; &quot; &quot;*&quot; echo else printf %$&#123;space&#125;s &quot; &quot; printf %$&#123;filler&#125;s &quot; &quot; |tr &quot; &quot; &quot;*&quot; printf %$&#123;space&#125;s &quot; &quot; echo fidone Python版本： 1234567lines = input("Please iput the lines number:")if isinstance(lines,int): for line in range(1,lines+1): space=lines-line filler=2*line-1 print(" "*space+"*"*filler+" "*space) 4 后续六个字符串：efbaf275cd、4be9c40b8b、44b2395c46、f8c8873ce0、b902c16c8b、ad865d2f63是通过对随机数变量RANDOM随机执行命令：echo $RANDOM|md5sum|cut –c1-10后的结果，请破解这些字符串对应的RANDOM值。 12345678910111213141516171819#!/bin/bash# ┌───────────────────────────────────────────────────────┐# │Script Name | decrypt.sh ▮▮▮▮▮▮▮▮ │# │Date | 2017-07-04 ▮▮ │# │Author | Yu Longjun ▮▮▮▮▮▮▮▮▮▮▮▮ │# │Blog | http://www.yulongnjun.com ▮▮ │# │Version | 1.0 ▮▮▮▮▮ │# │Description | This is description. ▮▮ │# └───────────────────────────────────────────────────────┘encrypt_num_array=(efbaf275cd 4be9c40b8b 44b2395c46 f8c8873ce0 b902c16c8b ad865d2f63)for raw_num in `seq 0 65535`;do encrypt_num=`echo $raw_num | md5sum | cut -c 1-10` for num in $&#123;encrypt_num_array[*]&#125;; do if [ "$encrypt_num" == "$num" ];then echo "$num --&gt; $raw_num" fi donedone 5 编写脚本/root/bin/copycmd.sh(1) 提示用户输入一个可执行命令名称(2) 获取此命令所依赖到的所有库文件列表(3) 复制命令至某目标目录(例如/mnt/sysroot)下的对应路径下；如：/bin/bash ==&gt; /mnt/sysroot/bin/bash/usr/bin/passwd==&gt; /mnt/sysroot/usr/bin/passwd(4) 复制此命令依赖到的所有库文件至目标目录下的对应路径下：如：/lib64/ld-linux-x86-64.so.2 ==&gt; /mnt/sysroot/lib64/ld-linux-x86-64.so.2(5)每次复制完成一个命令后，不要退出，而是提示用户键入新的要复制的命令，并重复完成上述功能；直到用户输入quit退出 123456789101112131415161718192021222324#!/bin/bash# ┌───────────────────────────────────────────────────────┐# │Script Name | copycmd.sh ▮▮▮▮▮▮▮▮ │# │Date | 2017-07-04 ▮▮ │# │Author | Yu Longjun ▮▮▮▮▮▮▮▮▮▮▮▮ │# │Blog | http://www.yulongnjun.com ▮▮ │# │Version | 1.0 ▮▮▮▮▮ │# │Description | This is description. ▮▮ │# └───────────────────────────────────────────────────────┘while true;do read -p "请输入一个可执行的命令的名称(quit 退出):" command if [ "$command" == "quit" ]; then exit else cmd_path=`which $command` mkdir -p /mnt/sysroot$cmd_path cp $cmd_path /mnt/sysroot$cmd_path list=`ldd /bin/ls|grep -o "/lib.* "|tr -d " "` [ -e /mnt/sysroot/lib64 -a -e /mnt/sysroot/lib ] || mkdir /mnt/sysroot/&#123;lib64,lib&#125; for i in $list;do cp $i /mnt/sysroot$i done fidone]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>practice</tag>
        <tag>flow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Axure RP 8 安装与汉化]]></title>
    <url>%2Fsoftware%2F20170704-axure-rp8%2F</url>
    <content type="text"><![CDATA[Axure RP Pro 是专为 Rapid Prototype Design 而生，它可以辅助产品经理快速设计完整的产品原型，并结合批注、说明以及流程图、框架图等元素将产品完整地表述给各方面设计人员，如 UI、UE 等等，并在讨论中不断完善。 Axure RP 能帮助网站需求设计者，快捷而简便的创建基于网站构架图的带注释页面示 意图、操作流程图、以及交互设计，并可自动生成用于演示的网页文件和规格文件，以提供演示与开发。 Axure RP Pro 功能包括站点地图、流程设计、页面框架设计、简单交互设 计等，可以生成HTML、Word等格式。RP操作比用Dreamweaver简单多了，图片、文字、输入框等等所有组件全是可视化操作，可以很方便的实 现网站交互内容的原型设计，可将以前死板的页面版式全部替换为有点击、链接效果的网页!同时可以为网站设计提供AJAX式的交互处理，RP提供一种动态层 的组件，在同一页设定不同状态的效果，然后用链接、按钮等触发即可产生动态效果，这样网站设计就变得更加生动，意图也就更加直观。 下载链接:http://pan.baidu.com/s/1c25ZO7u 密码:61w2 包含内容： AxureRP-Setup.dmg是原版包，也可以去官网下载最新版。 sn.txt 是序列号 AxureRP8_CHSxxx.zip是汉化包 AxureRP_for_chromexxx.zip是chrome插件。 依次安装即可。]]></content>
      <categories>
        <category>software</category>
      </categories>
      <tags>
        <tag>Axure RP 8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS5、CentOS6启动流程]]></title>
    <url>%2Flinux%2F20170703-03-init%2F</url>
    <content type="text"><![CDATA[这三篇文章讲的都很好，可以看一下，这里就不写了（偷个懒(⊙﹏⊙)b） http://os.51cto.com/art/201407/446819.htm http://www.mamicode.com/info-detail-1165638.html http://www.cnblogs.com/gbk66/p/5900964.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>init</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell进阶--函数]]></title>
    <url>%2Flinux%2F20170703-02-function%2F</url>
    <content type="text"><![CDATA[系统里的函数declare -f 查看系统里的函数 declare -f |grep &quot;.* ()&quot;|tr -d &quot; ()&quot;：用grep可以过滤出函数名 declare -f 函数名 ：查看特定的函数 unset 函数名 ：和变量一样的取消一样，可以unset取消函数 自定义函数语法一： 123function f_name &#123; 函数体&#125; 语法二： 123function f_name () &#123; 函数体&#125; 语法三(最常见，记住这个用法就行）： 123f_name () &#123; 函数体&#125; 函数调用函数的调用： 可在交互式环境下定义函数，即直接在bash命令行里写函数和调用函数。 可将函数放在脚本文件中作为它的一部分，即在同一个脚本文件里调用函数。 可放在只包含函数的单独文件中，即其他shell脚本可以调用这个文件，用. path/to/functions_file或者source path/to/functions_file 函数调用很简单：无参数：function_name有参数：functions_name arg1 arg2 ... argN functions_name为函数名，arg为argument（参数）的意思。在函数体中当中，可使用$1, $2调用这些参数；还 可以使用$@, $*, $#等特殊变量 函数返回值函数有两种返回值： 函数的执行结果返回值： (1) 使用echo等命令进行输出 (2) 函数体中调用命令的输出结果 函数的退出状态码： (1) 默认取决于函数中执行的最后一条命令的退出状态码 (2) 自定义退出状态码，其格式为： return 从函数中返回，用最后状态命令决定返回值。 return 0 无错误返回。 return 1-255 有错误返回。 函数中的局部变量当函数中有变量的时候，和函数体外的变量容易冲突，这时候可以用局部变量 在函数中定义局部变量的方法：local NAME=VALUE 递归函数函数直接或间接调用函数自身。 阶乘： 123456789#!/bin/bashfact() &#123; if [ $1 -eq 0 -o $1 -eq 1 ]; then echo 1 else echo $[$1*$(fact $[$1-1])] fi&#125;fact $1 fork炸弹： 123:()&#123; :|:&amp; &#125;;:# 冒号在这里其实就是函数名，换成另外的单词你就理解了：bomb() &#123; bomb | bomb &amp; &#125;; bomb 菲波那切数列： 123456789101112131415161718read -p "请输入数列个数：" nfib()&#123; if [ $1 -eq 0 ] ; then ret=0 echo $ret elif [ $1 -eq 1 ]; then ret=1 echo $ret elif [ $1 -gt 1 ]; then let ret=$(fib $[$1-1])+$(fib $[$1-2]) echo $ret fi&#125;for i in `seq $n`; do fib $idone 环境函数(全局函数）export -f func或者declare -fx func declare中，-f的意思是function，-x的意思是export]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[shell进阶--流程]]></title>
    <url>%2Flinux%2F20170703-01-flow%2F</url>
    <content type="text"><![CDATA[由于条件判断和循环跟其他语言都大同小异，学过编程的话很好理解，这里只贴出格式，不具体写用法了。（select菜单会详细讲一下） 条件判断if条件判断普通if条件判断： 123456789if 判断条件1; then 条件为真的分支代码elif 判断条件2; then 条件为真的分支代码elif 判断条件3; then 条件为真的分支代码else 以上条件都为假的分支代码fi 嵌套if条件判断： 12345678910111213141516171819202122232425if 判断条件1; then 条件为真的分支代码else if 判断条件2; then 条件为真的分支代码 else 条件为真的分支代码 fifi``` ### case条件判断```bashcase 变量引用 inPAT1) 分支1 ;;PAT2) 分支2 ;;#...省略*) 默认分支esac 循环for循环普通for循环： 123for 变量名 in 列表;do 循环体done 嵌套for循环： 123456for 变量名1 in 列表1;do 循环体1 for 变量名2 in 列表2;do 循环体2 donedone while循环123while CONDITION; do 循环体done until循环123until CONDITION; do 循环体done 循环中使用continue和breakcontinue 结束本次循环，还会进入下一轮循环break 结束全部循环，不会进入下一轮循环 循环工中使用shift跳过参数列表中的某项用于处理参数不确定的情况，shift比较好用 while循环的特殊用法（遍历文件的每一行）123while read line; do循环体done &lt; /PATH/FROM/SOMEFILE select 菜单 select 循环主要用于创建菜单，按数字顺序排列的菜单项将显示在标准错误上，并显示 PS3 提示符，等待用户输入。 用户输入菜单列表中的某个数字，执行相应的命令 用户输入被保存在内置变量 REPLY 中。 可以和case结合使用。 下面举个和select和case结合使用的例子：1234567891011121314151617181920212223242526PS3="Please choose your food(Input No.): "select food in "exit" "huimian" "juejiangmian" "laomo" "yangroutang"do case $food in "exit") echo Your choice is $REPLY echo "Thanks!" exit ;; "huimian"|"juejiangmian") echo Your choice is $REPLY echo "12 yuan" ;; "laomo") echo Your choice is $REPLY echo "15 yuan" ;; "yangroutang") echo Your choice is $REPLY echo "20 yuan" ;; *) echo "Dont's have this food" ;; esacdone trap 信号捕捉 trap &#39;触发指令&#39; 信号 ：自定义进程收到系统发出的指定信号后，将执行触发指令 ，而不会执行原操作 trap &#39;&#39; 信号 ：信号忽略信号的操作 trap &#39;-&#39; 信号 ：恢复原信号的操作 trap -p：列出自定义信号操作]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>if elif else</tag>
        <tag>for</tag>
        <tag>case</tag>
        <tag>while</tag>
        <tag>until</tag>
        <tag>select</tag>
        <tag>trap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计划任务at、cron]]></title>
    <url>%2Flinux%2F20170630-02-at-cron%2F</url>
    <content type="text"><![CDATA[at 未来的某时间点执行一次任务 batch 系统自行选择空闲时间去执行此处指定（用的比较少） cron 周期性运行某任务 at系统中的服务名叫atd at [option] TIME [option] at -l 显示计划任务at -c NO: 按编号（NO）查看具体作业任务at -d NO按编号（NO）删除计划任务-f /path/from/somefile：从指定的文件中读取任务-m:当任务被完成之后，将给用户发送邮件，即使没有标准输出 TIME定义出什么时候进行 at 这项任务的时间 HH:MM [YYYY-mm-dd] noon, midnight, teatime（4pm） tomorrow now+#{minutes,hours,days, OR weeks} HH:MM: 在今日的 HH:MM 进行，若该时刻已过，则明天此时执行任务 1at 02:00 HH:MM YYYY-MM-DD: 规定在某年某月的某一天的特殊时刻进行该项任务 1at 02:00 2016-09-20 HH:MM[am|pm] [Month] [Date] 12at 04pm March 17at 17:20 tomorrow HH:MM[am|pm] + number [minutes|hours|days|weeks]: 在某个时间点再加几个时间后才进行该项任务 12at now + 5 minutesat 02pm + 3 days 查看生成的计划任务文件：ls /var/spool/at 执行at命令的用户的黑白名单：/etc/at.{allow,deny} 白名单：/etc/at.allow 默认这个文件不存在，只有该文件中的用户才能执行at命令。白名单优先级高，有了白名单，黑名单不生效了（同一个用户又在白名单又在黑名单，只生效白名单，即允许此用户） 黑名单：/etc/at.deny 默认存在，拒绝该文件中用户执行at命令，而没有在at.deny文件中的使用者则可执行。 如果两个文件都不存在，只有 root 可以执行at命令 cron安装包叫cronie 默认一般安装并启用服务了,服务名叫crond： 可以查看服务状态，如果没启动，start启动一下：CentOS 6: service crond statusCentOS 7: systemctl status crond 计划周期性执行的任务提交给crond，到指定时间会自动运行。 /etc/crontab文件系统cron任务：系统维护作业。 x：特定值x。给定时间点有效取值范围内的值* ：每{分钟，小时，日，月等}。给定时间点上有效取值范围内的所有值x,y ：x或y。离散取值。x-y ：x到y。连续取值。/x ：每过多少{分钟，小时，日，月等}执行。在指定时间范围上，定义步长。 简写 意义 @reboot Run once after reboot. @yearly 0 0 1 1 * @annually 0 0 1 1 * @monthly 0 0 1 @weekly 0 0 0 @daily 0 0 * @hourly 0 示例：每3小时echo和wall命令 10 */3 * * * centos /bin/echo “howdy”;/usr/bin/wall “welcome to Magedu!” 用户cron任务：crontab命令 日志：/var/log/cron crontab命令用户cron：crontab命令定义，每个用户都有专用的cron任务文件： /var/spool/cron/USERNAME 命令用法： crontab [-u user] [-l | -r | -e] [-i] -l: 列出所有任务 -e: 编辑任务 -r: 移除所有任务 -i：同-r一同使用，以交互式模式移除指定任务 -u USERNAME: 仅root可运行-u选项，管理某个用户的cron任务。 /etc/cron.{allow,deny}控制计划任务的用户黑白名单。 规则同前面讲到的at.{allow,deny} Notes: 运行结果的标准输出和错误以邮件通知给相关用户。 (1) COMMAND &gt; /dev/null (2) COMMAND &amp;&gt; /dev/null 对于cron任务来讲，%有特殊用途；如果在命令中要使用%， 则需要转义，将%放置于单引号中，则可不用转义 anacron个人电脑用的比较多，时间段属于人的作息时间段。 安装包叫cronie-annacron]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>bonding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kill命令]]></title>
    <url>%2Flinux%2F20170630-01-kill%2F</url>
    <content type="text"><![CDATA[用法：，kill -SIGNAL pid SIGNAL是信号，kill -l可以查看支持的信号。 下面是系统支持的信号，用man 7 signal可以查询到： 常用信号： 1) SIGHUP: 无须关闭进程而让其重读配置文件2) SIGINT: 中止正在运行的进程；相当于Ctrl+c3) SIGQUIT:相当于ctrl+\9) SIGKILL: 强制杀死正在运行的进程15) SIGTERM：终止正在运行的进程18) SIGCONT：继续运行19) SIGSTOP：后台休眠 例子 123kill -9 4365kill -19 4444 # 让4444休眠kil -18 4444 # 让4444继续运行 pkill-SIGNAL：指定信号 -u uid: effective user，生效者 -U uid: real user，真正发起运行命令者 -t terminal: 与指定终端相关的进程 -l: 显示进程名（pgrep可用） -a: 显示完整格式的进程名（pgrep可用） -P pid: 显示指定进程的子进程 例子：pkill -t dev/tty3 运行状态前台运行后台运行后台休眠 kill -19 xxx 后台休眠kill -18 xxx 后台继续运行bg 1 后台运行（在同一个终端窗口执行）fg 1 前台运行（在同一个终端窗口执行） 可以让程序不依赖终端运行的两种方法： nohup COMMAND &amp;&gt;/dev/null &amp; screen;COMMAND 同时运行多个进程，提高效率 方法1 vim all.sh 123f1.sh&amp;f2.sh&amp;f3.sh&amp; 方法2 (f1.sh&amp;);(f2.sh&amp;);(f3.sh&amp;) 方法3 { f1.sh&amp; f2.sh&amp; f3.sh&amp; }]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>kill</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[进程]]></title>
    <url>%2Flinux%2F20170628-02-process%2F</url>
    <content type="text"><![CDATA[优先级： 命令pstree-p 把各个子线程都详细显示出来 ps 进程状态（process state）UNIX风格：ps -efBSD风格：ps aux 还有用到o参数，选项显示定制的信息：pid、comm、%cpu、%mem、state、tty、euser、ruser、psr psr:用的哪颗cpu ps axo pid,tid,class,rtprio,ni,pri,psr,pcpu,stat,comm ps -eo stat,euid,ruid,tty,tpgid,sess,pgrp,ppid,pid,pcpu,comm ni: nice值 pri: priority 优先级(数越大优先级越高，跟前面图里的system优先级反过来） psr: processor CPU编号 rtprio: 实时优先级 示例： ps -C PROCESSNAME单独显示某个进程，可以加o参数扩展。 STAT：进程状态 R：running S: interruptable sleeping D: uninterruptable sleeping T: stopped Z: zombie +: 前台进程 l: 多线程进程 L：内存分页并带锁 N：低优先级进程 &lt;: 高优先级进程 s: session leader，会话（子进程）发起者 nice 调整优先级运行某个命令，指定优先级：nice -n -XX COMMAND :xx范围是-20~19 在运行中的程序，修改优先级：renice -n -XX PID:xx范围是-20~19 pgrep 搜索进程信息-u uid: effective user，生效者 -U uid: real user，真正发起运行命令者 -t terminal: 与指定终端相关的进程 -l: 显示进程名 -a: 显示完整格式的进程名 -P pid: 显示指定进程的子进程 pidof按确切的程序名称显示pid命令绝对路径：/sbin/pidof用法：pidof COMMAND 12pidof httpdecho $? # 将来在脚本里可以判断进程是否存在 uptime显示启动状态显示当前时间，系统已启动的时间、当前上线人数，系统平 均负载（1、5、10分钟的平均负载，一般不会超过1） 12root@centos6 ~]# uptime 17:18:44 up 11:40, 2 users, load average: 0.00, 0.00, 0.00 系统平均负载: 指在特定时间间隔内运行队列中的平均进程数。 通常每个CPU内核的当前活动进程数不大于3，那么系统的性能良好。如果每个CPU内核的任务数大于5，那么此主机的性能有严重问题。 如果linux主机是1个双核CPU，当Load Average为6的时候说明机器已经被充分使用。 top显示进程使用系统资源情况 第一行：相当于uptime命令第二行：进程信息第三行：CPU使用情况第四行：内存使用情况第五行：交换分区swap使用情况 cpu那行： us：user, 用户空间占用 sy：system, 内核空间占用 ni：调整nice时间 id：idle, 空闲 wa：wait, 等待IO时间 hi：hard interupt, 硬中断 si：soft interupt, 软中断（模式切换） st：stole, 虚拟机偷走的时间 排序在打开top后，可以按快捷键进行自定义排序： P：以占据的CPU百分比,%CPU M：占据内存百分比,%MEM T：累积占据CPU时长,TIME+ 首部信息是否显示： uptime信息：l（字母l） tasks及cpu信息：t cpu分别显示：1（数字1） memory信息：m 退出命令：q 修改刷新时间间隔：s 终止指定进程：k 保存文件：W vmstat虚拟内存信息用法示例：vmstat 2 5: 每2秒刷一次，刷5次选项：-s: 显示内存的统计数据 显示中的各项含义： procs: r：可运行（正运行或等待运行）进程的个数，和核心数有关 b：处于不可中断睡眠态的进程个数(被阻塞的队列的长度) memory： swpd: 交换内存的使用总量 free：空闲物理内存总量 buffer：用于buffer的内存总量 cache：用于cache的内存总量 swap: si：从磁盘交换进内存的数据速率(kb/s) so：从内存交换至磁盘的数据速率(kb/s) io： bi：从块设备读入数据到系统的速率(kb/s) bo: 保存数据至块设备的速率 system： in: interrupts 中断速率，包括时钟 cs: context switch 进程切换速率 cpu： us:Time spent running non-kernel code sy: Time spent running kernel code id: Time spent idle. Linux 2.5.41前,包括IO-wait time. wa: Time spent waiting for IO.2.5.41前，包括in idle. st: Time stolen from a virtual machine.2.6.11前, unknown. iostat CPU和磁盘IO`iostat 1 10` 每1秒刷新一次，刷新10次 pmap 进程对应的内存映射pmap [options] pid [...] -x: 显示详细格式的信息 glances 监控其他机器C/S模式下运行glances命令 服务器模式： glances -s -B IPADDR IPADDR: 指明监听的本机哪个地址 客户端模式： glances -c IPADDR IPADDR：要连入的服务器端地址 dstat默认采用-cdngy参数 -c: 显示cpu相关信息 -C #,#,…,total -d: 显示disk相关信息 -D total,sda,sdb,… -g：显示page相关统计数据(交换分区） -m: 显示memory相关统计数据 -n: 显示network相关统计数据 -p: 显示process相关统计数据 -r: 显示io请求相关的统计数据 -s: 显示swapped相关的统计数据]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>bonding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网卡绑定(bonding)]]></title>
    <url>%2Flinux%2F20170626-05-bonding%2F</url>
    <content type="text"><![CDATA[就是将多块网卡绑定同一IP地址对外提供服务，可以实现高 可用或者负载均衡。当然，直接给两块网卡设置同一IP地址 是不可能的。通过bonding，虚拟一块网卡对外提供连接， 物理网卡的被修改为相同的MAC地址。 Mode 0 (balance-rr) 轮转（Round-robin）策略：从头到尾顺序的在每一个slave 接口上面发送数据包。本模式提供负载均衡和容错的能力 Mode 1 (active-backup) 活动-备份（主备）策略：只有一个slave被激活，当且仅当活动 的slave接口失败时才会激活其他slave。为了避免交换机发生混 乱此时绑定的MAC地址只有一个外部端口上可见 Mode 3 (broadcast) 广播策略：在所有的slave接口上传送所有的报文,提供容错能力 active-backup、balance-tlb 和 balance-alb 模式不需要 交换机的任何特殊配置。其他绑定模式需要配置交换机以便 整合链接。如：Cisco 交换机需要在模式 0、2 和 3 中使用 EtherChannel，但在模式4中需要 LACP和 EtherChannel 配置方法： 新建文件/etc/sysconfig/network-scripts/ifcfg-bond0： 123456789DEVICE=bond0BONDING_OPTS=&quot;miimon=100 mode=0&quot;BOOTPROTO=noneIPADDR=PREFIX=GATEWAY=DNS1=DNS2=DOMAIN= /etc/sysconfig/network-scripts/ifcfg-eth0 12345DEVICE=eth0BOOTPROTO=noneMASTER=bond0SLAVE=yesUSERCTL=no 同样的，也要修改eth1的ifcfg文件。 查看bond0状态：/proc/net/bonding/bond0 各种bond模式详细说明：详细帮助： /usr/share/doc/kernel-doc-version/Documentation/networking/bonding.txt https://www.kernel.org/doc/Documentation/networking/bonding.txt teamteam网络组：是将多个网卡聚合在一起方法，从而实现冗错和提 高吞吐量 网络组不同于旧版中bonding技术，提供更好的性能和扩展性 网络组由内核驱动和teamd守护进程实现. 多种方式runner broadcast roundrobin activebackup loadbalance lacp (implements the 802.3ad Link Aggregation Control Protocol) 命令格式： 创建team： nmcli con add type team con-name CNAME ifname INAME [config JSON] CNAME 连接名，INAME 接口名 JSON 指定runner方式 格式：&#39;{&quot;runner&quot;: {&quot;name&quot;: &quot;METHOD&quot;}}&#39; METHOD 可以是broadcast, roundrobin,activebackup, loadbalance, lacp 添加从属接口： nmcli con add type team-slave con-name CNAME ifname INAME master TEAM CNAME 连接名 INAME 网络接口名 TEAM 网络组接口名连接名若不指定，默认为team-slave-IFACE 查看team状态 1teamdctl team0 state down team 1nmcli connect down team0 up team 123nmcli connect up team0nmcli conect up team-slave-eth0nmcli connect up team-slave-eth1 down掉team，连slave也down掉。up的时候，只启动team，要手动启动各个slave。 实验： 1234567891011121314151617nmcli con add type team con-name team0 ifname team0 config ‘&#123;&quot;runner&quot;: &#123;&quot;name&quot;: &quot;loadbalance&quot;&#125;&#125;&apos;nmcli con mod team0 ipv4.addresses 192.168.1.100/24nmcli con mod team0 ipv4.method manualnmcli con add con-name team0-eth1 type team-slave ifname eth1 master team0nmcli con add con-name team0-eth2 type team-slave ifname eth2 master team0nmcli con up team0nmcli con up team0-eth1nmcli con up team0-eth2teamdctl team0 state; # nmcli dev dis eth1]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>bonding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS7网络配置]]></title>
    <url>%2Flinux%2F20170626-04-network-config-centos7%2F</url>
    <content type="text"><![CDATA[把网卡命名方式改为传统eth方式： vim /boot/grub2/grub.cfg linux16那行，最后添加net.ifnames=0 hostnamectlhostnamectl statushostnamectl set-hostname xxxxx：更改主机名，同时写到/etc/hostname配置文件里。 tips：记得也改下/etc/hosts里，最后加上新加的主机名 nmcli修改连接名 nmcli connection modify enp0s25 connection.id eth0 添加一条新的配置 nmcli connection add con-name eth1-lan type ethernet ifname eth1 nmcli connection modify eth0 xxxx.xxxx xxxx xxxx.xxxx xxxx指的是：connection.autoconnect yesipv4.method manual/autoipv4.addresses xxx.xxx.xxx.xxxipv4.gateway xxx.xxx.xxx.xxx 重启连接：nmcli connection reload 还有其他的： 对应关系： 去掉ip连接，不down设备：nmcli dev disconnect nmtui字符界面： 编辑连接、激活连接、设置主机名 bondbond也可以用bond来作 添加bonding接口 1nmcli con add type bond con-name mybond0 ifname mybond0 mode active-backup 添加从属接口 123nmcli con add type bond-slave ifname ens7 master mybond0nmcli con add type bond-slave ifname ens3 master mybond0 注：如无为从属接口提供连接名，则该名称是接口名称加类型构成 要启动绑定，则必须首先启动从属接口 123nmcli con up bond-slave-ens7nmcli con up bond-slave-ens3 启动绑定 1nmcli con up mybond0 down掉,并删除bond 12nmcli connect down mybond0nmcli connect del mybond0]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>bonding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS6网络配置]]></title>
    <url>%2Flinux%2F20170626-03-network-config-centos6%2F</url>
    <content type="text"><![CDATA[/etc/sysconfig/network-scripts/ifcfg-xxxip、mask、gateway、dns相关配置文件 1234567891011NAME=xxxDEVICE=XXXONBOOT=yesBOOTPROTO=dhcp/none #dhcp还是静态，不写就是手工指定。IPADDR=xx.xx.xx.xx # ip地址PREFIX=xx # 掩码，新格式# NETMASK=xxx.xxx.xxx.xxx # 掩码，旧格式DNS1=xxx.xxx.xxx.xxxDNS2=xxx.xxx.xxx.xxxHWADDR=xx:xx:xx:xx:xx:xx # 硬件MAC地址MACADDR=yy:yy:yy:yy:yy:yy # 手动修改MAC地址，要用MACADDR，而不能用HWADDR 例子： 123456DEVICE=eth0NAME=eth0IPADDR=192.168.100.100PREFIX=24GATEWAY=192.168.100.254DNS1=223.5.5.5 NetworkManager服务会在你修改完文件后，自动生成 /etc/resolv.conf，这个就是DNS地址。 /etc/resolv.confDNS和DOMAIN相关的配置文件。通常不用管，会自动生成。 DOMAIN是搜索域DOMAIN yulongjun.com ping www就是ping www.yulongjun.com /etc/sysconfig/network主机名配置文件 /etc/hosts本地的域名解析，优先解析hosts里定义的域名，然后没有才会去DNS里查。 tips：如果要修改顺序，可以更改/etc/nsswitch文件里的内容：hosts: file dns –&gt; hosts: dns file可以先使用dns，后使用/etc/hosts文件。 vim ifcfg-IFNAME:xxx给网卡设置另外一个地址，指定label设备名为网卡名:xxx 1234DEVICE=eth1:officeBOOTPROTO=noneIPADDR=5.5.5.5PREFIX=24 在实现一个网卡获取两个ip地址，一个dhcp自动获取，一个固定ip的时候，要注意，dhcp要先配置，然后新增的设为固定ip。 例如更改ifcfg-eth1里面的参数为dhcp模式，ifcfg-eth1:2里的参数为固定ip模式。 如果顺序反过来，会有问题。 /etc/sysconfig/network-scripts/route-IFNAME路由配置文件，永久生效。需要重启网络服务后才能生效。 两种写法： TARGET via GW如：10.0.0.0/8 via 172.16.0.1 每三行定义一条路由： 123ADDRESSXXX=TARGETNETMASKXXX=maskGATEWAYXXX=GW XXX是数字，一般从1开始写]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS6网络命令]]></title>
    <url>%2Flinux%2F20170626-02-network-commands-centos6%2F</url>
    <content type="text"><![CDATA[ifconfig route netstat ss setup ip {link, addr, route} ifconfig例子： 123ifconfig # 查看所有网卡信息ifconfig eth1 #查看eth1的网卡信息ifconfig eth1 172.17.111.100/16 # 临时设置ip routeroute -n : 查看路由route add [-net|-host] target [netmask Nm] [gw Gw][[dev] Interface] 例如： 1234# 主机route add -host 192.168.1.3 gw 172.16.0.1 dev eth0# 网段route add -net 192.168.0.0/24 gw 172.16.0.1 dev eth0 添加默认路由，默认路由即默认网关： 1234# 第一种写法：route add -net 0.0.0.0 netmask 0.0.0.0 gw 172.16.0.1# 简单写法：route add default gw 172.16.0.1 删除路由： 12route del -host 192.168.1.3route del -net 192.168.0.0/24 netstat常用参数： -t: tcp协议相关 -u: udp协议相关 -a: 所有状态 -n: 以数字显示IP和端口 -l: 处于监听状态 -p: 显示相关进程及PID 显示接口统计数据： netstat -i: 显示所有接口统计数据netstat –I=IFACE或者netstat -Ixxxx: 显示单个接口的统计数据 1watch -n1 netstat -Ieth0 ss新的ss效率更高，用法和netstat差不多。 有一些高级用法： 123ss -o state established &apos;( dport = :ssh or sport = :ssh )&apos; # 显示所有已建立的ssh连接ss -o state established &apos;( dport = :http or sport = :http )&apos; # 显示所有已建立的HTTP连接 setupsetup可以打开TUI（Text-based User Interface）基于文本的交互式界面。可以配置网络（Network configuration），配置防火墙(Firewall configuration)，启用关闭系统服务（System services）等等。 ip以前用的不多的新命令，很好很强大的命令。主要用到3个子命令 ip link、ip addr 、ip route ip linkip link show [IFNAME] ：显示网卡mac信息（link，链路层），不加网卡名就是全部，加上就是显示单个网卡的mac信息。ip link set up/down：设置网卡启用或关闭，在物理层禁止。 ip addrip addr show [IFNAME]：显示网卡ip信息，不加网卡名就是全部，加上就是显示单个网卡的ip信息。ip addr add xxx.xxx.xxx.xxx/xx dev IFNAME label IFNAME:LABELNAME：临时添加ip。 1ip addr add 192.168.100.100/24 dev eth1 label eth1:tmp-net ip addr del xxx.xxx.xxx.xxx/xx dev IFNAME： 1ip addr del 192.168.100.100/24 dev eth1 ip addr flush IFNAME：清楚网卡上所有ip地址，连非临时的都删掉。 1ip addr flush eth1 tips: 重启机器或者网络服务，所有临时添加的ip会消失。 ip routeip route add TARGET via gw 添加路由 12ip route add 192.168.0.0/24 via 172.16.0.1ip route add 192.168.1.13 via 172.16.0.1 ip route delete TARGET 删除路由 12ip route delete 192.168.0.0/24ip route delete 192.168.1.13 ip route show|list 显示路由ip route flush dev IFACE清空路由表 1ip route flush dev eth1]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[克隆虚拟机后，需要对克隆的虚拟机做的相关操作]]></title>
    <url>%2Flinux%2F20170626-01-clone-vm%2F</url>
    <content type="text"><![CDATA[模拟实验机器相关配置： 操作系统：CentOS6.9 两块网卡eth0 和eth1 1） 用自带的克隆虚机功能 启动两台机器，我们ifconfig看到两块网卡的名称、ip地址、mac地址都变了。 然后看到网卡配置文件里面也是旧的文件，还有一个重要的文件/etc//etc/udev/rules.d/70-persistent-net.rules也是错的： 我们先把/etc/udev/rules.d/70-persistent-net.rules下的前面两条注释掉，然后修改第三条第四条的eth2和eth3为eth0和eth1。 然后修改/etc/sysconfig/network-scripts/下的ifcfg-eth0 和ifcfg-eth1,把文件里的UUID和HWADDR给删掉，PART如果是DHCP的，就不用改，如果原来是固定IP的，把IP地址改一下。 可以重启电脑，如果不想重启电脑，可以用卸载然后重新加载网卡驱动模块的方法来使之生效： 我们用ethtool -i eth2可以看到网卡驱动型号： 然后我们可以先卸载，然后装载驱动: 123456789101112# 卸载网卡驱动：modprobe -r e1000#或者 `rmmod e1000`也可以# 装载网卡驱动：modprobe e1000service network restartservice NetworkManager restart 这时候发现，有一个网卡获取不到ip 这时候笔者更改了网卡的默认名字system eth0为eth0（同样的system eth1也改）,这时候再次重启NetworkManager服务，正常获取到了ip。这里猜测NetworkManager某些地方有bug（6里的NetworkManager真是个坑啊！），在名字方面不能跟原来的相同。 至此，克隆的CentOS6完全可用。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络知识(看书，略）]]></title>
    <url>%2Flinux%2F20170623-network-introduction%2F</url>
    <content type="text"><![CDATA[看《趣学CCNA》前5章内容，即可了解，这里略过。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络模型]]></title>
    <url>%2Flinux%2F20170621-network-model%2F</url>
    <content type="text"><![CDATA[OSI七层模型（理论） TCP/IP模型（现实）]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>network</tag>
        <tag>OSI</tag>
        <tag>TCP/IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑卷管理（LVM）]]></title>
    <url>%2Flinux%2F20170619-lvm%2F</url>
    <content type="text"><![CDATA[LVM：Logical Volume Management 逻辑卷管理 LVM是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。 传统磁盘管理：我们上层是直接访问文件系统，从而对底层的物理磁盘进行读取。 lvm工作原理：对底层的磁盘进行封装，以逻辑卷（logical volume）的方式呈现给上层应用，逻辑卷里可以方便的添加删除分区和硬盘。 基本的LVM术语概念：PV（Physical Volume）- 物理卷 物理卷在逻辑卷管理中处于最底层，它可以是实际物理硬盘上的分区（例如sdb1），也可以是整个物理硬盘（例如sdc），也可以是raid设备（例如md0）。 VG（Volume Group）- 卷组 卷组建立在物理卷之上，一个卷组中至少要包括一个物理卷，在卷组建立之后可动态添加物理卷到卷组中。一个逻辑卷管理系统工程中可以只有一个卷组，也可以拥有多个卷组。 LV（Logical Volume）- 逻辑卷 逻辑卷建立在卷组之上，卷组中的未分配空间可以用于建立新的逻辑卷，逻辑卷建立后可以动态地扩展和缩小空间。系统中的多个逻辑卷可以属于同一个卷组，也可以属于不同的多个卷组。 PE（Physical Extent）- 物理块 PE是整个LVM 最小的储存区块，也就是说，其实我们的资料都是由写入PE来处理的。简单的说，这个PE就有点像文件系统里面的block大小。 创建PV、VG、LV（增与查）0. 准备工作：在上述创建的那个虚机上增加两块硬盘，各200G。重启虚机，会识别为sdb和sdc，把sdc分区为3个分区：sdc1，sdc2，sdc3。 123fdisk /dev/sdcn来创建3个磁盘。t更改格式为8e（Linux LVM格式） 1. 创建PV（pvcreate）： 命令格式为：pvcreate /dev/DEVICE1 /dev/DEVICE2 /dev/DEVICE3DEVICE可以是sda，sdb，sdc这样的硬盘，也可以是sda1 ，sdb2，sdc3这样的分区，甚至是md0，md1这样的软raid。 实验操作： 1234pvcreate /dev/sdb /dev/sdc1 /dev/sdc2pvs # 简要显示pv组成部分pvdisplay #详细的显示pv的组成部分 2. 创建vg（vgcreate）: 命令格式为：vgcreate volume_group_name /dev/DEVICE1 /dev/DEVICE2 ... /dev/DEVICEN 实验操作： 123vgcreate vg2 /dev/sdc1 /dev/sdc2 /dev/sdc3vgdisplay vg2 #显示创建的vg2的信息 3. 创建lv（lvcreate） 命令格式为：lvcreate -L 大小 -n 名称 vg2参数L为LogicalVolumeSize的意思，大小的单位可为K，M，G，T，P，E参数n的意思是name 实验操作： vg2下创建lv_test1, lv_test2, lv_test33个lv，大小分别为10G，20G，30G 123456789101112131415161718# 创建3个lvlvcreate -L 10G -n lv_test1 vg2lvcreate -L 20G -n lv_test2 vg2lvcreate -L 30G -n lv_test3 vg2lvs # 简要的显示lv信息lvdisplay # 详细的显示lv信息# 格式化lvmkfs.ext4 /dev/vg2/lv_test&#123;1,2,3&#125; #创建目录，作为挂在目录mkdir test&#123;1,2,3&#125; # 挂载mount /dev/vg2/lv_test1 /test1mount /dev/vg2/lv_test2 /test2mount /dev/vg2/lv_test3 /test3 修改分区（改）1. lvextend、lvreduce、lvresize1.1 lvextend：扩展lv用法： 123lvextend -L [+][mMgGtT] /dev/VG_NAME/LV_NAMEresize2fs /dev/VG_NAME/LV_NAME 也可以使用 -l +100%free，可以划分剩下的全部的lv。 可以不用resize2fs，可以在命令后面加参数-r或--resizefs可以自动更新。 12345678910lvrextend -r -l +100%free /dev/VG_NAME/LV_NAME``` #### 1.2 `lvreduce`：减小lv用法：```bashlvreduce -L [-][mMgGtT] /dev/VG_NAME/LV_NAMEresize2fs /dev/VG_NAME/LV_NAME 可以不用resize2fs，可以在命令后面加参数-r或--resizefs可以自动更新。 123456789101112lvrextend -r -L -20G /dev/vg2/lv_test2``` #### 1.3 `lvresize`：修改lv大小用法：```bashlvresize [[+|-]大小 /dev/VGNAME/LVNAMEresize2fs /dev/VGNAME/LVNAME +为增加的意思，-为减少，省略掉即为直接设置大小。 resize2fs写入到内存，使硬盘信息和内存信息保持一致。 可以不用resize2fs，可以在命令后面加参数-r或--resizefs可以自动更新。 1234567891011# 先备份数据# 卸载逻辑卷umount /dev/vg2/lv_test3lvresize -L -5G /dev/vg2/lv_test3e2fsck -f /dev/vg2/lv_test3resize2fs /dev/vg2/lv_test3lvdisplay 2. vgextend、vgreduce2.1 vgextend：扩展vg12345pvcreate /dev/sdc3vgextend vg2 /dev/sdc3vgdisplay 2.2 vgreduce：缩减vg123vgreduce vg2 /dev/sdc3vgdisplay 3. lvrename、vgrename3.1 lvrename：修改lv名lvrename oldname newname 3.2 vgrename：修改vg名vgrename oldname newname 删除lv，vg，pv（删）lvremove、vgremove、pvremove12345lvremove /dev/vg2/lv_test2vgremove /dev/vg2pvremove /dev/sdb 图形界面操作感兴趣的同学可以试试。 运行命令system-config-lvm]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HPE x86服务器硬RAID做法]]></title>
    <url>%2Fserver%2F20170616-02-server-hard-raid%2F</url>
    <content type="text"><![CDATA[由于现在服务器普遍采用UEFI模式，这种模式更先进，支持的硬盘容量也更大（可以支持大于2TB的硬盘），所以老的Legency BIOS模式（不支持2TB以上硬盘）已经开始逐渐弃用。新的服务器基本默认全是UEFI模式，可以直接使用。 tips: UEFI模式下，默认用的GPT分区方法，所以在Linux下，不会只有3个主分区一个扩展主分区，而是最多支持128个主分区。 1) 开机启动，按F10进入硬件Intelligent Provisioning（简称IP） 2) 选择 HPE Smart Storage Administrator（HPE只能存储管理） 3) 进入下面界面后，点击左边的磁盘阵列卡（Smart Array Pxxx）,然后点配置(Congifure) 4) 如果是之前有阵列，如下图依次点击，Delete Array（删除阵列） 5) 然后重新Create Array（创建阵列） 6) 此处，选择Sort By Location(按位置排序），然后Select All(选择全部） 7) 磁盘RAID level，选择RAID 10(磁盘容量=n/2) 或者RAID 5（磁盘容量=n-1）。如果是生产环境，选择前者，测试环境可以选择后者。后面全部默认，然后点Create Logical Drive） tips: 关于RAID知识可以看这里 8) 创建完毕可以关闭界面 9) 重启]]></content>
      <categories>
        <category>server</category>
      </categories>
      <tags>
        <tag>HPE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘阵列（RAID）]]></title>
    <url>%2Flinux%2F20170616-01-raid%2F</url>
    <content type="text"><![CDATA[独立硬盘冗余阵列（RAID, Redundant Array of Independent Disks），旧称廉价磁盘冗余阵列（Redundant Array of Inexpensive Disks），简称磁盘阵列。其基本思想就是把多个相对便宜的硬盘组合起来，成为一个硬盘阵列组，使性能达到甚至超过一个价格昂贵、容量巨大的硬盘。根据选择的版本不同，RAID比单颗硬盘有以下一个或多个方面的好处：增强数据集成度，增强容错功能，增加处理量或容量。另外，磁盘阵列对于电脑来说，看起来就像一个单独的硬盘或逻辑存储单元。——维基百科 目前常用的有： 标准RAID RAID 0 RAID 1 RAID 5 RAID 6 混合RAID RAID 10 RAID 50 RAID 60 RAID 0 RAID 0亦称为带区集。它将两个以上的磁盘并联起来，成为一个大容量的磁盘。在存放数据时，分段后分散存储在这些磁盘中，因为读写时都可以并行处理，所以在所有的级别中，RAID 0的速度是最快的。但是RAID 0既没有冗余功能，也不具备容错能力，如果一个磁盘（物理）损坏，所有数据都会丢失。 RAID 1 两组以上的N个磁盘相互作镜像，在一些多线程操作系统中能有很好的读取速度，理论上读取速度等于硬盘数量的倍数，与RAID 0相同。另外写入速度有微小的降低。只要一个磁盘正常即可维持运作，可靠性最高。其原理为在主硬盘上存放数据的同时也在镜像硬盘上写一样的数据。当主硬盘（物理）损坏时，镜像硬盘则代替主硬盘的工作。因为有镜像硬盘做数据备份，所以RAID 1的数据安全性在所有的RAID级别上来说是最好的。但无论用多少磁盘做RAID 1，仅算一个磁盘的容量，是所有RAID中磁盘利用率最低的一个级别。 如果用两个不同大小的磁盘建RAID 1，可用空间为较小的那个磁盘，较大的磁盘多出来的空间也可以分区成一个区来使用，不会造成浪费。 \begin{aligned}Size&amp;=\min \left(S{1},S{2},S_{3}\dots \right)\end{aligned} RAID 5 RAID Level 5是一种储存性能、数据安全和存储成本兼顾的存储解决方案。它使用的是Disk Striping（硬盘分区）技术。RAID 5至少需要三块硬盘，RAID 5不是对存储的数据进行备份，而是把数据和相对应的奇偶校验信息存储到组成RAID5的各个磁盘上，并且奇偶校验信息和相对应的数据分别存储于不同的磁盘上。当RAID5的一个磁盘数据发生损坏后，可以利用剩下的数据和相应的奇偶校验信息去恢复被损坏的数据。RAID 5可以理解为是RAID 0和RAID 1的折衷方案。RAID 5可以为系统提供数据安全保障，但保障程度要比镜像低而磁盘空间利用率要比镜像高。RAID 5具有和RAID 0相近似的数据读取速度，只是因为多了一个奇偶校验信息，写入数据的速度相对单独写入一块硬盘的速度略慢，若使用“回写缓存”可以让性能改善不少。同时由于多个数据对应一个奇偶校验信息，RAID 5的磁盘空间利用率要比RAID 1高，便宜。 \begin{aligned}Size&amp;=(N-1)\times \min \left(S{1},S{2},\dots ,S_{N}\right)\end{aligned} RAID 6与RAID 5相比，RAID 6增加第二个独立的奇偶校验信息块。两个独立的奇偶系统使用不同的算法，数据的可靠性非常高，任意两块磁盘同时失效时不会影响数据完整性。RAID 6需要分配给奇偶校验信息更大的磁盘空间和额外的校验计算，相对于RAID 5有更大的IO操作量和计算量，其“写性能”强烈取决于具体的实现方案，因此RAID6通常不会通过软件方式来实现，而更可能通过硬件/固件方式实现。 同一数组中最多容许两个磁盘损坏。更换新磁盘后，数据将会重新算出并写入新的磁盘中。依照设计理论，RAID 6必须具备四个以上的磁盘才能生效。 可使用的容量为硬盘总数减去2的差，乘以最小容量，公式为： \begin{aligned}Size&amp;=(N-2)\times \min \left(S{1},S{2},S{3},\dots ,S{N}\right)\end{aligned} 同理，数据保护区域容量则为最小容量乘以2。 RAID 6在硬件磁盘阵列卡的功能中，也是最常见的磁盘阵列档次。 RAID 10 RAID 10是先镜射再分区数据，再将所有硬盘分为两组，视为是RAID 0的最低组合，然后将这两组各自视为RAID 1运作。 当RAID 10有一个硬盘受损，其余硬盘会继续运作。 RAID 50 RAID 5与RAID 0的组合，先作RAID 5，再作RAID 0，也就是对多组RAID 5彼此构成Stripe访问。由于RAID 50是以RAID 5为基础，而RAID 5至少需要3颗硬盘，因此要以多组RAID 5构成RAID 50，至少需要6颗硬盘。以RAID 50最小的6颗硬盘配置为例，先把6颗硬盘分为2组，每组3颗构成RAID 5，如此就得到两组RAID 5，然后再把两组RAID 5构成RAID 0。 RAID 50在底层的任一组或多组RAID 5中出现1颗硬盘损坏时，仍能维持运作，不过如果任一组RAID 5中出现2颗或2颗以上硬盘损毁，整组RAID 50就会失效。 RAID 50由于在上层把多组RAID 5构成Stripe，性能比起单纯的RAID 5高，容量利用率比RAID5要低。比如同样使用9颗硬盘，由各3颗RAID 5再组成RAID 0的RAID 50，每组RAID 5浪费一颗硬盘，利用率为(1-3/9)，RAID 5则为(1-1/9)。 RAID 60 RAID 6与RAID 0的组合：先作RAID 6，再作RAID 0。换句话说，就是对两组以上的RAID 6作Stripe访问。RAID 6至少需具备4颗硬盘，所以RAID 60的最小需求是8颗硬盘。 由于底层是以RAID 6组成，所以RAID 60可以容许任一组RAID 6中损毁最多2颗硬盘，而系统仍能维持运作；不过只要底层任一组RAID 6中损毁3颗硬盘，整组RAID 60就会失效，当然这种情况的概率相当低。 比起单纯的RAID 6，RAID 60的上层通过结合多组RAID 6构成Stripe访问，因此性能较高。不过使用门槛高，而且容量利用率低是较大的问题。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[磁盘分区管理]]></title>
    <url>%2Flinux%2F20170614-disk-management%2F</url>
    <content type="text"><![CDATA[硬盘机制磁盘分区信息是放在硬盘上的，而不是操作系统里，存在整个硬盘的第0个扇区（sector）。 MBR机制：Master Boot Record（主引导记录）占用空间为512bytes： bootloader（一段程序，引导加载器的程序）加载指定操作系统的内核：446bytes fat：64bytes，每16个bytes存放一个分区信息，最多只有4个分区。 55 AA：MBR有效性标记，2bytes GPT机制：大于2T的磁盘机制GPT全称是GUID Partition Table，GUID指的是全球统一标识符（Globals Unique Identifiers）。 GPT分区需要硬件有UEFI接口。 硬盘接口类型 IDE（ATA）：并口，每个控制器可接两块硬盘，早期PC机用的比较多，现在已经淘汰。133MB/s SCSI（Small computer System Interface）：并口，可以接 N多块硬盘。转速高，寿命长，早期的服务器用的很多。320MB/s，目前面临淘汰。 SAS （Serial Attached SCS）：串行SCSI，可以与SATA兼容。600MB/s 现在的x86服务器上，主流就是SAS口，可以支持SATA口的硬盘，也可支持SAS口的硬盘。 SATA（Serial ATA）：串行ATA口。目前很多台式机的机械硬盘是此类接口。 SATA1：150MB/s SATA2：300MB/s SATA3：600MB/s mSATA（mini-SATA）：mini SATA口。好多笔记本的SSD用mini-SATA口，目前最新版是M.2 mSATA M.2(mini-SATA 2)：socket3类型最高支持4GB/s。 PCI-e（PCI-express）：台式机的SSD，还有部分笔记本（比如苹果Macbook），用的是PCI-e，此接口比mSATA要快，最高支持8GB/s USB（Universal Serial Bus）：u盘、移动硬盘。 USB2.0：60MB/s USB3.0：500MB/s USB3.1（也叫type-c）：1.2GB/s 查看分区（查）df查看磁盘状态df：disk free * `-h`：human readable，人类可读 * `-T`：print type，输出格式。 12345[root@yulongjun ~]# df -hTFilesystem Type Size Used Avail Use% Mounted on/dev/sda2 ext4 50G 5.2G 42G 11% /tmpfs tmpfs 3.9G 0 3.9G 0% /dev/shm/dev/sda1 ext4 190M 39M 142M 22% /boot fdisk -l 列出所有磁盘信息。Id： 83：Linux可以使用的文件系统。 8e：指的是逻辑卷（logical volume） 常见的几种文件格式：基本：ext3（CentOS 5默认）， ext4（CentOS 6默认），xfs（CentOS 7 默认）光盘：iso9660可移动U盘：fat32（文件最大不能超过4G）， exfatwindows：ntfs（linux可编译安装ntfs-3g来识别） 创建分区（增）fdisk /dev/sda进入管理sda磁盘 常用的几个： m：menu，列出帮助菜单d：delete，删除n：new，新建p：print，列出t：调整分区IDl：list，列出内核支持的分区ID83 Linux（linux基础分区）5 extend（扩展分区）8e LVM（逻辑卷管理）ee GPT（2T以上硬盘）w：write，保存退出q：quit，不保存退出 new新分区的时候，如果是加主分区，只能加到第四个。如果想要更多的分区，可以把第四个加为扩展分区。然后再在扩展分区上创建分区。 删除分区（删）1234567fdisk /dev/sdad #进入删除模式# 输入要删除的分区号w #保存退出 修改分区格式（改）1234567891011121314151617fdisk /dev/sdat # 进入修改模式# 输入要修改的分区L # 列出可以修改的格式# 输入要修改的格式的代码w # 保存退出# 然后用到上面让内核识别的两个命令：kpartx -af /dev/sda # 强制添加分区到内核中partx -a /dev/sda # 再添加一次# 上述命令多次都不成功，只能重启了 通知内核识别新分区CentOS 5，7：partprobe 1parprobe /dev/sda CentOS 6 ：partx，kpartx 123kpartx -l /dev/sda # 列出sda中可重新加载到内核的分区kpartx -af /dev/sda # 强制添加分区到内核中partx -a /dev/sda # 再添加一次 CentOS 6 有时候不管用，只能重启系统(目前版本6.9，这个bug还没有修复。)，不过对于新硬盘，进行创建操作的时候，CentOS6不会有这个问题。 CentOS 5 和 7 使用partprobe即可，无bug。 创建文件系统 mkfs -t FSTYPE /dev/DEVICE mkfs.FSTYPE /dev/DEVICE mkswap /dev/DEVICE 创建交换分区。 mke2fs更加精细的操作 -t：ext2, ext3, ext4, xfs -L：label，指定卷标名 -b：block size ，（1024、2048、4096）默认是4k（4096） -i #: 为数据空间中每多少个字节创建一个inode；此大 小不应该小于block的大小 -N #：指定分区中创建多少个inode -I # 一个inode记录占用的磁盘空间大小，128—4096 -m #: 默认5%,为管理人员预留空间占总空间的百分比 -O FEATURE[,...]：启用指定特性 -O ^FEATURE：关闭指定特性 -o: 调整文件系统的默认挂载选项，–o acl 12mkfs.ext2 -O has_journal /dev/sda5# 上面虽然指定的格式是ext2，但是启动journal特性，所以就变成了ext3了。 acl权限：xfs创建的默认就带acl权限，ext系列的格式的需要手工指定。 卷标的操作：e2label /dev/DEVICE：查看卷标。e2label /dev/DEVICE LABEL_NAME：修改卷标。 示例： 1234mkfs -t ext4 /dev/sda5mkfs.ext4 /dev/sda5mke2fs.ext4 -t ext4 -L DB /dev/sda5mkswap /dev/sda6 查看磁盘分区的类型和uuid在Linux里，设备名是有可能变动的，比如我们添加一块IDE的磁盘，会变成sda，系统原来的sda会变成sdb，这样，我们就不能用设备名来唯一表示设备，这时候我们就用到UUID。 blkid /dev/DEVICE UUID 设备唯一标识 TYPE 文件系统类型 123blkid /dev/sda5/dev/sda5 UUID="XXXXXXXX" TYPE="ext4" 检测和修复文件系统检测修复ext类型的文件系统fsck DEVICE ：file system check ,文件系统检测fsck -t ext4 /dev/sda5检测文件系统错误fsck -r DEVICE:交互式检测文件系统fsck -y DEVICE：不用交互，一路确认检测文件系统。 检测修复xfs类型的文件系统，要用另外一个命令：xfs_check -y DEVICE tips：xfs_check命令需要安装xfsprogs包 挂载挂载的本质就是调用blkid查看设备UUID，然后查找到设备，进行挂载。 (1)手动挂载挂载交换分区：swapon /dev/sda6启用指定的交换分区swapon -a启用所有的交换分区swapon -s 显示交换分区的挂载情况卸载交换分区：swapoff /dev/sda6关闭指定的交换分区swapoff -a 关闭所有的临时交换分区 挂载普通分区：mount [-t FILETYPE] [-o option] /dev/DEVICE DIRECTORY参数详解： 卸载普通分区：umount /dev/DEVICE 或者umount /DIRECTORY，具体参数情况，见文章最后一节：附2：mount命令详解 查看挂载信息：mount 示例：123456789mount -t ext4 /dev/sda5 /u01mount -t iso9660 -o loop xxx.iso /mediamountumount /dev/sda5unmount /media (2)永久挂载修改/etc/fstab配置文件。fstab：file system table 123456789101112131415161718[root❄CentOS6 ~]☭ cat /etc/fstab ## /etc/fstab# Created by anaconda on Wed May 17 01:52:09 2017## Accessible filesystems, by reference, are maintained under &apos;/dev/disk&apos;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#UUID=964e8f82-ef35-4b1e-ae3f-9a95e36b5ee2 / ext4 defaults 1 1UUID=899d5f64-7a0c-4aba-b77f-a94f9c6df085 /app ext4 defaults 1 2UUID=ce69275d-68f5-46c7-975b-785e653d3d79 /boot ext4 defaults 1 2UUID=3aa122ef-cf5f-4198-81a7-48b799c9d914 swap swap defaults 0 0tmpfs /dev/shm tmpfs defaults 0 0devpts /dev/pts devpts gid=5,mode=620 0 0sysfs /sys sysfs defaults 0 0proc /proc proc defaults 0 0/dev/sr0 /media iso9660 defaults 0 0 字段从左到右含义： 要挂载的设备： 设备文件/dev/sda、/dev/sr0 卷名LABEL=&quot;ladel&quot; UUIDUUID=xxx 挂载点：挂载在那个目录下面。有的文件系统没有挂载点 ，例如swap，挂载点为swap。 文件系统类型：ext2,3,4、xfs、iso9660 挂载选项：多个选项间使用逗号分隔 loop：loop设备 ro：只读（默认rw） pri=xxx：priority，优先级，默认都是-1，你可以写0~65535，数值越大，优先级越高。 转储频率：0：从不备份（CentOS7 采用xfs，默认从不备份）1：每日备份（6 采用ext4，默认是1）2：每隔一天备份（很少使用） 自检次序：1：首先自检，通常只能被/使用2-9：顺序0：从不自检（默认从不自检） /etc/fstab里写挂载的几个特殊例子： 123456#目录挂目录/boot /mnt/boot none bind 0 0# loop文件挂目录/app/partfile /mnt/part loop 0 0#windows共享挂目录//winsvr_url/share_file /mnt/win cifs 0 0 实验：将swap分区迁移到高速磁盘上，优化性能，原swap不删除，作为次swap。(提示：利用pritory优先级） 1# TODO 卸载 查看挂载情况:findmnt MOUNT_POINT 查看正在访问指定文件系统的进程：lsof MOUNT_POINT或fuser -v MOUNT_POINT 终止所有在正访问指定的文件系统的进程：fuser -km MOUNT_POINT 卸载：umount DEVICEumount MOUNT_POINT 附1：其他常用命令findmntfindmnt /DEVICE或者findmnt PATH查找某个设备是否挂载，或者某个路径是否为挂载点。 123[root❄centos7 ~]☭ findmnt /dev/sdb5[root❄centos7 ~]☭ echo $?1 查看/mnt/sdb5是否是挂载点，如果不是，就把/dev/sdb5挂载在/mnt/sdb5上。 1findmnt /mnt/sdb5 &gt;&gt; /dev/null || mount /dev/sdb5 /mnt/sdb5 findfsfindfs LABEL=xxxxxfindfs UUID=xxxxx 查找dir对应的uuid的挂载磁盘12[root❄centos7 ~]☭ dir=/;findfs `egrep &quot;$dir[[:space:]]+&quot; /etc/fstab | cut -d&quot; &quot; -f1`/dev/sda2 tips: 一个设备可以挂载到不同的挂载点。不同的设备，可以挂载到同一个挂载点，最后一个挂载的设备生效。 ##附2：mount命令详解 参数解析： device：指明要挂载的设备； 设备文件：例如/dev/sda5 卷标：-L &#39;LABEL&#39;, 例如 -L &#39;MYDATA&#39; UUID, -U &#39;UUID&#39;：例如 -U &#39;0c50523c-43f1-45e7-85c0-a126711d406e&#39; 伪文件系统名称：proc, sysfs, devtmpfs, configfs dir：挂载点 事先存在；建议使用空目录 进程正在使用中的设备无法被卸载 -t vsftype 指定要挂载的设备上的文件系统类型 -fnrsvw： -r: readonly，只读挂载 -w: read and write, 读写挂载 -n: 不更新/etc/mtab，mount不可见(可以用cat /proc/mounts看到挂载） -a：自动挂载所有支持自动挂载的设备(定义在了/etc/fstab 文件中，且挂载选项中有auto功能) -L &#39;LABEL&#39;: 以卷标指定挂载设备 -U &#39;UUID&#39;: 以UUID指定要挂载的设备 -B, --bind: 绑定目录到另一个目录上 12mount -B /var/ftp/pub /var/www/html/# 这样可以实现同一份数据，在两个地方。 查看内核追踪到的已挂载的所有设备：cat /proc/mounts -o options：(挂载文件系统的选项)，多个选项使用逗号分隔 async：异步模式 sync：同步模式,内存更改时，同时写磁盘 atime/noatime：包含目录和文件的访问时间戳 diratime/nodiratime：目录的访问时间戳 auto/noauto：是否支持自动挂载,是否支持-a选项 exec/noexec：是否支持将文件系统上运行应用程序 dev/nodev：是否支持在此文件系统上使用设备文件 suid/nosuid：是否支持suid和sgid权限 remount：重新挂载 ro：只读 rw：读写 user/nouser：是否允许普通用户挂载此设备，默认管理员才能挂载 acl：启用此文件系统上的acl功能 Defaults：相当于rw, nosuid, dev, exec, auto, nouser, async 附3：loop特殊设备CentOS 6 默认只支持8个loop设备，7没有限制(每增加一个，control会去生成一个）: 123[root❄CentOS6 ~]☭ ls /dev/loop*/dev/loop0 /dev/loop2 /dev/loop4 /dev/loop6/dev/loop1 /dev/loop3 /dev/loop5 /dev/loop7 12[root❄centos7 ~]☭ ll /dev/loop*crw-rw---- 1 root disk 10, 237 Jun 14 10:17 /dev/loop-control 要想让6支持更多，要更改内核参数,kernel那一行，最后面加上max_loop=数量参数 123...kernel /vmlinuz-2.6.32-696.el6.x86_64 ro root=UUID=964e8f82-ef35-4b1e-ae3f-9a95e36b5ee2 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM rhgb quiet max_loop=100... losetup命令 指定loop号挂载，可以不用按顺序挂载 losetup -a ：显示系统挂载的loop设备(CentOS 7 不用加-a)losetup LOOP-DEVICE LOOP-FILE：关联loop文件到指定的loop设备上 12losetup /dev/loop9 /app/partfile9 #关联loop文件到指定的/dev/loop设备上mount /dev/loop9 /mnt/part9 # 把关联的设备挂载到目录上 挂载Windows共享文件： 123mkdir /mnt/winmount -o username=test,password=magedu //192.168.8.1/winshare /mnt/win]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-macOS磁盘管理工具]]></title>
    <url>%2Fmacos%2F20170614-apple-hfs%2F</url>
    <content type="text"><![CDATA[自带磁盘工具 macOS格式化磁盘的选项： 格式： 格式 英文 中文 HFS+ Mac OS Extended (Journaled) MacOS 扩展（日志式） HFSX MAC OS Extended (Case-sensitive, Journaled) MacOS 扩展（区分大小写，日志式） exFAT exFAT exFAT FAT32 MS-DOS(FAT) MS-DOS(FAT) 方案： GUID分区图（GPT）：使用启动基于Intel的MAC，或将磁盘当做非启动盘用于任何装有MAC OS X V10.4 或更高版本的MAC 主引导记录（MBR）：使用该磁盘启动DOS和Windows电脑，或者将磁盘配合需要DOS兼容分区或Windows兼容分区的设备来使用。 Apple分区图（APM）：使用该磁盘启动基于PowerPC的MAC 电脑，或将该磁盘当做非启动盘用于任何MAC电脑 小技巧：在我们格式化一个新的移动硬盘的时候，为了兼容macOS和Windows，通常会使用下面的方式：格式选择exFAT，方案选择主引导记录。 dd命令dd命令，通常在我们做iso镜像的启动盘、安装盘的时候使用。 具体教程可以看我之前写的一个文章：http://bbs.feng.com/read-htm-tid-9131601.html CentOS、RHEL、Fodera、Ubuntu都支持这种模式的制作方式，其他的暂未测试过。]]></content>
      <categories>
        <category>macos</category>
      </categories>
      <tags>
        <tag>macOS</tag>
        <tag>GUID</tag>
        <tag>MBR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-编译安装]]></title>
    <url>%2Flinux%2F20170609-04-make-install%2F</url>
    <content type="text"><![CDATA[编译安装，是指从源码进行编译，然后根据安装规则把编译好的文件，分发到预先设置好的目录，目录可以自定义。 我们以在 CentOS 7 上安装apache2.4为例： 在apache官网下载源码,解压。 yum groups install Development Tools安装开发包。 cd httpd-2.4 查看一下 README INSTALL ./configure --prefix=/app/appche24按照配置生成配置文件。 如果报错，查看下报错提示缺什么包，把缺失的包装上，一般新版本的选xxx-devel包就可以支持。 make编译 make install复制到规定的目录 1234567891011121314mkdir /app/apache24tar -xvf httpd-2.4.25.tar.bz2cd httpd-2.4.25yum groups install "Development Tools"./configure --prefix=/app/apache24 --sysconfdir=/etc/apache24 --enable-rewriteyum install apr-devel./configure --prefix=/app/apache24 --sysconfdir=/etc/apache24 --enable-rewriteyum install apr-util-devel./configure --prefix=/app/apache24 --sysconfdir=/etc/apache24 --enable-rewriteyum install pcre-devel./configure --prefix=/app/apache24 --sysconfdir=/etc/apache24 --enable-rewritemakemake install 把bin目录加到PATH里： vim /etc/profile.d/apache24.sh 1PATH=$PATH:/app/apache24/bin]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>make install</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-打包压缩命令tar、zip、split]]></title>
    <url>%2Flinux%2F20170609-03-extract-compress%2F</url>
    <content type="text"><![CDATA[tar(gz、bz2、xz) zip split 1. tartar的意思是Together ARchive（打包归档）。我们可以用来打包，也可以用来解压包，而且还支持打包后用各种格式压缩（gz、bz2、xz等）。 单个参数意义：f: 归档filev: verbose（注：详细），显示压缩过程的详细信息t: list,显示归档的内容x: extract,解压c: compress,压缩z: gzip格式压缩，后缀为.gzj: bzip2格式压缩，后缀为.bz2J: xz格式压缩，后缀为.xz 常用组合： 组合参数 意义 压缩文件后缀 cvf 原始tar包，不压缩 .tar zcvf 先tar，后gzip压缩 tar.gz 、tgz jcvf 先tar，后bzip2压缩 tar.bz2 、tbz2 Jcvf 先tar，后xz压缩 tar.xz、txz xvf 解压所有格式，通用解压命令 - tips: 在编写shell脚本打包的时候，我们通常不会用v选项，这样屏幕输出会比较乱，我们一般用echo在打包前后提示一下打包开始和打包完成，就ok了。举个我之前写的脚本例子： 1234567#!/bin/bashecho &quot;==&gt;开始打包文件夹...&quot;tar -czf Blog`date +%Y%m%d`.tar.gz Blogecho &quot;==&gt;打包成Blog`date +%Y%m%d`.tar.gz完毕.&quot;echo &quot;==&gt;移动打包文件到/Volumes/Transcend/Backup/Blog/...&quot;mv -f Blog`date +%Y%m%d`.tar.gz /Volumes/Transcend/Backup/Blog/echo &quot;==&gt;移动完毕.&quot; 执行效果就是这样： 12345LongDream❄MBP:Scripts]☭ ./backup_blog.sh==&gt;开始打包文件夹...==&gt;打包成Blog20170607.tar.gz完毕.==&gt;移动打包文件到/Volumes/Transcend/Backup/Blog/...==&gt;移动完毕. 2. zip&amp;unzip打包并压缩：zip -r xxx.zip xxx解压缩解包：unzip xxx.zip 举个解压oracle安装包的例子： 1unzip p13390677_112040_platform_1of7.zip &amp;&amp; unzip p13390677_112040_platform_2of7.zip &amp;&amp; unzip p13390677_112040_platform_4of7.zip splitsplit：切割打包。 默认包分割后的名字后缀是aa, ab, ac, ... -b SIZE 指定每个分割包的大小 -d DIGIT 每个分割包名后缀按数字命名：01, 02, ... 例子1（不加-d）: 12345678910[root❄centos7 bin]☭ ll -h bin.tar.gz -rw-r--r--. 1 root root 54M Jun 7 20:41 bin.tar.gz[root❄centos7 bin]☭ split -b 10M bin.tar.gz bin_[root❄centos7 bin]☭ ll -h bin_*-rw-r--r--. 1 root root 10M Jun 7 20:44 bin_aa-rw-r--r--. 1 root root 10M Jun 7 20:44 bin_ab-rw-r--r--. 1 root root 10M Jun 7 20:44 bin_ac-rw-r--r--. 1 root root 10M Jun 7 20:44 bin_ad-rw-r--r--. 1 root root 10M Jun 7 20:44 bin_ae-rw-r--r--. 1 root root 3.8M Jun 7 20:44 bin_af 例子2（加-b）: 1234567891011[root❄centos7 bin]☭ ll -h bin.tar.gz-rw-r--r--. 1 root root 54M Jun 7 20:41 bin.tar.gz[root❄centos7 bin]☭ split -b 10M -d bin.tar.gz bin_[root❄centos7 bin]☭ ll -h bin_[0-9]?-rw-r--r--. 1 root root 10M Jun 7 20:47 bin_00-rw-r--r--. 1 root root 10M Jun 7 20:47 bin_01-rw-r--r--. 1 root root 10M Jun 7 20:47 bin_02-rw-r--r--. 1 root root 10M Jun 7 20:47 bin_03-rw-r--r--. 1 root root 10M Jun 7 20:47 bin_04-rw-r--r--. 1 root root 3.8M Jun 7 20:47 bin_05 合并切割的包 例子：cat mybackup-parts* &gt; mybackup.tar.gz]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>tar</tag>
        <tag>zip</tag>
        <tag>split</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-程序包管理工具yum]]></title>
    <url>%2Flinux%2F20170609-02-yum%2F</url>
    <content type="text"><![CDATA[yum首先要有一个网络上或本地或远程的yum仓库。然后需要yum安装程序的机器去yum仓库下载yum元数据（包括包信息和依赖信息）到本地的cache里。当需要安装程序的时候，会查看yum源数据里是否有此包，并且查找此包的依赖信息，然后去yum仓库里面下载包和依赖包到本地cache里，并且进行安装。 校验码：远程仓库数据有可能发生改变，这样本地的信息就和仓库信息不符。这样根据校验码，来确定文件是否更改，这样可以节省网络带宽。（对于我们来说，yum一般配置本地仓库，yum源来自操作系统的iso镜像，一般不去校验） yum仓库(yum repository)yum的意思是Yellowdog Update Modifier yum repository存放了众多的rpm包，以及包的相关元数据文件（元数据存放于特定目录下：repodata）CentOS和Redhat安装操作系统的时候，anaconda也会去调用yum来安装（不过是调用的本地的光盘上的yum repository） tips: 无论任何仓库，repodata目录的父目录就是yum源的目录。比如CentOS 5是在光盘/Server/下,6和7在光盘/下。 网络文件服务器：CentOS已经配置好，联网可以直接使用（redhat不可使用，需要购买服务），是网络上的的文件服务器。 本地文件服务器： file://配置本地文件 ftp://配置本地ftp服务器 http://、https://配置本地http服务器 yum的配置文件： /etc/yum.confyum的公共配置文件 /etc/yum.repos.d/*.repo具体的每个仓库的配置，分开文件是为了方便管理和配置。 /etc/yum.conf 常用配置项： 123456789101112[main] #yum仓库idcachedir=/var/cache/yum/$basearch/$releasever #缓存和数据库文件，baseserach指的架构，如`x86_64`，releaserver指的是版本，如`6`或`7`keepcache=0 #缓存源文件和安装成功后的下载包是否保存debuglevel=2 #debug级别logfile=/var/log/yum.log #日志文件位置exactarch=1 #精确平台匹配obsoletes=1gpgcheck=1 #来源完整性和包完整性检查plugins=1 # 支不支持插件机制installonly_limit=5 # 允许同时安装几个程序包bugtracker_url=http://bugs.centos.org/set_project.php?project_id=19&amp;ref=http://bugs.centos.org/bug_report_page.php?category=yum #bug报告distroverpkg=centos-release #ditribution version pkg 发行版版本号获取 /etc/yum.repos.d/*.repo 常用配置项： 123456[repositoryid] # 仓库哦idname=Some name for this repository # 仓库名字baseurl=url://path/to/repository/ # 仓库地址enable=&#123;1|0&#125; # 是否可用gpgcheck=&#123;0|1&#125; # 是否检查gpggpgkey=URL # 如果检查，写gpgkey地址 yum安装日志：/var/log/yum.log 配置实战：yum仓库配置实战1-本地仓库： 挂载iso镜像到media目录 mount -t iso9660 -o loop /var/ftp/pub/CentOS-6.8-x86_64-bin-DVD1.iso /media 配置/etc/yum.repos.d/local.repo [local]name=Repo on Local Cdrom Mediabaseurl=file:///mediagpgcheck=0enable=1 创建缓存看是否成功 yum makecache yum仓库配置实战2-ftp仓库： 创建之前先删除之前的cache：rm -rf /var/cache/yum 配置/etc/yum.repos.d/ftp.repo [ftp]name=Repo on 172.17.0.1baseurl=ftp://172.17.0.1/pub/CentOS6u9gpgcheck=0enable=1 创建缓存看是否成功 yum makecache yum用法： yum makecache：创建yum缓存 yum repolist显示启用的仓库列表 yum grouplist显示包组 yum list显示所有可用包 yum list vsftpd*显示和vsftpd*匹配的包 yum list installed显示已安装的包 yum install package1 [package2] [...]可以安装一个或多个包 -y参数，即yes，表示不用交互询问，直接安装。 yum reinstall package1 [package2] [...] 可以重新安装一个或多个包 yum update [package1] [package2]不跟包名的话，是更新所有包，带包名的是更新具体的包 yum check-update检查可用升级 yum remove|erase package1 [package2]卸载程序包 yum info查看程序包信息 yum provides COMMAND1 [COMMANDN]COMMAND命令是由哪个包提供的。 yum search xxx模糊搜索程序包 yum clean all清除所有缓存(如果还清不了，可以手动删除rm -rf /var/cache/yum) 非常常用的几个个命令：yum clean allyum makecacheyum -y install xxx yyy zzzyum remove 自制第三方yum源createrepo /rpmdir会在目录下面生成一个repodata文件夹，存放了包的元数据。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>yum</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-程序包管理工具rpm]]></title>
    <url>%2Flinux%2F20170609-01-rpm%2F</url>
    <content type="text"><![CDATA[rpm就是一个程序的打包工具。在安装软件的时候，可以把不同的目录拷贝到相应的目录下（如二进制文件，配置文件等等。） RedHat里全名叫Redhat Package Management SuSe为了避免冲突，就叫RPM Package Management（类似于GNU，GNU is not a unix） rpm包格式： 程序报名-版本号-打包号.操作系统号.架构.rpm如：vsftpd-3.0.2-21.el7.x86_64.rpm 插入小练习，查看rpm包架构： 123456789# 第一种方法ls *.rpm |rev|cut -d. -f2|rev|sort | uniq -c 1337 noarch 2494 x86_64 # 第二种方法ls *rpm |egrep -o &quot;[^.]+\.rpm&quot; | cut -d. -f1 |sort |uniq -c 1337 noarch 2494 x86_64 包里面内容： path/files 要安装的文件和相对路径 metadata 元数据 scripts 脚本(安装前脚本、安装后脚本、卸载前脚本、卸载后脚本） 包安装信息数据库：/var/lib/rpm * 程序包名称及版本 * 依赖关系 * 功能说明 * 包安装后生成的各文件路径及校验码信息 程序包的来源1. 系统发版的光盘或官方的服务器；CentOS官网：https://www.centos.org/download/阿里云：http://mirrors.aliyun.com网易：http://mirrors.163.com清华：https://mirror.tuna.tsinghua.edu.cn/中科大：http://mirrors.ustc.edu.cn/浙大：http://mirrors.zju.edu.cn/ 2. 项目官方站点举几个例子： https://grafana.com/grafana/download https://portal.influxdata.com/downloads#influxdb 3. 第三方组织Fedora-EPEL： Extra Packages for Enterprise Linux 基本上镜像站都提供epel源。 4. 搜索引擎：http://pkgs.org http://rpmfind.net http://rpm.pbone.net https://sourceforge.net/ 5. 自己制作注意：第三方包建议要检查其合法性 来源合法性 程序包的完整性 rpm命令安装卸载-i：install ,安装-v：verbose,显示详情-h：显示进度条-e：erase，删除（卸载）-U upgrade，升级--force强制安装（不能用于强制卸载）--test： 测试安装，不真正执行安装。--nodeps：忽略依赖性（一般不忽略）--oldpackage 降级安装（一般不降级） 内核安装默认是不覆盖安装的，装了多个kernel，可以修改grub.conf修改默认启动内核顺序。可以rpm -e卸载旧的kernel 查询rpm -q PackageName查询某个包rpm -qa 查询已安装的所有包rpm -qa |grep xxx 模糊过滤rpm -qf FILE 硬盘上的文件(file)是来自于哪个rpm包（可以是二进制程序，也可以是配置文件等）rpm -qi xxx.rpm 查询安装包详细信息（information）rpm -ql xxx.rpm 查看某个包安装后在系统里的所有文件rpm -qc xxx 查询安装包后的配置（config）文件位置rpm -q --scripts查询程序自带的脚本 rpm校验rpm --import xxx/yyy/RPM-GPG-KEY-CentOS-7：导入包完整性校验文件rpm -K xxxxx.rpm校验某个包 查找安装gpg-pubkeyrpm -qa &quot;gpg-*&quot; 1234567[root❄centos7 Packages]☭ rpm --import /media/RPM-GPG-KEY-CentOS-7 [root❄centos7 Packages]☭ rpm -K tree-1.6.0-10.el7.x86_64.rpm [root❄centos7 Packages]☭ rpm -qa &quot;gpg-*&quot;gpg-pubkey-f4a80eb5-53a7ff4b[root❄centos7 Packages]☭ rpm -e gpg-pubkey [root❄centos7 Packages]☭ rpm -K tree-1.6.0-10.el7.x86_64.rpm tree-1.6.0-10.el7.x86_64.rpm: RSA sha1 ((MD5) PGP) md5 NOT OK (MISSING KEYS: (MD5) PGP#f4a80eb5)]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>rpm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-误删系统重要文件，无法启动，如何恢复（以删除库文件为例）]]></title>
    <url>%2Flinux%2F20170609-01-rescure%2F</url>
    <content type="text"><![CDATA[模拟误删（删除库文件）ldd 命令 ldd 查看命令依赖的库文件: 我们可以看到好多命令都依赖libc.so.6文件，我们就删除这个。 123456789[root❄centos6 ~]☭ ldd /bin/ls...libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f7b1d9c0000)...[root❄centos6 bin]☭ ldd /bin/rm...libc.so.6 =&gt; /lib64/libc.so.6 (0x00007f169c741000)...[root❄centos6 bin]☭ rm -rf /lib64/libc.so.6 删除之后，系统都无法重启，只能硬关机。 如何恢复系统用安装光盘或安装u盘启动系统，选择Rescure installed system进入救援模式。 选择no 我们可以看到提示，Linux系统会尝试挂载在/mnt/sysimage目录下，我们点继续。 提示你系统已经挂载到/mnt/sysimage 启动一个shell df查看一下挂载情况，我们可以看到系统原来的根，挂载在了/mnt/sysimage下面。 我们从当前运行环境中的lib64的文件里，复制一份，把原来删掉的文件，给恢复回来： 重启恢复。 CentOS 7和6差不多，就是光盘启动后，选择的是TroubleShooting–&gt;Rescure]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>rescure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS重装后初始设置(运维和开发人员自用版，可参考）]]></title>
    <url>%2Fmacos%2F20170608-macos-init-config%2F</url>
    <content type="text"><![CDATA[macOS作为类unix的系统，很多命令和功能和Linux等兼容度非常高，又支持很多国外流行软件、国内桌面软件（QQ、百度网盘、迅雷之类的），经常作为Linux运维人员和开发人员的首选系统。下面列出一些常用的软件和设置。 1. 软件1.1 安装app store里的一些常用应用App Store里搜索下载： Xcode（必装，后续有其他软件调用Xcode的组件） Numbers（mac下的表格软件） Pages（mac下的文件处理软件） Keynote（mac下的演示软件） 印象笔记 qq 微信 yy 钉钉 网易云音乐 Dr.Unarchive最好用的解压缩软件 1.2 打开允许来自“任何来源”的app的使用终端下运行下面命令： 1sudo spctl --master-disable 1.3 安装非app store 里的应用（可点击名字去下载页面） 搜狗拼音 搜狗五笔 迅雷 百度云 腾讯微云 RealVNC：vnc工具 Teamviewer：远程桌面连接工具 Visual Studio Code：微软出的最好用的轻量级编辑器（比Atom、Sublime Text都好用） Shadowsocks-NG：科学上网软件，要自行购买vps，安装配置见博文用搬瓦工和Shadowsocks搭梯子 Lantern：免费科学上网软件，不是很稳定。 Google Chrome：翻墙后可以直接下载，然后还可以在Chrome应用商店搜索下载下面的插件： Adblock Plus：屏蔽网页广告 印象笔记·剪藏：剪藏网页到印象笔记 Markdown Here：在邮件里写markdown直接转换为html格式，也可用在支持html裸格式的博客之类的。 Firefox Developer Edition：前段开发者最爱，同时支持不翻墙同步账号密码和标签。 1.4 收费软件 不鼓励破解和用网上序列号注册。如果前期学习的话，可以试用。如果后面长期用，还是付费的好。当然，笔者比较穷，都是在这个网站找资源： 爱情守望者 Office 2016 for mac 微软的优秀Office套件 CleanMyMac 3：Mac清理优化软件 Hands Off!：优秀的防火墙软件，可以针对具体软件来防，比如防止某些软件连接激活服务器（比如禁止Adobe全系列的网络连接） 禁止网络，只要Deny掉就可以了。 Adobe Photoshop/Illutrator/Acrobat/Dreamveaver：推荐用Adobe Creative Cloud安装全系列，然后见科学教程。 Idea/PyCharm/WebStorm：商业化的IDE软件，开发人员必备。官网下载，然后在这里找科学码。 Mweb：mac下最好用的Markdown编辑软件。支持同步到印象笔记、为知笔记；发布到WordPress博客；同时针对Hexo，Jekyll等静态博客做了优化；支持导出html、pdf、docx、rtf、图片格式。 笔者博客用的hexo，Mweb针对hexo的预览做了优化（采用外部模式方法），所以相当好用，以前markdown插入图片的预览是个问题，这个外部模式解决了我的痛点） Navicat Premium数据库工具，这个不用说了吧，搞数据库开发的好多都用这个软件，支持主流的数据库软件：MySQL、Oracle、PostgreSQL、SQLite、SQL Server、MariaDB。 iStat Menus：状态栏软件。可显示CPU、内存、电池、网络、日历、温度等。 状态栏效果： 下拉效果： Monodraw：强大的ASCII艺术编辑器，用来在脚本里画图用（虽然没什么卵用……但好看啊……） 我也写了个脚本，可以自动生成（当然是根据从Monodraw画出来的格式来编写啦） 官网的图： iThoughtX：比Xmind强大的脑图软件，支持markdown（最爱md）。 OmniGraffle：流程图软件，被誉为 Mac 上的 Visio，主要用于绘制流程图、图表、组织结构图、UI界面设计等等，支持 Visio 导入/导出。 VMware Fusion：等同于Windows和Linux下的VMware Workstation Pro 1.5 安装brew macOS没有类似apt-get、yum、dnf这种软件包管理器，Homebrew这个第三方的软件包管理器非常好用。 终端下运行： 1/usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; tips：Node.js、Python3、nmap、bash-completion等都可以用brew来安装。 2. CLI设置2.1 终端设置 更改计算机名字为MBP，改短点，这样‘终端’显示效果比较好。偏好设置–&gt;共享： 或者可选择自定义终端显示，vim ~/.bash_profile（没有则创建）： 123# PS1export PS1="[\[\033[01;32m\]\u❄h\[\033[00m\]:\[\033[01;34m\]\W\[\033[00m\]]☭ " 显示效果： 使ls命令显示文件文件夹颜色，同时设置ll别名。 vim ~/.bash_profile（没有则创建） 12345# LS_COLORexport LS_OPTIONS='--color=auto' # 如果没有指定，则自动选择颜色export CLICOLOR='Yes' #是否输出颜色# aliasalias ll="ls -l" 2.2 vim设置vim ~/.vimrc，设置vim参数（没有则创建）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&quot; Configuration file for vimset modelines=0 &quot; CVE-2007-2438&quot; Normally we use vim-extensions. If you want true vi-compatibility&quot; remove change the following statementsset nocompatible &quot; Use Vim defaults instead of 100% vi compatibilityset backspace=2 &quot; more powerful backspacing&quot; Don&apos;t write backup file if vim is being called by &quot;crontab -e&quot;au BufWrite /private/tmp/crontab.* set nowritebackup nobackup&quot; Don&apos;t write backup file if vim is being called by &quot;chpass&quot;au BufWrite /private/etc/pw.* set nowritebackup nobackupsyntax on&quot; 语法高亮set hlsearch&quot; 高亮搜索set incsearch&quot; 搜索逐字符高亮set ai&quot; 自动缩进set showmatch&quot; 括号成对匹配set matchtime=5&quot; 对应括号高亮的时间（单位为十分之一秒）set autoread&quot; 设置当文件被外部编辑器改动时候自动载入autocmd InsertLeave * se noculautocmd InsertEnter * se cul&quot; 用浅色高亮当前行set tabstop=4set expandtabset autoindent&quot; Tab键设置为4个空格set softtabstop=4set shiftwidth=4&quot; 统一缩进为4set number&quot; 显示行号colorscheme default &quot; 设置颜色主题,default,elflord,morning,peachpuff,slate,blue,delek,evening,murphy,ron,torte,darkblue,desert,koehler,pablo,shine,zellnerset ruler&quot; 在编辑过程中，在右下角显示光标位置的状态行 3 偏好设置3.1 通用只勾选了使用暗色菜单栏和Dock（根据个人喜好），默认浏览器给切换到Chrome（根据个人喜好），打开了handoff，关于handoff见官网解释。 3.2 安全与隐私关闭所有软件的定位（除了系统服务的不关）： 关闭所有访问通讯录的软件： 3.3 iCloud去掉照片和邮件的同步，iCloud Drive里只保留偏好设置的同步 3.4 互联网账户只保留iCloud，其它Game Center,微博之类的都删掉 3.5 App Store去掉自动下载在其他Mac上购买的应用，购买项目由始终需要改为15分钟后需要。免费下载改始终需要为存储密码。 3.6 用户与群组关闭客人用户。 3.7 日期与时间去掉勾选在菜单栏中显示日期和时间(前面软件iStat Menus已经显示了）]]></content>
      <categories>
        <category>macos</category>
      </categories>
      <tags>
        <tag>macOS</tag>
        <tag>Serria</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第一阶段测试题]]></title>
    <url>%2Flinux%2F20170608-exam%2F</url>
    <content type="text"><![CDATA[第1题题目： yum源的配置与使用1)创建一个本地yum源base源指向https://mirrors.aliyun.com/centos/7/os/x86_64/epel源指向https://mirrors.aliyun.com/epel/7Server/x86_64/2) 安装开发包组 解答： 123456789101112131415161718192021222324252627282930#!/bin/env bash# Filename: config_yum.sh# Author: Yu Longjun# 移动原repo文件到backup目录下yumdir=/etc/yum.repos.d/mkdir $yumdir/bakcupmv $yumdir/*.repo $yumdir/backup# 挂载光盘umount /dev/sr0 &gt; /dev/nullmount /dev/sr0 /media &amp;&gt;/dev/null# 增加base.repo文件cat &gt;base.repo &lt;&lt;EOF[base]name=centos7-basebaseurl=https://mirrors.aliyun.com/centos/7/os/x86_64/gpgcheck=0[epel]name=centos7-epelbaseurl=https://mirrors.aliyun.com/epel/7Server/x86_64/gpgcheck=0EOF# yum元数据缓存，安装"Devlopment Tools"包组yum makecache &gt;/dev/nullyum groups install "Development Tools" &gt;/dev/null 第2题题目： 复制/etc/ssh/sshd_config 到/tmp/中并更名为sshd_config.bak。将/tmp/sshd_config.bak文件中所有以非#号开头与包含空白字符的行保存至/tmp/sshd_config中。 解答： 123cp /etc/ssh/sshd_config /tmp/sshd_config.bakcat /tmp/sshd_config.bak |grep &quot;^[^#]&quot;|grep &quot;^[^[:space:]*$]&quot; 第3题题目： 编写脚本/root/bin/sysinfo.sh显示当前主机系统信息，包括主机名，操作系统版本，内核版本,CPU型号，内存大小，硬盘分区。 解答： 12345678910#!/bin/env bash# Filename: sysinfo.sh# Author: Yu Longjunecho "主机名 ： $HOSTNAME"echo "系统版本： `cat /etc/centos-release`"echo "内核版本： `uname -r`"echo "CPU型号 ：`cat /proc/cpuinfo | grep "model name"|cut -d: -f2|head -1`"echo "内存大小： `free -m |grep Mem|tr -s " "|cut -d" " -f2` MB"echo -e "硬盘分区：\n`df -hT |egrep -o "^/dev/sd.*\&gt;"|tr -s " " | cut -d" " -f1,3|sort`" 第4题题目： 给root用户定义别名命令vimnet，相当于vim /etc/sysconfig/network-scripts/ifcfg-ens33，并使root执行history命令时，显示每个命令执行的具体时间。 解答： 用root用户执行下面ch_bashrc.sh脚本： 123456789101112#!/bin/env bash# Filename: ch_bashrc.sh# Author: Yu Longjuncat &gt;&gt;~/.bashrc &lt;&lt;EOF# shortnamealias vimnet="vim /etc/sysconfig/network-scripts/ifcfg-ens33"# history with timestampexport HISTTIMEFORMAT="%F %T "EOF 第5题题目： 指出软链接与硬链接的异同之处(至少四处) 解答： 软链接就相当于Windows的快捷方式，删掉源文件，快捷方式和就失效了，软链接就找不到源文件了。 硬链接相当于多个链接指向同一份数据存储区域，每多一个硬链接，硬链接数+1，如果一个文件，有n个硬链接，删除n-1个硬链接，源文件还在，直到删除所有硬链接，才会删除源文件。 1. 复制（cp） 在复制过程中，复制软连接相当于复制了快捷方式，速度很快，而且可以跨分区。 在复制过程中，复制硬链接分为两种情形： 在同一分区复制，相当于多创建一个链接指向原数据存储位置，速度很快。 在不同分区复制，相当于把原来分区的数据拷贝过去存储，同时创建一个指向新数据区域的指针，速度比较慢。 2. 删除（rm）在删除过程中，删除软连接相当于删除了快捷方式，源文件还在。在删除过程中，删除硬连接相当于删除了一个到数据块的指针，，除非删除所有硬链接文件，源文件才删除。 3. 移动（mv）在移动过程中，移动软连接相当于移动了快捷方式而已。在移动过程中，移动硬连接分为两种情形： 在同一分区移动，相当于创建了一个新inode，指向数据块，并把原来的inode删掉 在不同分区移动，要把数据块复制到新分区，然后在新分区创建新的inode号指向新的数据块，并且把原来分区的inode号和数据块都删掉。 4. 软连接支持对目录创建，硬链接不支持 ln dir1 dir2不成功 ln /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/ens33成功 第6题题目： 下载编译安装httpd 2.4最新版本，写出安装过程。 解答： 见博文04-编译安装 第7题题目： 过滤ifconfig命令结果中所有大于0且小于255的三位数 解答： 1ifconfig | egrep -o &quot;\&lt;((1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-4]))&quot; 第8题题目： 将用户mage被误删除的的家目录恢复，复制/etc/shadow到mage家目录中。并设置只有用户wang可以读取/home/mage/shadow。 解答： 1234567891011121314# root用户执行下面命令mkdir /home/magechown mage:mage /home/mage# ll -d /home/wang 查看到wang家目录的权限为700chmod 700 /home/mage# 切换到mage用户su - magecp /etc/skel/\.* .cp -r /etc/skel/.mozilla/ .# root用户执行下面命令cp /etc/shadow /magesetfacl -m u:wang:r shadow 第9题题目： 统计/var/log/httpd/access.log日志访问频繁前十的地址，并从大到小排序。 解答： 1cat access.log |cut -d" " -f1 |sort |uniq -c |sort -nr|head -n10 第10题题目： 开启两个终端，将终端1 中输入命令的执行结果输出，并同时输出到终端2 。 解答： COMMAND为终端1输入的命令： 1COMMAND | tee &gt;/dev/pts/1 第11题题目： 误删除/lib64/libc.so.6系统库文件，如何恢复之，实验说明。 解答： 见博文01-误删系统重要文件，无法启动，如何恢复（以删除库文件为例） 第12题题目： 误删除rpm包命令，如何恢复之，实验说明。 解答： 前5步同11题前5步。 后面的步骤： 123cd /mnt/sysimage/media/Packagesrpm -ivh rpm-4.11.3-21.el7.x86_64.rpm 重启进入系统，rpm可以使用了。 第13题题目： 计算2+4+6+…+96+98+100之和。 解答： 1echo &#123;2..100..2&#125;|tr &quot; &quot; &quot;+&quot;|bc 第14题题目： 取/etc/sysconfig/network-scripts/ifcfg-ens33基名，用两种方法实现。 解答： 12345# 第一种：basename /etc/sysconfig/network-scripts/ifcfg-ens192 ifcfg-ens33# 第二种：echo /etc/sysconfig/network-scripts/ifcfg-ens33 | egrep -o "[^/]+$" 第15题题目： 对/etc/目录，分别执行命令，实现以下功能(1)按从大到小顺序显示文件列表(2)只显示隐藏文件(3)只显示目录(4)按mtime时间显示文件列表(5)按atime时间显示文件列表 解答： 12345ll -S /etcll -d /etc/.*ll -d /etc/*/ll -t /etcll -u /etc 第16题题目： 编写/root/bin/excute.sh，实现与用户交互，判断用户给予的参数是否可读，可写，可执行。 解答： 123456789101112#!/bin/env bash# Author: Yu Longjunread -p &quot;请输入文件绝对路径: &quot; pathif [ -e $path ]; then [ -r $path ] &amp;&amp; echo &quot;文件可读&quot; || echo &quot;文件不可读&quot; [ -w $path ] &amp;&amp; echo &quot;文件可写&quot; || echo &quot;文件不可写&quot; [ -x $path ] &amp;&amp; echo &quot;文件可执行&quot; || echo &quot;文件不可执行&quot;else echo &quot;文件不存在&quot;fi 第17题题目： 编写/root/bin/create.sh可以生成新的脚本包括作者、联系方式、版本、时间和描述等，并且可以直接对其进行编辑，编辑完后自动加上执行权限。 解答： 123456789101112131415161718#!/bin/env bash#! Author: Yu Longjuncomments=&quot;&quot;&quot;#!/bin/env bash\n# Author: Yu Longjun\n# Tel: 010-8888888\n# Version: 0.1\n# Date: `date +%F`\n# Description: description\n&quot;&quot;&quot;if [ $# -gt 1 ]; then echo &quot;参数太多,请只输入一个参数&quot;elif [ $# -eq 1 ]; then if [ -e $1 ]; then echo &quot;存在文件$1&quot; else echo -e $comments &gt;$1 chmod +x $1 /usr/bin/vim + -c o $1 fielse echo &quot;没有输入参数，请输入一个参数&quot;fi 第18题题目： 写一个脚本，让它可以传递两个参数后，实现对该参数的加、减、乘、除运算并输出运算后的值。 解答： 123456789101112131415161718192021#!/bin/env bash# Author: Yu Longjunif [ $# -ne 2 ]; then echo &quot;参数个数不对，请输入两个参数&quot;else let result1=$1+$2 let result2=$1-$2 let result3=$1*$2 echo $1 + $2 = $result1 echo $1 - $2 = $result2 echo $1 \* $2 = $result3 if [ $2 -eq 0 ];then echo &quot;$1 / $2 无结果，因为除数不能为0&quot; else let result4=$1/$2 echo $1 / $2 = $result4 fifi 第19题题目： 编写/root/bin/wcfile.sh统计/etc目录中的目录的个数，文件的个数，并求出/etc/目录中的目录和文件个数的总和。 解答： 12345678910#!/bin/env bash# Author: Yu Longjunetc_dir_count=`ls -d /etc/*/ |wc -l`etc_sum_count=`ls -d /etc/* |wc -l`let etc_file_count=$etc_sum_count-$etc_dir_countecho &quot;目录个数为：$etc_dir_count&quot;echo &quot;文件个数为： $etc_file_count&quot;echo &quot;目录文件总和为：$etc_sum_count&quot; 第20题题目： 编写/root/bin/baketc.sh 查找/etc/目录中超过1天未修改的文件，将其压缩备份至/bakup目录。若之前没有备份过则备份之，若存在的备份文件超过了2分钟则备份之，否则退出。备份的格式为YYYY-MM-DD-hh-mm-ss.xz（Y表示年，M表示月，D表示日，h表示时，m表示分，s表示秒） 解答： 12345678#!/bin/env bash# Author: Yu Longjun# 查找backup下的文件(注意不要用/backup要用/backup/*)不超过两分钟的文件，如果没有，就打包文件。flag=`find /backup/* -mmin -2`if [ -z $flag ]; then find /etc/ -mtime +1 |xargs tar -Jcvf /backup/`date +"%Y-%m-%d-%H-%M-%S"`.tar.xzfi]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-bash配置相关]]></title>
    <url>%2Flinux%2F20170605-05-bash-profile%2F</url>
    <content type="text"><![CDATA[配置文件bash的配置文件，按生效范围划分，存在两类： 全局配置： 配置文件 内容 /etc/profile 为登录程序而设置的系统广泛使用的环境和启动程序。官方建议不动，想自定义全局配置的话建议改/etc/profile.d里面的内容。 /etc/profile.d/*.sh 各种配置，如256term.sh（终端配色方案）、vim.sh（vim相关配置）、less.sh（less命令相关）、colorls.sh（命令显示颜色相关）、lang.sh（语言相关）等。 /etc/bashrc 主要设置用户的PS1、history、umask等的全局设置。 个人配置： 配置文件 内容 ~/.bash_profile 个人定义的一些bash变量。 ~/.bashrc 个人定义的一些别名和功能。 tips：个人配置其实写上面哪个文件都行，都可以生效。 登录方式登录方式分为两种方式： 交互式登录。分为下面几种： 直接通过终端输入账号密码登录 使用 su - UserName 切换的用户 执行顺序：/etc/profile –&gt; /etc/profile.d/*.sh –&gt; ~/.bash_profile –&gt; ~/.bashrc –&gt; /etc/bashrc 非交互式登录。分为下面几种： 使用su UserName切换用户 图形界面下打开的终端 执行脚本 任何其它的bash实例 执行顺序： ~/.bashrc –&gt; /etc/bashrc –&gt; /etc/profile.d/*.sh 退出配置退出配置保存在用户的~/.bash_logout文件中，在退出登录shell时运行。 通常配置里写： 创建自动备份 清除临时文件]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>bashrc</tag>
        <tag>bash_profile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-条件测试]]></title>
    <url>%2Flinux%2F20170605-04-shell-condition%2F</url>
    <content type="text"><![CDATA[条件测试分为三种，我们通过man test可以看到大部分的条件测试。 test 命令可以写成[ EXPRESSION ]，这种写法，中括号两边要有空格，表达式是比较的，每一段都要有空格。 tips: 还有一种[[ EXPRESSION ]]的写法，这种写法支持通配符（globing）,一般用通配符的时候才会去用，尽量用单中括号来写。 test命令可以判断三类条件： 数值比较 字符串比较 文件比较 算术比较 表达式 英文描述 中文描述 n1 -eq n2 equal 等于 n1 -ge n2 greater equal 大于等于 n1 -gt n2 greater than 大于 n1 -le n2 less equal 小于等于 n1 -lt n2 less than 小于 n1 -ne n2 not equal 不等于 字符串比较[ EXPRESSION ] tips：[ EXPRESSION ]这种方法的时候，&gt;``&lt;号记得加\转义。 表达式 描述 str1 == str2 相同 str1 != str2 不同 str1 &lt; str2 ascii值比较 str1 &gt; str2 ascii值比较 -n str1 not zero ，是否为非空 -z str1 zero ，是否为空 tips: test 命令和测试表达式使用标准的数学比较符号来表示字符串比较，而用文本代码来表示数值比较。这个细微的特性被很多程序员理解反了。如果你对数值使用了数学运算符号， shell会将它们当成字符串值，可能无法得到正确的结果。 [[ EXPRESSION ]]先看看系统中用[[ EXPESSION ]]的例子： 1234[root❄CentOS6 ~]☭ cat /etc/rc.d/rc.sysinit| grep &quot;\[\[&quot;if [[ &quot;$system_release&quot; == *&quot;Red Hat&quot;* ]]; thenelif [[ &quot;$system_release&quot; == *Fedora* ]]; thenelif [[ &quot;$system_release&quot; =~ &quot;CentOS&quot; ]]; then 这个文档很长，但是很多都是用的单括号，只有涉及到通配符的，才会去用双中括号，所以，我们可以借鉴大牛的做法：普通测试都用单中括号，只有用到通配符的时候，才去用双中括号。 文件比较我们可以使用不同的条件标志测试不同的文件系统相关的属性。 [ -f $file_var ]：如果给定的变量是文件，则返回真。 [ -d $dict_var ]：如果给定的变量是目录，则返回真。 [ -e $file_dict_var ]：如果给定的变量是文件或文件夹，则返回真。 [ -r $readable_var ]：如果给定的变量包含的文件可读，则返回真。 [ -w $write_var ]：如果给定的变量包含的文件可写，则返回真。 [ -x $x_fvar ]：如果给定的变量包含的文件可执行，则返回真。 [ -c $char_var ]：如果给定的变量包含的是一个字符设备文件的路径，则返回真。 [ -b $block_var ]：如果给定的变量包含的是一个块设备文件的路径，则返回真。 [ -L $link_var ]：如果给定的变量包含的是一个符号链接，则返回真。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>test</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-运算]]></title>
    <url>%2Flinux%2F20170605-03-shell-operation%2F</url>
    <content type="text"><![CDATA[算术运算略，后续在写 tips: $RANDOM：随机数0~32767echo $[$RANDOM%50]：随机数0-49 赋值运算普通赋值： =增强型赋值： +=, -=, *=, /=, %=自增自减：不推荐使用。 tips: 不推荐使用自增自检，在c语言里，为了计算机好识别，采用了自增自减。在比较新的语言，如Python，Ruby都不使用自增自减（很容易实现，但是不做），代码读起来不流畅，JavaScript支持自增自减，但是官方也不推荐使用。 布尔运算true false 略，后续再写]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-位置变量和退出码]]></title>
    <url>%2Flinux%2F20170605-02-shell-position-variables-and-exit-code%2F</url>
    <content type="text"><![CDATA[位置变量位置变量：在脚本代码中调用通过命令行传递给脚本的参数 $1, $2, ...$9, ${10}：对应脚本后参数的位置，超过两位数后要加花括号。 $0: 脚本本身（绝对路径） 1234567#!/bin/bashecho The script name is `basename $0`echo "1st arg is $1"echo "2st arg is $2"echo "9st arg is $9"echo "10st arg is $&#123;10&#125;" 结果： 123456[LongDream❄MBP:Scripts]☭ ./testarg.sh &#123;1..10&#125;The script name is testarg.sh1st arg is 12st arg is 29st arg is 910st arg is 10 $*: 传递给脚本的所有参数，全部参数合为一个字符串 $@: 传递给脚本的所有参数，每个参数为独立字符串 $#: 传递给脚本的参数的个数 tips: $@ $* 只在被双引号包起来的时候才会有差异 shift NUM可以用来想做移动位置参数。 12345678910111213141516171819202122232425# shift.sh echo 1st arg is $1echo 2st arg is $2echo 9st arg is $9echo the args counts is $#echo all args are "$*"shiftecho 1st arg is $1echo 2st arg is $2echo 9st arg is $9echo the args counts is $#echo all args are "$*"shiftecho 1st arg is $1echo 2st arg is $2echo 9st arg is $9echo the args counts is $#echo all args are "$*"shift 2 可以看到执行效果：12345678910111213141516[root❄centos7 bin]☭ ./shift.sh &#123;a..z&#125;1st arg is a2st arg is b9st arg is ithe args counts is 26all args are a b c d e f g h i j k l m n o p q r s t u v w x y z1st arg is b2st arg is c9st arg is jthe args counts is 25all args are b c d e f g h i j k l m n o p q r s t u v w x y z1st arg is c2st arg is d9st arg is kthe args counts is 24all args are c d e f g h i j k l m n o p q r s t u v w x y z 退出状态0代表命令运行成功1-255代表命令运行失败 脚本中可以自定义退出码，比如exit 100$?上一个命令运行时候的退出状态码。 &amp;&amp; 与 ||&amp;&amp;：前面命令成功，则运行后面命令。||：前面命令失败，则运行后面命令。 练习题实现自动生成sh脚本的模板。 实现： 12345678#!/bin/env bashecho "#!/bin/bash" &gt;&gt; $1echo "# Script Name: `basename $1`" &gt;&gt;$1echo "# Author: Yu Longjun" &gt;&gt;$1echo "# Version: 0.1" &gt;&gt;$1echo "# Date: `date +%F`" &gt;&gt;$1echo "# Description: " &gt;&gt;$1]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>position variables</tag>
        <tag>exit code</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-全局变量和局部变量]]></title>
    <url>%2Flinux%2F20170605-01-shell-global-and-local-variables%2F</url>
    <content type="text"><![CDATA[全局变量对于shell会话和所有生成的子shell都是可见的。局部变量则只对创建它们的 shell可见。 全局变量（global variables）用printenv就可以打印全局变量，里面会包括系统生成的全局环境变量和用户自定义的环境变量。 下面列出部分CentOS的全局环境变量： 12345678910111213141516171819XDG_SESSION_ID=226HOSTNAME=centos7.yulongjun.com #主机名TERM=xterm-256color #颜色方案SHELL=/bin/bash #当前使用的shellHISTSIZE=1000 #历史命令最大条目SSH_CLIENT=172.17.251.64 50610 22 # ssh client信息，也就是我登录的地址信息SSH_TTY=/dev/pts/2 # 我的终端号USER=root # 当前用户名LS_COLORS=xxxxxxxxxxxxxxxx #ls时文件的配色，太长了省略MAIL=/var/spool/mail/root # 当前用户的系统邮件存放的位置PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin # 命令的查找路径PWD=/app/scripts # 当前目录LANG=en_US.UTF-8 # 当前使用的语言HISTCONTROL=ignoredups # 历史命令控制选项，当前只有一个：去除连续重复的命令SHLVL=1 #shell 层级，当前只有一层shellHOME=/root # 当前用户家目录LOGNAME=root # 登录用户名SSH_CONNECTION=172.17.251.64 50610 172.17.37.200 22 # ssh连接信息，两端的信息都有OLDPWD=/app # 前一个工作目录 可以只打印某一个全局环境变量，有两种方法，记得要用echo调用变量的话，要在变量名前面加一个$： 12345[root@centos7 ~]# printenv HOME/root[root@centos7 ~]# echo $HOME/root 全局变量可用于子shell中(也可以用于当前shell下运行的脚本中，其实运行脚本就是在子shell中运行的）： 1234567891011121314[root@centos7 ~]# bash[root@centos7 ~]# bash[root@centos7 ~]# ps -f --forestUID PID PPID C STIME TTY TIME CMDroot 25202 25198 0 14:22 pts/2 00:00:00 -bashroot 26148 25202 0 15:44 pts/2 00:00:00 \_ bashroot 26180 26148 0 15:45 pts/2 00:00:00 \_ bashroot 26207 26180 0 15:45 pts/2 00:00:00 \_ ps -f --forest[root@centos7 ~]# echo $HOME/root[root@centos7 ~]# exitexit[root@centos7 ~]# exitexit 局部变量（local variables）没有专门的命令查看局部变量，只有一个set命令，会显示当前bash进程设置的所有变量，包括全局和局部。 123456789101112131415161718[root@centos7 ~]# setABRT_DEBUG_LOG=/dev/nullBASH=/bin/bashBASHOPTS=checkwinsize:cmdhist:expand_aliases:extglob:extquote:force_fignore:histappend:interactive_comments:login_shell:progcomp:promptvars:sourcepathBASH_ALIASES=()BASH_ARGC=()BASH_ARGV=()BASH_CMDS=()BASH_COMPLETION_COMPAT_DIR=/etc/bash_completion.d...HISTSIZE=1000HOME=/rootHOSTNAME=centos7.yulongjun.comHOSTTYPE=x86_64ID=0IFS=$&apos; \t\n&apos;... 创建局部变量和全局变量创建局部变量的方法很简单，就是变量名=值，例如var=10。 把局部变量export之后就是全局变量了。 12345678910111213141516[root@centos7 ~]# var=10[root@centos7 ~]# echo $var10[root@centos7 ~]# bash[root@centos7 ~]# echo $var[root@centos7 ~]# exitexit[root@centos7 ~]# echo $var10[root@centos7 ~]# export var[root@centos7 ~]# echo $var10[root@centos7 ~]# bash[root@centos7 ~]# echo $var10 调用变量$变量名即可调用变量，既返回(return)变量的值。 例如： 123456789101112131415[root@centos7 ~]# var=10[root@centos7 ~]# echo $var10[root@centos7 ~]# echo $USERroot[root@centos7 ~]# echo &quot;My hostname is $HOSTNAME&quot;My hostname is centos7.yulongjun.com[root@centos7 ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin[root@centos7 ~]# export PATH=$PATH:/root/bin[root@centos7 ~]# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin tips：PATH=$PATH:/root/bin看起来可能有点绕，其实就是$PATH取出原有的PATH的值，然后和后面的:/root/bin字符串连接起来，然后把连接后的字符串赋值给PATH。 删除环境变量unset 变量名 unset之后，调用变量就是空的。12[root@centos7 ~]# unset var[root@centos7 ~]# echo $var]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>variables</tag>
        <tag>Shell</tag>
        <tag>local variables</tag>
        <tag>global variables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-vim简明教程(附快速记忆方法)]]></title>
    <url>%2Flinux%2F20170602-01-vim%2F</url>
    <content type="text"><![CDATA[vim分为四种模式： 普通模式（normal mode） 插入模式（insert mode） 可视模式（visual mode） 命令模式（excute mode） 下面整理了常用的快捷键和记忆方法（结合英文的记忆方法法） 普通模式光标移动： 按键 效果 记忆方法 h j k l 向左/下/上/右移动 ←↑↓→ w 移动到下个单词开头 word W 移动到下个单词开头(包含标点) Word e 移动到下个单词结尾 end E 移动到下个单词结尾(单词含标点) End b 移动到上个单词开头 back B 移动到上个单词结尾(单词含标点) Back 0 移动到行首 hard ⇤ ^ 移动到行首的非空白符 soft ⇤ $ 移动到行尾 ⇥ H 当前屏幕的第一行 High M 当前屏幕的中间 Middle L 当前页的的最后一行 Low gg 移动到文件第一行 goto line1 G 移动到文件最后一行 Goto EOF 5G 移动到第五行 -Goto line5 查找： 按键 效果 记忆方法 f{char}/F{char} 在行内向下/向上查找字符{char} （光标在字符上） find/Find t{char}/T{char} 在行内向下/向上查找字符{char}（光标在字符前面） till /Till ;/, 跟f/F/t/T结合使用，，跟查找顺序相同/相反的下一个匹配项 - /pattern 文档向下查找匹配项 - ?pattern 文档内向上查匹配项 - n/N 跟/和?结合使用，跟查找顺序相同/相反的下一个匹配项 next/Next 剪切, 复制, 粘贴： 按键 效果 记忆方法 yy 复制当前行 yank 5yy 复制 5 行 5次yank yw 当光标在单词首字母处，复制当前单词 yank word yaw 当光标在单词内部，复制当前单词（单词后面空格也复制） yank around word yiw 当光标在单词内部，复制当前单词（单词后面空格不复制） yank inside word p 在光标后粘贴 paste P 在光标前粘贴 Paste dd 剪切当前行 delete 2dd 剪切 2 行 2次delete dw/dW 光标在单词首字母处，剪切当前单词 delete word daw/daW 剪切当前单词(后面有空格也剪切) delete around word diw/diW 剪切当前单词(后面有空格也剪切) delete inside word D 剪切, 从光标位置到行末 Delete ⇥ x 向后剪切掉一个字符，不用进入插入模式 向后x掉 X 向前剪切掉一个字符，不用进入插入模式 向前X掉 J 去掉行尾的换行符，即连接两行 Join lines u 撤销 undo &lt;ctrl-r&gt; 重做 redo 滚屏： 按键 效果 记忆方法 &lt;Ctrl + b&gt; 向后滚动一屏 backwards &lt;Ctrl + f&gt; 向前滚动一屏 forwards &lt;Ctrl + d&gt; 向后滚动半屏 down &lt;Ctrl + u&gt; 向前滚动半屏 up 插入模式 按键 效果 记忆方法 i 从光标前开始插入字符 insert I 从行首开始插入字符 Insert a 从光标后开始插入字符 append A 从行尾开始插入字符 Append o 在当前行之下另起一行, 开始插入字符 open a new line O 在当前行之上另起一行, 开始插入字符 Open a new line s 删除当前字符，然后进入插入模式(替换) substitute S 删除当前行，然后进入插入模式（替换） substitute r 替换当前字符（其实是属于replace模式） replace R 替换连续的几个字符（属于replace模式） Replace cw/cW 删掉一个单词/带标点的单词，然后进入插入模式 change C 删除光标所在行的光标后面的内容 Change &lt;Esc&gt; 退出插入模式 - 可视模式（visual mode） 按键 效果 记忆方法 v 选择字符 visual V 选择行 Visual line &lt;ctrl-v&gt; 选择块 visual block gv 重复上次的高亮区域 - o 结合可视模式用的o，回到活动端点 - vw 光标在单词首字母处，选择单词 visual word vaw 选择单词（包括单词后面的空格） visual around world viw 选择单词（不包括单词后面的空格） visual inside world vit 选择标签内的内容（html） visual inside tags 命令行模式 按键 效果 记忆方法 :w 保存、写入 write :x/:wq 保存并退出 write quit :q! 直接退出 quit r filename 读文件内容到当前文件中 read filename w filename 将当前文件内容另存到另一个文件 write filename !command 执行命令 !command r!command 读入命令的输出 read !command :set number 设置行符 略 :syntax on/:syntax off 开启/关闭代码高亮 略 替换命令： :s/target/replacement/：替换当前行的第一个target为replacement :s/target/replacement/g：替换当前行的所有的target为replacement :n,$s/target/replacement/：替换第n到最后一行的第一个target为replacement :n,$s/target/replacement/g：替换第n到最后一行的所有的target为replacement :%s/target/replacement：替换所有行的第一个target为replacement :%s/target/replacement/g：替换所有行的所有的target为replacement 用#或+作为分隔符，/作为匹配项中的内容: :s#target/#/replacement#g：替换所有行的第一个target/为/replacement :%s+/oradata/apras/+/user01/apras1+g:替换所有行的/oradata/apras/为/user01/apras1/ 颜色&lt;ctrl+v+[&gt; 在颜色方案前面插入上述三个按键，效果是蓝色的^[（并不是字符^[，只是这三个键呈现在屏幕的效果） 呈现的效果是这样： tips：颜色方案参考回显命令中的打印带颜色的回显]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-文本处理练习题]]></title>
    <url>%2Flinux%2F20170531-03-practice%2F</url>
    <content type="text"><![CDATA[grep练习题1、显示/proc/meminfo文件中以大小s开头的行(要求：使用两 种方法) 12345678# 第一种：grep "^[Ss] /proc/meminfo# 第二种：grep -i "^s" /proc/meminfo# 第三种：grep -i "^\(S\|s\)" /proc/meminfo# 第四种：grep -e "^s" -e "^S" /proc/meminfo 2、显示/etc/passwd文件中不以/bin/bash结尾的行 1grep -v "/bin/bash$" /etc/passwd 3、显示用户rpc默认的shell程序 1grep "^rpc\&gt;" /etc/passwd| cut -d : -f1,7 4、找出/etc/passwd中的两位或三位数 1grep -o "\&lt;[[:digit:]]\&#123;2,3\&#125;\&gt;" /etc/passwd 5、显示CentOS7的/etc/grub2.cfg文件中，至少以一个空白字符开头的且后面存非空白字符的行 1grep "^[[:space:]]\+[^[:space:]]\+" /etc/grub2.cfg 6、找出“netstat -tan”命令的结果中以‘LISTEN’后跟任意多个空白字符结尾的行 1netstat -tan | grep "\&lt;LISTEN\&gt;[[:space:]]*" 7、显示CentOS7上所有系统用户的用户名和UID 1234# 第一种方法：cat /etc/passwd |cut -d: -f1,3 | grep ":[[:digit:]]\&#123;,3\&#125;$"# 第二种方法：cat /etc/passwd |cut -d: -f1,3 | grep "\&lt;[[:digit:]]\&#123;,3\&#125;\&gt;$" 8、添加用户bash、testbash、basher、sh、nologin(其shell 为/sbin/nologin),找出/etc/passwd用户名同shell名的行 1234567echo bash testbash basher sh nologin | xargs -n1 useraddchsh -s /sbin/nologin nologin# grep：cat /etc/passwd | grep "\(^.*\&gt;\).*/\1$"# egrep 扩展：cat /etc/passwd | egrep "(^.*\&gt;).*/\1$" 9、利用df和grep，取出磁盘各分区利用率，并从大到小排序 1234第一种方法：df |grep "/dev/sd"|tr -s " "|cut -d" " -f1,5|sort -nr -k2第二种方法：df |grep "/dev/sd"|egrep -o "[[:digit:]]+%"|sort -nr egrep练习题1、显示三个用户root、mage、wang的UID和默认shell 1egrep "^(root|mage|wang)\&gt;" /etc/passwd | cut -d: -f1,3,7 2、找出/etc/rc.d/init.d/functions文件中行首为某单词(包括下划线)后面跟一个小括号的行用[:alnum:]或者单词边界\&gt;、\b123egrep -o "^[[:alnum:]_]+\(\)" /etc/rc.d/init.d/functionsegrep -o "^.*\&gt;\(\)" /etc/rc.d/init.d/functionsegrep -o "^.*\b\(\)" /etc/rc.d/init.d/functions 3、使用egrep取出/etc/rc.d/init.d/functions中其基名 123456# 文件，或者目录后面不带/：echo /etc/rc.d/init.d/functions |egrep -o "[[:alnum:]]+$"echo /etc/rc.d/init.d/functions |egrep -o "[^/]+$"# 文件或目录，后面带不带/都行，通用写法：echo /etc/rc.d/init.d/functions/ |egrep -o "[[:alnum:]]+/?$"echo /etc/rc.d/init.d/functions/ |egrep -o "[^/]+/?$" 4、使用egrep取出上面路径的目录名 12 5、统计last命令中以root登录的每个主机IP地址登录次数 1last |tr -s " " |cut -d" " -f1,3|egrep "root.*[[:digit:]].+"|sort|uniq -c 6、利用扩展正则表达式分别表示0-9、10-99、100-199、 200-249、250-255 12345echo &#123;0..1000&#125; |egrep -o "\&lt;[0-9]\&gt;"echo &#123;0..1000&#125; |egrep -o "\&lt;[1-9][0-9]\&gt;"echo &#123;0..1000&#125; |egrep -o "\&lt;1[0-9]&#123;2&#125;\&gt;"echo &#123;0..1000&#125; |egrep -o "\&lt;2[0-4][0-9]\&gt;"echo &#123;0..1000&#125; |egrep -o "\&lt;25[0-5]\&gt;" 7、显示ifconfig命令结果中所有IPv4地址 1ifconfig | egrep -o "\&lt;(([0-9]|[1-9][0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])\.)&#123;3&#125;([0-9]|[1-9][0-9]|1[0-9]&#123;2&#125;|2[0-4][0-9]|25[0-5])" 8、将此字符串：welcome to magedu linux 中的每个字符 去重并排序，重复次数多的排到前面 1echo welcom to magedu linux |grep -o . |sort|uniq -c|sort -nr]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>RegExp</tag>
        <tag>grep</tag>
        <tag>egrep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-正则表达式(RegExp)]]></title>
    <url>%2Flinux%2F20170531-02-regexp%2F</url>
    <content type="text"><![CDATA[Regular Expression，正则表达式，简写为RegExp。 BRE ：Basic RegExp，基本正则表达式 ERE ：Extend RegExp，扩展正则表达式 PCRE ：Perl Compatible RegExp，Perl兼容的正则表达式 基本正则表达式字符匹配 . 匹配任意单个字符：如r..t [] 匹配指定范围内的任意单个字符：如[abc]r [^] 匹配指定范围外的任意单个字符：如[^abc]r [:alnum:] 字母和数字 [:alpha:] 代表任何英文大小写字符，亦即 A-Z, a-z [:lower:] 小写字母 [:upper:] 大写字母 [:blank:] 空白字符（空格和制表符） [:space:] 水平和垂直的空白字符（比[:blank:]包含的范围广） [:cntrl:] 不可打印的控制字符（退格、删除、警铃…） [:digit:] 十进制数字 [:xdigit:]十六进制数字 [:graph:] 可打印的非空白字符 [:print:] 可打印字符 [:punct:] 标点符号 匹配次数 * 匹配前面的字符任意次，包括0次（贪婪模式，尽可能长的匹配） .* 任意长度的任意字符 \? 匹配其前面的字符0或1次 \+ 匹配前面的字符1次或多次 \{n\} 匹配前面的字符n次 \{m,n\} 匹配前面的字符至少m次，至多n次 \{,n\} 匹配前面的字符至多n次 \{n,\} 匹配前面的字符至少n次 位置锚定位置锚定：定位出现的位置 ^ 行首锚定，用于模式的最左侧(要跟字符匹配里的[^]区分开，那个是在中括号里面的) $ 行尾锚定，用于模式的最右侧 ^PATTERN$ 用于模式匹配整行 ^$ 空行 ^[[:space:]]*$ 空白行 \&lt; 或 \b 词首锚定，用于单词模式的左侧 \&gt; 或 \b 词尾锚定；用于单词模式的右侧 \&lt;PATTERN\&gt; 匹配整个单词 分组分组：\(\)将一个或多个字符捆绑在一起，当作一个整体进 行处理，如：\(root\)\+ 分组括号中的模式匹配到的内容会被正则表达式引擎记录于 内部的变量中，这些变量的命名方式为: \1, \2, \3, …\1 表示从左侧起第一个左括号以及与之匹配右括号之间的 模式所匹配到的字符 示例： \(string1\+\(string2\)*\) \1 ：string1\+\(string2\)* \2 ：string2 后向引用：引用前面的分组括号中的模式所匹配字符，而非模式本身 grep &quot;\(root\).*\1&quot; /etc/passwd 或者\| 示例： a\|b: a或b C\|cat: C或cat \(C\|c\)at:Cat或cat 扩展正则表达式egrep == grep -E egrep其实很简单，就是把grep里的斜线去掉了，不过有些还没有去掉。 字符匹配： . 任意单个字符 [] 指定范围的字符 [^] 不在指定范围的字符 次数匹配： *：匹配前面字符任意次 ?：0或1次 +：1次或多次 {m}：匹配m次 {m,n}：至少m，至多n次 位置锚定： ^：行首 $：行尾 \&lt;, \b :语首 \&gt;, \b :语尾 分组： () 后向引用：\1, \2, … 或者：| 示例： a|b: a或b C|cat: C或cat (C|c)at:Cat或cat]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>RegExp</tag>
        <tag>grep</tag>
        <tag>egrep</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-文本处理工具(cut、sort、tr、grep等)]]></title>
    <url>%2Flinux%2F20170531-01-text-tools%2F</url>
    <content type="text"><![CDATA[命令目录，查看某一个命令可点击直接跳转： 文件查看 cat tac rev more less 按行截取 head tail 转化内容 tr 按列操作 cut paste 分析文本 wc sort uniq diff、patch 按关键字过滤 grep 文件查看cat查看文件，从第一行到最后一行全部显示。 参数： -E: 显示行结束符$ -A：显示不可打印字符，通常查看脚本是否有多加空格tab回车之类的。 -n：对显示出的每一行进行编号 -s：显示行号，压缩连续的空行，只显示一行空行 -b ：空行不加行号，等同于nl命令 示例： 1234567891011121314151617181920212223242526272829303132333435363738394041424344[root❄centos7 app]☭ cat catfileAAA BBBCCC DDDEEEGGGHHH[root❄centos7 app]☭ cat -A catfile AAA^IBBB$CCC DDD$EEE$$GGG$$$HHH$[root❄centos7 app]☭ cat -n catfile 1 AAA BBB 2 CCC DDD 3 EEE 4 5 GGG 6 7 8 HHH[root❄centos7 app]☭ cat -b catfile 1 AAA BBB 2 CCC DDD 3 EEE 4 GGG 5 HHH[root❄centos7 app]☭ cat -s catfileAAA BBBCCC DDDEEEGGGHHH tac从后往前显示文件，和cat相反。 示例： 123456789[root❄centos7 app]☭ tac catfileHHHGGGEEECCC DDDAAA BBB revreverse lines水平反转每一行里的字母。 示例1，水平翻转文件： 123456789[root❄centos7 app]☭ rev catfile BBB AAADDD CCCEEEGGGHHH 示例2，倒序+水平翻转文件： 123456789[root❄centos7 app]☭ tac catfile | revHHHGGGEEEDDD CCCBBB AAA more分页显示，显示到最后会退出。 空格或者f下一页 b上一页（管道后跟more无法上一页） less分页显示，显示到最后也不会退出，要按q退出。 翻屏操作： 键盘按键 效果 空格键 向下滚动一屏 b 向上滚动一屏 j 向下移动一行 k或回车 向上移动一行 gg 跳转至第一行 5g 跳转到第5行 G 跳转到最后一行 q 退出 文本搜索: 键盘按键 效果 /keyword 向下查找，不区分大小写 ?keyword 向上查找，不区分大小写 n 查找下一个，与查找方向相同 N 查找上一个，与查找方向相反 按行截取head取文件头部的行，默认头10行。 参数： -c NUM：显示几个字符 -n NUM：显示几行 12# 取随机数里，数字或字母的字符，取前20个字符，可以用来做随机密码。cat /dev/urandom | tr -dc "0-9a-zA-Z" | head -c 20 tail显示文件尾部的行，默认最后10行。 参数： -c NUM：只显示倒数几个字符。 -n NUM：示倒数几行。 -f：follow，跟踪这个文件的变动，用来看日志比较多 例子：1tail -n0 -f /var/log/messages &amp;` -n0只显示新增加的内容，命令最后加一个&amp;，表示放在后台运行，可以去运行其他命令不受影响。当日志变动的时候，会在前台打印出一条，按回车可以继续运行其他命令。 转化内容trtr [OPTION]... SET1 [SET2]把输入的数据当中的字符，凡是在SET1定义范围内出现的，通通对位转换为SET2出现的字符 参数： tr SET1 SET2 &lt; /path/from/somefile对位转化SET1中的字符为SET2中的字符 tr -d SET1 &lt; /path/from/somefile删除指定集合里出现的字符 tr -s &quot;\n&quot; /path/from/somefile把指定的连续的字符以一个字符表示，压缩。 tr -cComplement ,取字符集的补集，通常与其他参数结合使用，例如-dc 123456789101112[root❄centos7 ~]☭ tr &apos;a-z&apos; &apos;A-Z&apos;aabbcc33AABBCC33^C[root❄centos7 ~]☭ tr &apos;a-z&apos; &apos;A-Z&apos; &lt;/etc/issue\SKERNEL \R ON AN \M[root❄centos7 ~]☭ tr &apos;a-z&apos; &apos;A-Z&apos; &lt; /etc/issue &gt; /app/issue2[root❄centos7 ~]☭ cat /app/issue2\SKERNEL \R ON AN \M tips：如果是输入后再输出到同一个文件，就会清空这个文件，所以最好不要这么用，下面是一个错误示范： 12345[root❄centos7 ~]☭ cd /app/[root❄centos7 app]☭ cp issue2 issue3[root❄centos7 app]☭ tr &apos;a-z&apos; &apos;A-Z&apos; &lt; issue3 &gt;issue3[root❄centos7 app]☭ cat issue3[root❄centos7 app]☭ 追加是可以的，在原有文件基础上再追加一段： 1234567891011[root❄centos7 app]☭ cat issue2\SKERNEL \R ON AN \M[root❄centos7 app]☭ tr &apos;A-Z&apos; &apos;a-z&apos; &lt; issue2 &gt;&gt; issue2[root❄centos7 app]☭ cat issue2\SKERNEL \R ON AN \M\skernel \r on an \m dc结合使用： 123456789[root❄centos7 app]☭ echo &#123;a..z&#125; &gt;f1[root❄centos7 app]☭ cat f1a b c d e f g h i j k l m n o p q r s t u v w x y z[root❄centos7 app]☭ tr -d &apos;f-n&apos; &lt;f1a b c d e o p q r s t u v w x y z[root❄centos7 app]☭ cat f1a b c d e f g h i j k l m n o p q r s t u v w x y z[root❄centos7 app]☭ tr -dc &apos;f-n&apos; &lt; f1fghijklmn[root❄centos7 app]☭ 按列操作cutcut可以实现分割每一行，并且指定输出列的字段。 -d DELIMITER：指定分隔符（delimiter） -f FIELDS：取指定字段（fileds） 示例1： 123456789# 以冒号作为分隔符，取1到3、7的字段cut -d: -f1-3,7 /etc/passwd# 结果root:x:0:/bin/bash...LongDream:x:1000:/bin/bashyu:x:1001:/bin/bashalice:x:1002:/bin/bashtom:x:1003:/bin/bash 示例2： 123456789101112# 取磁盘利用率,先tr压缩空格为一个空格，然后以空格作为分隔符，取第五个字段df |tr -s " "| cut -d" " -f5# 结果Use%5%0%0%1%0%1%22%0% paste把多个文件的多行进行合并，逐行进行合并。 参数： -d指定分隔符,默认是tab -s把每个文件里的多行合成一行，每个文件一行。 示例： 1234567891011121314151617181920212223[root❄centos7 app]☭ cat file1nameaaabbbccc[root❄centos7 app]☭ cat file2age182022[root❄centos7 app]☭ paste file1 file2name ageaaa 18bbb 20ccc 22[root❄centos7 app]☭ paste -d: file1 file2name:ageaaa:18bbb:20ccc:22[root❄centos7 app]☭ paste -s file1 file2name aaa bbb cccage 18 20 22 分析文本wc直接运行原wc命令：输出文件中的行数、单词数、字节数 参数： -c：输出字节数 -m：输出字符数 -l：输出行数 -L：输出最长的行的长度。 -w： 输出单词统计数。 sort排序，默认是按照字符的大小来排列 -t ：指定分隔符 -k：以哪一个为分割 -n：按数字大小排列，从小到大 -r：反向，从打到小 -u：删除重复的行 示例1：1234# 以冒号分割，取第三列UID，nr表示按照数值大小倒序（由大到小）排列。sort -t: -k3 -nr /etc/passwd# 先cut只留第一列用户名和第三列UIDcut -d: -f1,3 /etc/passwd | sort -t: -k2 -nr 示例2： 12# 取分区使用率最大值df |tr -s " " "%"|cut -d% -f5|sort -nr|head -n1 uniq唯一，从输入中删除前后相接的重复的行 -c: 显示每行重复出现的次数 -d: 仅显示重复过的行 -u: 仅显示不曾重复的行 常和sort 命令一起配合使用，示例： 1234# 把用户名列表进行排序（重复的会在相邻的行），然后统计重复用户出现的次数：sort userlist.txt | uniq -c# 登录用户的登录次数：`last | cut -d ' ' -f1|sort| uniq -c |sort -nr` diff、patchdiff比较两个文件的区别 1234567# 比较两个文件的区别，发现第5行有区别diff foo.conf-broken foo.conf-works5c5&lt; use_widgets = no---&gt; use_widgets = yes patch可以用diff生成的patch来修复另一个文件。 123diff -u foo.conf-broken foo.conf-works &gt; foo.patch #把差异写到foo.patch补丁里cp foo.conf-broken foo.conf-broken.bak #备份一下patch -b foo.conf-broken foo.patch # 从差异的补丁进行恢复 按关键字过滤grep文本过滤工具 不带参数普通用法： grep root /etc/passwd 带参数： -v：显示不被pattern匹配到的行 -i：忽略字符大小写 -n：显示匹配的行号 -c：统计匹配的行数 -o：仅显示匹配到的字符串 -q：静默模式，不输出任何信息(quiet，可以结合echo $?状态码使用) -A NUM after, 包含匹配行的后NUM行 -B NUM:before, 包含匹配行的前NUM行 -C NUM context, 包含匹配行的前后各NUM行 -e: 实现多个选项间的逻辑or关系 -w匹配整个单词(字母、下划线、数字汉字，这几个连在一起算一个单词) -E使用ERE -F相当于fgrep，不支持正则表达式 参考书籍：关于grep的各个参数的具体用法，可以看《Linux Shell 脚本攻略（第2版）》4.3章节，有详细说明，这里不做赘述。 例子： 12# 扫描172.16.252。0段的机器，如果主机是up状态（Host is up）的，显示之前一行（-B1）,前一行有ip地址，然后grep for是因为只有有ip的那行有for，然后以空格为分隔符，取第五个，就是ip地址。nmap -v -sP 172.17.252.0/24 |grep -B1 "Host is up."|grep for|cut -d" " -f5]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>grep</tag>
        <tag>cut</tag>
        <tag>sort</tag>
        <tag>wc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-权限和ACL练习题]]></title>
    <url>%2Flinux%2F20170527-04-practice%2F</url>
    <content type="text"><![CDATA[1、在/testdir/dir里创建的新文件自动属于g1组，组 g2的成员如：alice能对这些新文件有读写权限，组g3 的成员如：tom只能对新文件有读权限，其它用户（不 属于g1,g2,g3）不能访问这个文件夹。 前期准备： 1234567[root❄centos7 ~]☭ echo g&#123;1,2,3&#125; |xargs -n 1 groupadd[root❄centos7 ~]☭ tail -n 3 /etc/groupg1:x:1002:g2:x:1003:g3:x:1004:[root❄centos7 ~]☭ useradd alice -g g2[root❄centos7 ~]☭ useradd tom -g g3 题目解答： 12345chgrp g1 /testdir/dirchmod g+s /testdir/dirsetfacl -Rm d:g:g2:rw /testdir/dirsetfacl -Rm d:g:g3:r /testdir/dirchmod o= /testdir/dir 2、备份/testdir/dir里所有文件的ACL权限到/root/acl.txt中，清除/testdir/dir中所有ACL权限 ，最后还原ACL权限。 题目解答： 123456789101112# 备份权限[root❄centos7 /]☭ getfacl -R /testdir/dir &gt; /root/acl.txt# 清除/testdir/dir权限[root❄centos7 /]☭ setfacl -Rb /testdir/dir# 还原ACL权限，两种方法。# 第一种绝对路径法：setfacl --set-file=/root/acl.txt /testdir/dir# 第二种相对路径法（查看acl.txt看是相对路径testdir/dir,是相对于/的，所以先cd到/,这步不能省）：[root❄centos7 /]☭ cd /[root❄centos7 /]☭ setfacl --restore /root/acl.txt]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>acl</tag>
        <tag>facl</tag>
        <tag>permission</tag>
        <tag>practice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-访问控制列表（ACL）]]></title>
    <url>%2Flinux%2F20170527-03-acl%2F</url>
    <content type="text"><![CDATA[ACL：Access Control List 访问控制列表 除了文件原本的权限位设置，可以自定义用户访问控制。 CentOS 7 默认创建的xfs和ext4文件系统具有ACL功能 CentOS 7 之前版本，系统安装时候创建的文件系统有ACL功能，默认手工创建的ext4文件系统无ACL功 能，需手动增加。 比如在6上新加一块磁盘，创建了sdb1分区，可以用下面命令使其支持ACL： 123tune2fs –o acl /dev/sdb1mount –o acl /dev/sdb1 /mnt/test 基本命令用法getfacl XXX获取文件或文件夹的权限： 123456789101112131415161718[root❄centos7 app]☭ touch a[root❄centos7 app]☭ getfacl a# file: a# owner: root# group: rootuser::rw-group::r--other::r--[root❄centos7 app]☭ mkdir dir1[root❄centos7 app]☭ getfacl dir1# file: dir1# owner: root# group: rootuser::rwxgroup::r-xother::r-x setfacl(set file acl)给一个文件加了facl的话，整个权限位后面会有个+号，表示设置了facl权限。 -m 参数，表示modify 禁止yu用户访问file1 12345678910[root❄centos7 app]☭ setfacl -m u:yu:000 file1[root❄centos7 app]☭ getfacl file1# file: file1# owner: root# group: rootuser::rw-user:yu:---group::r--mask::r--other::r-- 禁止yu组访问file2 12345678910[root❄centos7 app]☭ setfacl -m g:yu:000 file2[root❄centos7 app]☭ getfacl file2# file: file2# owner: root# group: rootuser::rw-group::r--group:yu:---mask::r--other::r-- 禁止其它用户访问file1 12345678[root❄centos7 app]☭ setfacl -m o:0 file3[root❄centos7 app]☭ getfacl file3# file: file3# owner: root# group: rootuser::rw-group::r--other::--- 默认的从上到下的权限依次是： #owner:文件的拥有者#group：文件的拥有组user：自定义的用户（可多个）group：自定义的组（可多个）other：自定义的其它用户（可多个） 如果一个用户属于多个自定义的组，权限是这几个组的权限的累加。 setfacl的其他用法setfacl -Rb *：清除所有acl setfacl -M TEXT：利用列表来批量设置权限 1234567891011121314[root❄centos7 ~]☭ cat file.acl u:yu:-g:yu:rwx[root❄centos7 ~]☭ setfacl -M file.acl xxx[root❄centos7 ~]☭ getfacl xxx# file: xxx# owner: root# group: rootuser::rw-user:yu:---group::rw-group:yu:rwxmask::rwxother::rw- -d 参数（default的意思），默认在facl权限下的目录，新创建的文件和文件夹继承上一级目录的权限。例如： setfacl -md d:u:yu:rwx /app/dir2 那么在dir2下新创建的目录就默认继承dir2的acl权限。 -x删除某个权限： setfacl -x u:yu:rwx /app/dir2 -X按照文件里的内容来批量删除权限： setfacl -X file.acl xxx acl中的maskfacl的mask是高压线，是影响自定义用户和自定义组的权限，有了mask后，会与自定义的用户和组的权限做逻辑与，这之后是自定义用户和组的真实权限。 默认是没有高压线的，默认值一般都是所有自定义用户和组的最高权限累加。 如果设置了，就相当于设定了高压线，高于这个高压线的权限，会降低到高压线之下。 12345678910111213141516171819202122232425[root❄centos7 app]☭ touch file4[root❄centos7 app]☭ setfacl -m u:yu:rwx file4 [root❄centos7 app]☭ setfacl -m g:yu:rw- file4[root❄centos7 app]☭ getfacl file4# file: file4# owner: root# group: rootuser::rw-user:yu:rwxgroup::r--group:yu:rw-mask::rwxother::r--[root❄centos7 app]☭ setfacl -m m:r-- file4[root❄centos7 app]☭ getfacl file4# file: file4# owner: root# group: rootuser::rw-user:yu:rwx #effective:r--group::r--group:yu:rw- #effective:r--mask::r--other::r-- 我们可以看到，设置了高压线后，自定义用户和自定义组的有效值（effective）是不超过mask的r-- facl权限的复制、备份和恢复复制acl权限： getfacl file1 | setfacl --set-file=- file2 主要的文件操作命令cp和mv都支持ACL，只是cp命令需要 加上-p 参数。但是tar等常见的备份工具是不会保留目录 和文件的ACL信息。 123456789101112# 得到app目录及其目录里所有文件的acl权限（-R参数），写入到/root/app.acl文件里[root❄centos7 app]☭ getfacl -R /app &gt;/root/app.acl# 去掉/app目录及其目录里所有文件的acl权限（-b去除）[root❄centos7 app]☭ setfacl -R -b /app# 恢复权限# 绝对路径法：setfacl --set-file=/root/app.acl /app#相对路径法（需要在要恢复的文件夹或文件的上一级目录运行）：setfacl --restore /root/app.acl]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>acl</tag>
        <tag>facl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-特殊权限]]></title>
    <url>%2Flinux%2F20170527-02-special-permission%2F</url>
    <content type="text"><![CDATA[X（大写）X：给目录x权限，不给文件x权限（当文件本来就有x权限的话会重新赋予x权限） 例如： chmod -R +X dir2 对dir2文件夹本身和dir2里的文件夹加执行权限对文件的话： 如果这个文件原来没有x权限，就不加。 如果这个文件任意位置有x权限，就会给这个文件重新增加执行权限。 SUIDSUID属性一般运用在可执行文件上，当用户执行该执行文件时，会临时拥有该执行文件所有者的权限。 表现在权限位上就是一个s： 12[root❄centos7 ~]☭ ll /bin/passwd-rwsr-xr-x. 1 root root 27832 Jun 10 2014 /bin/passwd 必须放在二进制的、可执行的程序上才有用，比如/bin/passwd就有suid权限。 当普通用户使用passwd执行修改密码时，发给root去执行写入/etc/shadow的操作。（写/etc/shadow的这个操作，普通用户对/etc/shadow无写权限，这时候就需要s权限） 用法： chown u+s BIN chown u-s BIN chown 4xxx BIN tips：当一个文件或文件夹没有x权限时候，表明SUID无效，表现为一个S（大写）： 123456[root❄centos7 app]☭ chmod a-x file1[root❄centos7 app]☭ ll file1-rw-r--r-- 1 root root 0 May 29 16:27 file1[root❄centos7 app]☭ chmod u+s file1[root❄centos7 app]☭ ll file1-rwSr--r-- 1 root root 0 May 29 16:27 file1 SGIDSGID于SUID不同，SGID属性可以应用在目录或可执行文件上。 当SGID属性应用在目录上时，该目录中所有建立的文件或子目录的拥有组都会是该目录的拥有组。当SGID属性应用在可执行文件上时，其他用户在使用该执行文件时就会临时拥有该执行文件拥有组的权限。 chmod g+s BINchmod g-s BINchmod 2xxx BIN Sticky 粘滞位Sticky作用在目录上才有意义。 在目录上加这个权限，那在这个目录里的文件，只有拥有者用户自己或者root可以删，其他普通用户不能删。 chmod o+t DIRchmod o-t DIRchmod 1xxx DIR 1234[root❄centos7 app]☭ chmod 7770 haha[root❄centos7 app]☭ lltotal 0-rwsrws--T. 1 root root 0 May 25 19:49 haha tips：执行完更改特殊权限位，最好ll看一下有没有问题，要养成这样的好习惯。 chattrchattr +i xxx 锁定该文件 ，不允许修改，删除，重命名，重定向方式清空也不可以。 chattr -i xxx 去掉锁定。 chattr +a xxx只能&gt;&gt;追加内容，不能删除和减少。 chattr -a xxx去掉只能追加这种锁定 ls是看不到attr属性的，可以用lsattr显示特定属性。 12345678910111213[root❄centos7 app]☭ touch file1[root❄centos7 app]☭ chattr +i file1[root❄centos7 app]☭ lsattr file1----i----------- file1[root❄centos7 app]☭ echo &quot;yulongjun&quot; &gt; file1-bash: file1: Permission denied[root❄centos7 app]☭ echo &quot;yulongjun&quot; &gt;&gt; file1-bash: file1: Permission denied[root❄centos7 app]☭ mv file1 /tmpmv: cannot remove ‘file1’: Operation not permitted[root❄centos7 app]☭ rm file1rm: remove regular empty file ‘file1’? yrm: cannot remove ‘file1’: Operation not permitted 123456789101112[root❄centos7 app]☭ touch file2[root❄centos7 app]☭ chattr +a file2[root❄centos7 app]☭ lsattr file2-----a---------- file2[root❄centos7 app]☭ echo &quot;yulongjun&quot; &gt; file2-bash: file2: Operation not permitted[root❄centos7 app]☭ echo &quot;yulongjun&quot; &gt;&gt; file2[root❄centos7 app]☭ cat file2yulongjun[root❄centos7 app]☭ rm file2rm: remove regular file ‘file2’? yrm: cannot remove ‘file2’: Operation not permitted]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>special permission</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-文件和目录权限]]></title>
    <url>%2Flinux%2F20170527-01-permission%2F</url>
    <content type="text"><![CDATA[权限位rwxrwrwx：左三位：定义user（owner）的权限，属主权限中三位：定义group的权限，属组权限有三位：定义other的权限，其他的权限 进程对文件的访问权限应用模型：进程的属主与文件的属主是否相同。如果相同，则应用属主权限；否则去检查金证的属于是否属于文件的属组；如果是，则应用属组权限，否则，就只能引用other权限。 权限：r：readable，可读w：writable，可写x：executble，可执行 文件 r：可获取文件的数据 w：可修改文件的数据 x：可将此文件运行为进程 目录 r：可使用ls命令获取旗下的所有文件列表 w：可修改次目录下的文件列表，即创建或删除文件 x：可cd值此目录中，且可使用ll来获取到所有文件详细属性信息 权限组合机制： ---：二进制000，十进制0 --x：二进制001，十进制1 -w-：二进制010，十进制2 -wx：二进制011，十进制3 r--：二进制100，十进制4 r-x：二进制101，十进制5 rw-：二进制110，十进制6 rwx：二进制111，十进制7 文件权限管理命令chmod的三种用法： chmod [OPTION]… MODE[,MODE]… FILE…chmod [OPTION]… OCTAL-MODE FILE…chmod [OPTION]… –reference=RFILE FILE… 三类用户： u：属主 g：属组 o；其他 a：所有 (1)MODE表示法：chmod [OPTION]... MODE[,MODE]... FILE... 赋值表示法：直接操作一类用户的所有权限位： u= g= o= a= 授权表示法：直接操作一类用户的一个或多个权限位： u+、u- g+、g- o+、o- a+、a- 1234567891011121314151617181920212223touch testchmod g=rw testllchmod ug=r testllchmod u=rwx,g=rw,o= testllchmod u-x testllchmod o+r testllchmod ug+x testllchmod g-wx testllchmod u-r testllchmod +x test # +x对所有都有效llchmod +w test # +w只对属主有效llchmod u+x,g+w testll (2)八进制表示法：chmod [OPTION]… OCTAL-MODE FILE… 1234chmod 660 testllchmod 777 testll (3)参考某个文件的权限：chmod [OPTION]… –reference=RFILE FILE… 1chmod --reference=/etc/fstab test (4)chmod选项 -R ：recursive ，递归修改 从属关系管理命令主要有两个命令chown，chgrp chown更改文件的的属主(属组也能更改) -R：递归修改 用法：chown -R USERNAME[:FILENAME] FILENAME 例如：chown -R oracle:oinstall /u01 chgrp更改文件的属组 -R：递归修改 用法：chgrp -R GROUPNAME FILENAME用到很少，chown可以修改属组，这个命令就被打入冷宫了，很少用。 新建文件和目录的默认权限（umask）新建的文件和文件夹都有一个默认权限，那么是如何实现的呢？就是用umask实现的。 umask原理： 针对新建文件：666-umask后的权限就是新建文件的权限 针对新建目录：777-umask后的权限就是新建目录的权限。 我们可以看/etc/bashrc里面的umask规则： ![]/images/1496043520606.png) 看代码，即UID 大于199，umask为002，否则umask为022 普通用户UID在CentOS 6下大于500，CentOS7下大于1000，所以umask值为002，默认创建的文件和文件夹权限分别为666-002=664、777-002=775。 而root用户，UID为0，umask值为022，默认创建的文件和文件夹权限分别为666-022=644、777-022=755。 练习题1、当用户xiaoming对/testdir 目录无执行权限时，意味着无 法做哪些操作？ 无法cd到这个目录，无法删除、移动这个文件。 2、当用户xiaoqiang对/testdir 目录无读权限时，意味着无 法做哪些操作？ 无法查看这个目录，也无法ls查看这个目录里面内容。 3、当用户wangcai 对/testdir 目录无写权限时，该目录下的只读文件file1是否可修改和删除？ 不可以修改和删除。 4、当用户wangcai 对/testdir 目录有写和执行权限时，该目 录下的只读文件file1是否可修改和删除？ 可以修改和删除，但是没有读权限的话，只能盲找。 5、复制/etc/fstab文件到/var/tmp下，设置文件所有者为 wangcai读写权限，所属组为sysadmins组有读写权限，其他人无权限 123cp /etc/fstab /var/tmp/chown wangcai:sysadmins /var/tmp/fstabchmod 660 /var/tmp/fstab 6、误删除了用户wangcai的家目录，请重建并恢复该用户家目录及相应的权限属性 1234rm -rf /home/wangcaicp -r /etc/skel/ /home/wangcaichmod 700 /home/wangciachown -R wangcai:wangcai /home/wangcai]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>permission</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-实践：Linux用户、组和密码相关文件被破坏如何恢复系统]]></title>
    <url>%2Flinux%2F20170526-05-recovery-system%2F</url>
    <content type="text"><![CDATA[我们先看一下用户用户组和密码相关文件： 123456789[root❄centos7 ~]☭ ll /etc/passwd* /etc/shadow* /etc/group* /etc/gshadow*-rw-r--r--. 1 root root 988 May 28 02:34 /etc/group-rw-r--r--. 1 root root 975 May 28 02:29 /etc/group-----------. 1 root root 801 May 28 02:34 /etc/gshadow----------. 1 root root 790 May 28 02:19 /etc/gshadow--rw-r--r--. 1 root root 2247 May 28 02:19 /etc/passwd-rw-r--r--. 1 root root 2247 May 28 02:19 /etc/passwd-----------. 1 root root 1257 May 28 02:19 /etc/shadow----------. 1 root root 1262 May 28 02:19 /etc/shadow- 我们可以看到每一个文件都有一个带-后缀的文件。如果我们不小心破坏了原来的文件，可以用后面的带-文件恢复，基本上能恢复所有的账号和密码。 下面进行破坏模拟实验： CentOS 7 破坏模拟1、 在/etc/passwd文件的首行root行加注释#，保存退出： 12#root:x:0:0:root:/root:/bin/bash... 2、 然后重启服务器，发现进不去系统。 3、 然后我们重启下，然后再按e修改启动方法： 4、 进入到这个界面后，找到linux16那行： 5、 在这行的尾部添加init=/bin/bash，然后按&lt;Ctrl-x&gt;后启动： 6、 然后我们进入bash： 7、 这时候系统是无法写入的，我们要重新以rw的模式remount一下，命令为mount -o rw,remount / 8、 我们可以修复被修改的/etc/passwd文件，记得先给原/etc/passswd做个备份，然后再复制/etc/passwd-文件去覆盖/etc/passwd，（如果有其他文件如/etc/shadow等被破坏，也可以用此方法） tips：备份是个好习惯，更改重要文件前最好都要先备份下。 9、重启服务器（reboot，init，shutdown等重启命令都无法使用，只能硬重启），正常进入操作系统。 CentOS 6 破坏模拟1、 在/etc/shadow文件的首行root行加注释#，保存退出： 12#root:$6$6P086Sg8NUDccylN$O1g3CU1Jhfxp.RwBvwylXF.yVml2dQvAGIAAM.LzIDDqsueS8FrFWfDa/S4JZ.4uLs./iQ9f6Ifm7qCYlVV6U.:17313:0:99999:7:::... 2、 然后重启服务器，发现可以进入系统，但是输入root密码后，提示不正确（不输入密码直接进也进不去）。 3、 然后我们重启下，在这个界面按任意键进入启动菜单： 4、按a修改启动内核启动参数： 5、进入到这个界面后，在后面输入init=/bin/bash，然后按回车： 6、 然后我们进入bash： 7、 这时候系统是无法写入的，我们要重新以rw的模式remount一下，命令为mount -o rw,remount /（和CentOS 7 一样，就不贴图了） 8、 我们可以修复被修改的/etc/shadow文件，记得先给原/etc/shadow做个备份，然后再复制/etc/shadow-文件去覆盖/etc/shadow，（如果有其他文件如/etc/shadow等被破坏，也可以用此方法）（和CentOS7 一样，就不贴图了） tips：备份是个好习惯，更改重要文件前最好都要先备份下。 9、重启服务器（reboot，init，shutdown等重启命令都无法使用，只能硬重启），正常进入操作系统。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>recovery</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-用户和组练习题]]></title>
    <url>%2Flinux%2F20170526-04-user-group-practice%2F</url>
    <content type="text"><![CDATA[练习1创建用户gentoo，附加组为bin和root，默认shell为 /bin/csh，注释信息为”Gentoo Distribution” 123[root❄centos7 skel]☭ useradd gentoo -G bin,root -s /bin/csh -c &quot;Gentoo Distribution&quot;[root❄centos7 skel]☭ cat /etc/passwd |grep gentoogentoo:x:1001:1001:Gentoo Distribution:/home/gentoo:/bin/csh 练习2创建下面的用户、组和组成员关系 名字为admins 的组 用户natasha，使用admins 作为附属组 用户harry，也使用admins 作为附属组 用户sarah，不可交互登录系统，且不是admins 的成员， natasha，harry，sarah密码都是centos 123456789101112131415161718[root❄centos7 skel]☭ groupadd admins[root❄centos7 skel]☭ useradd natasha -G admins[root❄centos7 skel]☭ useradd harry -G admins[root❄centos7 skel]☭ useradd sarah -s /sbin/nologin[root❄centos7 skel]☭ echo centos | passwd --stdin natashaChanging password for user natasha.passwd: all authentication tokens updated successfully.[root❄centos7 skel]☭ echo centos | passwd --stdin harryChanging password for user harry.passwd: all authentication tokens updated successfully.[root❄centos7 skel]☭ echo centos | passwd --stdin sarahChanging password for user sarah.passwd: all authentication tokens updated successfully.[root❄centos7 skel]☭ tail -n 3 /etc/passwdnatasha:x:1002:1003::/home/natasha:/bin/bashharry:x:1003:1004::/home/harry:/bin/bashsarah:x:1004:1005::/home/sarah:/sbin/nologin]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>user</tag>
        <tag>group</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-用户和用户组的管理命令]]></title>
    <url>%2Flinux%2F20170526-03-user-group-cmmands%2F</url>
    <content type="text"><![CDATA[用到的命令主要有： 组命令：groupadd、groupdel、groupmod、gpasswd、groupmems、 newgrp 用户命令：useradd、userdel、usermod、passwd 其他相关命令： getent：getent passwd USERNAME、getent shadow GROUPNAME chage chsh id su 用户组命令：groupaddgroupadd GROUP_NAME:创建新组 可选参数： -g GID：手动指定GID，默认是上一个组的GID+1 -r：创建系统组 groupmodgroupmod GROUP_NAME 修改组信息 -g GID：修改GID -n NEW_NAME OLD_NAME：修改组名 groupdelgroupdel GROUP_NAME：删除组 例子：12345678groupadd grpgroupadd -g 2000 grp1groupadd -r sysgrpgroupadd -r -g 306 sysgrp1groupmod -g 702 sysgrp1groupmod -n newsysgrp1 sysgrp1groupdel grp gpasswdgpasswd GROUP_NAME：给组加密码，一般默认不加，不加的话，用户就不能用newgrp切换属组（组密码那里是两!!，表示无密码，无密码的组，用户也不能临时切换到这个组。） groupmems12345678910用法：groupmems [选项] [动作]选项： -g, --group groupname 更改组 groupname，而不是用户的组(只 root)动作： -a, --add username 将用户 username 添加到组成员中 -d, --delete username 从组的成员中删除用户 username -p, --purge 从组中移除所有成员 -l, --list 列出组中的所有成员 比如下面，就是列出admins组里的所有成员，然后删除其中的一个成员natasha： 1234[root❄centos7 skel]☭ groupmems -g admins -lnatasha harry [root❄centos7 skel]☭ groupmems -g admins -lharry newgrpnewgrp GROUPNAME 临时切换主组，如果切换不属于的组，要输入组密码。如果不属于的组而且没有设置组密码，则无法切换。 用户命令useradduseradd ：创建新用户或者更新默认新用户信息 -u：指定UID -g：指定基本组GID，组名必须存在才行，不能用来新建GID。 -G GROUP1[,GROUP2,...[GROUPN]]：指定GID，指明用户的附加组，多个组之间用逗号分隔。 -c COMMENT：注释信息 -d HOME_DIR：指定家目录（本质是通过复制/etc/skel目录并重命名实现的），如果目录路径本身就存在，则不会为用户复制/etc/skel下的内容。 -s SHELL： 指定用户的默认shell，可用于所有shell列表存在的shell(shell列表：/etc/shells) -r ：创建系统用户 -D：修改创建用户时候的默认选项（man useradd可以看一下详情,，其实更改的就是/etc/default/useradd） 1234567891011121314151617181920useradd dockeruseradd -u 3000 openstackuseradd -u 3001 -g 3001 openshiftuseradd -g cloudgroup cloudstackuseradd -G grp,grp1 archlinuxuseradd -c &quot;Hacker Linux&quot; kalilinuxuseradd -d /opt/sybase sybasemkdir /tmp/test1useradd -d /tmp/test1 test1useradd -s /bin/csh test2useradd -s /sbin/nologin test3 tips：要查看useradd的默认规则，可以查看/etc/default/useradd文件。 123456789[root❄centos7 default]☭ cat /etc/default/useradd# useradd defaults fileGROUP=100HOME=/homeINACTIVE=-1EXPIRE=SHELL=/bin/bashSKEL=/etc/skelCREATE_MAIL_SPOOL=yes 我们可以看到有个SKEL，skel里存放的是环境变量文件，当创建新用户时候，会复制一份到用户家目录里面。 123[root❄centos7 default]☭ cd /etc/skel/[root❄centos7 skel]☭ ls -a. .. .bash_logout .bash_profile .bashrc .mozilla usermodusermod：修改用户属性 -u UID：修改用户的UID-g：修改用户所属的基本组-G ：修改用户的附加组，原来的附加组会被覆盖掉-a：append，和-G结合使用，表示新添加的组，兵不会覆盖掉原来的组-c COMMENT：修改注释信息-d：修改用户的家目录，用户原有的文件不会被转移到新位置。-m：move-home，更改用户主目录，和-d配合使用，会移动原来家目录的文件。-l：修改用户名 -s SHELL： 修改用户的默认shell，可用于所有shell列表存在的shell(shell列表：/etc/shells) -L：Lock，锁定用户密码（在原来的密码字符串之前加一个!） -U：unlock，解锁用户的密码（删掉!） userdeluserdel：删除用户-r：删除用户并删除其家目录和mail spool passwdpasswd修改用户自己的密码passwd USERNAME修改其他用户的密码（root有此权限） -- stdin 参数 echo ‘test2&#39; | passwd --stdin test2记住echo后面的字符串要用弱引用，强引用的话如果密码串里有特殊字符，就会不是原始密码了。 其他相关命令chagechage：change age 更改用户密码过期时间 chage USERNAME：不跟参数，会进入一个交互式的模式来修改各个时间，如下面：设置最短修改密码时间为0；最长密码时间为92天过期，上次更改密码时间保持默认那天（20170526），快过期的开始警告设置的在快过期前7天开始，密码过期后，7天内要修改密码，否则变为不活动（Inactive，即锁住），最后一条是设置账号过期时间，设置为默认不过期（-1）。 12345678910[root❄centos7 skel]☭ chage gentooChanging the aging information for gentooEnter the new value, or press ENTER for the default Minimum Password Age [0]: 0 Maximum Password Age [99999]: 92 Last Password Change (YYYY-MM-DD) [2017-05-26]: Password Expiration Warning [7]: 7 Password Inactive [-1]: 7 Account Expiration Date (YYYY-MM-DD) [-1]: 带参数，下面是参数设置，和上面类似，就是把交互式变成了参数选项： chage [option] USERNAME -d LAST_DAY -d 0:第一次登陆，强制让你该口令。 -E –expiredate EXPIRE_DATE -I –inactive INACTIVE -m –mindays MIN_DAYS -M –maxdays MAX_DAYS -W –warndays WARN_DAYS –l 显示密码策略 示例： 12345chage -d 0 tom #下一次登录强制重设密码chage -m 0 –M 42 –W 14 –I 7 tomchage -E 2016-09-10 tom idid [USERNAME]：显示用户的uid和gid，USERNAME省略的话表示只显示当前用户的id信息。 -u显示用户id -g显示基本组id -G显示所有组id -n显示名字而不是id susu命令：switch user，切换用户 登录式切换：会通过读取目标用户的配置文件来重新初始化 su - USERNAME*su -l USERNAMEl就表示login 非登陆式切换：不会读取目标用户的配置文件来进行初始化 su USERNAME Note：管理员可无密码切换到其他任意用户。 参数：-c &#39;COMMAND&#39;：仅以目标用户登录，后执行后面的命令，然后就退出 示例：su -test -c &#39;whoami&#39; getentget entries from Name Service Switch libraries从命名服务切换库获得条目 getent passwd [USERNAME]：不输入USERNAME默认是全部用户，如果输入了，就是指定的那个 12[root❄centos7 skel]☭ getent passwd gentoogentoo:x:1001:1001:Gentoo Distribution:/home/gentoo:/bin/csh getent shadow [GROUPNAME]：不输入GROUPNAME默认是全部用户组信息，如果输入了，就是指定的那个 12[root❄centos7 skel]☭ getent shadow gentoogentoo:!!:17312:0:92:7:7:: finger查看用户信息 12345678910[root❄ centos7 ~]☭ finger rootLogin: root Name: rootDirectory: /root Shell: /bin/bashOn since Sun May 28 16:17 (CST) on pts/0 from 172.17.251.64 4 seconds idleOn since Wed May 24 20:40 (CST) on :0 from :0 (messages off)On since Wed May 24 20:40 (CST) on pts/2 from :0 3 days 19 hours idleNo mail.No Plan.]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>user</tag>
        <tag>group</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-用户和组文件]]></title>
    <url>%2Flinux%2F20170526-02-user-group-file%2F</url>
    <content type="text"><![CDATA[/etc/passwdroot:x:0:0:root:/root:/bin/bash 1234whatis passwdman 5 passwdname:password:UID:GID:GECOS:directory:shell name：用户名 password：密码占位符x UID：User ID GID：Group ID GECOS：注释信息 directory：用户的家目录 shell：用户的默认shell，登录时候默认的shell程序 tips：用户的注释信息（GECOS）可以通过chfn更改： 12345678[root❄centos7 ~]☭ chfn LongDreamChanging finger information for LongDream. Name [LongDream]: Yu LongjunLongjun Office []: Beijing Office Phone []: 010-8888888 Home Phone []: 188888888Finger information changed. 1234567[root❄centos7 ~]☭ finger LongDreamLogin: LongDream Name: Yu LongjunDirectory: /home/LongDream Shell: /bin/bashOffice: Beijing, 010-8888888 Home Phone: +1-888-888-8888Last login Thu May 25 08:59 (CST) on pts/0No mail.No Plan. /etc/shadowroot:$6$gzEdnxzX$na8Z8EEYpr.piY4jcCsC.52.4HG0uo6aSbvdg5Yu1TJetkmKGfYElSS//AebAglmUoW.Z5QodAHYpatAfg7pR/:16967:0:99999:7::: 以冒号分割的各个段落的意思： account:用户名 encrypted passwd：加密后密码 。两个$符号之间： 第一个6表示加密算法的第六种，sha512sum 第二个是salt（随机数） 第三个是真正的密码块 如果整段为!!，表明被锁定，不能登录，去掉，就可以以空密码登录。可以直接改shadow文件，给有密码的前面加个！号，这个账号就无法登录了。 date of last passwd change：最后一次更改密码的日期。最近一次更改密码的时间，从1970年1月1日开始的天数。如果是0，表示用户要在下次登录时候更改密码。空表示被禁用了 minimum passwd age：最小密码年龄。更改密码后，最短多长时间能更改密码，默认是0，随时可以更改 maximum password age：最大密码年龄。在更改密码多少天后，用户必须要更改密码，默认是99999，相当于永不过期 password warning period：密码警告时间段密码过期之前，提前警告用户的天数，一般是7，提前7天通知你。 password inactivity period：密码禁用期密码过期了之后，仍可以用旧密码的天数，过了这个天数，就给你锁定了，只能用root去解锁了。默认是空，永远可以用。 account expiration date：账号过期日期账户要多少天过期，从1970年1月1日起的天数来表示，如果过期了，这个账号就不能使用了。默认是空，永不过期。 reserved field：保留字段，后续功能添加 /etc/grouproot:x:0: group_name:passwd:GID:user_list group_name：组名 passwd：组密码 GID：组ID user_list：附加组的用户 tips：由于安全问题，group的密码一般不设置，所以gshadow文件也不用去研究，基本不用。所以只讲了上面三个文件，不用研究/etc/gshadow]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>group</tag>
        <tag>passwd</tag>
        <tag>shadow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-什么是用户和组]]></title>
    <url>%2Flinux%2F20170526-01-whatis-user-group%2F</url>
    <content type="text"><![CDATA[3A Authentication：认证（用户的识别（密码、指纹、虹膜）） Authorization：授权（用户的访问权限控制） Accounting：审计（用户的动作记录，监督权限的使用） 用户：user组：group（组在有些系统里组被称为角色） 用户用户类别: 管理员 普通用户 系统用户 登录用户 用户标识： UID(User ID)，范围为16bits（0-65535） 管理员：0 普通用户：1-65535 系统用户：1-499（CentOS6）1-999（CentOS7） 登录用户：500-60000（CentOS6），1000-60000（CentOS7）、 名称解析：UserName &lt;–&gt; UID根据名称解析库进行：/etc/passwd 组组类别1 管理员组 普通用户组 系统组 登录组 组标识： GID(Group ID)，范围为16bits（0-65535） 管理员：0 普通用户组：1-65535 系统组：1-499（CentOS6）1-999（CentOS7） 登录组：500-60000（CentOS6），1000-60000（CentOS7）、 名称解析：GroupName &lt;–&gt; GID根据名称解析库进行：/etc/group 组类别2一个用户属于多个组，可以划分为： 用户主组(基本组)(g)：primary group 附加组(G)：supplymentary group 用户必须属于主组，有且只有一个主组，附加组可有可无，可以有多个。 组类别3私有组：组名同用户名，且只包含一个用户公共组：组内包含了多个用户 认证信息通过比对事先存储的信息，与登录时候提供的信息否一致。密码：passwd /etc/shadow(用户密码)/etc/gshadow(组密码) 加密算法： 对称加密：加密和解密使用同一个密码 非对称加密：加密和解密使用一对儿秘钥。 公钥：public key 私钥：private key 单向加密：只能加密，不能解密：提取数据特征码 定长输出（跟原来的数据量多大没关系） 雪崩效应（数据的一点点差别，加密后差别很大 ）echo &quot;How are you?&quot; | md5sumecho &quot;How are you&quot; | md5sum 算法： md5：message digest（消息摘要）version5， 128位 sha：secure hash algorithm（安全的哈希算法） sha1sum sha224sum sha256sum sha384sum sha512sum 账户输入密码后，加点salt（添加随机数），这样保证不同用户用同样的密码，加密后结果不一样。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>user</tag>
        <tag>group</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IO重定向和管道练习题]]></title>
    <url>%2Flinux%2F20170524-03-practice%2F</url>
    <content type="text"><![CDATA[1、将/etc/issue文件中的内容转换为大写后保存至/tmp/issue.out文件中 答案（两种方法）： 123tr &apos;a-z&apos; &apos;A-Z&apos; &lt; /etc/issue &gt; /tmp/issuecat /etc/issue | tr &apos;a-z&apos; &apos;A-Z&apos; &gt; /tmp/issue 解析： 第一种方法用的输入输出重定向，是把/etc/issue文件作为tr命令的输入，然后输出到/tmp/issue; 第二种方法是用的管道和输出重定向，把cat命令的输出，作为tr命令的输入，然后结果输出到/tmp/issue 2、将当前系统登录用户的信息转换为大写后保存至/tmp/who.out文件中 答案： 1who |tr &apos;a-z&apos; &apos;A-Z&apos; &gt; /tmp/who.out 解析： who输出的登录信息，作为tr命令的输入，然后tr命令的输出重定向到/tmp/who.txt文件 3、一个linux用户给root发邮件，要求邮件标题为”help”，邮件正文如下： Hello, I am 用户名,The system version is here,please help me to check it ,thanks!操作系统版本信息 答案： 1234cat &lt;&lt;EOF | mail -s help root&gt;Hello, I am $USER,The system version is here,please help me to check it ,thanks! &gt;My OS is `cat /etc/centos-release`&gt;My OS&apos;s Kernel is `uname -a` 解析： 先用cat &lt;&lt;EOF来把下面行的输入信息 输出出来，作为后面mail命令的输入。 4、将/root/下文件列表，显示成一行，并文件名之间用空格隔开 答案： 1ls -1 |tr &apos;\n&apos; &apos; &apos; 解析： ls显示所有文件行，用tr命令把尾部的\n替换成空格 5、计算1+2+3+..+99+100的总和 答案： 1echo &#123;1..100&#125; |tr &apos; &apos; &apos;+&apos; |bc 解析： echo输出100个数，然后把中间的空格转化为加号，然后再传给bc计算 6、删除Windows文本文件中的‘^M’字符 答案： 1cat windows.txt |tr -d &quot;\r&quot; 解析： 7、处理字符串“xt.,l 1 jr#!$mn 2 c*/fe 3 uz 4”，只保留其中的数字 和空格 答案： 1echo “xt.,l 1 jr#!$mn 2 c*/fe 3 uz 4” | tr -d &quot;^0-9 &quot; 解析： -d选项是delete的意思，^是非的意思，0=9是数字，后面跟一个空格，组合起来就是非数字和空格，然后-d删除 8、将PATH变量每个目录显示在独立的一行 答案： 1echo $PATH | tr &quot;:&quot; &quot;\n&quot; 解析： 冒号转化为换行 9、将指定文件中0-9分别替代成a-j 答案： 1tr &quot;0-9&quot; &quot;a-j&quot; file 解析： 略 10、将文件中每个单词（由字母组成）显示在独立的一行，并无空行 答案： 1cat 1.log |tr -c &apos;a-zA-Z&apos; &apos;\n&apos; 解析： cat命令的输出，作为tr命令的输入，-c选项是选择补集，即不是字母的单词]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>inode</tag>
        <tag>link</tag>
        <tag>hard link</tag>
        <tag>soft link</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-管道]]></title>
    <url>%2Flinux%2F20170524-02-pipe%2F</url>
    <content type="text"><![CDATA[管道（|）基本用法管道（|）用来连接命令 命令格式： COMMAND1 | COMMAND2 | COMMAND3 | ... 实现了把前一个命令的标准输出作为后面命令的输入。 tips: IO重定向和管道的区别：IO重定向，是把前一个命令的标准输出，重定向到普通文件或者设备文件。而管道，是把前一个命令的标准输出作为后面命令的标准输入。一个是到文件，一个是作为后面命令的标准输入。 例子： ls -C | tr &#39;a-z&#39; &#39;A-Z&#39;：ls出来的文件列表，作为tr的输入，这样小写被转化为大写。 ls -l /etc |less：ls输出的文件列表，作为less的输入，这样可以逐行显示 echo &quot;test email&quot; | mail-s &quot;test&quot; user@example.com：echo输出的字符串，作为mail 的输入 cat /etc/issue | tr &#39;a-z&#39; &#39;A-Z&#39;：略 who | head -2 | tr &#39;a-z&#39; &#39;A-Z&#39; | tr -d &#39;0-9&#39;：略 echo &#39;Text through stdin&#39; | cat - file.txt tips：在上面代码中，-指的就是前面输出的内容，再举个例子：123456[root@fedora Scripts]# touch 1 2 3 4 5[root@fedora Scripts]# ls [0-9]1 2 3 4 5[root@fedora Scripts]# ls [0-9] |xargs rm -rf -[root@fedora Scripts]# ls [0-9]ls: cannot access &apos;[0-9]&apos;: No such file or directory 2&gt;&amp;1和|&amp;当既有标准输出又有错误输出的时候，标准输出是可以输入到后面命令的，错误输出是无法输入到后面命令。可以使用2&gt;&amp;1和|&amp;实现错误的也输入到后面命令。1234567891011121314151617181920[root❄centos7 app]☭ ls /app /err | tr &apos;a-z&apos; &apos;A-Z&apos;ls: cannot access /err: No such file or directory/APP:F1ISSUE2ISSUE3[root❄centos7 app]☭ ls /app /err 2&gt;&amp;1 | tr &apos;a-z&apos; &apos;A-Z&apos;LS: CANNOT ACCESS /ERR: NO SUCH FILE OR DIRECTORY/APP:F1ISSUE2ISSUE3[root❄centos7 app]☭ ls /app /err |&amp; tr &apos;a-z&apos; &apos;A-Z&apos;LS: CANNOT ACCESS /ERR: NO SUCH FILE OR DIRECTORY/APP:F1ISSUE2ISSUE3 teetee：read from standard input and write to standard output and files从标准输入读，写到标准输出和文件。当然如果再跟一个管道，标准输出就输出到另外一个命令了。 12345678910111213# 写到屏幕和文件/tmp/issue.tee[root❄centos7 ~]☭ cat /etc/issue | tee /app/issue.tee\SKernel \r on an \m[root❄centos7 ~]☭ cat /app/issue.tee\SKernel \r on an \m# 写到/tmp/issue.tee ，写到标准输出的通过管道又传递了一次[root❄centos7 ~]☭ cat /etc/issue | tee /tmp/issue.tee | tr 'a-z' 'A-Z'\SKERNEL \R ON AN \M]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>pipe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-IO重定向]]></title>
    <url>%2Flinux%2F20170524-01-io-redirection%2F</url>
    <content type="text"><![CDATA[标准IO（Standard Input/Output）可用于做输入的设备： 键盘设备、文件系统上的常规文件、网卡等。 可用于做输出的设备： 显示器、文件系统上的常规文件、网卡等。 程序的数据流有三种： 输入的数据流：&lt;– 标准输入（stdin(standard input)）,默认接受来自键盘的输入。 输出的数据流：–&gt; 标准输出（stdout(standard output)），默认输出到终端窗口。 错误输出流：–&gt; 标准错误（stderr(standard error)），默认输出到终端窗口。 fd:file descriptor，文件描述符标准输入：0标准输出：1标准错误：2 echo $?（bash脚本编程中，很多判断基于$?这个来决定） IO重定向(Input/Output Redirection) 输入本来默认是键盘，我们改成其他输入，就是输入重定向 ：例如从文本文件里输入。本来输出的位置是显示器，我们改成其他输出，就是输出重定向：例如输出到文件。 set -C： 禁止覆盖输出重定向到已存在的文件(在这种模式下，如果你非要覆盖，可以使用&gt;|） set +C： 关闭上述特性 /dev/null 特殊设备，空设备，也叫黑洞，数据进去都没了。 输出重定向（Output Redirection）的几种方法1. 正常输出重定向：&gt;（覆盖输出）、&gt;&gt;（追加输出）例子：1234567891011121314&gt;&gt;&gt; cat /etc/issue &gt; /tmp/issue.out#开两个ssh连接,我们先看第二个的终端所在的虚拟设备文件，然后在第一个输入#第二个ssh：&gt;&gt;&gt; tty/dev/pts/1#第一个ssh输入：&gt;&gt;&gt; cat /etc/issue &gt; /dev/pts/1# 我们看到第二个ssh输出了信息#追加输出ls /var &gt; /tmp/test.outcat /etc/fstab &gt;&gt; /tmp/test.out 2. 错误输出重定向：2&gt;（覆盖输出）、2&gt;&gt;（追加输出）123ls /etc/issue1111 2&gt;&gt; /tmp/error.outcatt /etc/issue 2&gt;&gt; /dev/null 3. 正确和错误的都进行输出重定向： 新写法：COMMAND &amp;&gt; /path/to/somefile、COMMAND &amp;&gt;&gt; /path/to/somefile 老写法：COMMAND &gt; /path/to/somefile 2&gt;&amp;1、COMMAND &gt;&gt; /path/to/somefile 2&gt;&amp;1 1234ls /boot /err &amp;&gt; /tmp/all1.log # 新写法ls /boot /err &amp;&gt;&gt; /tmp/all2.log # 新写法ls /boot /err &gt; /tmp/all3.log 2&gt;&amp;1 # 老写法ls /boot /err &gt;&gt; /tmp/all4.log 2&gt;&amp;1 # 老写法 输入重定向（Input Redirection）的方法tr命令tr [OPTION]... SET1 [SET2]把输入的数据当中的字符，凡是在SET1定义范围内出现的，通通对位转换为SET2出现的字符 tr SET1 SET2 &lt; /path/from/somefile对位转化SET1中的字符为SET2中的字符 tr -d SET1 &lt; /path/from/somefile删除指定集合里出现的字符 tr -s &quot;\n&quot; /path/from/somefile把指定的连续的字符以一个字符表示，压缩。 tr -cComplement ,取字符集的补集，通常与其他参数结合使用，例如-dc 123456789101112[root❄centos7 ~]☭ tr &apos;a-z&apos; &apos;A-Z&apos;aabbcc33AABBCC33^C[root❄centos7 ~]☭ tr &apos;a-z&apos; &apos;A-Z&apos; &lt;/etc/issue\SKERNEL \R ON AN \M[root❄centos7 ~]☭ tr &apos;a-z&apos; &apos;A-Z&apos; &lt; /etc/issue &gt; /app/issue2[root❄centos7 ~]☭ cat /app/issue2\SKERNEL \R ON AN \M tips：如果是输入后再输出到同一个文件，就会清空这个文件，所以最好不要这么用，下面是一个错误示范： 12345[root❄centos7 ~]☭ cd /app/[root❄centos7 app]☭ cp issue2 issue3[root❄centos7 app]☭ tr &apos;a-z&apos; &apos;A-Z&apos; &lt; issue3 &gt;issue3[root❄centos7 app]☭ cat issue3[root❄centos7 app]☭ 追加是可以的，在原有文件基础上再追加一段： 1234567891011[root❄centos7 app]☭ cat issue2\SKERNEL \R ON AN \M[root❄centos7 app]☭ tr &apos;A-Z&apos; &apos;a-z&apos; &lt; issue2 &gt;&gt; issue2[root❄centos7 app]☭ cat issue2\SKERNEL \R ON AN \M\skernel \r on an \m dc结合使用 123456789[root❄centos7 app]☭ echo &#123;a..z&#125; &gt;f1[root❄centos7 app]☭ cat f1a b c d e f g h i j k l m n o p q r s t u v w x y z[root❄centos7 app]☭ tr -d &apos;f-n&apos; &lt;f1a b c d e o p q r s t u v w x y z[root❄centos7 app]☭ cat f1a b c d e f g h i j k l m n o p q r s t u v w x y z[root❄centos7 app]☭ tr -dc &apos;f-n&apos; &lt; f1fghijklmn[root❄centos7 app]☭ Here documents输出到屏幕，或创建多行文档)：&lt;&lt;终止词 例子： 123456789101112131415161718192021[root❄centos7 app]☭ cat &lt;&lt;EOF&gt; yulongjun&gt; zhaoweiqiang&gt; EOFyulongjunzhaoweiqiang[root❄centos7 app]☭ cat &gt;cat.out &lt;&lt;END&gt; yulongjun&gt; zhaoweiqiang&gt; hanjinze&gt; songda&gt; END[root❄centos7 app]☭ cat /tmp/cat.out yulongjunzhaoweiqianghanjinzesongda -代表输出流 此段来源于：https://www.cnblogs.com/dachenzi/p/6790596.html 搭配cat cat -：如果指定cat的文件为-，表示从标准输入读取（和直接使用cat，好像没什么区别） 搭配| echo 123 | cat -：表示把管道符前面的输出流，在交给cat执行一遍（这就很牛逼了） 例子： 如果操作系统没有scp命令，只有ssh，那么是不是就不能远程拷贝了（前提：没有openssh-clients软件包） 利用-，就可以实现： 1cat jdk.tar.gz | ssh 192.168.56.101 &apos;cat - &gt; /tmp/jdk.tar.gz&apos; cat jdk.tar.gz 产生输出流， 在管道后面的 - ,则可以接受输出流，并重定向到 /tmp/jdk.tar.gz]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>IO</tag>
        <tag>IO Redirection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-inode、软硬链接]]></title>
    <url>%2Flinux%2F20170522-05-inode-link%2F</url>
    <content type="text"><![CDATA[关于inode是什么，可以看这篇文章：http://www.cnblogs.com/adforce/p/3522433.html 如何查看inodell -di /boot / /app查看文件和文件夹的inode号 df -i查看挂载点文件夹的inode号 做inode增长实验创建60万个文件的方法1（效率不高）：for i in {1..600000}; do touch file$1; echo file$i is created;done创建60万个文件的方法2（效率高）：echo file{1..600000} | touch删除前20万个文件：echo file{1..200000} |args rm 在创建的过程中，可以另开一个窗口，用下面命令，每隔1秒运行df -hi命令，可以查看inode的增长情况：watch -n1 df -hi tips：文件粉碎工具shred：shred -uzvn10 FILE重复随机写入10次覆盖源文件，然后最后删除此文件。 硬链接、软链接软链接就相当于Windows的快捷方式，删掉源文件，快捷方式和就失效了，软链接就找不到源文件了。 硬链接相当于多个链接指向同一份数据存储区域，每多一个硬链接，硬链接数+1，如果一个文件，有n个硬链接，删除n-1个硬链接，源文件还在，直到删除所有硬链接，才会删除源文件。 1. 复制（cp） 在复制过程中，复制软连接相当于复制了快捷方式，速度很快，而且可以跨分区。 在复制过程中，复制硬链接分为两种情形： 在同一分区复制，相当于多创建一个链接指向原数据存储位置，速度很快。 在不同分区复制，相当于把原来分区的数据拷贝过去存储，同时创建一个指向新数据区域的指针，速度比较慢。 2. 删除（rm）在删除过程中，删除软连接相当于删除了快捷方式，源文件还在。在删除过程中，删除硬连接相当于删除了一个到数据块的指针，，除非删除所有硬链接文件，源文件才删除。 3. 移动（mv）在移动过程中，移动软连接相当于移动了快捷方式而已。在移动过程中，移动硬连接分为两种情形： 在同一分区移动，相当于创建了一个新inode，指向数据块，并把原来的inode删掉 在不同分区移动，要把数据块复制到新分区，然后在新分区创建新的inode号指向新的数据块，并且把原来分区的inode号和数据块都删掉。 4. 软连接支持对目录创建，硬链接不支持 ln dir1 dir2不成功 ln /etc/sysconfig/network-scripts/ifcfg-ens33 /etc/ens33成功 tips1：当我们看到一个磁盘，使用空间没满，但是却提示”no space left on device”，那可能是inode用完了。 tips2：如何删除数量很多的文件（比如前面做实验创建的60万个文件。）:技巧是使用管道|和xargs，管道|后面会讲，管道是指的是前面命令的输出作为后面命令的输入。xargs，前面命令多个输出，可以用xargs一个个的传给后面的命令，而不是已下载全传给后面命令，可以解决参数太长的情况。 ls | xargs rm 5. 如何写软连接相对路径 软连接写相对路径，要根据软连接文件的路径来写。 例如要在把/etc/issue 软连接到/app/d1/d2/d3/ilink 1ln -s ../../../../../etc/issue /app/d1/d1/d3/ilink 6. 如何软链接设备文件 设备文件比较特殊，如果要创建设备文件的链接，需要用到mknod命令： 12345[root@centos7 etc]# ll /dev/sdabrw-rw----. 1 root disk 8, 0 May 22 09:06 /dev/sda #得到主设备号和复设备号。mknod /app/sda b 8 0ll -i /dev/sda /app/sda 软链接练习： 创建一个目录tomcat-8.5.23，创建一个软连接tomcat到这个目录；在创建一个目录tomcat-9.0.1, 把tomcat的软连接指向新的tomcat-9.0.1目录。 123ln -sv tomcat-8.5.23 tomcatrm -rf tomcatln -sv tomcat-9.0.1 tomcat 创建一个目录/mnt/lfs/tools， 然后创建/mnt/lfs/tools的软连接/tools，一般有这样的命令：ln -sv /mnt/lfs/tools /tools，如何可以更短？ 12mkdir -v /mnt/lfs/toolsln -sv /mnt/lfs/tools / file命令常用选项: -b 列出文件辨识结果时，不显示文件名称 -f 列出文件中文件名的文件类型 -F 使用指定分隔符号替换输出文件名后默认的”:”分隔符 -L 查看对应软链接对应文件的文件类型 --help 显示命令在线帮助 file /etc/system-relase file命令就是查看的文件头部的信息，可以用hexdump查看源文件源码信息 （也可以用xxd命令看）。 如果是二进制文件，会显示二进制的头信息。 如果是文本文件，就直接是文本。 1234567891011121314[root@centos7 ~]# hexdump -C -n 100 /bin/ls00000000 7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00 |.ELF............|00000010 02 00 3e 00 01 00 00 00 48 4b 40 00 00 00 00 00 |..&gt;.....HK@.....|00000020 40 00 00 00 00 00 00 00 18 c4 01 00 00 00 00 00 |@...............|00000030 00 00 00 00 40 00 38 00 09 00 40 00 1e 00 1d 00 |....@.8...@.....|00000040 06 00 00 00 05 00 00 00 40 00 00 00 00 00 00 00 |........@.......|00000050 40 00 40 00 00 00 00 00 40 00 40 00 00 00 00 00 |@.@.....@.@.....|00000060 f8 01 00 00 |....|00000064[root@centos7 ~]# hexdump -C -n 100 /etc/issue00000000 5c 53 0a 4b 65 72 6e 65 6c 20 5c 72 20 6f 6e 20 |\S.Kernel \r on |00000010 61 6e 20 5c 6d 0a |an \m.|00000016 readlink命令读取软连接指向的真实路径 12[root❄centos7 ~]☭ readlink /etc/redhat-release centos-release]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>inode</tag>
        <tag>link</tag>
        <tag>hard link</tag>
        <tag>soft link</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-Linux文件类型和文件相关命令]]></title>
    <url>%2Flinux%2F20170522-04-linux-file-command%2F</url>
    <content type="text"><![CDATA[文件类型ll后可以看到文件详情： -：常规文件（内部类型是什么，用file命令） d：directory，目录文件 b：blobk device，块设备文件，支持以“block”为单位进行随机访问 major number：主设备号，用来表示设备类型，进而确定要加载的驱动程序 minor number：次设备号，用于表示同一类型中的不同设备。 如下图的1,3 、1,5 c：character device，字符设备文件，支持以”character”为单位进行线性访问 l：symbolic link，符号链接文件（软连接文件） p：pipe，命名管道 s：socket，套接字文件 路径绝对路径：以斜线开始的，从/开始的路径。 /etc/syscofnig/network-scrpits绝对路径分为两段，dirname和basename： 1234[root@centos7 app]# dirname /etc/sysconfig/network-scripts/ifcfg-ens192/etc/sysconfig/network-scripts[root@centos7 app]# basename /etc/sysconfig/network-scripts/ifcfg-ens192ifcfg-ens192 相对路径：不以斜线开始的路径。 xxx/yyy ：当前目录下的xxx子目录的yyy子目录 .：当前目录 ..：上级目录 ~：用户的家目录 -：切换上次输入的目录，和命令配合使用 tips:-原理，系统记住上一个工作目录，存储在环境变量OLDPWD，可以用echo $OLDPWD查看到。 常用命令：1. pwd：显示工作目录printing working directory 2. cd：切换目录change directory cd：切换到家目录 cd ~：切换到家目录 cd ~USERNAME：切换到用户USERNAME的家目录 cd -：在上一次所在目录与当前目录来回切换（PWD to OLDPWD） cd ..：切换到上级目录 cd /path/to/directory 切换到一个绝对目录 cd path/to/directory：切换到一个相对目录 3. ls：列出内容list(用/var目录做例子) -a， --all：列出所有文件包含隐藏文件 -A，--almost-all：列出除.和..之蛙所有的文件 -F：-F参数在目录名后加了正斜线（/），以方便用户在输出中分辨它们。类似地，它会在可执行 文件（比如上面的my_script文件）的后面加个星号，以便用户找出可在系统上运行的文件。 l，--long：长格式信息，列出文件的详细属性，命令可以简写为ll，alias ll=&#39;ls -l --color=auto&#39; -h，--human-readable：size用人类可读的格式表示 -d, --directory：查看目录本身而非内部的文件详情 -r, --reverse：反转排序（降序） -R, --recursive 递归显示（基本不用这个，递归显示用tree命令更直观） -t：按修改时间排序 4. cat：文本文件输出到终端concatenate 拼接 可以单独cat：cat /etc/issue 可以连接多个文件显示cat /etc/issue /etc/redhat-release 123456789[root@CentOS6 app]# cat /etc/issueCentOS release 6.9 (Final)Kernel \r on an \m[root@CentOS6 app]# cat /etc/issue /etc/redhat-releaseCentOS release 6.9 (Final)Kernel \r on an \mCentOS release 6.9 (Final) 参数： cat -s file：删除多余的空白行（多行空白行，删除到只剩一行空白行） 12345678910111213141516171819$ cat multi_blanks.txt line 1line2line3line4$ cat -s multi_blanks.txt #压缩相邻的空白行 line 1line2line3line4 cat -T file：将制表符显示为^I 用来看脚本里的制表符，通常我们写脚本要避免用制表符，而用多个空格。所以此命令通常用来查询哪里用了制表符。 cat -n file：显示行号 123456789$ cat lines.txt line line line$ cat -n lines.txt 1 line 2 line 3 line 5. tac： 跟cat相反，从后面行往前面行输出stat 查看文件状态可以看到atime(Access)、mtime(Modify)、Ctime(Change)，还有文件的权限，文件拥有者和拥有组，还有文件大小相关的内容。12345678910[root@centos7 ~]# stat /etc/issue File: ‘/etc/issue’ Size: 23 Blocks: 8 IO Block: 4096 regular fileDevice: 802h/2050d Inode: 134320235 Links: 1Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Context: system_u:object_r:etc_t:s0Access: 2017-05-22 09:07:34.832999490 +0800Modify: 2016-11-30 02:12:59.000000000 +0800Change: 2017-05-17 16:53:20.670996867 +0800 Birth: - 6. 文件通配符（globbing） * ：匹配零个或多个字符 ? ：匹配任何单个字符 ~ ：当前用户家目录 ~longdream ：用户longdream的家目录 ~+ ：当前工作目录 ~-：前一个工作目录 [0-9]：匹配数字范围，匹配一个 [a-z]或[A-Z]：字母，不区分大小写 tips：因为不区分大小写，[a-d]代表的就是匹配AaBbCcDd中的一个 [a-z0-9]：字母或数字 [wang]：匹配列表中的任何的一个字符,匹配w或a或n或g [^wang] 匹配列表中的所有字符以外的字符，除了w、a、n、g 只显示隐藏文件： 预定义的字符类：man 7 glob [:digit:]：任意数字，相当于0-9 [:lower:]：任意小写字母 [:upper:]: 任意大写字母 [:alpha:]: 任意大小写字母 [:alnum:]：任意数字或字母 [:blank:]：水平空白字符 [:space:]：水平或垂直空白字符 [:punct:]：标点符号 [:print:]：可打印字符 [:cntrl:]：控制（非打印）字符 [:graph:]：图形字符 [:xdigit:]：十六进制字符 练习练习1：显示/var/log目录下所有以l开头，以一个小写字母结尾，且中间出现一位任意字符的文件或目录。 练习2：显示/etc目录下，以任意一位数字开头，且以非数字结尾的文件或目录。 练习3：显示/etc目录下，以非字母开头，后面跟一个字母及其他任意长度任意字符的文件或目录。 练习4：复制/etc目录下，所有以m开头，以非数字结尾的文件或目录到/tmp/test目录。 练习5：复制/usr/share/man目录下，所有以man开头，后跟一个数字结尾的文件或目录至/tmp/test/man目录下。 练习6：复制/etc目录下，所有以.conf结尾，且以m,n,r,p开头的文件或目录到/tmp/conf.d/目录下。 答案： 123456789101112131415ls -d /var/l?[[:lower:]]ls -d /etc/[0-9]*[^0-9]ls -d /etc/[^a-z][a-z]*mkdir /tmp/testcp -r /etc/m*[^0-9] /tmp/test/mkdir /tmp/test/mancp -r /usr/share/man/man[0-9] /tmp/test/manmkdir /tmp/conf.dcp -r /etc/[mnrp]*.conf /tmp/conf.d/ 7. touchtouch [OPTION]... FILE... -a ：仅改变 atime和ctime -m ：仅改变 mtime和ctime -t [[CC]YY]MMDDhhmm[.ss]：指定atime和mtime的时间戳 -c ：如果文件不存在，则不予创建 9. cpcp ：copy cp /etc/fstab /app ：复制单个文件到目录下面。cp /etc/{fstab,issue} /app/dir/，cp /etc/fstab /etc/bashrc /app/dir/：复制多个文件到某目录下。cp /etc/fstab /app/1.txt ：复制并覆盖目标文件（没有则创建）。 参数 -f ，--force：强制覆盖目标文件。 -r，-R，`--recursive：递归复制目录 (-r == -R)。cp -r /var/log /app/log -d：复制符号链接本身，而不是他指向的文件。cp -d /etc/system-release /app -a，--archive：归档，等同于-dr -p：复制文件的原来属性，等同于下面的--preserv=[mode,ownership,timestamp] --preserv[=ATTR_LIST] mode：权限 ownership：属主属组 timestamp：时间戳 links：复制链接的源文件 xattr context all -v, --verbose：看到详细信息 练习： 1) 定义别名命令baketc，每天将/etc/目录下所有 文件，备份到/testdir独立的子目录下，并要求子目 录格式为 backupYYYY-mm-dd，备份过程可见 2) 创建/testdir/rootdir目录，并复制/root下所有 文件到该目录内，要求保留原有权限 解答： 1) 1alias baketc=cp -av /etc/* /testdir/`date -u` 2) 1cp -ap /root/* /testdir/rootdir tips: 小技巧 利用通配符来复制：cp -a /etc/passwd{,.bak}相当于 cp -a /etc/passwd /etc/passwd.bak 10. mvmv：move -f 强制移动或覆盖 移动文件（不在同一目录下），重命名文件（在同一目录下）移动目录或重命名目录 11. rename重命名后缀 rename [options] expression replacement file... 选项 1234567891011EXAMPLES Given the files foo1, ..., foo9, foo10, ..., foo278, the commands rename foo foo0 foo? rename foo foo0 foo?? will turn them into foo001, ..., foo009, foo010, ..., foo278. And rename .htm .html *.htm will fix the extension of your html files. 1234mkdir testcd texttouch &#123;1,2,3,4,5&#125;.txtrename -v .txt .txt.bak *.txt rmrm： remove 删除文件-i：interactive，交互式-r 递归删除-f 强制删除 rm -rf /(5是可以删除根的，6和7是不可以删除的) 我们可以留一个习惯：不用的文件，不要直接删除，可以用mv move到某个专用目录（比如都移动到/tmp下） 12. tree参数： -d: 只显示目录 -L level：指定显示的层级数目 -P pattern: 只显示由指定pattern匹配到的路径 13. mkdir、rmdirmkdir：make directory -p 递归创建 -v ：verbose，创建目录的详情 -m mode，权限 rmdir:remove empty directory(只能删除空目录) 练习1：创建test1/x/y1, test1/x/y2, test1/x/y1/a, test1/x/y1/b 练习2： test2目录下面创建a_c, a_d,b_c, b_d 练习3：在test3目录下创建以下目录结构： 1234567891011121314151617181920test3├── bin├── etc│ └── sysconfig│ └── network-scripts├── sbin├── usr│ ├── bin│ ├── lib│ ├── lib64│ ├── local│ │ ├── bin│ │ ├── etc│ │ ├── lib│ │ └── sbin│ └── sbin└── var ├── cache ├── log └── run 练习3：创建/testdir/dir1/x, /testdir/dir1/y, /testdir/dir1/x/a, /testdir/dir1/x/b, /testdir/dir1/y/a, /testdir/dir1/y/b 练习4：创建/testdir/dir2/x, /testdir/dir2/y, /testdir/dir2/x/a, /testdir/dir2/x/b 练习5：创建/testdir/dir3, /testdir/dir4, /testdir/dir5, /testdir/dir5/dir6, /testdir/dir5/dir7 答案： 123456789101112mkdir -pv test1/x/&#123;y1/&#123;a,b&#125;,y2&#125;mkdir -pv test2/&#123;a,b&#125;&#123;c,d&#125;mkdir -pv test3/&#123;bin,sbin,etc/sysconfig/network-scripts,usr/&#123;bin,sbin,local/&#123;bin,sbin,etc,lib&#125;,lib,lib64&#125;,var/&#123;cache,log,run&#125;&#125;mkdir -pv /testdir/dir1/&#123;x,y&#125;/&#123;a,b&#125;mkdir -pv /testdir/dir2/&#123;x/&#123;a,b&#125;,y&#125;mkdir -pv /testdir/dir&#123;3,4,5/dir&#123;6,7&#125;&#125; 14. 延伸删除大文件的方法（没被释放删除，空间还占用的文件）： 实验：在一个会话下，dd if=/dev/zero of=bigfile bs=1M count=2048，vim bigfile，在vim模式下不要退出。 打开另外一个会话窗口，删除文件，结果发现，文件是删除了，但是空间没有释放，因为vim占用着文件。 这时候我们就可以用&gt;来搞定： 12&gt; bigfile3rm -rf bigfile3 &gt; 和&gt;&gt;开单章讲，在后面的IO和重定向部分。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>File Type</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-bash快捷键]]></title>
    <url>%2Flinux%2F20170522-03-shotcuts%2F</url>
    <content type="text"><![CDATA[bash中命令运行中相关快捷键和相关命令 &lt;Ctrl + l&gt;：清屏，相当于clear命令。 &lt;Ctrl + c&gt;：取消（终止）前台运行命令，可以用来中断当前运行的命令。&lt;Ctrl + z&gt;：挂起命令到后台，并暂停。命令状态为Stopped。jobs：查看后台的命令，每个命令都有一个序号，从1开始。bg %NUM：将挂起在后台的命令，让其在后台继续运行，NUM为命令的序号。如果不写序号，默认继续运行最后一个挂起的命令。jobs查看命令状态变成Running状态。fg %NUM：后台命令送到前台运行。kill %n：中断(Terminted)后台某个命令。（Stopped状态的和Running状态都可以中断） COMMAND &amp;：把命令送到后台。 nohup COMMAND &amp; [&gt;XXX]：将程序放在后台运行，退出会话也不退出。（如果没有退出会话，jobs可以看到程序，按照序号杀死，如果关了会话的话，jobs是看不到的，杀死程序进程需要找到程序的进程号，用kill杀掉。 1nohup ping github.com &amp; 关于&gt;xxx后面重定向时候会学到。默认nohup 明星会重定向到运行程序的当前目录下的nohup.out。 bash控制和移动快捷键：&lt;Ctrl-a&gt;：光标跳转至行首 （用的最多）&lt;Ctrl-e&gt;：光标跳转至行位 （用的最多）&lt;Ctrl-u&gt;：从光标所在位置删除到行首&lt;Ctrl-k&gt;：从光标所在位置删除到行尾&lt;Alt-f&gt;：光标向右移动到一个单词的末尾&lt;Alt-b&gt;：光标向左移动到一个单词的首部]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>shotcuts</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-历史命令用法]]></title>
    <url>%2Flinux%2F20170522-02-history%2F</url>
    <content type="text"><![CDATA[history命令： 直接运行，显示历史命令，默认最多记录1000行 新执行的命令存在内存里，正常退出shell后，会写在~/.bash_history -c清空 -a把当前内存里存的历史命令存入~/.bash_history文件。 *-p 可以有这种用法，反引号里写命令，可以不保存命令到历史（可以用来做坏事哦） 1history -p `hostname` `ifcofnig` -s伪造历史命令，其实没执行： history -s &quot;rm -rf /*&quot;（可以用来吓唬人） tips：printenv可以打印出系统全局变量，可以看到有个HISTSIZE，echo $HISTSIZE可以看到此全局环境变量设置的是1000，这个就是历史命令大小，这个全局环境变量的参数的配置信息存在/etc/profile文件里。 利用历史命令来执行的方法： 常用的几个记住就好： !!：重复执行上一个命令 ! NUM：执行序号为NUM的命令： !-NUM：倒数第几个命令 !:NUM：上一个命令的以空格分割的索引 Esc .：可以重复上面一个命令的最后的参数比如我们本来要编辑一个文件，发现输入错了，输入的cd，可以再输入vim Esc .就把上一条的参数弄过来了。 设置HISTTIMEFORMAT 这样就可以显示每条命令执行的时间。例如设置 ~/.bash_profile里添加用户系统环境变量export HISTTIMEFORMAT=&quot;%F %T &quot;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>history</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-Linux系统层级结构标准]]></title>
    <url>%2Flinux%2F20170522-01-fhs%2F</url>
    <content type="text"><![CDATA[Linux Foundation有一套标准规范: FHS: Filesystem Hierarchy [‘haɪərɑːkɪ] Standard（文件系统层级标准）目前最新的标准是2.3版本：http://refspecs.linuxfoundation.org/FHS_2.3/ /bin ：所有用户可用的基本命令程序文件 /sbin ：系统用户管理命令 /boot： boot loader的静态文件（kernel，initramfs（initrd），grub等） /dev ：存储特殊文件（tty虚拟终端之类）和设备文件（字符设备（键盘、显示器）、块设备（硬盘、光盘）） tips：主设备名，次设备名如/dev/null就是1,3，/dev/zero就是1,5 /etc：配置文件 /home：非root用户的家目录 /root：root用户的额家目录 /lib：为系统启动或者根文件系统上的应用程序（/bin,/sbin）等提供共享库，以及为内核提供内核模块 libc.so.*：动态链接的c库 ld*：运行时链接器/加载器 modules：用于存储内核模块的目录 /lib64：64位系统特有的存放64位共享库的路径 /media：便携式设备的挂载点（如光盘cdrom、u盘floppy）。 /mnt：临时文件系统挂载点。 /opt：附加程序的安装位置 /srv：当前主机为服务提供的数据 /tmp：临时文件（temporary files）（可供所有用户执行写入操作） /usr：全局共享只读文件（Universial Shareable Read-only）（第二主要的层级目录） bin：非系统启动时用到的程序 sbin：非系统启动时用到的系统程序 include：c程序的头文件（header files） lib：程序依赖的库 lib64：程序依赖的库（64位） local：用来安装本地应用程序（又一个层级目录），第三方程序（比如在MacOS下，brew安装的程序都会安装在usr/local/bin下） share：命令man手册页，命令自带文档 /usr/share/dict/words 暴力破解的密码表(弱口令) src：某些程序的源代码 tips ：CentOS 7 都是把根目录的一些目录软连接到/usr下的目录 /var：可变数据文件（系统日志、缓存文件） log cache mail等 /proc：基于内存的虚拟文件系统（一切皆文件，把实时的内核参数和进程的信息进行可视化）（系统调优经常用到） /proc/cpuinfo： /proc/partitions： /sys：sysfs虚拟文件系统，提供了一种比proc更为理想的访问内存数据的途径，为管理Linux设备提供了一种统一模型的接口（see also: https://www.ibm.com/developerworks/cn/linux/l-cn-sysfs/）（**系统条有经常用到**）]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>FHS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[作业]]></title>
    <url>%2Flinux%2F20170519-10-practice%2F</url>
    <content type="text"><![CDATA[1、别名，内部，外部，hash 优先级？ 别名&gt;内部&gt;hash&gt;外部 2、实现screen 协助 一台screen -S协助名称 另外一台screen -ls列出目前开的协助会话(session)，找到上面协助名称对应的session号。 screen -x SESSION，SESSION为上面的会话号码。如果是只有一个的话，可以只输入screen -x 3、显示昨天是星期几 1234[root@centos7 ~]# date --date=&quot;yesterday&quot; +%aSun[root@centos7 ~]# date --date=&quot;yesterday&quot; +%ASunday 4、显示当前时间，格式：2016-06-18 10:20:30 date +&quot;%F %T&quot; 5、设置当前日期为2019-08-07 06:05:10 date 080706052019.10 6、在本机字符终端登录时，除显示原有信息外，再显示当前登录终端号，主机名和当前时间 man issue发现不详细，然后看到提示有12SEE ALSO motd(5), agetty(8), mingetty(8) agetty也不详细，mingetty最详细，我们找到想要的信息了： 12345678910111213141516171819202122ISSUE ESCAPES mingetty recognizes the following escapes sequences which might be embedded in the /etc/issue file: \d insert current day (localtime), \l insert line on which mingetty is running, \m inserts machine architecture (uname -m), \n inserts machine’s network node hostname (uname -n), \o inserts domain name, \r inserts operating system release (uname -r), \t insert current time (localtime), \s inserts operating system name, \u resp. \U the current number of users which are currently logged in. \U inserts &quot;n users&quot;, where as \u only inserts &quot;n&quot;. \v inserts operating system version (uname -v). 可以根据需要来自定义了,vim /etc/isue 123Terminal: \lHostname: \nLocaltime: \t 7、今天18：30自动关机，并提示用户shutdown -h 18:30 &quot;System will shutdown at 18:30, please backup you data&quot;]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>homework</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09-延伸小技巧screen、script、scriptreplay]]></title>
    <url>%2Flinux%2F20170519-09-screen-script-scriptreplay%2F</url>
    <content type="text"><![CDATA[screen详解：http://www.cnblogs.com/mchina/archive/2013/01/30/2880680.html script &amp; scriptreplayhttp://www.cnblogs.com/cornell/p/3833955.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>command</tag>
        <tag>screen</tag>
        <tag>script</tag>
        <tag>scriptreplay</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07-回显命令]]></title>
    <url>%2Flinux%2F20170519-07-echo%2F</url>
    <content type="text"><![CDATA[echo参数 -E：禁止对在字符串里的那些转义符进行解释，默认echo命令，就带此参数 -n：不自动换行 -e：让\转义符（escapes）生效，同时不会自动加回车。下面是常用的几个转义符，如果字符串中出现以下字符，则特别处理，而不会将它当成一般文字输出： 转义字符意义ASCII码值（十进制）\a响铃(BEL)007\b退格(BS) ，将当前位置移到前一列008\f换页(FF)，将当前位置移到下页开头012\n换行(LF) ，将当前位置移到下一行开头010\r回车(CR) ，将当前位置移到本行开头013\t水平制表(HT) （跳到下一个TAB位置）009\v垂直制表(VT)011\代表一个反斜线字符’’\’092\’代表一个单引号（撇号）字符039\”代表一个双引号字符034\? 代表一个问号 063 \0空字符(NULL)000\ooo1到3位八进制数所代表的任意字符三位八进制\xhh1到2位十六进制所代表的任意字符二位十六进制 打印带颜色的回显：示例：echo -e &#39;\033[43;31;5mFBI Warning!\033[0m&#39; 格式: echo -e “控制码字符串\033[0m” 控制码控制显示字体 如果有多个以m结尾的控制码可以使用分号隔开，在最后加m，例如：\033[43;31;5m 中间写字符串 最后\033[0m结束属性，这样后面的输入不会变颜色。（可以不加最后的这段自己测试下是什么样子） 控制码的说明： \033[0m 关闭所有属性 \033[1m 设置高亮度 \033[4m 下划线 \033[5m 闪烁 \033[7m 反显 \033[8m 消隐 \033[30m -- \033[37m 设置前景色 \033[40m -- \033[47m 设置背景色 \033[nA 光标上移n行 \033[nB 光标下移n行 \033[nC 光标右移n行 \033[nD 光标左移n行 \033[y;xH设置光标位置 \033[2J 清屏 \033[K 清除从光标到行尾的内容 \033[s 保存光标位置 \033[u 恢复光标位置 \033[?25l 隐藏光标 \033[?25h 显示光标 字体背景颜色范围: 40—-49 40: 黑 41: 红 42: 绿 43: 黄 44: 蓝 45: 紫 46: 青 47: 白 字体前景色范围: 30———–39 30: 黑 31: 红 32: 绿 33: 黄 34: 蓝 35: 紫 36: 青 37: 白 引用双引号：&quot;：弱引用，转化变量 单引号：&#39;：强引用不转化，输出原格式 反向单引号：`：反向单引号里运行的是命令，然后把命令作为前面命令的参数 1234567891011121314151617181920212223[root@centos7 ~]# echo &apos;$USER&apos;$USER[root@centos7 ~]# echo $USERroot[root@centos7 ~]# echo &quot;$USER&quot;root[root@centos7 ~]# echo &apos;$USER&apos;$USER[root@centos7 ~]# echo `$USER`bash: root: 未找到命令...[root@centos7 ~]# echo echo $USERecho root[root@centos7 ~]# echo &quot;echo $USER&quot;echo root[root@centos7 ~]# echo &apos;echo $USER&apos;echo $USER[root@centos7 ~]# echo `echo $USER`root[root@centos7 app]# touch oper`date +%F`.log[root@centos7 app]# lsoper2017-05-20.log 命令行扩展：$( ) 或 `` 把一个命令的输出打印给另一个命令的参数1234[root@centos7 ~]# echo &quot;This system&apos;s name is $(hostname) &quot;This system&apos;s name is centos7.yulongjun.com [root@centos7 ~]# echo &quot;i am `whoami` &quot;i am root 括号扩展：{ } 打印重复字符串的简化形式 123456789101112131415161718[root@centos7 app]# echo file&#123;1,3,5&#125;file1 file3 file5[root@centos7 app]# touch file&#123;1,3,5&#125;[root@centos7 app]# lsfile1 file3 file5[root@centos7 app]# rm -rf file&#123;1,3,5&#125;[root@centos7 app]# echo &#123;1..10&#125;1 2 3 4 5 6 7 8 9 10[root@centos7 app]# echo &#123;a..z&#125;a b c d e f g h i j k l m n o p q r s t u v w x y z[root@centos7 app]# echo &#123;z..a&#125;z y x w v u t s r q p o n m l k j i h g f e d c b a[root@centos7 app]# echo &#123;A..z&#125;A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z[root@centos7 app]# echo &#123;000..20..2&#125;000 002 004 006 008 010 012 014 016 018 020/* 三位数字，最大为20，步长为2]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>command</tag>
        <tag>echo</tag>
        <tag>quote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06-日期时间相关命令]]></title>
    <url>%2Flinux%2F20170519-06-date-datetime%2F</url>
    <content type="text"><![CDATA[1. date默认直接输入date显示当前系统时间 高级使用方法： date [OPTION]... [+FORMAT] date [-u|--utc|--universal] [MMDDhhmm[[CC]YY][.ss]] 第一种用法是一种显示时间方法： 1234[root@centos7 ~]# date +&quot;%Y%m%d&quot;20170519[root@centos7 ~]# date +&quot;%F %T&quot;2017-05-19 20:01:17 FORMAT的多种格式，可以通过man date来查看具体格式， 下面列出常用的： %F ：年月日全格式，例如2016-06-21 %T ：时间全格式，例如13:14:42 %Y：年 %m：月 %d：日 %H：小时 %M 分 -%S 秒 %s：从1970年1月1日00:00:00开始的秒数 第二种用法是用来更改时间的： 更改的时间格式为MMDDhhmm[[CC]YY][.ss] MM:month DD:day hh:hour mm:minute CC:centery YY:year ss:second 看可选项我们能明白，必须写月日小时分钟，可以只写年的两位，不写世纪，也可以年和世界都不写（就是不更改年），秒可写可不写。例如设置到2012年12月21日 11:11:11 12[root@centos7 ~]# date 122111112012.11Fri Dec 21 11:11:11 CST 2012 2. clock硬件时钟（clock==hwclock）clock又或者hwclock，是一样的命令。 主要用到两个： -s --hctosys：硬件时钟（hardware clock）to 系统时钟（system time），把系统时间调成和硬件时钟一样。-w, --systohc：系统时钟（system time） to 硬件时钟（hardware clock），把硬件时钟调成和系统时钟一样。 3. ntpdatentpdate IP：如htpdate 172.17.0.1 tips：前提是IP所在的那台机器启用了NTP服务，NTP服务后面我们会学，这里先了解下。 4. 更改时区CentOS 6和7都支持的命令：tzselect，是一个交互式的命令。 先让你选择洲，这里我选的5 Asia，然后选择国家，这里我选的9 China，然后选择时区，这里我选的1 Beijing。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859[root@centos7 ~]# tzselect Please identify a location so that time zone rules can be set correctly.Please select a continent or ocean. 1) Africa 2) Americas 3) Antarctica 4) Arctic Ocean 5) Asia 6) Atlantic Ocean 7) Australia 8) Europe 9) Indian Ocean10) Pacific Ocean11) none - I want to specify the time zone using the Posix TZ format.#? 5Please select a country. 1) Afghanistan 18) Israel 35) Palestine 2) Armenia 19) Japan 36) Philippines 3) Azerbaijan 20) Jordan 37) Qatar 4) Bahrain 21) Kazakhstan 38) Russia 5) Bangladesh 22) Korea (North) 39) Saudi Arabia 6) Bhutan 23) Korea (South) 40) Singapore 7) Brunei 24) Kuwait 41) Sri Lanka 8) Cambodia 25) Kyrgyzstan 42) Syria 9) China 26) Laos 43) Taiwan10) Cyprus 27) Lebanon 44) Tajikistan11) East Timor 28) Macau 45) Thailand12) Georgia 29) Malaysia 46) Turkmenistan13) Hong Kong 30) Mongolia 47) United Arab Emirates14) India 31) Myanmar (Burma) 48) Uzbekistan15) Indonesia 32) Nepal 49) Vietnam16) Iran 33) Oman 50) Yemen17) Iraq 34) Pakistan#? 9Please select one of the following time zone regions.1) Beijing Time2) Xinjiang Time#? 1The following information has been given: China Beijing TimeTherefore TZ=&apos;Asia/Shanghai&apos; will be used.Local time is now: Fri May 19 20:31:30 CST 2017.Universal Time is now: Fri May 19 12:31:30 UTC 2017.Is the above information OK?1) Yes2) No#? 1You can make this change permanent for yourself by appending the line TZ=&apos;Asia/Shanghai&apos;; export TZto the file &apos;.profile&apos; in your home directory; then log out and log in again.Here is that TZ value again, this time on standard output so that youcan use the /usr/bin/tzselect command in shell scripts:Asia/Shanghai CentOS 7还有一个非交互式的命令： 时间状态：timedatectl status 列出时区timedatectl list-timezones 更改时区timedatectl set-timezones 洲/城市 6和7都支持一个非交互式的方法，是直接覆盖文件： cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 5. cal显示日历，用法： cal [options] [[[day] month] year] cal：显示当月 cal -y：显示当年日历 cal [[[day] month] year]：如cal 21 12 2012 123456789[root@centos7 ~]# cal 21 12 2012 December 2012 Su Mo Tu We Th Fr Sa 1 2 3 4 5 6 7 8 9 10 11 12 13 14 1516 17 18 19 20 21 2223 24 25 26 27 28 2930 31]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>command</tag>
        <tag>date</tag>
        <tag>clock</tag>
        <tag>ntpdate</tag>
        <tag>timezone</tag>
        <tag>cal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-关机重启命令]]></title>
    <url>%2Flinux%2F20170519-05-shutdown-reboot%2F</url>
    <content type="text"><![CDATA[poweroff ：关机 halt ：关机，CentOS 6 断电，CentOS 7 不断电 reboot ：重启 init方式： init 0：关机 init 3：CLI界面(Command Line Interface) init 5：图形界面(GUI Graphycal U Interface) init 6：重启 shutdown：关机或重启 USAGE：shutdown [OPTIONS] ... TIME [MESSAGE] OPTIONS: -h：halt，关机 -r：reboot，重启 -c：cancel，取消关机或重启 TIME: now: 立刻 +m: 相对时间表示法，多久之后；例如 +3 hh:mm: 绝对时间表示，指明具体时间 MESSAGE：关机前发送的消息广播 shutdown的几个例子： shutdown -h now shutdown -r now shutdown -r +60 shutdown -h 18:30 &quot;系统即将关机&quot; shutdown -c]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>command</tag>
        <tag>poweroff</tag>
        <tag>halt</tag>
        <tag>reboot</tag>
        <tag>init</tag>
        <tag>shutdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-别名的使用方法]]></title>
    <url>%2Flinux%2F20170519-04-useage-of-alias%2F</url>
    <content type="text"><![CDATA[主要用到两个命令：alias&amp;unalias` alias：显示当前使用的别名。 alias NAME=&quot;XXXX&quot;：给某段命令或路径加别名 unalias NAME：去掉某个命令的别名 1234567891011121314151617181920212223242526272829[root@centos7 ~]# alias cdnet=&quot;cd /etc/sysconfig/network-scripts/&quot;[root@centos7 ~]# cdnet[root@centos7 network-scripts]# pwd/etc/sysconfig/network-scripts[root@centos7 network-scripts]# aliasalias cdnet=&apos;cd /etc/sysconfig/network-scripts/&apos;alias cp=&apos;cp -i&apos;alias egrep=&apos;egrep --color=auto&apos;alias fgrep=&apos;fgrep --color=auto&apos;alias grep=&apos;grep --color=auto&apos;alias l.=&apos;ls -d .* --color=auto&apos;alias ll=&apos;ls -l --color=auto&apos;alias ls=&apos;ls --color=auto&apos;alias mv=&apos;mv -i&apos;alias rm=&apos;rm -i&apos;alias which=&apos;alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde&apos;[root@centos7 network-scripts]# unalias cdnet[root@centos7 network-scripts]# alias alias cp=&apos;cp -i&apos;alias egrep=&apos;egrep --color=auto&apos;alias fgrep=&apos;fgrep --color=auto&apos;alias grep=&apos;grep --color=auto&apos;alias l.=&apos;ls -d .* --color=auto&apos;alias ll=&apos;ls -l --color=auto&apos;alias ls=&apos;ls --color=auto&apos;alias mv=&apos;mv -i&apos;alias rm=&apos;rm -i&apos;alias which=&apos;alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde&apos; 不用别名，用原始命令的话，可以用下面的方法： \COMMAND &#39;COMMAND&#39; &#39;/PATH/TO/COMMAND&#39; 如下图，ls原始命令是带颜色的，用\ls后不带颜色了 永久保存alias的方法： 针对一个用户的环境变量：更改vim ~/.bash_rc文件 针对全局用户：更改/etc/bash_rc文件（不推荐，不同用户有不同需要和偏好） 下面是某些alias设置1234alias vi =&quot;vim&quot;alias cdnet=&quot;cd /etc/sysconfig/network-scripts&quot;alias vimnet1=&quot;vim /etc/syscofig/network-scripts/eth0&quot;alias vimnet2=&quot;vim /etc/syscofnig/network-scripts/eth1&quot; 更改完成后，可以用source或.命令来使之立即生效，或者重启shell： 12[root@centos7 ~]# source ~/.bashrc [root@centos7 ~]# . ~/.bashrc]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>alias</tag>
        <tag>unalias</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-命令信息相关命令]]></title>
    <url>%2Flinux%2F20170519-03-command-info%2F</url>
    <content type="text"><![CDATA[1. type COMMAND查看命令类型shell builtin（shell 内建命令)： cd、ls、pwd、wall等。 enable或help出来的全是内部命令。help有简单的参数提示。 外部命令：非shell内建命令都是外部命令。 内部和外部都有的命令，优先使用内部命令。 type -a COMMAND：可以列出某命令的所有位置（包括内部和外部） type -P COMMAND：可以显示命令的路径 12345678910111213[root@centos7 ~]# type cdcd is a shell builtin[root@centos7 ~]# type -a cdcd is a shell builtincd is /usr/bin/cd[root@centos7 ~]# type -P cd/usr/bin/cd[root@centos7 ~]# type -a cdcd is a shell builtincd is /usr/bin/cd enable -n COMMAND1 ... COMMANDN：禁用一个或多个命令 enable COMMAND1 ... COMMANDN：启动一个或多个命令 上面这两个命令造成的影响，重登录bash失效。 2. which查看外部命令的路径which -a查看PATH变量下所有路径有没有此命令 which --skip-alias有的命令的别名，加此参数可以路过别名 3. whereis列出外部命令本身路径，和man帮助文件命令12[root@centos7 ~]# whereis cdcd: /usr/bin/cd /usr/share/man/man1/cd.1.gz /usr/share/man/man1p/cd.1p.gz 4. hash哈希表，缓存表系统初始hash表为空，当外部命令执行时，默认会从 PATH路径下寻找该命令，找到后会将这条命令的路径记录到 hash表中，当再次使用该命令时，shell解释器首先会查看hash 表，存在将执行之，如果不存在，将会去PATH路径下寻找。利 用hash缓存表可大大提高命令的调用速率。 hash：显示命令缓存，还有用了几次（hits） hash -l：列出（list）缓存过的命令 hash -p PATH NAME：可以给命令建立缓存 hash -d COMMAND：删除某个命令的缓存 hash -t COMMAND：列出单个别名的路径 hash -r：清空hash表 12345678910111213141516171819202122232425[root@centos7 ~]# hashhits command 2 /usr/bin/file 8 /usr/bin/ls[root@centos7 ~]# hash -lbuiltin hash -p /usr/bin/file filebuiltin hash -p /usr/bin/ls ls[root@centos7 ~]# hash -p /usr/bin/file f[root@centos7 ~]# hash -lbuiltin hash -p /usr/bin/file filebuiltin hash -p /usr/bin/file fbuiltin hash -p /usr/bin/ls ls[root@centos7 ~]# hash -d f[root@centos7 ~]# hash -lbuiltin hash -p /usr/bin/file filebuiltin hash -p /usr/bin/ls ls[root@centos7 ~]# hash -t file/usr/bin/file[root@centos7 ~]# hash -r[root@centos7 ~]# hashhash: hash table empty]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>hash</tag>
        <tag>command</tag>
        <tag>type</tag>
        <tag>which</tag>
        <tag>whereis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-查看命令的帮助]]></title>
    <url>%2Flinux%2F20170519-02-command-help%2F</url>
    <content type="text"><![CDATA[whatis模糊操作命令的帮助，并列出在哪个手册，比如下面搜索date，会列出条目和简介里带date字眼的所有命令字段。前面是命令，后面是简介。命令后面括号里的数字指的就是man手册的序号。 12345MBP:~ LongDream$ whatis datecal(1), ncal(1) - displays a calendar and the date of easterdate(1) - display or set date and timeiwidgets_datefield(n), iwidgets::datefield(n) - Create and manipulate a date field widgetntpdate(8) - set the date and time via NTP 执行的whatis 命令是查询数据库提供的，并不一定是最新的，可手动更新数据库，第一次运行可能会有点慢： CentOS 6 上：makewhatis CentOS 7 上：mandb help &amp; --help大部分命令都有--help选项，也是一个简要的帮助文档。 manman后的格式和内容详细的帮助文档：man COMMAND NAME 功能能行说明 SYNOPSIS 语法概要 []表示可选选项 &lt;&gt;表示必需提供的 |多选一 {}分组的 ...前面的内容可以出现多次 DESCRIPTION 描述 OPTIONS 选项 EXAMPLES 使用示例 AUTHOR 作者 REPORTING BUGS 报告bug方式 COPYRIGHT 版权 SEE ALSO 参考其他 man的序号 1-9 用户命令 系统调用 库调用 设备文件 文件格式 游戏 杂项 管理命令 whatis COMMAND：可以查看命令有哪几个manual 12345[root@centos7 ~]# whatis tartar (1) - manual page for tar 1.26tar (5) - format of tape archive files[root@centos7 ~]# whatis ifcfgifcfg (8) - simplistic script which replaces ifconfig IP management 默认查的是序号最小的那个man手册，如要要查其它的，可以用man 序号 COMMAND来查看命令,例如man 5 tar man手册操作方法翻屏操作：很多特别像vim的命令 键盘按键 效果 空格键 向下滚动一屏 b 向上滚动一屏 j 向上移动一行 k或回车 向下移动一行 gg 跳转至第一行 5g 跳转到第5行 G 跳转到最后一行 q 退出 文本搜索 键盘按键 效果 /keyword 向下查找，不区分大小写 ?keyword 向上查找，不区分大小写 n 查找下一个，与查找方向相同 N 查找上一个，与查找方向相反 man选项：man -M /path/to/somedir： 某些程序man目录不在标准的/usr/share/man到指定目录下查找命令手册并打开它。 程序自带帮助文档很多会放在/usr/share/doc/APP-VERSION下 README：程序的相关信息 INSTALL：安装帮助 CHANGES：版本改动信息 主流发行版官方文档http://www.redhat.com/doc等 程序的官方文档官方站点上会有docs/documents/documentation等字眼 善用搜索引擎google搜pdf文档这样： centos filetype:pdf 搜站点里资源都这样：centos site:centos.org 书籍出版社 O’REILLY Wrox 人民邮电出版社 机械工业出版社 电子工业出版社 清华大学出版社 图灵社区]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>command</tag>
        <tag>man</tag>
        <tag>help</tag>
        <tag>whatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-命令基本格式]]></title>
    <url>%2Flinux%2F20170519-01-command-base-type%2F</url>
    <content type="text"><![CDATA[命令的基本格式COMMAND [OPTIONS...] [ARGUMENTS...] 命令（COMMAND) 选项（OPTIONS）： 短格式， 如-a、-h、-l、-r、-f等 长格式， 如--forest,--exclude=PATTERN等 参数（ARGUMENTS)：通常为文件 可以执行多个命令，用逗号分隔开ls;pwd;hostname;who 这个不是特别好，如果中间有报错的命令，下面的命令也会继续执行。后面会学一个命令&amp;，可以在前面命令执行成功后再执行后面命令ls&amp;pwd&amp;hostn&amp;who，执行到hostn就会报错，后面的命令不予执行。当然也有好的 \回车后可以再接着写这种方式常用在长格式的命令下面用，段落更清晰。 下面举个例子。tar命令暂时没讲，知道\的作用就行： 12345678[root@centos7 ~]# tar -czvf \&gt; 20170519.tar.gz \&gt; * \&gt; --exclude=&quot;test&quot;anaconda-ks.cfginitial-setup-ks.cfg 执行中的命令退出 ctrl + d：正常退出 ctrl +c：强制退出]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>command</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08-换行（LF）和回车（CR）详解]]></title>
    <url>%2Flinux%2F20170518-08-lf-cr%2F</url>
    <content type="text"><![CDATA[我们打开Visual Studio Code编辑器，可以看到右下角有这个LF，这是VS Code的默认行尾序列的符号： 点开后，我们可以到，有两种模式可选，LF，CRLF： 为什么是这样呢，这两种模式有什么区别呢？ 在Linux下，默认换行的话，是LF模式，见下图两个红框部分： Linux下创建的LinuxFIle文件，用Linux的编辑器在里面写了三行文本。然后我用Python显示出转义符，可以看到是\n，这里的\n就是指的是换行符（LF） 然后我们在Windows下用记事本，写一个文件WindowsFile.txt，然后上转到Linux上去同样的方法查看。可以看到是\r\n，\r指的就是回车（CR），\r\n连起来就是回车换行（CRLF） 也就是说：在Linux里编辑文件，一行结束后跟的是\n；在Windows里用自带的记事本编辑文件，一行结束后跟的是\r\n tips：在Windows下有很多编辑器，是默认支持LF的方式，如Visual Studio Code、Sublime Text、Notepadd++，而且默认的编码格式是UTF-8，所以，大家在Windows下写Linux脚本，或打开Linux下的文件，可以用上面的编辑器，而不要用Windows自带的记事本。 CR和LF是缩写，其实他们的全称分别是：Carriage-Return和Line-Feed。追本溯源的说，CR(Carriage-Return)和LF(Line-Feed)这两个词来源于打字机的发明和使用。 打字机的纸张向下卷动一行，就是换行(LF, Line-Feed) 将打印头从最右边归位到最左边，就是回车(CR, Carriage-Return) 如果把一个Windows记事本建立的文档，放到Linux里用的话，要用dos2unix来转换一下后，再使用。当然最好的方法还是用专门的编辑器，不要用记事本。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>LF</tag>
        <tag>CR</tag>
        <tag>CRLF</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 7 安装步骤]]></title>
    <url>%2Flinux%2F20170517-02-install-centos7%2F</url>
    <content type="text"><![CDATA[1. 虚拟机软件Windows和Linux下推荐使用VMware Workstation macOS下推荐使用VMware Fusion 2. 虚拟机分配的资源因为用的软件不一样，这里设置方法无法截图，但大至如下： 2CPU/1G内存/200G硬盘 去掉打印机等没用的硬件（macOS要去掉打印机和摄像头） 光盘开始选择空白光盘，不要在这里选择iso安装，会有一个简易的自动安装，等完成设置后，再手动编辑设置改为iso镜像。同时勾选连接光盘和已连接。 3. 系统安装过程1、 加载光盘后，默认的选项是第二行：Test this media &amp; install CentOS Linux 7(测试媒介&amp;安装CentOS Linux 7)，按键盘↑选择Install CentOS Linux7（安装CentOS 7） 2、 在安装过程中使用的语言，这里使用默认的英文 3、 日期和时间DATA &amp; TIME 4、 软件选择SOFTWARE SELECT 这里选择Sever with GUI或者GNOME Desktop 5、 安装位置 选择I will configure partitioning（我将会配置分区），然后Done进入分区页面 默认分区方式为lvm，这里改为Standard Partition标准分区。 然后按+号开始划分分区 点Done后，弹出变更摘要Summary of changes，点Accept Changes 6、 NETWORK &amp; HOSTNAME（网络和主机名） 把网卡都打开 Host name自定义一下，然后点Apply 7、 设置结束，开始安装 8、 安装过程中，设置下root密码，然后创建一个用户 设置root密码 创建用户，勾选下作为管理员Make this user Administrator 9、 安装完后点重启 10、 重启后，接受下协议 11、 完成设置 12、 ifconfig检查两个网卡是否都起来了，是否有ip地址分配，如果没有，可以cat /etc/sysconfig/network-scripts/ifcfg-网卡名，看看里面有一行ONBOOT=no(这条配置决定了开机是否启动网卡)，如果是no, 可以用nano或gedit或vim等工具来更改为yes，这样下次重启后就永久生效了，然后ifup 网卡名实时启动网卡（避免重启）。 13、关闭系统，拍一个快照。 关机位置在这：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>CentOS 7</tag>
        <tag>install</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 6 安装步骤]]></title>
    <url>%2Flinux%2F20170517-01-install-centos6%2F</url>
    <content type="text"><![CDATA[1. 虚拟机软件Windows和Linux下推荐使用VMware Workstation macOS下推荐使用VMware Fusion 另外还有Virtual Box也非常好用。 2. 虚拟机分配的资源 2CPU/1G内存/200G硬盘 去掉打印机等的没用的硬件 光盘开始选择空白光盘，不要在这里选择iso安装，会有一个简易的自动安装，等完成设置后，再手动编辑设置改为iso镜像。同时勾选连接光盘和已连接。 3. 系统安装过程1、 选择“Install or upgrade an existing system”（安装或升级现存的系统） 2、 检测光盘有无问题，这里跳过，不检测 3、 下一步 4、 在安装过程中使用的语言，这里选英文 5、 键盘，选择U.S. English 6、 磁盘这里，选择基本存储设备（Basic Storage Devices） 7、 删除磁盘上的分区和文件系统，选Yes, discard any data 8、 hostname主机名填一个自定义的主机名。 9、 然后点击左下角的配置网络（Configure Network）来配置网卡信息。 10、 这里我们两个网段都有DHCP，所以可以都自动获取，但是这两个网卡默认都是不启用的，所以要打开自动连接（Connect automatically） 11、 都打开后，点击下一步 12、 这里选择时区，点地图上的点，点到上海，下面会自动变为Asia/Shanghai，然后关闭UTC时间（System clock uses UTC前面的勾去掉） 13、 设置root用户密码 14、 安装方式，选择自定义布局Create Custom Layout 15、 点击sda或者free，然后点Create创建分区。 16、 分区，各分区大小(这里给个参考，可跟据自己机器自由选择) 分区 大小（GB） 大小（MB） /boot 1 1024 / 100 102400 /app 50 51200 swap 2 2048 最后剩下的不用管，后面做lvm等磁盘实验用，先不分。 17、 这是让你确认是否开始格式化，点Format格式化 18、 是否开始给硬盘分区，点Write change to disk 19、 这里选择默认的就好。 tips：生产中有时候用u盘来给服务器装操作系统，这里要看到boot loader（很小的一小段空间，446字节，但很重要）的安装位置是不是/dev/sda，不要不小心把boot loader装到u盘(/dev/sdb)了，这里要手动改一下设备的。如果没注意，很有可能写到u盘里，这样u盘也无法使用了，然后拔掉u盘后，系统也无法启动了。 20、 选择安装包，选择Desktop，然后再自定义选择Customize now。 21、 自定义就可以选一些包了，比如选一个中文支持包，选一个KDE桌面包（跟据个人爱好，我还是比较喜欢Gnome） 包里其实有还有些可选包，在包上点右键，可以选择Select all optional packages，即选择所有可选包。我们可以看到Chinese Support的包为6 of 7，右键选完后，就变为7 of 7了。 22、 然后进入安装界面，开始安装包 23、 等待安装，安装完后，点击reboot 24、 进入一个欢迎界面，还有些设置要设置一下。 25、 同意协议 26、 创建用户，这里可以自己创建一下，或不填跳过，都可以。 27、 设置时间 28、 开启kdump，这里选择No，可以不用重启操作系统，如果按Yes，需要重启操作系统。 29、 安装完毕 30、 ifconfig检查两个网卡是否都起来了，是否有ip地址分配，如果没有，可以cat /etc/sysconfig/network-scripts/ifcfg-网卡名，看看里面有一行ONBOOT=no(这条配置决定了开机是否启动网卡)，如果是no, 可以用nano或gedit或vim等工具来更改为yes，这样下次重启后就永久生效了，然后ifup 网卡名实时启动网卡（避免重启）。 31、 关闭系统，拍一个快照。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>install</tag>
        <tag>CentOS 6</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux各发行版时间线2017年版本]]></title>
    <url>%2Flinux%2F20170516-01-linux-distribution-timeline%2F</url>
    <content type="text"><![CDATA[时间线目前版本16.12，于2017年2月1日发布，参见wikipedia svg图片太长，无法显示，可以点击链接下载:https://upload.wikimedia.org/wikipedia/commons/1/1b/Linux_Distribution_Timeline.svg]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Timeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学会科学上网——用搬瓦工和Shadowsocks搭梯子]]></title>
    <url>%2Flinux%2F20170515-04-shadowsocks%2F</url>
    <content type="text"><![CDATA[在学习Linux的过程中，经常遇到查阅英文文档被墙，以及不能使用Google的痛苦。这时候我们就需要科学上网。 购买vps云服务器http://banwagong.cn/其实这个网站不是搬瓦工的官网，但是可以跳转到英文版的搬瓦工的镜像网站，里面还有中文的介绍和优惠码，还有安装说明。 根据个人喜好，选择付费方案，kvm架构六机房的好处是可以换机房，在某个机房速度慢的时候，可以重新换个机房。一般是选固定洛杉矶和佛利蒙的机房，相同的价格，流量加倍，目前是1TB流量： 跳转到英文网站，选择付费方案（月付、季付、半年付、年付），一般选年付比较便宜，然后点Add to Crat，添加到购物车： 跳转到了结算页面，这里填入优惠码，然后可以省一点钱，然后点checkout结账： 填一些个人信息，地址国家选中国后，随便填，手机号也可以随便填(不会给中国的手机发短信的)。邮箱和密码要记住，是这个搬瓦工网站的账号和密码，支付方式选择alipay。 点Pay Now 就会跳转到支付宝的支付页面，手机扫描支付就可以了（支付宝支持汇率自动计算）。 配置虚拟机在搬瓦工的页面：点击Client Area登录 然后点击Services –&gt; My Services 进入服务面板，可以看到购买的虚拟机，有虚拟机的ip地址和主机名。点KiwVM Control Panel进入虚拟机控制面板。 点start启动虚拟机，还可以看到主控面板上服务器的ip地址、ssh端口号： 还有其他选项，可以修改服务器root密码什么的（随机生成一个root的密码）： 还可以更换操作系统(默认安装的CentOS 32位的系统)： 剩下的自己可以研究下，比如ssh终端之类的。 Shadowsocks国外服务端安装ssh登录到服务器上： 运行下面3个命令进行一键安装（参考：https://teddysun.com/342.html） 12345wget --no-check-certificate -O shadowsocks.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks.shchmod +x shadowsocks.sh./shadowsocks.sh 2&gt;&amp;1 | tee shadowsocks.log 安装过程中会让你设置shadowsocks服务的密码和端口号 然后就开始自动化安装了。 最后会出现这样一个页面，就安装服务成功了。 服务的管理方法，CentOS6是这样的： service shadowsocks stop 关闭service shadowsocks start 启动service shadowsocks restart 重启 在自己电脑上配置客户端客户端下载地址(GitHub上)： Windows tips：Windows有可能提示要下载.NET Framwork，根据提示自行查找.NET的版本安装。 macOS Android iOS tips: 或者App Store 里搜索Shadowrocket，可以使用，付费的，目前可能有其他免费的客户端，大家可以自行搜索下。(2017.10.14更新：Shadowrocket被下架，目前有一个shadowWingy) Windows下载后，在D盘(C盘会有权限问题，要以管理员运行)新建个文件夹，把解压后的Shadowsocks.exe放进去，然后启动后，在右下角小图标处，点配置服务器，可以直接输入ip、端口号（shadowsocks的服务端口号，不是ssh端口号）、选择加密方法、输入密码（shadowsocks服务的密码），然后选择启动服务。 还可以点生成二维码，共享给他人，其他人可以通过扫描二维码添加服务器，而不用手动设置。 macOS直接拖到应用程序里就可以了，配置方法和Windows类似。 手机端的，可以自行摸索，大同小异。 贴一个macOS的服务器设置页面：]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>bandwagon</tag>
        <tag>Shadowsocks</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工欲善其事必先利其器 —— Xmanager Enterprise 5 和 RealVNC 5/6 介绍]]></title>
    <url>%2Flinux%2F20170515-03-xmanager-realvnc%2F</url>
    <content type="text"><![CDATA[Xmanager Enterprise 5Xmanager Enterprise 5 是一个套件，内含Xshell、Xftp、X 有很多强大的功能，支持多种设备和系统的登录，Xshell支持的协议有：TELNET(远程登录) 、RLOGIN（UNIX登录）、 SSH（Linux登录） 、SFTP（SSH协议的SFTP） 、SERIAL（串口协议，通常用来直连网络设置如交换机、路由器等使用，替代了原来Windows下的超级终端） 主要用到两个工具： xshell界面 xftp 特色： 界面漂亮，扁平化设计 保存会话连接，会话可分组 保存账号密码，可自动登陆 可把常用的连接，放到界面的标签上，这样更便于连接 锁屏 设置方法： 效果 可以在设置里面更改多长时间不用锁屏 可以切换编码以适应系统，默认utf-8 可更改配色 可映射TUI（Text-based User Interface）基于文本用户界面到本地 可映射GUI（Graphical User Interface）图形用户界面到本地 tips：可以不用设置xdisplay，用最开始登陆时候的用户登录，就可以直接输入命令打开图形界面。（比如Oracle安装时候的OUI界面，直接用oracle用户直接登录，而不是root登录后切换oracle用户） 可以自定义复制和粘贴的方法 比如勾选这两个选项，选择即复制，然后去掉空白符： 鼠标，默认设置的是鼠标中间滚轮按下，是粘贴，右键，是弹出菜单。 如果喜欢SecureCRT风格的方式， 可以改为右键粘贴，不过我一般是用默认的中键，不会误操作。 下载地址：netsarang.com 下载链接是发到你邮箱的，所以要填你姓名和邮箱地址，然后点Submit提交，然后你就可以去邮箱收一下了。 RealVNC 5.x 的 VNC Address Book (本地) 可保存会话 可以保存主机信息（地址、端口、密码） 可以设置锁定密码 可以导出导入到另一台机器 RealVNC 6.x的 VNC Address Book（云）新的RealVNC 6支持云同步会话地址（不同步密码，密码比较重要，同步到云上也不安全，但本地会保存密码） 支持label标签(同上面5的文件夹，不过可以加多个标签)，可按标签分组机器。 手机版的RealVNC也有，可以自动同步过来地址（不同步密码） 下载地址：RealVNC]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>Xmanager</tag>
        <tag>realVNC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[本文章集合所遵循的开源协议以及开源协议介绍(GPL,LGPL,BSD,MIT,Apache,CC)]]></title>
    <url>%2Flinux%2F20170515-02-six-pl%2F</url>
    <content type="text"><![CDATA[图片来自于：https://github.com/phodal/licenses 版权声明：本博客所有文章除特别声明外，均采用CC BY-NC-SA 3.0许可协议。转载请注明出处。 下面介绍了各种协议。内容转载自https://www.oschina.net/question/54100_9455 什么是许可协议？ 什么是许可，当你为你的产品签发许可，你是在出让自己的权利，不过，你仍然拥有版权和专利（如果申请了的话），许可的目的是，向使用你产品的人提供 一定的权限。 不管产品是免费向公众分发，还是出售，制定一份许可协议非常有用，否则，对于前者，你相当于放弃了自己所有的权利，任何人都没有义务表明你的原始作者身份，对于后者，你将不得不花费比开发更多的精力用来逐个处理用户的授权问题。 而开源许可协议使这些事情变得简单，开发者很容易向一个项目贡献自己的代码，它还可以保护你原始作者的身份，使你至少获得认可，开源许可协议还可以阻止其它人将某个产品据为己有。以下是开源界的 5 大许可协议。 GNU GPLGNU General Public Licence (GPL) 有可能是开源界最常用的许可模式。GPL 保证了所有开发者的权利，同时为使用者提供了足够的复制，分发，修改的权利： 可自由复制你可以将软件复制到你的电脑，你客户的电脑，或者任何地方。复制份数没有任何限制。 可自由分发在你的网站提供下载，拷贝到U盘送人，或者将源代码打印出来从窗户扔出去（环保起见，请别这样做）。 可以用来盈利你可以在分发软件的时候收费，但你必须在收费前向你的客户提供该软件的 GNU GPL 许可协议，以便让他们知道，他们可以从别的渠道免费得到这份软件，以及你收费的理由。 可自由修改如果你想添加或删除某个功能，没问题，如果你想在别的项目中使用部分代码，也没问题，唯一的要求是，使用了这段代码的项目也必须使用 GPL 协议。 需要注意的是，分发的时候，需要明确提供源代码和二进制文件，另外，用于某些程序的某些协议有一些问题和限制，你可以看一下 @PierreJoye 写的 Practical Guide to GPL Compliance 一文。使用 GPL 协议，你必须在源代码代码中包含相应信息，以及协议本身。 GNU LGPLGNU 还有另外一种协议，叫做 LGPL （Lesser General Public Licence），它对产品所保留的权利比 GPL 少，总的来说，LGPL 适合那些用于非 GPL 或非开源产品的开源类库或框架。因为 GPL 要求，使用了 GPL 代码的产品必须也使用 GPL 协议，开发者不允许将 GPL 代码用于商业产品。LGPL 绕过了这一限制。 BSDBSD 在软件分发方面的限制比别的开源协议（如 GNU GPL）要少。该协议有多种版本，最主要的版本有两个，新 BSD 协议与简单 BSD 协议，这两种协议经过修正，都和 GPL 兼容，并为开源组织所认可。 新 BSD 协议（3条款协议）在软件分发方面，除需要包含一份版权提示和免责声明之外，没有任何限制。另外，该协议还禁止拿开发者的名义为衍生产品背书，但简单 BSD 协议删除了这一条款。 MITMIT 协议可能是几大开源协议中最宽松的一个，核心条款是： 该软件及其相关文档对所有人免费，可以任意处置，包括使用，复制，修改，合并，发表，分发，再授权，或者销售。唯一的限制是，软件中必须包含上述版权和许可提示。 这意味着： 你可以自由使用，复制，修改，可以用于自己的项目。 可以免费分发或用来盈利。 唯一的限制是必须包含许可声明。 MIT 协议是所有开源许可中最宽松的一个，除了必须包含许可声明外，再无任何限制。 ApacheApache 协议 2.0 和别的开源协议相比，除了为用户提供版权许可之外，还有专利许可，对于那些涉及专利内容的开发者而言，该协议最适合（这里有 一篇文章阐述这个问题）。 Apache 协议还有以下需要说明的地方: 永久权利一旦被授权，永久拥有。 全球范围的权利在一个国家获得授权，适用于所有国家。假如你在美国，许可是从印度授权的，也没有问题。 授权免费，且无版税前期，后期均无任何费用。 授权无排他性任何人都可以获得授权 授权不可撤消一旦获得授权，没有任何人可以取消。比如，你基于该产品代码开发了衍生产品，你不用担心会在某一天被禁止使用该代码。 分发代码方面包含一些要求，主要是，要在声明中对参与开发的人给予认可并包含一份许可协议原文。 Creative CommonsCreative Commons (CC) 并非严格意义上的开源许可，它主要用于设计。Creative Commons 有多种协议，每种都提供了相应授权模式，CC 协议主要包含 4 种基本形式： 署名权必须为原始作者署名，然后才可以修改，分发，复制。 保持一致作品同样可以在 CC 协议基础上修改，分发，复制。 非商业作品可以被修改，分发，复制，但不能用于商业用途。但商业的定义有些模糊，比如，有的人认为非商业用途指的是不能销售，有的认为是甚至不能放在有广告的网站，也有人认为非商业的意思是非盈利。 不能衍生新作品你可以复制，分发，但不能修改，也不能以此为基础创作自己的作品。 这些许可形式可以结合起来用，其中最严厉的组合是“署名，非商用，不能衍生新作品”，意味着，你可以分享作品，但不能改动或以此盈利，而且必须为原 作者署名。在这种许可模式下，原始作者对作品还拥有完全的控制权，而最宽松的组合是“署名”，意味着，只要为原始作者署名了，就可以自由处置。]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>GPL</tag>
        <tag>LGPL</tag>
        <tag>BSD</tag>
        <tag>MIT</tag>
        <tag>Apache</tag>
        <tag>CC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[笔记所使用的markdown语法]]></title>
    <url>%2Flinux%2F20170515-01-markdown%2F</url>
    <content type="text"><![CDATA[Markdown是什么？Markdown是web上的一种格式文本。Markdown功能强大，我们可以使用Markdown控制文档的显示、把文字格式为粗体或斜体、添加图像、创建列表等等。特别值得一提的是，Markdown编写只需普通文字与一些非字母字符，比如#或*，易于学习与掌握。 Markdown文件的后缀是.md或.markdown 此篇教程就是用Markdown编写的，而且采用的是Github的markdown扩展语法——GFM(GitHub Flavored Markdown) markdown软件 Mweb（目前使用的） 支持平台： web/chrome app/windows/macOS 官网： http://zh.mweb.im/ 收费情况：试用14天，收费版98/年。 优点： 对 Hexo、Octpress、Jekyll 等静态博客做了优化，以方便插入图片和实时预览。可以同步到各种平台，如印象笔记，Wordpress等。支持图片存储到七牛云。 缺点：只有Mac版和iOS版本 网上评价：https://zhuanlan.zhihu.com/p/21760223 马克飞象（用过两年） 支持平台： web/chrome app/windows/macOS 官网： https://maxiang.io/ 收费情况：试用10天，无免费版，收费版79/年。 优点：有浏览器就能使用，可随便贴图，而不用指定图片路径，可与印象笔记同步，可导出md源格式，导出生成pdf或html格式。 缺点：收费，联网才能使用。 网上评价：https://www.zhihu.com/question/24676344 Visual Studio Code 支持平台：Windows/macOS/Linux 官网：https://code.visualstudio.com/ 收费情况：完全免费 优点：界面美观，插件众多，一个功能强大的编辑器，支持众多语法，支持git，微软随便弄一个团队做出业的产品，就比市面上的类似产品，如Atom,Sublime Text好很多,打开大文件速度非常快，没有Atom卡慢的问题，比Sublime Text安装插件的方式更简洁。 缺点：markdown的话，无法贴图，要放好位置，手动敲入图片路径（不知道是否有插件能支持） 网上评价：https://www.zhihu.com/question/29984607 简书博客平台 支持平台：Web/Android/iOS 官网：www.jianshu.com 收费情况：完全免费 优点：支持直接贴图，可以做为博客使用，界面美观 缺点：无法导出md源文件(连图片都导出的那种功能)，无法导出pdf Markdown语法讲解文本加粗、斜体和超链接代码： 12用Markdown让词句有**加粗**或*斜体*的效果很容易.你也可以创建一个文本超链接，像这样：[连接到Github](https://github.com) 代码的效果： 用Markdown让词句有加粗或斜体的效果很容易.你也可以创建一个文本超链接，像这样：连接到Github 列表代码： 12345678910111213141516171819202122232425有时候你想要带数字编号的列表1. 一2. 二3. 三有时候你想要`·`作为编号，以下3个符号`*`、`+`、`-`可以混用* 可以用`*`* 继续用`*`+ 可以用`+`+ 继续用`+`- 可以用`-`- 继续用`-`还可以用分级形式1. 第一章2. 第二章3. 第三章 - 你好 - 再见 代码的效果： 有时候你想要带数字编号的列表 一 二 三 有时候你想要·作为编号，以下3个符号*、+、-可以混用 可以用* 继续用* 可以用+ 继续用+ 可以用- 继续用- 还可以用分级形式 第一章 第二章 第三章 你好 再见 图片代码： 123Github的Yaktocat![ Yaktocat图片](https://octodex.github.com/images/yaktocat.png) 代码的效果： Github的Yaktocat 标题标题由#+标题表示，从#到######一共六个等级的标题代码： 123#标题1##标题2###标题3 代码的效果： 引用如果你想要引用某些东西，在行前面用&gt;符号。例如： 代码： 123456&gt; 我是一个运维，刚开始学编程,我觉得：&gt;&gt; Markdown真是一个好用的工具。&gt;&gt; 编程是一个不停实践的过程。&gt; ——*于龙君* 代码的效果： 我是一个运维，刚开始学编程,我觉得： Markdown真是一个好用的工具。编程是一个不停实践的过程。 ——于龙君 代码加阴影和高亮无论是一小行代码加阴影还是一整段程序加阴影，都可以用反引号`。 ####代码： 代码的效果： 段落中出现的代码可以用，如/usr/bin/env python3,整个大段的程序代码也可以用： 1234#!/usr/bin/env python3# -*- encoding: utf-8 -*-print('Hello,World!')print('Hello,Markdown') 任务列表代码： 123456我是一个程序员小白，我在学编程，我的学习列表是：- [x] Markdown使用方法- [x] git&amp;github使用方法- [x] Python3.5学习- [ ] Python3实战项目- [ ] shell 代码的效果： 我是一个程序员小白，我在学编程，我的学习列表是： [x] Markdown使用方法 [x] git&amp;github使用方法 [x] Python3学习 [ ] Python3实战项目 [ ] Shell 这个属于GMF语法 表格表格支持左对齐:----，右对齐----:，居中对齐 :--:，如果没有:，默认左对齐。 代码： 123456| 我喜欢的电子产品 | 价格(USD) | 喜欢程度 || :------------ | ----: | :--: || Macbook pro | 1399-2799| ★★★★★ || Surface Book | 1499-3199 | ★★★★★ || ipad pro | 599-1129 | ★★★☆☆ || iphone plus | 869-969 | ★★★★☆ | 代码的效果： 我喜欢的电子产品 价格(USD) 喜欢程度 Macbook pro 1399-2799 ★★★★★ Surface Book 1499-3199 ★★★★★ ipad pro 599-1129 ★★★☆☆ iphone plus 869-969 ★★★★☆]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆（heap）]]></title>
    <url>%2Fpython%2F20170514-08-heap%2F</url>
    <content type="text"><![CDATA[二叉堆（Binary Heap）堆是一种特殊的基于树的数据结构，他满足堆的特性：所有的节点要么全大于，要么全等于，要么全小于他的每一孩子节点。 满二叉树Full Binary Tree：树上一个每一个节点要么有0个，要么有2个子节点。 完全二叉树Complete Binary Tree：除最后一层外，若其余层都是满的，并且最后一层或者是满的，或者是在右边缺少连续若干节点。 二叉堆Binary Heap：二叉堆是一种特殊的堆，二叉堆是完全二叉树或者是近似完全二叉树。二叉堆满足堆特性：父节点的键值总是保持固定的序关系于任何一个子节点的键值，且每个节点的左子树和右子树都是一个二叉堆。当父节点的键值总是大于或等于任何一个子节点的键值时为大顶堆Max Heap； 当父节点的键值总是小于或等于任何一个子节点的键值时为小顶堆Min Heap。 大顶堆： 小顶堆： 二叉堆的性质（这里假设根节点索引是0)： 索引为1的左子节点的索引是(2*i+1) 索引为i的右子节点的索引是(2*i+2) 索引为i的父节点的索引是(i-1)//2 下面部分文字和图片源自chao_yu的博客，做了部分更改，原文是用c实现的，这里图片中还保留数组字眼，不影响理解。(允许我懒一下，后面课程我都没跟上大部队，写笔记比较费时间) 你一定发觉了，最小的一个元素就是列表第一个元素，那么二叉堆这种有序列表如何入队呢？看图： 假设要在这个二叉堆里入队一个单元，键值为2，那只需在列表末尾加入这个元素，然后尽可能把这个元素往上挪，直到挪不动，经过了这种复杂度为Ο(logn)的操作，二叉堆还是二叉堆。 出堆一定是出列表的第一个元素，这么来第一个元素以前的位置就成了空位，我们需要把这个空位挪至叶子节点，然后把列表最后一个元素插入这个空位，把这个“空位”尽量往上挪。这种操作的复杂度也是Ο(logn)，比Ο(n)强多了吧？ Python3实现代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import mathimport randomclass Heap: def __init__(self): self.__data = [] def insert(self, value): self.__data.append(value) idx = len(self.__data) - 1 parent = math.floor((idx - 1) / 2) while parent &gt;= 0 and self.__data[parent] &lt; value: self.__data[idx] = self.__data[parent] self.__data[parent] = value idx = parent parent = math.floor((idx - 1) / 2) def pop(self): if not self.__data: raise Exception('Empty') ret = self.__data[0] value = self.__data.pop() self.__data[0] = value idx = 0 left = 2 * idx + 1 right = 2 * idx + 2 while len(self.__data) &gt; left: tmp_idx = left if len(self.__data) &gt; right and self.__data[right] &gt; self.__data[left]: tmp_idx = right if self.__data[tmp_idx] &gt; value: self.__data[idx] = self.__data[tmp_idx] self.__data[tmp_idx] = value else: return ret idx = tmp_idx left = 2 * idx + 1 right = 2 * idx + 2 return ret def remove(self, i): if len(self.__data) - 1 &lt; i: raise Exception('Empty') ret = self.__data[i] value = self.__data.pop() self.__data[i] = value idx = i left = 2 * idx + 1 right = 2 * idx + 2 while len(self.__data) &gt; left: tmp_idx = left if len(self.__data) &gt; right and self.__data[right] &gt; self.__data[left]: tmp_idx = right if self.__data[tmp_idx] &gt; value: self.__data[idx] = self.__data[tmp_idx] self.__data[tmp_idx] = value else: return ret idx = tmp_idx left = 2 * idx + 1 right = 2 * idx + 2 return ret def view(self): print(self.__data)if __name__ == '__main__': heap = Heap() for _ in range(8): i = random.randint(0, 100) print('i is ', i) heap.insert(i) heap.view() #heap.pop() heap.remove(1) heap.view()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>heap</tag>
        <tag>binary heap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07-tree]]></title>
    <url>%2Fpython%2F20170514-07-tree%2F</url>
    <content type="text"><![CDATA[一个树形的数据结构就是树，有根节点，有子树。 二叉树binary tree：是每个节点最多有两个子树的树结构。通常子树被称作左子树left subtree和右子树right subtree。二叉树常被用于实现,二叉查找树Binary Search Tree 和 二叉堆Binary Heap。 构建一个二叉树，类似于这种： 代码： tree.py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150from stack import Stackfrom queue import Queueclass Node: def __init__(self, value): self.value = value self.left = None self.right = None def __str__(self): return '&#123;0&#125;&lt;--&#123;1&#125;--&gt;&#123;2&#125;'.format(self.left, self.value, self.right) def __repr__(self): return self.__str__()class Tree: def __init__(self, node): self.root = node def add_left(self, tree): self.root.left = tree def add_right(self, tree): self.root.right = tree @property def left(self): return self.root.left @property def right(self): return self.root.right @left.setter def left(self, value): self.root.left = value @right.setter def right(self, value): self.root.right = value @left.deleter def left(self): del self.root.left @right.deleter def right(self): del self.root.right # 先序遍历,用递归的方式。 # 先访问root，再访问左子树，再访问右子树。 # fn为访问函数。 def visit_first(self, fn): fn(self.root.value) if self.left: self.left.visit_first(fn) if self.right: self.right.visit_first(fn) # 中序遍历，用递归的方式。 # 先访问左子树，再访问root，再访问右子树。 # fn为访问函数。 def visit_middle(self, fn): if self.left: self.left.visit_middle(fn) fn(self.root.value) if self.right: self.right.visit_middle(fn) # 后续遍历，用递归的方式。 # 先访问左子树，再访问右子树，最后访问root。 # fn为访问函数。 def visit_last(self, fn): if self.left: self.left.visit_last(fn) if self.right: self.right.visit_last(fn) fn(self.root.value) # 先序遍历，非递归的方式。 # 利用栈来实现。 # fn为访问函数。 def iter_visit_first(self, fn): stack = Stack() stack.push(self) #push整棵树 while stack.top: #当栈顶不为空的时候 p = stack.pop() #弹出栈顶 fn(p.root.value) #先访问root # 入栈先入右，再入左，因为先进后出，所以后入左，出的时候就先出左。 if p.right: stack.push(p.right) if p.left: stack.push(p.left) # 层序遍历。 # 利用序列来实现。 # fn为访问函数。 def visit_level(self, fn): queue = Queue() queue.put(self) while not queue.empty(): p = queue.get() fn(p.root.value) if p.left: queue.put(p.left) if p.right: queue.put(p.right)if __name__ == '__main__': d = Tree(Node('D')) e = Tree(Node('E')) b = Tree(Node('B')) b.left = d b.right = e f = Tree(Node('F')) g = Tree(Node('G')) c = Tree(Node('C')) c.left = f c.right = g a = Tree(Node('A')) a.left = b a.right = c from functools import partial p = partial(print, end='') a.visit_first(p) print() a.visit_middle(p) print() a.visit_last(p) print() a.iter_visit_first(p) print() a.visit_level(p) # ABDECFG 先序遍历，递归方法 # DBEAFCG 中序遍历，递归方法 # DEBFGCA 后序遍历，递归方法 # ABDECFG 先序遍历，非递归的方法，用栈的方法。从左到右，从上到下。 #TODO 中序、后序遍历的非递归方法 # ABCDEFG 层序遍历，一层层遍历，用序列的方法。从第一层到最后一层]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06-哈希（hash）]]></title>
    <url>%2Fpython%2F20170514-06-hash%2F</url>
    <content type="text"><![CDATA[在Python里面叫字典（dict），在ruby里面叫哈希（hash），在java里面叫map，是一种kv结构（key-value)，也是一种线性结构。 散列表（Hash table，也叫哈希表），是根据 键-值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。给定表M，存在函数f(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为哈希(Hash）表，函数f(key)为哈希(Hash) 函数。 下图为拉链法： 定义来自百度百科。 先介绍几个概念： 拉链法： 求模：fn(k)%n（除法比乘法慢，有效率问题） 平方位移法：fn(k)**2 +n（乘法，快一些） 斐波那契位移法：hash(k)*fab(64) （64位系统上） 开地址法： g = current + 1 （ 很少用，有效率问题） 其他g函数（效率较高的g函数，暂略过） 介绍几个小技巧,便于理解后面的代码。 列表复制： 123456In [8]: list1 = ['pig', 'monkey', 'lion', 'tiger']In [9]: list2 = list1[:]In [10]: list2Out[10]: ['pig', 'monkey', 'lion', 'tiger'] 枚举函数enumerate——给可迭代对象加上索引：1234567891011list1 = ['pig', 'monkey', 'lion', 'tiger']for index, item in enumerate(list1): print (index, item) In [4]: for index, item in enumerate(list1): ...: print (index, item) ...:0 pig1 monkey2 lion3 tiger hash（map）用python代码来表示：map.py:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class Node: def __init__(self, key, value): self.key = key self.value = value def __eq__(self, other): return self.key == other.key# 拉链法实现hash（map）class Map: def __init__(self, init_size, hash=hash): #初始化槽位 self.__slot = [[] for _ in range(init_size)] # for _ in range(init_size): # self.__slot.append([]) self.__size = init_size self.hash = hash def put(self, key, value): node = Node(key, value) # 拉链法的简单方法：求模。 # 虽然效率有问题，可以暂时用来做演示。 address = self.hash(node.key) % self.__size self.__slot[address].append(node) def get(self, key, default=None): _key = self.hash(key) address = _key % self.__size for node in self.__slot[address]: if node.key == key: return node.value return default def remove(self, key): address = self.hash(key) % self.__size try: self.__slot[address].remove(Node(key, None)) except ValueError: pass # for idx, node in enumerate(self.__slot[address][:]): # if node.key == key: # self.__slot[address].pop(idx)if __name__ == '__main__': map = Map(16) for i in range(20): map.put(i, i) map.remove(3) for i in range(20): print(map.get(i, 'not set'))]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>hash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05-队列（queue）]]></title>
    <url>%2Fpython%2F20170514-05-queue%2F</url>
    <content type="text"><![CDATA[自写一个简单的队列队列结构和链表差不多。队列遵循先进先出的pop()方法。 写一个简单的队列：（看不懂代码的可以去复习一下链表，有相似之处，再来看队列，就觉得很简单了） queue.py 1234567891011121314151617181920212223242526272829303132333435class Node: def __init__(self,value): self.value = value self.next = Noneclass Queue: def __init__(self): self.head = None self.tail = None def put(self,value): node = Node(value) if self.head is None: self.head = node self.tail = node else: self.tail.next = node self.tail = node def pop(self): if self.head is None: raise Exception('empty queue') node = self.head self.head = node.next return node.valueif __name__ == '__main__': q = Queue() for i in range(10): q.put(i) for _ in range(10): # 下划线的意思是我取到的数不要，直接丢弃，默认用法，可参考cookbook第一章，是一种pythonic的小技巧 print(q.pop()) python标准库中的队列（Queue)复杂的就暂时不说了，我们来看下Python标准库中Queue的用法。或者from queue import Queue后，help(Queue)查看用法： 说简单说几个，如果要看详细的，请参考Python标准库 __init__(self, maxsize=0)：Queue有一个maxsize属性，可以用来限制队列的大小。 empty(self)：判断队列是不是空队列 full(self)：判断队列有没有满 get(self, block=True, timeout=None)：从队列移除并返回一个元素，block=True的时候如果队列为空，则阻塞住，timeout可以设置阻塞的时间，超过timeout设置的时间再抛出异常，没设置就持续阻塞。 get_nowait(self)：重定向到get方法上，只不过block为false，不阻塞，当队列为空的时候，直接抛出一个空异常。 join(self)：队列多用于多线程的情况下，join方法可以等待其他所有持有这个队列的线程退出，再运行。 put(self, item, block=True, timeout=None)：给队列加元素，block=True的时候，当队列满了，则阻塞住，timeout可以设置阻塞的时间，超过timeout设置的时间再抛出异常，没设置就持续阻塞。 put_nowait(self)：重定向到put方法上，只不过默认block为false，不阻塞，当队列为空的时候，直接抛出一个空异常。 qsize(self)：返回当前队列的长度。 队列暂时提一下，后续多线程和并发会讲到具体用法。 分布式队列：httpsqs，kafka，redis，AMQP， qpid 后面会讲到用队列实现RPC ——远程过程调用协议 Remote Procedure Call Protocol 双端队列（deque）和普通队列不同的是，他是两段都可以进，两段都可以出，分左和右。 我们来看下标准库的实现，用代码理解(只列举部分，可以自己去试验)： 1234567891011121314151617181920212223242526272829303132333435363738In [1]: from collections import dequeIn [9]: dq = deque([1,2,3,4,5])In [10]: dqOut[10]: deque([1, 2, 3, 4, 5])In [11]: dq.append(6)In [12]: dqOut[12]: deque([1, 2, 3, 4, 5, 6])In [13]: dq.appendleft(0)In [14]: dqOut[14]: deque([0, 1, 2, 3, 4, 5, 6])In [15]: dq.pop()Out[15]: 6In [16]: dqOut[16]: deque([0, 1, 2, 3, 4, 5])In [17]: dq.popleft()Out[17]: 0In [18]: dqOut[18]: deque([1, 2, 3, 4, 5])In [19]: dq.extend([6,7,8])In [20]: dqOut[20]: deque([1, 2, 3, 4, 5, 6, 7, 8])In [28]: dq.extendleft([0,-1,-2])In [29]: dqOut[29]: deque([-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8]) 环形队列初始化固定的size，元素链接成一个环，head和tail连接起来。deque也是一个环装队列123456789101112131415161718192021In [2]: from collections import dequeIn [3]: ring = deque(maxlen=5)In [4]: ringOut[4]: deque([])In [5]: ring.append(1)In [6]: ringOut[6]: deque([1])In [7]: ring.extend([2,3,4,5])In [8]: ringOut[8]: deque([1, 2, 3, 4, 5])In [9]: ring.append(6)In [10]: ringOut[10]: deque([2, 3, 4, 5, 6]) 有什么作用呢，历史记录就可以用环装队列来保存，当记录过多，就把之前的记录顶替掉了，免得满了，占太多空间。比如默认存200条历史记录，当超过200条时候，就把最开始的历史记录删掉。还比如日志分析时候，要往报错的信息往前查看几行信息，而不是光报错那行，就可以用环装队列。每匹配一条，就把他放在环装队列里面，当匹配到的那条报警了，我们就把整个环装队列的内容都返回回去，这样就能看到前面的几行了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04-stack应用-规则解析]]></title>
    <url>%2Fpython%2F20170514-04-stack-parse-rules%2F</url>
    <content type="text"><![CDATA[写一个简单的规则解析，只涉及表达式的True和False的解析。 定义的匹配 规则是： 表达式用两个#括起来，类似于#expr#，只计算表达式的布尔值：True或False。 非用!， 与用&amp;， 或用| 。 可以用()来改变优先级。 先理解一个python的re.match的用法，下面代码用的到： re是一个正则表达式处理函数。re.match的函数原型为：re.match(pattern, string, flags)。 第一个参数是正则表达式，如果匹配成功，则返回一个Match，否则返回一个None； 第二个参数表示要匹配的字符串； 第三个参数是标致位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。 解析式样式类似于这样：‘(#e1# &amp; #e2#) |(!#e3# &amp; #e4#)’具体的可能是这样：&#39;(#abc# &amp; #324#) | (!#def# &amp; #789#)&#39; 我们来分析下： 读到(，(直接入栈。 读到第一个#，入栈。 遇到&amp;、|、!，直接入栈。 继续读，后面的如果不是#，就全部读入一个字符串，把这个字符串表达式入栈。 遇到第二个# ，就去读栈顶，把栈顶的那个字符串取出来，然后再取出栈顶#，两个#抵消，把中间的字符串表达式入栈.这时候要查看表达式的前一个元素，如果前一个元素是!，直接对表达式取反（加一个Not前面），然后把取反后的布尔值入栈。 遇到)，把栈顶值取出来，然后再取出栈顶，也就是(，两个括号抵消，把之前那个栈顶值入栈。 遇到空格，pass 注意一个小陷阱，表达式里面也有可能有运算符或者括号，这时候要加一个flags来验证是不是在表达式内部的。 下面是规则解析的代码（缺详细注释，后续添加）：rule_parser.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107# #expr# &amp; | ! ()# (#e1# &amp; #e2#) |(!#e3# &amp; #e4#)from stack import Stack# '(#abc# &amp; #324#) | (!#def# &amp; #789#)'# 定义一个match匹配函数，接受表达式，当前的一行日志line，fn。# fn为正则匹配函数,fn接收两个参数，expr和line。def match(exprs, line, fn): stack = Stack() is_expr = False #flags expr = [] for c in exprs: if c == '#': if not is_expr: is_expr = True else: is_expr = False v = fn(line, ''.join(expr)) expr = [] if stack.top is None: stack.push(v) continue s = stack.pop() if s == '!': v = not v if stack.top is None: stack.push(v) continue s = stack.pop() if s == '&amp;': if isinstance(stack.top.value, bool): v = stack.pop() and v stack.push(v) else: raise Exception('wrong expr') elif s == '|': if isinstance(stack.top.value, bool): v = stack.pop() or v stack.push(v) else: raise Exception('wrong expr') elif s == '(': stack.push(s) stack.push(v) else: raise Exception('wrong expr') else: if is_expr: expr.append(c) else: if c in '(&amp;!|': stack.push(c) elif c.strip() == '': pass elif c == ')': v = stack.pop() if not isinstance(v, bool): raise Exception('wrong expr') s = stack.pop() if s == '!': v = not v s = stack.pop() if s == '(': stack.push(v) else: raise Exception('wrong expr') else: raise Exception('wrong expr') while stack.top: v = stack.pop() if not isinstance(v, bool): raise Exception('wrong expr') s = stack.pop() if s == '!': v = not v s = stack.pop() if s == '&amp;': v2 = stack.pop() if not isinstance(v2, bool): raise Exception('wrong expr') v = v and v2 elif s == '|': v2 = stack.pop() if not isinstance(v2, bool): raise Exception('wrong expr') v = v or v2 else: raise Exception('wrong expr') if stack.top is None: return v else: stack.push(v)if __name__ == '__main__': import re line = 'abc 123 def 456 asd 789' exprs = '(#abc# &amp; #324#) | (!#def# &amp; #789#)' # False def callback(line, expr): return re.match(expr, line) is not None print(match(exprs, line, callback))#TODO 优化两个程序， 使其模块化]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-stack应用--表达式解析]]></title>
    <url>%2Fpython%2F20170514-03-stack-parse-expr%2F</url>
    <content type="text"><![CDATA[先写一个简单的算术表达式计算器，有数字，有括号，有运算符（暂时不考虑运算符优先级）。 假如是这样的算数表达式：(3+4)*5/((2+3)*3) 先分析解析思路，纸上谈兵（有点长，慢慢看）： 读到(，(直接入栈。 读到3， 是数字，此时栈顶是(，不是运算符，所以就入栈3。 读到+，是运算符，直接入栈。 读到4，是数字，此时的栈顶是运算符+，所以把+出栈，然后再把此时的栈顶3也出栈，运算出来，等于7，然后把运算出来的结果7入栈。 读到)，此时的栈顶是数字的话，就出栈并且暂存下，然后再出栈一次，就把(也出栈了，两个括号抵消了，把暂存的7继续入栈。 读到*，是运算符，直接入栈。 读到5，是数字，此时的栈顶是运算符*，所以把*出栈，然后再把此时的栈顶7也出栈，运算出来，等于35，然后把运算出来的结果35入栈。 读到/，是符号，直接入栈。 读到(，(直接入栈。 读到(，(直接入栈。 读到2， 是数字，此时栈顶是(，不是运算符，所以就入栈2。 读到+，是运算符，直接入栈。 读到3，是数字，此时栈顶是运算符+,把+出栈，然后再把此时的栈顶2也出栈，运算出来，等于5，然后把运算出来的结果5入栈。 读到*，是运算符，直接入栈。 读到3，是数字，此时栈顶是运算符*,把*出栈，然后再把此时的栈顶5也出栈，运算出来，等于15，然后把运算出来的结果15入栈。 读到)，此时的栈顶是数字的话，就出栈并且暂存下，然后再出栈一次，就把(也出栈了，两个括号抵消了，把暂存的15继续入栈。17.栈还是有元素，就继续从头开始上面的步骤，此时可以用到迭代来做，如果没有元素了，就把最后pop出来的那个栈顶给return出来，就是最终结果。 我们总结出一套规则： 读到(和运算符，(和运算符就直接入栈。 读到空格，就直接pass就好，不做任何操作 读到数字，查看此时的栈顶，如果不是运算符，就入栈此数字，如果是运算符，就把运算符出栈，然后再出一次栈顶（把运算符之前的数字取出来），进行运算，然后把运算出来的结果入栈。 读到)，此时的栈顶是数字的话，就出栈并且暂存下，然后再出栈一次，就把(也出栈了，两个括号抵消了，再把暂存的数字继续入栈。（还要考虑一种情况，就是多打个)的话，就没有(和他匹配，这时候就得raise Exception） 迭代，直到最后栈为空 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172from stack import Stackfunc_map = &#123; '+': lambda x, y: x+y, '*': lambda x, y: x*y, '/': lambda x, y: x/y, '-': lambda x, y: x-y&#125;# (3 + 4) * 5 / ((2+3) *3)def cacl(expr): stack = Stack() for c in expr: # 读到`(`和运算符，`(`和运算符就直接入栈。 if c in '(+-*/': stack.push(c) # 读到空白符（space、制表符tab、换行符\n)，直接pass elif c.strip() == '': pass # else剩下的就是读到数字和`)`的情况 else: # 读到数字，查看此时的栈顶： # 如果是运算符，就把运算符出栈，然后再出一次栈顶（把运算符之前的数字取出来）; # 如果不是运算符，就入栈此数字； # 进行运算，然后把运算出来的结果入栈。 if c != ')': c = int(c) # 这里假设c是正整数，暂时不考虑浮点数的情况 # 如果栈顶是运算符： if stack.top.value in '+-/*': s = stack.pop() if not isinstance(stack.top.value, (int, float)): raise Exception('wrong expr') v = stack.pop() v = func_map[s](v, c) stack.push(v) # 如果栈顶是数字： else: stack.push(c) # 读到`)`，把右括号之前的数字pop出来，然后再把之前的`(`也pop出来，括号抵消了，把数字存进去。 # 如果`)`前面不是数字，就报错。 # 如果pop数字后，再pop出来的不是`(`，也报错。 if c == ')': if isinstance(stack.top.value, (int, float)): v = stack.pop() if stack.top.value == '(': stack.pop() stack.push(v) else: raise Exception('wrong expr') else: raise Exception('wrong expr') #当栈顶不为空时候，还要继续运算： while stack.top: c = stack.pop() if not isinstance(c, (int, float)): raise Exception('wrong expr') if stack.top.value in '+-/*': s = stack.pop() if not isinstance(stack.top.value, (int, float)): raise Exception('wrong expr') v = stack.pop() v = func_map[s](v, c) if stack.top is None: return v stack.push(v) else: raise Exception('wrong expr')if __name__ == '__main__': print(cacl('(3 + 4) * 5 / ((2+3) *3)'))#TODO 实现带优先级的算术表达式解析]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-stack]]></title>
    <url>%2Fpython%2F20170514-02-stack%2F</url>
    <content type="text"><![CDATA[栈就好像是一个桶，往里加数据（push）和拿数据（pop），先进入桶的在桶底，后进入的在桶顶（top），出数据的时候先出桶顶的。遵循后进先出原则（LIFO, Last In First Out）。 python代码实现： 12345678910111213141516171819202122232425class Node: def __init__(self,value): self.value = value self.next = Noneclass Stack: def __init__(self): self.top = None def push(self,value): node = Node(value) node.next = self.top self.top = node def pop(self): node = self.top self.top = node.next return node.valueif __name__ == '__main__': stack = Stack() for i in range(10): stack.push(i) while stack.top: print(stack.pop()) 代码解读图示： stack.push(i)： stack.pop()：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>stack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-单向链表（Singly linked list）]]></title>
    <url>%2Fpython%2F20170514-01-singly-linked-list%2F</url>
    <content type="text"><![CDATA[链表（Linked list）的定义： 链表中的每个元素通常被叫做节点（node）。每个节点包括两个域，一个是存储数据的，叫数据域（data field）或者信息域（information field），另一个域存储指向下一个节点的地址，叫做指针域（pointer field）。 单向链表（Singly linked list）是最简单的一种链表，每个节点也包含两个域，一个数据域和一个指针域，这个指针域只指向下一个节点，所以叫单向链表。 单向链表的第一个节点称为头节点（head node），最后一个节点称为尾节点（tail node)，尾节点的指针域为空（None），不指向下一个节点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Node: def __init__(self, data): self.data = data self.next = Noneclass LinkedList: def __init__(self): self.head = None self.tail = None def append(self, data): node = Node(data) if self.head is None: self.head = node self.tail = node else: self.tail.next = node self.tail = node def iter(self): if not self.head: return cur = self.head yield cur.data while cur.next: cur = cur.next yield cur.data def insert(self, idx, value): cur = self.head cur_idx = 0 while cur_idx &lt; idx-1: cur = cur.next if cur is None: raise Exception('list length less than index') cur_idx += 1 node = Node(value) node.next = cur.next cur.next = node if node.next is None: self.tail = node def remove(self, idx): cur = self.head cur_idx = 0 while cur_idx &lt; idx-1: cur = cur.next if cur is None: raise Exception('list length less than index') cur_idx += 1 cur.next = cur.next.next if cur.next is None: self.tail = curif __name__ == '__main__': linkedlist = LinkedList() for i in range(10): linkedlist.append(i) linkedlist.insert(2, 10) linkedlist.remove(2) for data in linkedlist.iter(): print(data) main运行图解： 1) linkedlist = LinkedList() head和tail都指向空的linkedlist。 2) for i in range(10): linkedlist.append(i) 循环添加元素： linkedlist.append(0)： linkedlist.append(1) linkedlist.append(2) ….. linkedlist.append(9) 3) linkedlist.insert(2,10) insert插入 4) linkedlist.remove(2) remove移除： 5) for data in linkedlist.iter(): print(data) iter迭代输出：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>singly linked list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模块化（Modularization)]]></title>
    <url>%2Fpython%2F20170513-modularization%2F</url>
    <content type="text"><![CDATA[基本概念：在Python中，模块（modules)、包（packages）和库（librarys）的概念并没有那么清晰，通常我们认为： 一个.py文件就是一个模块（module），模块名就是文件名,例如foo.py，模块名就是foo。 一个目录，包含了__init__.py就是一个包。 当一个包或者若干包，包含一个setup.py就认为是一个可分发的库。 导入模块语法： 12import modulefrom module import submodule 例子： 123456789101112131415161718192021222324252627In [1]: import os #直接导入一级模块，明明空间要带全In [2]: os.path.basename('/a/b/c.py')Out[2]: 'c.py'In [3]: import os.path # 直接导入二级模块，命名空间要带全In [4]: os.path.basename('/a/b/c.py')Out[4]: 'c.py'In [5]: import os.path.basename #最底层模块无法直接导入---------------------------------------------------------------------------ImportError Traceback (most recent call last)&lt;ipython-input-5-fceecbf39c0c&gt; in &lt;module&gt;()----&gt; 1 import os.path.basenameImportError: No module named 'os.path.basename'; 'os.path' is not a packageIn [6]: from os import path #从一级导入二级模块In [8]: path.basename('/a/b/c.py') #只写二级模块名就可以了Out[8]: 'c.py'In [9]: from os.path import basename #从二级导入三级模块In [10]: basename('/a/b/c.py') # 只写三级模块就可以了Out[10]: 'c.py' 导入模块后重命名当模块名字太长，或者有函数和模块重名，就可以用as来重命名： 12345678In [7]: def basename(): ...: return 'ha ha ha' ...:In [8]: from os.path import basename as bIn [9]: b('/a/b/c.py')Out[9]: 'c.py' 自定义模块导入模块其实是导入了模块所在的文件本身。 在我们写自定义模块的时候，要尽量避免那种全局语句，比如下面这两个.py文件，main.py调用foo.py： foo.py: 1234print("I'm foo")def bar(): print("I'm bar in foo") main.py: 12import foofoo.bar() 执行main.py,输出： 12I'm fooI'm bar in foo 即使是from foo import bar()，输出结果还是上面那两句话,第一行print(&quot;I&#39;m foo&quot;)就是一个全局语句，这是我们不想看到的结果，我们可能只想输出I&#39;m bar in foo所以， 要尽量避免全局语句。 7.4 绝对导入和相对导入举个例子，我们有个magedu的包，我们定义了三个模块 foo.py, bar.py, main.py: 我们看到，这三个是同级的，所以bar.py可以相对引入foo.py：from foo import fn。 也可以像main.py一样，打全绝对路径，绝对引入foo.py：from magedu.bar import bar 如果是不同级呢，我们建一个sub子模块，子模块下有个x.py，再在sub下建一个子模块subsub，下面有个y.py: 我们看到x.py和y.py都是写的相对路径，而main中的3个语句都可以执行（main中有的写相对路径，有的写绝对路径）。 如果是不同的包之间引用，一般是用绝对导入。 7.5 循环导入应该尽量避免循环导入: 7.6 发布自己的库写一个setup.py就可以了 1234567from distutils.core import setupsetup(&#123; name='package_name', version='0.0.1', package=[], requires:=[]&#125;) name是起的模块名，version是版本，packages有三个，包括二级包和三级包，requires是需求什么模块。 例子： 在目录下运行下面命令生成模块： build将会在build文件夹下面创建包，install就会把这个包安装。 由于要写出所有的packages，如果有很多，一个个写太麻烦了，我们还可以用setuptools来简化packages的写法： 12345678from setuptools import setup, find_packagessetup( name='magedu', version='0.0.1' packages=find_requests(), install_requires=['requests=2.9.0']) packages直接写find_requests()会自动找多层的包，install_requires可以写具体的版本（不指定就是安装最新版） pip命令：install、uninstall就不用说了，有个freeze介绍一下。 pip freeze会把当前所有安装的库列出来: 1234pip freeze &gt;requirements.txt#在一个新的环境中，就可以导入requirements里面的所有包，保持和原来环境一样。pip install -r requirements.txt]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Modularization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常（Exception）]]></title>
    <url>%2Fpython%2F20170513-exception%2F</url>
    <content type="text"><![CDATA[异常处理的一般语法处理异常的基本语法： 123456try: blockexcept ExceptionClass as e: passfinally: pass 异常处理是以try开始。 当异常发生时按照一定规则执行except块，可以存在多个except块。 可选的finally块，无论如何都会执行，通常用于清理工作。 except块的选择： 判断try块中抛出的异常实例，是否是ExceptionClass的实例 如果是，执行异常处理语句，可选的as e ，把跑出的异常实例赋值给e（例如try TypeError as e: print(e)) 如果不是，继续往下执行 单独的except捕获任意异常 需要注意except的顺序，子类异常在前，父类异常在后。 看下面几个例子理解下： 1234567891011121314151617In [1]: 3/0---------------------------------------------------------------------------ZeroDivisionError Traceback (most recent call last)&lt;ipython-input-1-a0641230c7a8&gt; in &lt;module&gt;()----&gt; 1 3/0ZeroDivisionError: division by zeroIn [2]: try: ...: 3/0 ...: except ZeroDivisionError as e: ...: print(e) ...: finally: ...: print('finally')division by zerofinally 在try的时候遇到错误，就把错误跑出来，终止try错误语句下面所有语句的运行，来到了except，最后来到了finally。 可以匹配多个except 语句，符合哪个，就执行哪个，总是从上往下匹配的： 12345678In [3]: try: ...: 3/0 ...: except SyntaxError as e: ...: print(e) ...: except ZeroDivisionError as e: ...: print(e)division by zero 当我们try的时候遇到错误，就去试第一个except，发现不是SyntaxError异常，所以再去找第二个except，发现符合ZeroDivisionError异常，就输出division by zero。 因为是从上往下匹配的，所以，异常总是把子类异常写在前面，父类异常写在后面，要不只会运行父类，不会运行子类了，后面的代码就没有意义了，也不知道具体的异常是什么（细化的异常，而不是一个大类的异常）。（下面例子中ZeroDivisionError是Exception的子类）： 12345678In [4]: try: ...: 3/0 ...: except Exception: ...: print('Exception') ...: except ZeroDivisionError: ...: print('ZeroDivisonError')Exception 下面例子说明，finally一定会执行，即使前面是return语句： 12345678910111213In [5]: def p(): ...: print('ha ha ha') ...:In [6]: def main(): ...: try: ...: return p() ...: finally: ...: print('finally')In [7]: main()ha ha hafinally finally通常用在try打开文件、连接后，运行中出现异常后，退出关闭用，例如： 比如： 123456789In [11]: try: ....: f = open(./Untitled.ipynb') ....: 3/0 ....: except: ....: print('eeee') ....: finally: ....: f.close()eeee 抛出异常（raise exceptions）123456789101112131415161718In [12]: def fn(i): ....: if i &lt; 0: ....: raise Exception('i&lt;0') ....:In [13]: fn(-2)---------------------------------------------------------------------------Exception Traceback (most recent call last)&lt;ipython-input-13-aaa95363b839&gt; in &lt;module&gt;()----&gt; 1 fn(-2)&lt;ipython-input-12-3d515a291510&gt; in fn(i) 1 def fn(i): 2 if i &lt; 0:----&gt; 3 raise Exception('i&lt;0') 4Exception: i&lt;0 未处理异常，会向顶层抛出，直到最顶层： 1234567891011121314151617181920212223242526272829303132333435In [17]: def main(): ....: def fn(): ....: def fn2(): ....: 3/0 ....: return fn2() ....: return fn() ....:In [18]: main()---------------------------------------------------------------------------ZeroDivisionError Traceback (most recent call last)&lt;ipython-input-18-58ca95c5b364&gt; in &lt;module&gt;()----&gt; 1 main()&lt;ipython-input-17-25d5853cd075&gt; in main() 4 3/0 5 return fn2()----&gt; 6 return fn() 7&lt;ipython-input-17-25d5853cd075&gt; in fn() 3 def fn2(): 4 3/0----&gt; 5 return fn2() 6 return fn() 7&lt;ipython-input-17-25d5853cd075&gt; in fn2() 2 def fn(): 3 def fn2():----&gt; 4 3/0 5 return fn2() 6 return fn()ZeroDivisionError: division by zero 我们可以在运行时候处理：比如上面的例子，我们用try...except...来处理： 123456In [19]: try: ....: main() ....: except Exception as e: ....: print(e) ....:division by zero 异常的继承层次BaseException：最顶层的异常，基异常，下面四中都是BaseException的子类 Exception：除了下面三种特殊的，异常一般都继承自Exception。 GeneratorExit：生成器退出的异常。 KeyboardInterrupt：键盘中断，运行程序中&lt;Ctrl-C&gt;就会抛出这个异常。 SystemExit：在解释器中sys.exit()出抛出这个异常，解释器就会退出。（在python自带的IDE和IDLE解释器中exit()就可以， ipython中，exit就可以） 让我们用代码来理解后面三种异常。GeneratorExit： 123456789101112131415161718192021222324252627282930In [21]: def gen(): ....: c = 0 ....: while True: ....: yield c ....: c += 1 ....: if c &gt; 3: ....: raise GeneratorExit() ....:In [22]: for x in gen(): ....: print(x) ....:0123---------------------------------------------------------------------------GeneratorExit Traceback (most recent call last)&lt;ipython-input-22-19e03e2de58a&gt; in &lt;module&gt;()----&gt; 1 for x in gen(): 2 print(x) 3&lt;ipython-input-21-15634e9f6ed8&gt; in gen() 5 c += 1 6 if c &gt; 3:----&gt; 7 raise GeneratorExit() 8GeneratorExit: KeyboardInterrupt： 12345678910111213141516171819202122In [25]: try: ....: while True: ....: print('ha ha ha') ....: except KeyboardInterrupt: ....: print('exit...') ....: ha ha haha ha haha ha haha ha haha ha haha ha haha ha haha ha haha ha haha ha haha ha haha ha haha ha haha ha haexit... #运行时候按&lt;CTRl-C&gt; 中断后会输出exit。。。 SystemExit： 123456[yulongjun@MBP python3.6 ]$ pythonPython 3.6.1 (default, Mar 24 2016, 18:14:05)[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)] on darwinType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.exit() # 或者用'exit()', &lt;Ctrl-d&gt; 1234567891011121314[yulongjun@MBP python3.6 ]$ ipythonWARNING: IPython History requires SQLite, your history will not be savedPython 3.5.1 (default, Mar 24 2016, 18:14:05)Type "copyright", "credits" or "license" for more information.IPython 4.1.2 -- An enhanced Interactive Python.? -&gt; Introduction and overview of IPython's features.%quickref -&gt; Quick reference.help -&gt; Python's own help system.object? -&gt; Details about 'object', use 'object??' for extra details.In [1]: exit() # 或者用'exit', 'quit', &lt;Ctrl-D&gt;[yulongjun@MBP python3.6 ]$ 除了上面三种，其他的异常都是Excption和Excption的派生类（子类） 自定义异常当一个类继承自Exception及其派生类，那么就是自定义的异常。下面举两个简单的例子，都是Exception的自定义子类异常： 1234567891011In [1]: class MyException(Exception): ...: pass ...:In [4]: raise MyException('my exception')---------------------------------------------------------------------------MyException Traceback (most recent call last)&lt;ipython-input-4-d9d6f87445e1&gt; in &lt;module&gt;()----&gt; 1 raise MyException('my exception')MyException: my exception 12345678910111213141516171819202122In [5]: class InputLessZeroException(Exception): ...: pass ...:In [6]: def fn(i): ...: if i &lt; 0: ...: raise InputLessZeroException() ...:In [7]: fn(-2)---------------------------------------------------------------------------InputLessZeroException Traceback (most recent call last)&lt;ipython-input-7-aaa95363b839&gt; in &lt;module&gt;()----&gt; 1 fn(-2)&lt;ipython-input-6-862835565304&gt; in fn(i) 1 def fn(i): 2 if i &lt; 0:----&gt; 3 raise InputLessZeroException() 4InputLessZeroException:]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[魔术方法/特殊方法(Magic Methods)]]></title>
    <url>%2Fpython%2F20170512-04-magic-method%2F</url>
    <content type="text"><![CDATA[总是以双下划线开始，双下划线结束的方法，就叫做魔术方法（magic methods）。 init可以读作dunder init 对象的创建与销毁 __new__：创建对象的方法（用到很少，只有元编程的时候用到） __init__：初始化对象（用到比较多） __del__：当销毁对象的时候调用（用的也不多，但是也会用到）（比如程序里打开了一些资源，在python运行时，并没有释放，只有等程序运行完了才释放，这时候就可以用del来提前释放） 看个例子： 可视化对象在我们打印类A的实例a时候，并不能输出可视化： 我们可以用__repr__、 __str__、__bytes__这三种来加强可视化： print(a)就相当于先调用a的__repr_ 方法， 即相当于print(repr(a))。类似的 str(a)，就是调用a的__str__方法。法；bytes(a)，就是调用a的__bytes__方法。 比较运算符重载(reload)例子： 这里我们不能比较p1和p2的大小，但是如果我们想比较呢，用什么方法？ 可以采用下面的方法，就可以比较了： 我们分别比较下： bool函数12345678910111213141516171819202122232425262728In [79]: bool([]) # 空列表是FlaseOut[79]: FalseIn [80]: bool(0) # 数字0是FalseOut[80]: FalseIn [81]: bool(None) # None是FalseOut[81]: FalseIn [82]: bool(p1) # 上一节的p1实例是TrueOut[82]: TrueIn [85]: class Grok: ....: def __init__(self, val): ....: self.val = val ....: def __bool__(self): ....: return not self.val ....:In [86]: grok1 = Grok(True)In [87]: bool(grok1) #grok1是True，定义中bool是返回not True，也就是FalseOut[87]: FalseIn [88]: grok2 = Grok(False)In [89]: bool(grok2) #grok2是False，定义中bool是返回not False，也就是TrueOut[89]: True 我们随便dir([])，发现其实类默认是没有__bool__方法的，那么是如何使用bool()函数的呢？——其实使用__len__方法的值，值如果是0，就是False，值如果是非0，就是True： 1234567891011121314151617181920212223In [91]: class List: ....: def __init__(self,*args): ....: self.val = args ....: ....: def __len__(self): ....: return len(self.val) ....:In [92]: lst = List(1, 2, 3)In [93]: len(lst)Out[93]: 3In [94]: bool(lst)Out[94]: TrueIn [95]: lst2 = list()In [96]: len(lst2)Out[96]: 0In [97]: bool(lst2)Out[97]: False 如果我们定义了__bool__方法，那么就按照__bool__方法的定义来： 123456789101112131415In [98]: class List: ....: def __init__(self,*args): ....: self.val = args ....: ....: def __bool__(self): ....: return True ....: ....: def __len__(self): ....: return len(self.val) ....:In [99]: lst3 = List()In [100]: bool(lst3)Out[100]: True hash() 与可hash对象 当我们重写了__hash__后，变为pass，那么就用hash就会抛出异常 系统如果是64位的，hash值是64位的 可调用对象函数是可调用对象： 123456789In [106]: class Function: .....: def __call__(self,name): .....: print('I am callable, my name is &#123;0&#125;'.format(name)) .....:In [107]: func = Function()In [109]: func('magedu')I am callable, my name is magedu callable函数经常用在装饰器上： 单例（singleton)：单例模式可以保证系统中一个类只有一个实例而且该实例易于外界访问，从而方便对实例个数的控制并节约系统资源。如果希望在系统中某个类的对象只能存在一个，单例模式是最好的解决方案。 下面写一个单例，用到可调用对象： （后面会详细讲单例，这里不做赘述） 针对反射的方法12345678910111213class Grok: X = 1 Y = 2 Z = 3 def __init__(self, x, y, z): self.x = x self.y = y self.z = z def method(self): passgrok = Grok(1, 2, 3) __dict__反射： 12In [112]: grok.__dict__Out[112]: &#123;'x': 1, 'y': 2, 'z': 3&#125; __class__反射： 12In [113]: grok.__class__Out[113]: __main__.Grok __dir__反射： 12345678910111213141516171819202122232425262728293031323334In [113]: grok.__dir__Out[113]:['Y', '__repr__', 'Z', '__eq__', '__getattribute__', '__str__', '__reduce__', '__delattr__', '__ne__', '__le__', '__dir__', '__hash__', '__sizeof__', '__weakref__', 'z', '__class__', '__setattr__', '__format__', '__lt__', '__new__', '__dict__', '__reduce_ex__', 'y', '__module__', 'method', 'X', '__gt__', '__doc__', '__ge__', 'x', '__init__', '__subclasshook__'] __getattr__、__setattr__、__delattr__反射： with语句与 __enter__ 、__exit__一般打开文件，连接数据库，连接socket等，结束后都要close()退出操作： 1234567891011f = open('./notes.adoc')f.readline()f.close()pymysql.connect()...pymysq.close()socket.connect().....socket.close() 但是如果用with，就不用写close()，在离开with段后会自动退出： 12with open('./notes.adoc') as f: f.readline() 这是利用的什么原理呢？就是__enter__ 、__exit__： 12345678910111213141516171819In [145]: class Resouce: def __init__(self): print('init') def __enter__(self): print('enter') print(self) return self def __exit__(self, *args, **kwargs): print('exit')In [146]: with Resouce() as res: print(res)Out[146]:initenter&lt;__main__.Resouce object at 0x7f00d443f588&gt;&lt;__main__.Resouce object at 0x7f00d443f588&gt;exit 看上面代码，用with语法后，先执行了__init__，然后进入__enter__，最后进入__exit__自动退出。 with语法可以自动清理不用的资源，如果一段程序老是打开不关闭，打开的多了，就会造成资源的浪费，最终导致系统崩溃，所以要养成用with语法的习惯。 描述器（descriptor）只要能实现getter、setter、deleter方法的类就叫描述器。 用代码来说明。 point表示平面坐标上的一个点： 1234In [110]: class Point: .....: def __init__(self, x, y): .....: self.x = x .....: self.y = y 上面代码只是一个简单的定义，x可以是数字也可以是字符串，如何能保证坐标点是数字呢？可以用描述器来实现： point的getter和setter方法都可以实现，而且当setter的时候是非数值，就会报TypeError错误，并提示&quot;excepted int or float&quot; 可以看之前继承那里的property、setter、getter、deleter来理解。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OOP</tag>
        <tag>Encapsulate</tag>
        <tag>Inheritance</tag>
        <tag>Polymorphism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03-多继承、MRO算法]]></title>
    <url>%2Fpython%2F20170512-03-multi-inheritance-mro%2F</url>
    <content type="text"><![CDATA[多继承 举个例子，有一个人有亲爹有干爹，可以继承亲爹的，也可以继承干爹的一些东西，这就是多继承的概念。 用代码来理解： 123456789101112131415161718192021In [23]: class A: ....: def method_from_a(self): ....: print('I am method of a') ....:In [24]: class B: ....: def method_from_b(self): ....: print('I am method of b') ....:In [25]: class C(A,B): ....: pass ....:In [26]: c = C()In [28]: c.method_from_a()I am method of aIn [29]: c.method_from_b()I am method of b 如果多继承里面，两个父类有相同的方法，怎么办呢，我们用代码来验证下： 123456789101112131415161718192021222324252627In [30]: class A: ....: def method(self): ....: print('method of A') ....:In [31]: class B: ....: def method(self): ....: print('method of B') ....:In [33]: class C(A,B): ....: pass ....:In [34]: c = C()In [35]: c.method()method of AIn [33]: class C(B,A): ....: pass ....:In [34]: c = C()In [35]: c.method()method of B 看上面代码可以看到，如果继承的方法名字相同，就继承前面那个。 但是如果B继承自A，又是另外一种情况，C继承A和B会报错： 12345678910111213141516171819202122In [30]: class A: ....: def method(self): ....: print('method of A') ....:In [36]: class B(A): ....: def method(self): ....: print('method of B') ....:In [37]: class C(A,B): ....: pass ....:---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-37-779918bf027e&gt; in &lt;module&gt;()----&gt; 1 class C(A,B): 2 pass 3TypeError: Cannot create a consistent method resolutionorder (MRO) for bases A, B 但如果是C(B,A)呢？那就没问题了： 123456789101112In [38]: class C(B,A): ....: pass ....:In [38]: class C(B,A): ....: pass ....:In [39]: c=C()In [41]: c.method()method of B 这是什么原理呢?其实是遵循一种MRO（method resolution order）原理： MROMRO详解：你真的理解Python中MRO算法吗？ 本地优先级：根据声明顺序从左往右查找 单调性：所有子类中，也应该满足其查找顺序 MRO通过C3算法计算出来。 C3算法： 12class B: --&gt; mro(B) =[B,O] #这里的O相当于Object，最顶级的类class B(A1,A2,...) --&gt; mro(B)+merge(mro(A1), mro(A2), ... , [A1, A2, ...]) C3算法merge步骤： 顺序遍历列表 首元素满足以下条件，否则遍历下一个序列 在其他序列也是首元素 在其他序列里不存在 从所有序列中移除此元素，合并到MRO序列中 重复执行，直到所有序列为空或无法执行下去 下面看上面四中情况的MRO算法： 12345678C(A,B) --&gt; mro(C(A, B)) =[C] + merge(mro(A), mro(B), [A, B]) =[C] + merge([A, O], [B, O], [A, B]) =[C,A] + merge([O], [B,O], [B]) =[C, A, B] + merge([O], [O]) =[C, A, B, O] 12345678C(B, A) -&gt; mro(C(A, B)) =[C] + merge(mro(B), mro(A), [B, A]) =[C] + merge([B, O], [A, O], [B, A]) =[C, B] + merge([O], [A, O], [A]) =[C, B, A] + merge([O], [O]) =[C, B, A, O] 123456789C(A, B), B(A) -&gt; mro(C(A, B)) =[C] + merge(mro(A), mro(B), [A, B]) =[C] + merge([A, O], ([B] + merge(mro(A), [A]), [A, B]) =[C] + merge([A, O], ([B] + merge([A, O], [A])), [A, B]) =[C] + merge([A, O], ([B, A] + merge([O])), [A, B]) =[C] + merge([A, O], [B, A, O], [A, B]) raise TypeError 12345678C(B, A), B(A) -&gt; mro(C(B, A)) =[C] + merge(mro(B), mro(A), [B, A]) =[C] + merge([B, A, O], [A, O], [B, A]) =[C, B] + merge([A, O], [A, O], [A]) =[C, B, A] + merge([O], [O]) =[C, B, A, O] 第三段如果难以理解，可以拆分一下，如下： 1234567891011121314B(A) -&gt; //先算出mro(B(A)) mro(B(A)) =[B] + merge(mro(A), [A]) =[B] + merge([A, O], [A]) =[B, A] + merge([O]) =[B, A, O]C(A, B), B(A) -&gt; //把mro(B(A))的结果带入下面公式 mro(C(A,B)) =[C] + merge(mro(A), mro(B), [A, B]) =[C] + merge([A, O], [B, A, O], [A, B]) raise TypeError 其实可以找出一个简单规律，比如这样： 1234class Aclass B(A)class C(B)class D(C) 那么就可以用下面几种多继承都是正确的： 123456789class E(D,C,B,A)class E(D,C,B)class E(D,C,A)class E(D,B,A)class E(D,B)class E(D,A)class E(C,B,A)class E(C,A)class E(B,A) 相当于一个层级的多继承，先继承底层的类，再继承高层的类，这样就不会抛出 TypeError。 所有类都有__mro__方法 12In [54]: C.__mro__Out[54]: (__main__.C, __main__.B, __main__.A, object) Python的多继承是一剂毒药，容易让人上瘾，确实有让人爽的地方 我们看下面一个代码例子，Document是文档，Word和Excel是Document的格式，输出模式有两个，一个是屏幕Monitor，一个是HP打印机HP。 如果是单继承模式，我们对一个文档的输出模式有m(文档格式)_n(输出模式)种，要一个个写太累了。就比如上面写了2_2=4种单继承的类（WordWithMonitor,ExcelWithMonitor,WordWithHP,ExcelWithHP）。如果我们有10种格式，4种输出模式，难道我们要写40种单继承的类吗？这里就可以用到多继承了： 我们创建Monitor类和HP类，如果有另外的输出模式，就再新建一个类就可以了，然后都可以用多继承来调用。如果是Monitor类有变化，我们只要修改Monoitor类就可以了，而不会影响其他的代码。 一个新类，组合两个类，成为一个新类，在Python中就叫MixIn。两个类，一个存数据，一个存方法，单独调用方法是没用的，组合起来就有组合的效果。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>multi-inheritance</tag>
        <tag>MRO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02-OOP的三大特征：封装、继承、多态]]></title>
    <url>%2Fpython%2F20170512-02-oop-three-principles%2F</url>
    <content type="text"><![CDATA[oop的三大特性： 封装（Encapsulate） 继承（Inheritance） 多态（Polymorphism） 下面三个小节详细说明了这三个特性。 封装（Encapsulate）封装有两种划分方法： 按可见范围分类划分： 类级（class level）：（类和实例都可以访问，所有实例共享） 类变量（class variable） ：类定义时确定的，没在__init__初始化实例方法里面的 类方法 （class method） 实例级（instance level）： （只有实例可以访问） 实例变量（instance variable）：被定义在__init__初始化实例方法里面的，实例初始化的后的变量 实例方法（instance method） 按私有与公有划分： 私有（private）：只有类内部可以访问。 私有类变量（private class variable） 私有类方法（private class method） 私有实例变量（private instance variable） 私有实例方法（private instance method） 公有（public）：类外可以访问。 公有类变量（public class variable） 公有类方法（public class method） 公有实例变量（public instance variable） 公有实例方法（ public instance method） 定义规则： 私有（private）的变量和方法，通常在变量和方法前面加__。 类变量（class variable）通常在定义类后定义类变量；实例变量（instance variable）是定义在__init__方法里面的。 实例方法是直接def(self)创建；类方法通常使用装饰器@classmethod后，再def ClassMethod(cls)创建。 下面让我们用代码来理解下以上概念：（代码有点多，可以看后面注释一步步理解，看懂这一波代码你就就能懂封装了,后面继承也能用到这段代码） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465class A: # 定义类名后定义类变量 __private_class_var = 'private class var' # 定义私有类变量 public_class_var = 'public class var' # 定义公有类变量 def __init__(self): # 用__init__定义实例变量 self.__private_instance_var = 'private instance var' # 定义私有实例变量 self.public_instance_var = 'public instance var' # 定义公有实例变量 def __private_instance_method(self): # 定义私有实例方法，尝试打印出所有的变量 try: print(self.__private_class_var) # 尝试打印私有类变量 except: pass try: print(self.public_class_var) # 尝试打印公有类变量 except: pass try: print(self.__private_instance_var) # 尝试打印私有实例变量 except: pass try: print(self.public_instance_var) # 尝试打印私有实例变量 except: pass def public_instance_method(self): # 定义公有实例方法，尝试打印所有的变量 try: print(self.__private_class_var) # 尝试打印私有类变量 except: pass try: print(self.public_class_var) # 尝试打印公有类变量 except: pass try: print(self.__private_instance_var) # 尝试打印私有实例变量 except: pass try: print(self.public_instance_var) # 尝试打印私有实例变量 except: pass @classmethod def __private_class_method(cls): # 使用@classmethod创建私有类方法 try: print(cls.__private_class_var) # 尝试打印私有类变量 except: pass try: print(cls.public_class_var) # 尝试打印公有类变量 except: pass @classmethod def public_class_method(cls): # 使用@classmethod创建公有类方法 try: print(cls.__private_class_var) # 尝试打印私有类变量 except: pass try: print(cls.public_class_var) # 尝试打印公有类变量 except: pass 上面代码： 行号 定义类型 英文 1 定义类名 class name 2-3 定义类变量 class variable(private &amp; public) 5-7 定义实例变量 instance variable(private &amp; public) 9-25 定义私有实例方法 private instance method 27-43 定义公有实例方法 public instance method 45-54 定义私有类方法 private class method 56-65 定义公有类方法 public class method 下面我们验证下类和实例的变量和方法的互访情况：(a=A()，a是类A的实例) 由上图可以得出结论： 类私有变量、类私有方法、实例私有变量和实例私有方法，类和实例都不能直接访问。 类可以访问类级的公有类变量和公有类方法，但是不能访问实例级的的公有实例变量和公有实例方法。 实例可以访问类级的公有类变量和公有类方法，也能访问实例级的的公有实例变量和公有实例方法。 类可以用公有类方法调用私有类变量和公有类变量，但是不能调用私有实例变量和公有实例变量。 实例可以用公有实例方法调用私有类变量和公有类变量，也能调用私有实例变量和公有实例变量。 —- 类能否访问 实例能否访问 类是否能够通过公有类方法调用私有变量 实例是否能够通过公有实例方法调用私有变量 私有类变量 × × √ √ 私有类方法 × × - - 私有实例变量 × × × √ 私有实例方法 × × - - 公有类变量 √ √ √ √ 公有类方法 √ √ - - 公有实例变量 × √ × √ 公有实例方法 × √ - - property函数、 getter、setter、deleter方法让我们看一段代码： 1234567891011121314151617class A: def __init__(self, input_x): self.__x = input_x def getx(self): print('inside the getter') return self.__x def setx(self, input_x): print('inside the setter') self.__x = input_x def delx(self): print('inside the deleter') del self.__x x = property(getx, setx, delx, "I'm the 'x' property") 上面代码中： __x是一个隐藏的变量，我们通常不会直接去调用隐藏的变量，而是用getter方法getx，相应的，我们改变隐藏的变量，也用相应的setter方法setx，还有删除隐藏变量的deleter方法delx。 property函数调用了上述三种方法，property函数的用法是：property(fget=None, fset=None, fdel=None, doc=None)，这里的fget是getx，fset是setx，fdel是delx，doc是&quot;I&#39;m the &#39;x&#39; property.&quot;。下面是使用的代码： 当然也可以有更简便的方法： property函数的代码还可以装饰器来写，更加简洁,和上面实现的功能是一样的： 12345678910111213141516171819class A: def __init__(self, input_x): self.__x = input_x @property def x(self): "I am the 'x' property." print("inside the getter") return self.__x @x.setter def x(self, value): print("inside the setter") self.__x = value @x.deleter def x(self): print("inside the deleter") del self.__x 继承（Inheritance）继承的多种方法 继承（inheritance）是一种实现代码复用的一个好方法。 当我们定义的新类用到之前定义过的旧类的一些属性和方法的时候，我们可以重写一个新类（比较笨拙的方法），也可以用继承的方法，来获得旧类的变量和方法。 新类可以保留部分旧类变量和方法（使用super()函数），可以重写（也说覆盖，override）旧类的部分变量和方法，可以新增（add）变量和方法。 在继承中，原始的旧类叫做父类（parent class）、超类（super class）、基类（base class），继承得到的新类叫做孩子类（child class），子类（sub class），衍生类（derived class）。 用代码来说明 前面有定义过一个Door的类，下面是类Door的定义： 12345678910class Door: def __init__(self,number,status): self.number=number self.status=status def open(self): self.status='openning' def close(self): self.status='closed' 我们重新定义一个新类叫LockableDoor（可锁的门） 1234567891011121314151617181920class LockableDoor: def __init__(self,number,status,is_lockked): self.number = number self.status = status self.is_locked = is_locked def open(self): if not self.is_locked: self.status = 'openning' else: print('is locked') def close(self): self.status = 'closed' def lock(self): if self.status == 'closed': self.is_locked = True else: print('is openning') 定义新类是笨方法，尤其是当旧类的属性和方法很多的时候，不能全部去重写一遍吧！ 我们看到这两个类有很多相似的部分，所以可以用Door当做父类，用LockableDoor当做Door的子类，继承Door： 123456789101112131415161718class LockableDoor(Door): # 定义Door的子类LockableDoor def __init__(self,number,status,is_locked):# 新增了一个is_locked属性（attritube） super(LockableDoor,self).__init__(number,status) # number，status可以直接继承Door的这两个属性，是相同的，用super（）函数来继承 self.is_locked = is_locked # 新加的is_locked属性的定义，新的属性 def open(self): # 覆盖（override）open方法 if not self.is_locked: # 如果门没锁，LockableDoor的门可以像Door一样调用open方法 super(LockableDoor,self).open() #此处为像超类Door一样调用open方法 else: # 如果们锁了，门就打不开了 print("It is locked, can't open.")# 只能输出“它锁住了，不能打开” #LockableDoor和Door的close方法都一样，没必要重写close方法，就可以省略了，直接继承 def lock(self):# 新增一个lock方法，这个超类中完全没有，下面是lock方法的定义 if self.status == 'closed':# 如果门关了 self.is_locked = True # is_locked属性就是真值 else: # 如果门开着 print('It is openning')# 就输出“它开着" 上面代码用到了继承的很多方法： 单继承：close方法都一样，是直接继承的，所以不用写，省略就可以，直接继承过来。 重写（override）：LockableDoor的open方法是属于重写了原来的Door的open方法。 新增（add）：is_lock属于新增的变量，lock方法属于新增的方法。 调用父类（super()）：super(LockableDoor,self).__init__(number,status)调用了父类的number和status变量，super(LockableDoor,self).open()调用了父类的open方法。 继承与可见性继承可以继承父类的那些东西呢?这里与之前的封装有关系，我们可以封装那一节里定义的那个特别长的函数A来说明:之前的那个类是A，太长就不复制黏贴了。我们定义一个新类B，继承自父类A，b是类B的实例： 12345678910111213141516171819202122232425262728293031323334353637383940In [69]: def B(ClassName): passIn [70]: b=B()In [71]: dir(b)out[71]:['_A__private_class_method', '_A__private_class_var', '_A__private_instance_method', '_A__private_instance_var', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'public_class_method', 'public_class_var', 'public_instance_method', 'public_instance_var'] dir(b)可以查看b的属性和方法，中间那些我们都用不上，我们可以切片精简下： In [72]: dir(b)[:4]+dir(b)[:-5:-1] Out[72]: ['_A__private_class_method', '_A__private_class_var', '_A__private_instance_method', '_A__private_instance_var', 'public_instance_var', 'public_instance_method', 'public_class_var', 'public_class_method'] 我们运行下面代码： 以上代码说明：1. 私有的方法、变量（无论类还是实例的）是不可继承的2. 公有的方法、变量（无论类还是实例）是可以继承的3. 子类的公有的方法（包括类和实例）是可以访问父类的私有变量的 继承后重写私有变量和公有变量，私有变量不变，公有变量变为重写后的变量 看一段代码： 类C我们继承A并重写（override）私有变量和公有变量我们发现私有变量没有变化，而公有变量变了，说明重写（override）只能重写公有变量，不能重写私有变量，私有变量还是父类A的私有变量。（dir(c)后我们看到，私有变量有_A前缀，也有_C前缀，默认会先使用父类的_A前缀的私有变量。） 继承后重写公有方法和私有方法，公有方法改变，私有方法也改变 看一段代码理解： 重写公有方法后，公有方法改变（变为调用两次__method())；重写私有方法后，公有方法调用的私有方法也改变（改变为输出method of B) 多态（Polymorphism）不同的操作对不同的对象有多种表现形式。重写（override）就是多态的一种表现形式。 在讲继承的时候，已经把多态讲过了，这里就略过了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>OOP</tag>
        <tag>Encapsulate</tag>
        <tag>Inheritance</tag>
        <tag>Polymorphism</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-类（Class）的定义]]></title>
    <url>%2Fpython%2F20170512-01-define-class%2F</url>
    <content type="text"><![CDATA[Python中对象由两大部分组成： 数据（data）（也可以称之为变量（variable）或者属性（attribute））（下文类的理解中中有时候用变量，有时候用属性，其实说的是一个东西） 行为（behavior）（也可以称之为函数（functions）或者方法（methods））（下文类的理解中大部分用方法来表示） 类（class）和实例（instance）类和实例都是对象，但是类是一类对象的对象，实例是调用类后产生的对象。 实例是调用类后的产物，每次调用类，都会产生一个新实例。类相当于工厂，实例就是工厂生产的一个个产品，看个例子： 12345class A: #定义类A passa1=A() #实例化类A，a1就是类A的一个实例a2=A() #实例化类A，a2是类A的另一个实例 看上面代码，实现了定义一个类A（注意类名一般大写），调用类后产生两个实例a1和a2（调用类：在类后面加一对小括号）。 定义一个类我们定义了一个类Door，这个门有号码属性，有状态属性（打开状态或者关闭状态），还有两种方法（打开方法和关闭方法）number和status是属性（或者称之为变量），open()和close()是门的两种方法: 12345678class Door: def __init__(self, number, status): self.number = number self.status = status def open(self): self.status = 'openning' def close(self): self.status = 'closed' 解读代码： class Door：定义一个类Door，这是一个简写的定义类的语句，其实要写全应该是class Door(object):，object是最Python中最顶级的类，由object构造出一切类，Door就是object的子类。当然，定义出来的类还可以构造出子类，比如再定义一个类:class LockableDoor(Door):， 这里就定义了Door的一个子类LockableDoor，LockableDoor继承了Door的属性和行为。 构造函数：__init__，第一个参数是self（是约定俗成的，当类实例化后，self代表实例本身），后面的是参数number和status，这两个参数会赋值给实例变量（instance variable）self.number和self.status。 定义类的两个方法，open()和close()。在类实例化后，实例就有两个方法：打开和关闭。调用方法后，会把实例的status变量变为openning或者closed 下面是实例化后的效果： 123456789101112131415161718192021222324252627In [2]: door1 = Door(1, 'closed') # 实例化Door为door1实例In [3]: door2 = Door(1, 'openning') #实例化Door为door2实例In [4]: door1.status # 查看door1的状态Out[4]: 'closed'In [5]: door2.status # 查看door2的状态Out[5]: 'openning'In [6]: door1.open() # 运行door1实例的open()方法In [7]: door1.status # 查看door1的状态，发现变成'openning'Out[7]: 'openning'In [8]: door1.close() # 运行door1实例的close()方法In [9]: door1.status # 查看door1的状态，变为了'closed'Out[9]: 'closed'In [10]: door2.number = 222 # 变更door2的号码为222In [11]: door2.number # door2实例的号码就变为了222Out[11]: 222In [12]: door1.number # door1实例的号码不变，还是1Out[12]: 1]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[装饰器(Decorators)]]></title>
    <url>%2Fpython%2F20170511-08-decoractors%2F</url>
    <content type="text"><![CDATA[装饰器是给已有的函数加上一些小装饰（即一些小功能，这些小功能可能好多函数都会用到），但是不会让这个小装饰（小功能）侵入到原有的模块代码中。 ——引用于CoolShell.cn的文章：《Python修饰器的函数式编程》(下面部分内容摘抄自上述文章) 1 不带参数的装饰器先看一个没有用到装饰器的代码： 12345678910In [1]: import time ...: ...: def sleep(n): ...: time.sleep(n) ...: ...: start = time.time() ...: sleep(3) ...: print(time.time()-start) ...: 3.0056068897247314 上述代码的作用就是算出sleep(3)这个的执行时间。 我们可以改写代码，使得计算函数执行时间的代码作为一个函数，接收一个函数，然后计算这个函数的执行时间。 123456789101112import timedef timeit(fn, *args, **kwargs): start = time.time() ret = fn(*args,**kwargs) print(time.time()-start) return retdef sleep(n): time.sleep(n)timeit(sleep,3) 上面的代码，必须用timeit来传入sleep函数和sleep的参数来运行，我们可不可以提前装饰下要被装饰的函数，然后调用呢？可以的，如下： 1234567891011121314151617In [2]: import time ...: ...: def timeit(fn): ...: def wrapper(*args, **kwargs): ...: start = time.time() ...: ret = fn(*args, **kwargs) ...: print(time.time()-start) ...: return ret ...: return wrapper ...: ...: def sleep(n): ...: time.sleep(n) ...: ...: sleep = timeit(sleep) #提前把sleep函数装饰了。 ...: sleep(3) # 再调用函数的时候，就是调用被装饰过的sleep函数了。 ...: 3.000748872756958 Python提供了一种装饰器的语法糖，可以把这三行： 1234def sleep(n): time.sleep(n)sleep = timeit(sleep) 改为下面这三行: 123@timeitdef sleep(n): time.sleep(n) 完整版语法糖版的装饰器示例如下： 1234567891011121314151617In [3]: import time ...: ...: def timeit(fn): ...: def wrapper(*args, **kwargs): ...: start = time.time() ...: ret = fn(*args,**kwargs) ...: print(time.time()-start) ...: return ret ...: return wrapper ...: ...: @timeit ...: def sleep(n): ...: time.sleep(n) ...: ...: sleep(3) ...: 3.004680871963501 装饰器的本质是高阶函数，接受一个函数作为参数，并且返回一个函数。 装饰器通常会返回一个封装函数（上述代码中的wrapper），这个封装函数在传入的函数前后做一些事情。 详解上述代码： 我们在timeit装饰器里定义了一个封装函数wrapper，这个wrapper函数在我们真正传入的函数fn的前后做了一些操作，计算出了fn运行的时间差，并且把函数返回回去，然后把这个封装函数wrapper返回回去，替换原来的fn。 @timeit 然后定义的函数sleep，意味着sleep = timeit(sleep)，即sleep即为传入的函数，然后经过wrapper的封装，返回回来，替换原来的sleep。 2 装饰器里的@wraps用法我们看这样一个例子： 1234567891011121314151617181920212223In [3]: def timeit(fn): ...: def wrapper(*args, **kwargs): ...: start = time.time() ...: ret = fn(*args, **kwargs) ...: print(time.time() - start) ...: return ret ...: return wrapper ...: ...: @timeit ...: def add(x, y): ...: '''x + y''' ...: return x + y ...: In [4]: help(add)Help on function wrapper in module __main__:wrapper(*args, **kwargs)In [5]: add.__name__Out[5]: 'wrapper' 我们本意是求add的用法，和add的名字，结果给的不是我们想要的，是因为我们调用add的帮助和名字，实际上调用的是wrapper，所以我们返回的都是wrapper的help和name。 这里就可以在装饰器timeit里面用@wraps(fn)方法，这个方法在标准库functools里面。这样我们就可以得到想要的结果了，查看帮助是add的帮助,name是add的name： 1234567891011121314151617181920212223242526In [6]: from functools import wraps ...: ...: def timeit(fn): ...: @wraps(fn) # 下面的函数wrapper就集成了传入的函数的属性。 ...: def wrapper(*args, **kwargs): ...: start = time.time() ...: ret = fn(*args, **kwargs) ...: print(time.time() - start) ...: return ret ...: return wrapper ...: ...: @timeit ...: def add(x, y): ...: '''x + y''' ...: return x + y ...: In [7]: help(add)Help on function add in module __main__:add(x, y) x + y In [8]: add.__name__Out[8]: 'add' 3 带参数的装饰器一个程序占用的时间分为用户时间（包括sleep等待的时间），和cpu时间（cpu时间不包括sleep等待的时间）。下面是cpu时间的例子： 12345678910n [12]: import time ...: ...: def sleep(n): ...: time.sleep(3) ...: ...: start = time.clock() ...: sleep(3) ...: time.clock() - start ...: Out[12]: 0.0009540000000001214 我们想写一个装饰器，既可以返回用户时间，又可以返回cpu时间，默认返回的是用户时间（可以用一个开关作为参数，来控制返回的是什么时间），这时候我们就用到了带参数的装饰器： 12345678910111213141516171819202122232425262728293031In [13]: import time ...: ...: def timeit(cpu_time = False): ...: # 根据传入的参数，来决定是用time.clock还是time.time ...: time_func = time.clock if cpu_time else time.time ...: def dec(fn): ...: @wraps(fn) ...: def wrapper(*args, **kwargs): ...: start = time_func() ...: ret = fn(*args, **kwargs) ...: print(time_func() - start) ...: return ret ...: return wrapper ...: return dec ...: ...: # 下面注释，等同于@timeit()那段 ...: # def fun(n): ...: # time.sleep(n) ...: # return n ...: # fun = timeit()(fun) ...: # 其实上面这行，用柯里化就非常好理解了。 ...: ...: @timeit() # 参数为空，但也是带了参数了，空参数，传入的参数为默认参数，等同于@timeit(cpu_time = False） ...: def fun(n): ...: time.sleep(n) ...: return n ...: In [14]: fun(3)3.0001399517059326Out[14]: 3 上面是返回用户时间的装饰器。 如果后面几行像下面这样写，就是输出cpu时间了： 123456789In [15]: @timeit(cpu_time = True) # 传入cpu_time的参数为True，覆盖掉False默认参数 ...: def fun(n): ...: time.sleep(n) ...: return n ...: In [16]: fun(3)0.0007150000000000212Out[16]: 3 带参数的装饰器本质是一个函数，传入参数，返回一个新的装饰器。 带参数的装饰器只允许一层，如果像这样@xxx()()多层了，就会报错。 多个装饰器装饰的时候，采用就近原则，然后一层层装饰，比如： 123456@a@b@cdef fn(): pass# 上述等效于a(b(c(fn))) 4 装饰器的应用4.1 给函数调用做缓存1234567891011121314151617181920from functools import wrapsdef memo(fn): cache = &#123;&#125; miss = object() @wraps(fn) def wrapper(*args): result = cache.get(args, miss) if result is miss: result = fn(*args) cache[args] = result return result return wrapper @memodef fib(n): if n &lt; 2: return n return fib(n - 1) + fib(n - 2) 上面这个例子中，是一个斐波拉契数例的递归算法。我们知道，这个递归是相当没有效率的，因为会重复调用。比如：我们要计算fib(5)，于是其分解成fib(4) + fib(3)，而fib(4)分解成fib(3)+fib(2)，fib(3)又分解成fib(2)+fib(1)…… 你可看到，基本上来说，fib(3), fib(2), fib(1)在整个递归过程中被调用了两次。 而我们用decorator，在调用函数前查询一下缓存，如果没有才调用了，有了就从缓存中返回值。一下子，这个递归从二叉树式的递归成了线性的递归。 4.2 给函数打日志下面这个示例演示了一个logger的decorator，这个decorator输出了函数名，参数，返回值，和运行时间。 12345678910111213141516171819202122232425262728from functools import wrapsdef logger(fn): @wraps(fn) def wrapper(*args, **kwargs): ts = time.time() result = fn(*args, **kwargs) te = time.time() print("function = &#123;0&#125;".format(fn.__name__)) print(" arguments = &#123;0&#125; &#123;1&#125;".format(args, kwargs)) print(" return = &#123;0&#125;".format(result)) print(" time = %.6f sec" % (te-ts)) return result return wrapper @loggerdef multipy(x, y): return x * y @loggerdef sum_num(n): s = 0 for i in xrange(n+1): s += i return s print(multipy(2, 10))print(sum_num(100))print(sum_num(10000000)) 上面那个打日志还是有点粗糙，让我们看一个更好一点的（带log level参数的）： 1234567891011121314151617181920import inspectdef get_line_number(): return inspect.currentframe().f_back.f_back.f_lineno def logger(loglevel): def log_decorator(fn): @wraps(fn) def wrapper(*args, **kwargs): ts = time.time() result = fn(*args, **kwargs) te = time.time() print "function = " + fn.__name__, print " arguments = &#123;0&#125; &#123;1&#125;".format(args, kwargs) print " return = &#123;0&#125;".format(result) print " time = %.6f sec" % (te-ts) if (loglevel == 'debug'): print " called_from_line : " + str(get_line_number()) return result return wrapper return log_decorator 但是，上面这个带log level参数的有两具不好的地方，1） loglevel不是debug的时候，还是要计算函数调用的时间。2） 不同level的要写在一起，不易读。 我们再接着改进： 12345678910111213141516171819202122232425262728293031323334import inspect def advance_logger(loglevel): def get_line_number(): return inspect.currentframe().f_back.f_back.f_lineno def _basic_log(fn, result, *args, **kwargs): print "function = " + fn.__name__, print " arguments = &#123;0&#125; &#123;1&#125;".format(args, kwargs) print " return = &#123;0&#125;".format(result) def info_log_decorator(fn): @wraps(fn) def wrapper(*args, **kwargs): result = fn(*args, **kwargs) _basic_log(fn, result, args, kwargs) return wrapper def debug_log_decorator(fn): @wraps(fn) def wrapper(*args, **kwargs): ts = time.time() result = fn(*args, **kwargs) te = time.time() _basic_log(fn, result, args, kwargs) print " time = %.6f sec" % (te-ts) print " called_from_line : " + str(get_line_number()) return wrapper if loglevel is "debug": return debug_log_decorator else: return info_log_decorator 你可以看到两点，1）我们分了两个log level，一个是info的，一个是debug的，然后我们在外尾根据不同的参数返回不同的decorator。2）我们把info和debug中的相同的代码抽到了一个叫_basic_log的函数里，DRY原则。 以上例子来自于：http://coolshell.cn/articles/11265.html其他装饰器例子参考：https://wiki.python.org/moin/PythonDecoratorLibrary 英文不错的也可以参考英文文章，比如这篇https://qxf2.com/blog/python-decorators/，文章开头图片来自于此文章。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>decorators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高阶函数、闭包、偏函数、柯里化、匿名函数]]></title>
    <url>%2Fpython%2F20170511-07-higher-order-function%2F</url>
    <content type="text"><![CDATA[1 高阶函数 一个函数可以接收另一个函数作为参数，这种函数就称之为高阶函数。 如下面的add函数就是一个高阶函数，他接收一个函数作为参数：12345In [1]: def add(x, y, fn): ...: return fn(x) + fn(y) ...: In [5]: add(10, -20, abs) Out[5]: 30 在上述例子中，我们传入的x 为10， y 为 -20 ，传入的函数为abs函数，返回abs(10) + abs(20)。 上述高阶函数例子，我们也可以传入其他的函数，比如我们定义一个square（平方）函数，然后把这个平方函数传入到add高阶函数 123456In [8]: def square(x): ...: return x*x ...: In [9]: add(10, -20, square)Out[9]: 500 其实add(10, -20, square)就相当于 square(10) + square(20)，即10*10 + 20*20。 其他好多语言是无法直接传递函数作为参数到另外一个函数，要用到函数指针，而Python可以直接传递一个函数进来。 我们可以完善下上述程序，当没有指定传入参数fn的时候，就调用默认的default_add函数，即默认把传入的x和y相加后返回；如果指定了fn函数，就返回fn(x) + fn(y)的值。如下： 12345678910111213141516In [17]: def add(x, y, fn=None): ...: def default_add(a, b): ...: return a + b ...: if fn is None: ...: add = default_add ...: return add(x, y) ...: else: ...: return fn(x) + fn(y) ...: In [18]: add(10, -20) # 不指定默认函数，默认返回x + yOut[18]: -10In [19]: add(10, -20, abs) # 指定了默认函数为abs，就返回abs(x) + abs(y)Out[19]: 30 2 闭包 一个函数的返回值里有函数，就是闭包闭包也是高阶函数。 下面的这样的一个函数就是一个闭包。此函数的作用是做一个计数器，可以增加减少和重置的计数器：123456789101112131415161718192021222324252627282930313233In [1]: def make_counter(init): #给计数器一个初始值 ...: counter = [init] ...: def inc(): ...: counter[0] += 1 ...: def dec(): ...: counter[0] -= 1 ...: def get(): ...: return counter[0] ...: def reset(): ...: counter[0] = init ...: return inc, dec, get, reset ...: In [2]: inc, dec, get, reset = make_counter(0)In [3]: inc() # 增加In [4]: inc()In [5]: inc()In [7]: get() # 获取Out[7]: 3In [8]: dec() # 减少In [9]: get()Out[9]: 2In [10]: reset() # 重置In [11]: get()Out[11]: 0 上述还可以用nolocal的形式来写(nolocal方式可选学习)： 123456789101112131415In [12]: def make_counter(init=0): ...: counter = init ...: def inc(): ...: nonlocal counter ...: counter += 1 ...: def dec(): ...: nonlocal counter ...: counter -= 1 ...: def get(): ...: nonlocal counter ...: return counter ...: def reset(): ...: nonlocal counter ...: counter = init ...: return inc, dec, get, reset 总结一下： 参数是函数。 返回值是函数。满足以上两点任意一点的函数,就称之为高阶函数 3 偏函数偏函数英文叫partital，从functools模块导入。用法： partial(func, *args, **keywords) - new function with partial application of the given arguments and keywords. 意思为给func函数传入参数，并固定下来，即可以固定一个或多个参数。 123456789101112In [1]: from functools import partialIn [2]: help(partial)In [3]: partial(int,base=16)Out[3]: functools.partial(&lt;class 'int'&gt;, base=16)In [4]: hex_to_int = partial(int,base=16)In [6]: hex_to_int('0xaaaa')Out[6]: 43690 12345678910In [9]: lst = [ 3, 1, 2 , 5, 4]In [10]: sorted(lst)Out[10]: [1, 2, 3, 4, 5]In [15]: sorted_desc = partial(sorted, reverse = True) # sorted_desc即为降序排序，固话参数reverse为TrueIn [16]: sorted_desc(lst)Out[16]: [5, 4, 3, 2, 1] 来一个实际应用：比如我们经常创建数据库连接，而数据的某些信息是固定的，就可以用partial函数。 123from functools import partitalmy_connect = partital(connect, port=3000) #端口是固定的3000，就可以用偏函数固化。 4 柯里化柯里化是闭包的一种。 f(x, y , z) =&gt; g(x)(y)(z) 123456789101112In [17]: def add(x,y): #正常的函数 ...: return x + y ...: In [18]: def add(x): # 柯里化的函数 ...: def inner_add(y): ...: return x + y ...: return inner_add ...: In [19]: add(3)(5)Out[19]: 8 5 匿名函数 lambda parameters : expression 只能写在一行上。 前面是参数，后面是表达式。表达式作为返回值。1234567In [16]: lambda x, y : x + yOut[16]: &lt;function __main__.&lt;lambda&gt;&gt;In [17]: add = lambda x, y : x + yIn [18]: add(3, 5)Out[18]: 8 lambda函数也可以给默认值、可变参数、可变关键字参数，keyword-only参数 12345678910111213141516171819In [19]: add = lambda x, y = 1: x + yIn [20]: add(5)Out[20]: 6In [21]: add = lambda *x : sum(x) # 支持可变参数In [22]: add(1,2,3,4)Out[22]: 10In [39]: f = lambda **kw: kw # 支持可变关键字参数In [40]: f(a=0, b=1)Out[40]: &#123;'a': 0, 'b': 1&#125;In [41]: f = lambda x, *, y: x + y # 支持keyword-onlyIn [42]: f(1,y=3)Out[42]: 4 匿名函数通常和高阶函数配合使用，作为参数传入，或者作为返回值返回 一些短小的函数，我们就可以写匿名函数，而不用写好几行代码，一行匿名函数就够了。 1234In [2]: fib = lambda n: 1 if n==0 or n==1 else fib(n-1)+fib(n-2)In [3]: fib(10)Out[3]: 89 但是匿名函数最好不要定义递归的，匿名函数通常用来定义一些简单的函数。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>higher order function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生成器函数]]></title>
    <url>%2Fpython%2F20170511-06-generator-function%2F</url>
    <content type="text"><![CDATA[1 定义之前有说过生成器解析式。这里我们来写一下生成器的函数，生成器函数就是在函数体里写yield 12345678910111213141516171819In [85]: def gen(): ...: yield 0 ...: In [86]: g = gen()In [87]: type(g)Out[87]: generatorIn [89]: next(g)Out[89]: 0In [90]: next(g)------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-90-5f315c5de15b&gt; in &lt;module&gt;()----&gt; 1 next(g)StopIteration: 我们写一个复杂点的：12345678910111213141516171819In [91]: def gen(): ...: while True: ...: yield 0 ...: print('.....') ...: In [98]: g = gen()In [99]: next(g)Out[99]: 0In [100]: next(g).....Out[100]: 0In [101]: next(g).....Out[101]: 0 我们可以看出，一执行next，就到yield那条语句执行完返回后面的值，然后暂停，等待下一次next，下一次再next就先执行剩下的部分，然后执行到下一次循环，到yield那条语句，返回yield后面的值，然后又暂停。 生成器的定义和函数类似，但是有yield语句。 生成器执行到yield的时候会暂停，再次的时候，next会从暂停的地方继续执行到下一次yield。 我们再写一个： 1234567891011121314151617181920In [107]: def gen(x): ...: for i in range(x): ...: yield i ...: In [108]: g = gen(10)In [109]: for x in g: ...: print(x) ...: 0123456789 2 yield和returnyield 弹出值，暂停函数return返回值，并结束函数 yied可以和return并用： 12345678910111213141516171819In [111]: def gen(x): ...: for i in range(10): ...: yield i ...: return 'ok'In [112]: g = gen(10)In [113]: for x in g: ...: print(x) ...: 012456789 从上我们可以看出，当yield和return同时存在的时候，return的返回值会被忽略，但是return依然可以中断生成器。 3 协程协程是用户空间的轻量线程，泡在一个线程内，由用户空间调度。 生成器就可以实现协程。 1234567891011121314151617181920212223In [118]: def gen1(): ...: while True: ...: yield 'gen 1' ...: In [121]: def gen2(g): ...: while True: ...: yield 'gen 2' ...: print(next(g)) ...: In [122]: g = gen2(gen1())In [123]: next(g)Out[123]: 'gen 2'In [124]: next(g)gen 1Out[124]: 'gen 2'In [125]: next(g)gen 1Out[125]: 'gen 2' 4 生成器的应用 下面是一个计数器（counter），如果在单线程环境下，没有意义，可视用在多线程下，就有意义了。 1234567891011121314151617In [134]: def counter(init): ...: c = init ...: while True: ...: yield c ...: c +=1 ...: In [135]: c = counter(0)In [136]: next(c)Out[136]: 0In [137]: next(c)Out[137]: 1In [138]: next(c)Out[138]: 2 惰性求值(由于函数是在yield处暂停的，所以，可以用到时，才去计算) 比如下面的阶乘函数：12345678910111213141516171819202122232425262728In [147]: def factorial(): ...: ret = 1 ...: idx =0 ...: while True: ...: yield ret ...: idx +=1 ...: ret *= idx ...: In [148]: g = factorial()In [149]: next(g)Out[149]: 1In [150]: next(g)Out[150]: 1In [151]: next(g)Out[151]: 2In [152]: next(g)Out[152]: 6In [153]: next(g)Out[153]: 24In [154]: next(g)Out[154]: 120 我们可以使用生成器来替换递归。 所有的递归，都可以用生成器来替换。(这样就没有深度限制了) 12345678910111213141516In [159]: def g(n): ...: def fact(): ...: ret = 1 ...: idx = 0 ...: while True: ...: yield ret ...: idx += 1 ...: ret += idx ...: gen = factorial() ...: for _ in range(n): ...: next(gen) ...: return next(gen) ...: In [160]: g(10)Out[160]: 3628800 5 yield from用法1234In [161]: def gen(lst): ...: for x in lst: ...: yield x ...: 上面的我们迭代一个列表，用循环来yield，其实可以用yield from 可迭代对象的简写方法： 123In [163]: def gen(lst): ...: yield from lst ...: yield from我们也可用在元组、字符串、文件对象等可迭代对象。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>generator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[递归函数]]></title>
    <url>%2Fpython%2F20170511-05-recursive-function%2F</url>
    <content type="text"><![CDATA[我们来看一个斐波那契数列的定义： fn(0) = 1fn(1) = 1fn(n) = f(n-1) + f(n-2) 例如：1 1 2 3 5 8 13 21 34 我们用函数来定义就是： 123456789101112131415161718In [35]: def fib(n): ...: if n == 0: ...: return 1 ...: if n == 1: ...: return 1 ...: return fib(n-1) + fib(n-2)In [40]: fib(1)Out[40]: 1In [41]: fib(5)Out[41]: 8In [42]: fib(10)Out[42]: 89In [44]: fib(19)/fib(20)Out[44]: 0.618033985017358 # 0.618黄金分割比例 我们再看一个简单的： 阶乘（factorial）:fact(0) = 1fact(1) = 1fact(n) = n*fact(n-1) 用函数来写就是： 1234567891011121314151617In [51]: def fact(n): ...: if n == 0: ...: return 1 ...: if n == 1: ...: return 1 ...: return n * fact(n-1) ...: ...: In [52]: fact(1)Out[52]: 1In [53]: fact(5)Out[53]: 120In [54]: fact(10)Out[54]: 3628800 fact(5)的实际执行过程就是这样： ===&gt; fact(5)===&gt; 5 fact(4)===&gt; 5 (4 fact(3))===&gt; 5 (4 (3 fact(2)))===&gt; 5 (4 (3 (2 fact(1))))===&gt; 5 (4 (3 (2 1)))===&gt; 5 (4 (3 2))===&gt; 5 (4 6)===&gt; 5 24===&gt; 120 在Python中，递归深度是有限制的， 默认深度是1000， 我们可以更改默认深度，但是如果太大, 而且可能让服务器的cpu冲高。12345678In [55]: import sysIn [56]: sys.getrecursionlimit() # 默认递归深度为1000Out[56]: 1000In [66]: sys.setrecursionlimit(10000) # 更改递归深度为10000In [71]: fib(1000) # 这个要跑很久，cpu 100%,普通的电脑跑一两分钟跑不出来，服务器去跑很快，貌似因为服务器的指令集不同，感觉运算很快。 递归在Python中非常慢，而且有深度限制，所以尽量避免使用递归。 比如上面的阶乘函数，我们就可以用普通函数来写。12345In [73]: def fact(n): ...: ret = 1 ...: for x in range(1,n+1): ...: ret *= x ...: return ret 在Python当中我们可以完全不使用递归。 但是在写某些函数的时候，如果递归写起来非常简单，那么也可以写递归，但是递归层数最好不要太多。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>recursive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[函数注解（类型示意）]]></title>
    <url>%2Fpython%2F20170511-04-function-annotation%2F</url>
    <content type="text"><![CDATA[我们说过，Python是一种强类型的动态语言。但是也有类型示意，但是不是强制性质的，没有强制指定类型。 123456789101112In [14]: def add(x:int, y:int) -&gt; int: ...: return x+y ...: In [15]: add(1,2)Out[15]: 3In [16]: add(1.0, 2.0)Out[16]: 3.0In [17]: add('x', 'y')Out[17]: 'xy' 从上面代码而已看出，类型示意没有任何类型检查，仅仅只是一个示意而已。我们可以用help来查看类型示意： 123In [19]: help(add)Help on function add in module __main__:add(x:int, y:int) -&gt; int Python是一种自文档的语言，很多时候，我们需要自己来写用法示意，以前都是用三引号引起来，来说明函数的用法： 123456789101112131415161718In [21]: def add(x, y): ...: ''' ...: @param x int ...: @param y int ...: @return int ...: ''' ...: return x+y Help on function add in module __main__:In [24]: help(add)Help on function add in module __main__:add(x, y) @param x int @param y int @return int 有了上面的类型示意后，很多时候简单的就不用写了，类型示意就能明白 用处： 一种更清晰的自文档。（很多时候就不用写docstrings了，写个类型示意简单明了） 帮助IDE做检查（像PyCharm做检查时候） 可以通过这种机制，做类型检查（可以在装饰器里来限制传入参数的类型，装饰器里会讲）。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>annotation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[函数的参数]]></title>
    <url>%2Fpython%2F20170511-03-function-argument%2F</url>
    <content type="text"><![CDATA[1. 函数定义时的参数(parameter)1.1 普通参数init是普通参数： 123456789In [1]: def inc(init): ...: return init + 1 ...: In [2]: inc(3)Out[2]: 4In [3]: inc(4)Out[3]: 5 1.1 默认参数下面step = 1，是默认参数： 123456789101112In [4]: def inc(init, step=1): ...: return init + step ...: In [5]: inc(3, 2) # 覆盖掉默认参数Out[5]: 5In [6]: inc(3) # 不填默认参数，默认用默认参数1Out[6]: 4In [7]: inc(3, 1) # 效果和上面等同Out[7]: 4 使用默认参数，一时可以让api更简洁，又不失灵活性，可以用通用性的默认参数，也可以自定义参数值，比较灵活。 还有，当我们对旧函数重构功能时候，用默认参数，可以保持函数的向下兼容性。 默认参数要在非默然参数后面： 允许有多个默认参数，但是默认参数要放在参数列表的最后面，否则会报语法错误： 123456In [15]: def inx(step=1, x): ...: return x + step File "&lt;ipython-input-15-f03eeca5f89a&gt;", line 1 def inx(step=1, x): ^SyntaxError: non-default argument follows default argument 默认参数的值，如果是可变的，在函数体里不要修改： 12345678910 In [16]: def append(x, lst =[]): ...: lst.append(x) ...: return lst ...: In [17]: append(1)Out[17]: [1]In [18]: append(2)Out[18]: [1, 2] 我们可以看到，lst属于调用者的全局变量，这样的话重复调用函数会有问题。 如果实在要用，，可以采用下面的方式：1234567891011In [19]: def append(x, lst=None): ...: if lst is None: ...: lst = [] ...: lst.append(x) ...: return lstIn [28]: append(1)Out[28]: [1]In [29]: append(2)Out[29]: [2] 1.2 可变位置参数可变位置参数用*来定义，在函数体内，可变位置参数是一个元组。 123456789101112In [1]: def fn(*args): ...: print(args) ...: In [2]: fn()()In [3]: fn(1)(1,)In [5]: fn(1, 2, 3, 4, 5, 6)(1, 2, 3, 4, 5, 6) 1.3 可变关键字参数可变关键字参数用**来定义，在函数体内，可变位置参数是一个字典。 可变关键字参数的key都是字符串，并且符合标识符定义规范。123456In [6]: def fn(**kwargs): ...: print(kwargs) ...: In [7]: fn(a=5, b=6)&#123;'b': 6, 'a': 5&#125; 可变位置参数只能以位置参数的额形式调用 可变关键字参数只能以关键字参数的形式调用 1.4 可变关键字参数和可变关键字参数混用两者可以混用，但是可变位置参数必须在可变关键字参数前面。 1234567891011121314151617In [1]: def fn(*args, **kwargs): ...: print(args) ...: print(kwargs) ...: In [2]: fn(1, 2, 3, a=1, b=2)(1, 2, 3)&#123;'a': 1, 'b': 2&#125;In [3]: def fn(**kwargs, *args): #可变位置参数必须在可变关键字参数前面 ...: print(kwargs) ...: print(args) ...: File "&lt;ipython-input-3-ff68f43fac13&gt;", line 1 def fn(**kwargs, *args): ^SyntaxError: invalid syntax 1.5 普通参数，可变位置参数，可变关键字参数混用123456789101112In [4]: def fn(x, y, *args, **kwargs): ...: print(x) ...: print(y) ...: print(args) ...: print(kwargs) ...: In [5]: fn(1, 2, 3, 4, 5, 6, 7, a=1, b=2)12(3, 4, 5, 6, 7)&#123;'a': 1, 'b': 2&#125; 2. 函数调用时的参数(argument)首先定义一个函数：123In [23]: def minus(x, y): ...: return x-y ...: 下面是调用（传参）的情况： 2.1 位置参数（位置传参）按照对应的位置传入： 12345In [24]: minus(5, 3)Out[24]: 2In [25]: minus(3, 5)Out[25]: -2 2.2 关键字参数（关键字传参）按照关键字传参，无所谓位置12345In [26]: minus(x=3 , y=5)Out[26]: -2In [27]: minus(y=5, x=3)Out[27]: -2 2.3 位置参数和关键字参数混用混用的话，关键字参数要在位置参数后面，否则会报错。 12345678In [30]: minus(3 , y=5)Out[30]: -2In [31]: minus(x=3 , 5) File "&lt;ipython-input-31-37ba1a10f555&gt;", line 1 minus(x=3 , 5) ^SyntaxError: positional argument follows keyword argument 2.7 使用规则 可变参数后置 可变参数不和默认参数一起出现。 编写代码，通常遵循上面两条规则，否则容易混乱。 2.8 参数解构参数解构发生在函数调用的时候。 *可以把线性结构解包成位置参数。 1234567891011In [12]: def fn(a, b, c): ....: print(a, b, c) ....: In [13]: lst = [1, 2, 3]In [14]: fn(lst[0],lst[1], lst[2])1 2 3In [15]: fn(*lst) # 参数解构，fn(*lst)等价于fn(lst[0],lst[1], lst[2])1 2 3 **可以把字典解构成关键字参数。 1234567In [12]: def fn(a, b, c): ....: print(a, b, c) ....: In [16]: d = &#123;'a':1, 'b':2, 'c':3&#125;In [17]: fn(**d) 如果传入的参数解构的元素太多，可以在函数里采用*args的形式，来去掉多余的参数。 12345678In [22]: def fn(a, b, c, *args): ....: print(a, b, c) ....: In [23]: lst = [1, 2, 3, 4, 5]In [24]: fn(*lst)1 2 3 也可以用**kwargs，去掉多余的参数。 12345678In [25]: d = &#123;'a':1, 'b':2, 'c':3, 'd':4, 'e':5&#125;In [26]: def fn(a, b, c, *args, **kwargs): ....: print(a, b, c ) ....: In [27]: fn(**d)1 2 3 2.9 传参的一个顺序位置参数，线性结构解构，关键字参数，字典解构 解构的时候，线性结构的解构后是位置参数，字典结构成关键字参数。 在传入时候注意冲突。 尽量不要同时使用两种结构，除非你真的知道在做什么。 2.10 参数槽（keyword-only）*之后的参数，必须以关键字参数的形式传递，称之为参数槽（又叫keyword-only）。 1234567891011121314151617In [4]: def fn(a, b, *, c): ...: print(a, b ,c) ...: In [6]: fn(1, 2, c=3)1 2 3In [7]: fn(a=1, b=2, c=3)1 2 3In [22]: fn(1, 2, 3)---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-22-9ab6b0a400c3&gt; in &lt;module&gt;()----&gt; 1 fn(1, 2, 3)TypeError: fn() takes 2 positional arguments but 3 were given 参数槽通常和默认参数搭配使用 参数槽通常在我们认为一些参数通常不会改变的情况下使用，如果需要使用，必须要用关键字参数来调用。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>argument</tag>
        <tag>parameter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[函数执行过程]]></title>
    <url>%2Fpython%2F20170511-02-function-process%2F</url>
    <content type="text"><![CDATA[在学习函数初期，我们可以用pythontutor来分析函数的执行过程，帮助理解函数: 123456789def inc(x): return x + 1def inc_add(x, y): return inc(x) + inc(y) x = 1y = 2z = inc_add(x, y)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[函数基本定义和函数调用]]></title>
    <url>%2Fpython%2F20170511-01-function-define%2F</url>
    <content type="text"><![CDATA[定义函数（define）简单的定义一个函数的格式： 12def function_name(parameter_list): statements 结构为def，后面接函数名(function name)，然后括号括起来的参数列表（parameter list），参数以逗号分隔，然后再接冒号:，下面是函数语句(statements)。 函数语句里通常包含一个return语句，或者是yield语句，或者是二者都有。 写一个简单的函数： 123In [18]: def add(x, y): ....: return x+y ....: 函数调用（call）函数调用格式：参数名加括号括起来的传入参数（argument），传入参数以逗号分隔。 1function_name(argument_list) 例如上面的add函数调用： 12In [19]: add(1, 2)Out[19]: 3 作用域（scope）LGB原则： Built-in（内置作用域：open，range） Global（全局作用域：模块（或文件）最外层，或者在函数内里声明了global） Local（函数内的作用域） 在函数内搜索变量时候，先从函数本地作用域查找，如果没有，再去全局作用域查找，然后再去内置作用域查找: Local–&gt;Global –&gt; Built-in 当然，函数内可以有多层嵌套函数或匿名函数，以层级从内（inner）到外（outer）查找。 让我们看两个例子： 12345678910In [20]: def append(lst,x): ....: lst.append(x) ....: In [21]: lst = [1, 2, 3]In [22]: append(lst, 4)In [23]: lstOut[23]: [1, 2, 3, 4] 上面append(lst, 4)，在函数内没有找到lst的定义，所以去全局再找，在全局找到了lst，这时候给lst添加4后，改变了全局的lst值，我们可以看到lst变成了[1, 2, 3, 4] 下面这个例子比较绕，大家需要仔细体会： 1234567891011121314151617In [24]: def swap(x,y): ....: x, y = y, x ....: print(x, y) ....: In [25]: x = 3In [26]: y = 4In [27]: swap(x, y)4 3In [28]: xOut[28]: 3In [29]: yOut[29]: 4 上面这个例子是这样：我们运行swap(x, y)是在global作用域，是传入的全局的x 和 y的值，也就相当于运行swap(3, 4)，在函数内部，把4，3的值赋给了函数本地作用域里的x和y，也就是说函数本地的x = 4, y = 3。而我们最后运行查看x和y是在global作用域运行的，所以查看的是global作用域里的x和y 换一种表现形式大家更能明白： 1234567891011121314151617In [30]: def swap(globalx, globaly): ....: localx, localy = globaly, globalx ....: print(localx, localy) ....: In [31]: globalx = 3In [32]: globaly = 4In [33]: swap(globalx, globaly)4 3In [34]: globalxOut[34]: 3In [35]: globalyOut[35]: 4 返回值在函数里可以用return来定义函数的返回值。如果没有定义return语句，默认会返回一个None。 例如： 12345678In [43]: def add(x,y): ....: return x + y ....: In [44]: ret = add(1, 2)In [45]: print(ret)3 12345678In [39]: def fn(): ....: pass ....: In [40]: ret = fn()In [42]: print(ret)None 如果return后面跟多个值，会把多个值隐式封装成一个元组。 我们可以对return回来的元组进行解包操作，也是可以的。 12345678910111213141516171819In [46]: def unpack(lst): ....: head, *tail = lst ....: return head, tail ....: In [47]: lst1 = [1, 2, 3, 4]In [48]: all = unpack(lst1)In [49]: all # 我们看到返回的多个值被封装成元组Out[49]: (1, [2, 3, 4])In [50]: head, tail = all # 可以对返回值进行解包操作In [51]: headOut[51]: 1In [52]: tailOut[52]: [2, 3, 4] 一个函数可以有任意多个return语句，但是始终只会执行一个return语句，执行完return语句后，会返回到调用方作用域。原函数作用域里的其他语句都不执行，直接跳出来。如果在多层循环里，会跳出所有循环并跳出函数作用域。 12345678910111213141516In [56]: lst1Out[56]: [1, 2, 3, 4]In [57]: def append(lst,x): ....: return "No Operation" ....: lst.append(x) ....: return "Have Operation" ....: In [58]: ret = append(lst1, 5)In [59]: ret # 只返回了执行到的第一个return语句Out[59]: 'No Operation'In [60]: lst1 # return后面的语句都没有被执行Out[60]: [1, 2, 3, 4] 跳出多层循环的例子： 1234567891011121314In [4]: def loop(lst1, lst2): ...: for x in lst1: ...: for y in lst2: ...: print(x, y) ...: if y == 3: ...: return ...: ...: In [5]: loop(range(10), range(10))0 00 10 20 3]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>function</tag>
        <tag>define</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解 可迭代对象(Iterables)、迭代器(Iterators)、生成器(Generators)]]></title>
    <url>%2Fpython%2F20170510-07-iterables-interators-generators-verbose%2F</url>
    <content type="text"><![CDATA[英文原文：http://nvie.com/posts/iterators-vs-generators/ 翻译：（译者：于龙君，严禁未经本人同意的转载，如要转载，请联系本人，转载也请注明出处。） 有时我会混淆Python中的几个概念： 容器（container） 可迭代对象（iterable） 迭代器（iterator） 生成器（generator） 生成器表达式（generator expression） 列表、集合、字典解析式（{list, set, dict} comprehension） 故写下这个帖子作为袖珍指南。 容器（Containers）容器是指存有元素的数据结构，支持成员测试。容器是存在内存里的数据结构，通常它们的值也在内存里。在Python里内置的容器有： list, deque, … set, frozensets, … dict, defaultdict, OrderedDict, Counter, … tuple, namedtuple, … str 容器易于掌握，因为您可以将其视为现实生活中的容器：盒子，立方体，房屋，船舶等。 从技术上讲，一个对象是一个容器，可以询问它是否包含某个元素。您可以对列表，集合或元组执行这样的成员关系测试： 123456&gt;&gt;&gt; assert 1 in [1, 2, 3] # lists&gt;&gt;&gt; assert 4 not in [1, 2, 3]&gt;&gt;&gt; assert 1 in &#123;1, 2, 3&#125; # sets&gt;&gt;&gt; assert 4 not in &#123;1, 2, 3&#125;&gt;&gt;&gt; assert 1 in (1, 2, 3) # tuples&gt;&gt;&gt; assert 4 not in (1, 2, 3) 字典是检查keys： 1234&gt;&gt;&gt; d = &#123;1: 'foo', 2: 'bar', 3: 'qux'&#125;&gt;&gt;&gt; assert 1 in d&gt;&gt;&gt; assert 4 not in d&gt;&gt;&gt; assert 'foo' not in d # 'foo' is not a _key_ in the dict 你也可以查看一个字符串是否包含一个子字符串： 1234&gt;&gt;&gt; s = 'foobar'&gt;&gt;&gt; assert 'b' in s&gt;&gt;&gt; assert 'x' not in s&gt;&gt;&gt; assert 'foo' in s # a string "contains" all its substrings 最后一个例子有一点奇怪，不过它展示了容器接口如何隐式呈现对象。一个字符串不会将所有子字符串的副本存储在内存中，但是可以这样来使用。 注意：尽管大多数容器数据结构提供了生成包含的每个元素的方法，但他们并不一定生成一个容器，也可能是一个可迭代对象（稍后会说到）。 并不是所有的容器都必须是可迭代的。一个例子是Bloom filter。是一种概率性的数据结构，我们可以问他是否包含某个元素，但是不能返回其中的任何元素。 可迭代对象（Iterables ）如上所述，大多数容器也是iterable，但是还有更多的东西是iterable，比如说打开的文件，打开的套接字（sockets）等。容器通常是有限的；iterable可以是有限的，也可以表示无限的数据源。 iterable不只可以是数据结构，也可以是能返回迭代器（Iterator）的任何对象（目的是为了返回其中的元素）。听起来有一点绕，但iterable和iterator有一个重要的不同，我们来看例子更明了： 12345678910111213&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; y = iter(x)&gt;&gt;&gt; z = iter(x)&gt;&gt;&gt; next(y)1&gt;&gt;&gt; next(y)2&gt;&gt;&gt; next(z)1&gt;&gt;&gt; type(x)&lt;class 'list'&gt;&gt;&gt;&gt; type(y)&lt;class 'list_iterator'&gt; 从例子可以看出, x 是 iterable, y 和 z 是两个单独的iterator实例，这两个实例从iterablex生成值。 y 和 z 都保存各自的状态。在例子里， x 是一个数据结构 (list列表), y是list_iterator(列表迭代器）。 注意：Often, for pragmatic reasons, iterable classes will implement both __iter__() and __next__() in the same class, and have __iter__() return self, which makes the class both an iterable and its own iterator. It is perfectly fine to return a different object as the iterator, though. 最后，当你写下： 123 x = [1, 2, 3]for elem in x: ... 这是实际发生的： 当您反汇编此Python代码时，您可以看到显式调用GET_ITER, 这实际上就像调用 iter(x)。 FOR_ITER 是一个指令，它将相当于重复调用next()以获取每个元素，但是这不会从字节码指令中显示，因为它在解释器中针对速度进行了优化。 123456789101112&gt;&gt;&gt; import dis&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; dis.dis('for _ in x: pass') 1 0 SETUP_LOOP 14 (to 17) 3 LOAD_NAME 0 (x) 6 GET_ITER &gt;&gt; 7 FOR_ITER 6 (to 16) 10 STORE_NAME 1 (_) 13 JUMP_ABSOLUTE 7 &gt;&gt; 16 POP_BLOCK &gt;&gt; 17 LOAD_CONST 0 (None) 20 RETURN_VALUE 迭代器（Iterators）那么什么是 iterator 呢？它是一个有状态的帮助对象，当您调用 next() 时将生成下一个值。因此，任何具有__next__()方法的对象都是iterator，如何生成一个价值是无关紧要的。 所以迭代器是一个生产值的工厂。每次你要求它的“下一个”值，它知道如何计算它，因为它保存内部状态。 有无数的迭代器例子。所有的itertools函数都返回iterator。 有些生成无限序列(infinite sequences)： 123456&gt;&gt;&gt; from itertools import count&gt;&gt;&gt; counter = count(start=13)&gt;&gt;&gt; next(counter)13&gt;&gt;&gt; next(counter)14 有些生成从有限序列（finite sequences）生成无限序列： 12345678910&gt;&gt;&gt; from itertools import cycle&gt;&gt;&gt; colors = cycle(['red', 'white', 'blue'])&gt;&gt;&gt; next(colors)'red'&gt;&gt;&gt; next(colors)'white'&gt;&gt;&gt; next(colors)'blue'&gt;&gt;&gt; next(colors)'red' 有些从无限序列生成有限序列： 123456789&gt;&gt;&gt; from itertools import islice&gt;&gt;&gt; colors = cycle(['red', 'white', 'blue']) # infinite&gt;&gt;&gt; limited = islice(colors, 0, 4) # finite&gt;&gt;&gt; for x in limited: # so safe to use for-loop on... print(x)redwhitebluered 为了更好地了解interator的内部结构，我们来构建一个产生斐波那契数的interator： 1234567891011121314151617&gt;&gt;&gt; class fib:... def __init__(self):... self.prev = 0... self.curr = 1... ... def __iter__(self):... return self... ... def __next__(self):... value = self.curr... self.curr += self.prev... self.prev = value... return value...&gt;&gt;&gt; f = fib()&gt;&gt;&gt; list(islice(f, 0, 10))[1, 1, 2, 3, 5, 8, 13, 21, 34, 55] 请注意，这个类即是一个iterable（因为它运行一个__iter__() 方法），也是迭代器（因为它有一个__next__()方法）。 这个iterator中的状态完全保存在内部的prev和curr实例变量中，并用于后续调用迭代器。每一次调用next()都要做两件重要的事情： 修改状态，为了下一次调用next()做准备。 生成当前调用的结果。 中心思想：一个懒惰的工厂从外面看，interator就像一个懒惰的工厂，平常是空闲的，除非你向它请求一个值，他才会产生一个单个的值，然后再次进入空闲状态，直到你下次再调用，这样循环往复。 生成器（Generators）最后，我们终于要说generator了！generator是我最爱的Python语言功能。一个generator就是一个特殊的迭代器——优雅的那种。 一个生成器允许你像上面的斐波纳契序列迭代器示例一样，来编写迭代器，但是使用了一个优雅的简洁语法，避免在类里写__iter__()和__next__()方法。 让我们明确的说一下： 任何generator也是一个iterator（反之亦然！）; 因此，任何generator也是一个惰性生成值的工厂。 下面也是一个斐波那契序列工厂，但是是用generator写的： 123456789&gt;&gt;&gt; def fib():... prev, curr = 0, 1... while True:... yield curr... prev, curr = curr, prev + curr...&gt;&gt;&gt; f = fib()&gt;&gt;&gt; list(islice(f, 0, 10))[1, 1, 2, 3, 5, 8, 13, 21, 34, 55] 哇，是不是很优雅？注意美丽的魔法关键字： yield 让我们分解一下这里发生的一切：首先要注意的fib是一个普通的Python函数，没什么特别的。但是，请注意，在函数体里没有return关键字。函数的返回值将是一个generator（心中默念：一个iterator，一个懒惰的工厂，一个有状态的帮助对象）。 现在，当f = fib()被调用时，genrator（懒惰的工厂）被实例化并返回，此时不会执行任何代码：只是启动了一个初始化状态的空闲generator。要明白一点：prev, curr = 0, 1这行代码还没有被执行。 然后，这个generator实例被包在一个islice()函数里。lslice(f,0,10)本身也是一个迭代器，最初是空闲的。还是什么都没有发生。 然后，这个迭代器被包装在一个list()函数里，它将消耗它的所有参数，并从它构建一个列表。为了做到这一点，它将开始在islice()实例里调用next() ，这其实是转向f实例里调用next()。 但是一次只有一步。在第一次调用时，代码会最终运行一点：prev, curr = 0, 1这行执行了，然后再到while True循环开始，然后遇到yield curr语句。然后会生成一个值，这个是就是当前curr变量里的值，然后再次进入空闲状态。 这个产生的值传入到islice()，list()函数可以把第一次生成的值1加入到列表里。 因为还没到第10个，这时会要求islice()下一个值，然后传递到要求f下一个值，它将f从之前的状态“取消暂停” ，接着运行prev, curr = curr, prev + curr。然后再次进入while循环的下一次迭代，并且命中yield curr语句，返回下一个curr的值2。 直到输出列表为10个元素长度，这时list()要求 islice()第11个值时，islice()会引发StopIteration异常，表示已到达结束，并且列表将返回结果：10个项目的列表，其中包含了前10个斐波纳契数。请注意，generator不接收第十一个next()的调用。实际上，它不会再被使用了，之后会被垃圾回收。 生成器类型（Types of Generators）Python中有两种类型的generator：generator函数（function）和generator表达式(expression)。只要yield在其函数体内出现，就叫generator函数。我们刚才还看了一个例子就是这样的。关键字的yield的出现足以使函数成为generator函数。 假设你用下面的列表解析式（list comprehension）语法来构建一个平方数列表： 123&gt;&gt;&gt; numbers = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; [x * x for x in numbers][1, 4, 9, 16, 25, 36] 你可以用集合解析式(set comprehension)来做相同的事情：You could do the same thing with a set comprehension: 12&gt;&gt;&gt; &#123;x * x for x in numbers&#125;&#123;1, 4, 36, 9, 16, 25&#125; 或者是一个字典解析式(dict comprehension)： 12&gt;&gt;&gt; &#123;x: x * x for x in numbers&#125;&#123;1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36&#125; 但是您也可以使用生成器表达式（generator expression）（注意：这不是一个元组解析式(tuple comprehension），没有元组解析式这种东西） 1234567&gt;&gt;&gt; lazy_squares = (x * x for x in numbers)&gt;&gt;&gt; lazy_squares&lt;generator object &lt;genexpr&gt; at 0x10d1f5510&gt;&gt;&gt;&gt; next(lazy_squares)1&gt;&gt;&gt; list(lazy_squares)[4, 9, 16, 25, 36] 需要注意的是，因为我们用next()方法从lazy_squares读入第一个值，状态目前在第二个item上，所以，当我们通过调用list()来消费所有元素时，只会从第二个开始读入，因为我们已经用next()消费了一个了。（这也向我们展示了惰性求值行为）。这也是一个和上面例子一样的generator（当然了，也是iterator） 总结generator是一个不可思议的强大的编程结构。它们允许您编写具有较少中间变量和数据结构的流式代码。除此之外，它们具有更高的内存和CPU效率，他们也用更少的代码行来书写。 开始使用generator。 在代码中找到如下位置： 12345def something(): result = [] for ... in ...: result.append(x) return result 并替换为： 1234567def iter_something(): for ... in ...: yield x# def something(): # 当你真的需要一个列表结构时：# return list(iter_something())]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Iterables</tag>
        <tag>Iterators</tag>
        <tag>Generators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单了解 可迭代对象(Iterables)、迭代器(Iterators)、生成器(Generators)]]></title>
    <url>%2Fpython%2F20170510-06-iterables-interators-generators-simple%2F</url>
    <content type="text"><![CDATA[##可迭代对象（iterable） 列表、元组、集合、字典、字符串、bytes、bytearray、生成器（generator）、range等，都是可迭代对象（iterable）。 简单的可以这样理解：能用for...in循环来迭代、能用in成员运算符的，都是可迭代对象。 迭代器（iterator）通过iter函数，可以把可迭代对象（iterable）封装成为迭代器（iterator）。 123456789In [11]: r = range(3)In [12]: it = iter(r)In [13]: type(r)Out[13]: rangeIn [14]: type(it)Out[14]: range_iterator 迭代器的本质是一种封装。很多地方都隐式的调用了迭代器。for…in循环和成员运算符隐式的调用了iter函数。 next函数可以迭代迭代器。 当迭代器没有下一个元素的时候，next函数会抛出StopIteration异常。 没有下一个元素的时候，如果设置了默认返回值，迭代结束就不会抛出异常，而是返回默认返回值。 12345678910111213141516171819In [15]: next(it)Out[15]: 0In [16]: next(it)Out[16]: 1In [17]: next(it)Out[17]: 2In [18]: next(it)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-18-2cdb14c0d4d6&gt; in &lt;module&gt;()----&gt; 1 next(it)StopIteration: In [19]: next(it,3)Out[19]: 3 生成器（generator）生成器（generator）是迭代器的子集（iterator），迭代器中包含了生成器。 生成器可以用上节的生成器解析式来生成。 由于生成器也是迭代器，所以也可以使用next方法。 123456789101112131415161718192021222324In [35]: g = (x for x in range(3))In [36]: gOut[36]: &lt;generator object &lt;genexpr&gt; at 0x10e6c0e08&gt;In [37]: next(g)Out[37]: 0In [38]: next(g)Out[38]: 1In [39]: next(g)Out[39]: 2In [40]: next(g)---------------------------------------------------------------------------StopIteration Traceback (most recent call last)&lt;ipython-input-40-5f315c5de15b&gt; in &lt;module&gt;()----&gt; 1 next(g)StopIteration: In [41]: next(g,-1)Out[41]: -1 非生成器的迭代器的用处其他迭代器不如生成器那样能节省内存，还让可迭代对象只能next往后。那其他迭代器到底有什么用呢？ iter的一种使用场景，用的最少的一种场景。 123456789101112131415In [47]: lst = [ ....: ['温度', 28, 29, 32, 35, 30, 29, 27], ....: ['湿度', 30, 35, 45, 50, 39, 35, 30] ....: ]In [48]: for seq in lst: ....: it = iter(seq) ....: name = next(it) ....: print(name) ....: for x in it: ....: print(x, end=' ') ....: print() 温度28 29 32 35 30 29 27 湿度30 35 45 50 39 35 30 help(iter)我们可以看到第二种用法：iter(callable, sentinel) -&gt; iterator iter(…)iter(iterable) -&gt; iteratoriter(callable, sentinel) -&gt; iterator Get an iterator from an object. In the first form, the argument mustsupply its own iterator, or be a sequence.In the second form, the callable is called until it returns the sentinel. 第二种用法我们用的比较多，鉴于目前所学的知识还不够，后续会讲到。(可以实现非阻塞io）]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>Iterables</tag>
        <tag>Iterators</tag>
        <tag>Generators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解析式（Comprehensions)]]></title>
    <url>%2Fpython%2F20170510-05-comprehensions%2F</url>
    <content type="text"><![CDATA[解析式是用一个或多个迭代器来快速建立数据结构的一种方法。可以将for循环和if判断结合，创造出一种简单快捷的语法糖，更加Pythonic。 分为几种解析式： 列表解析式（list comprehensions） 集合解析式（set comprehensions） 字典解析式（dict comprehensions） 生成器解析式（generator comprehensions） 1. 列表解析式（list comprehensions）1.1 最基本语法[ expression for item in iterable ] 用正常的for循环来建立一个列表： 12345678In [2]: list1=[]In [3]: for i in range(6): ...: list1.append(i) ...: In [4]: list1Out[4]: [0, 1, 2, 3, 4, 5] 用列表解析式来建立： 1234In [5]: list1=[i for i in range(6)]In [6]: list1Out[6]: [0, 1, 2, 3, 4, 5] 1.2加上if条件表达式[ expression for item in iterable if condition ] 下面看示例： 1234In [8]: list2=[i for i in range(5) if i % 2 == 1]In [9]: list2Out[9]: [1, 3, 5] 上面代码用传统方法来写就是： 123456789In [10]: list2=[]In [11]: for i in range(6): ....: if i % 2 == 1: ....: list2.append(i) ....: In [12]: list2Out[12]: [1, 3, 5] 1.3 多个if条件表达式1234In [24]: list3=[i for i in range(6) if i % 2 == 1 if i &gt; 2]In [25]: list3Out[25]: [3, 5] 传统方法来写就是： 12345678910In [28]: list3=[]In [29]: for i in range(6): ....: if i%2 == 1: ....: if i &gt; 2: ....: list3.append(i) ....:In [30]: list3Out[30]: [3, 5] 1.4 笛卡尔积（多个for的嵌套循环）[ (expression1,expression2 for item1 in iterable1 for item2 in iterable2 ] 12345678In [1]: list1=[1,3,4]In [2]: list2=[2,4,6]In [3]: list3=[(x,y) for x in list1 for y in list2]In [4]: list3Out[4]: [(1, 2), (1, 4), (1, 6), (3, 2), (3, 4), (3, 6), (4, 2), (4, 4), (4, 6)] 用传统方法来写就是： 12345678910111213In [5]: list1=[1,3,5]In [6]: list2=[2,4,6]In [7]: list3=[]In [8]: for x in list1: ...: for y in list2: ...: list3.append((x,y)) ...:In [9]: list3Out[9]: [(1, 2), (1, 4), (1, 6), (3, 2), (3, 4), (3, 6), (5, 2), (5, 4), (5, 6)] 1.5 列表解析式加上zip()函数（可以不看，延伸部分）先理解zip()函数，zip()函数可以对多个序列进行并行迭代 1234567891011In [20]: uppers = ['A','B','C']In [21]: lowers = ['a','b','c']In [22]: nums = [1,2,3]In [23]: zip(uppers,lowers,nums)Out[23]: &lt;zip at 0x17c62a1c9c8&gt;In [24]: list(zip(uppers,lowers,nums))Out[24]: [('A', 'a', 1), ('B', 'b', 2), ('C', 'c', 3)] 下面两个是不一样的，第一个是笛卡尔积，第二个是zip()函数,结果是不一样的。笛卡尔积： 12345678910111213In [25]: list1=[(u,l,n) for u in uppers for l in lowers for n in lowers]In [26]: list1Out[26]:[('A', 'a', 'a'), ('A', 'a', 'b'), ('A', 'a', 'c'), ('A', 'b', 'a'), ('A', 'b', 'b'), ('A', 'b', 'c'), ('A', 'c', 'a'), ('A', 'c', 'b'), ('A', 'c', 'c'), ('B', 'a', 'a'), ('B', 'a', 'b'), ('B', 'a', 'c'), ('B', 'b', 'a'), ('B', 'b', 'b'), ('B', 'b', 'c'), ('B', 'c', 'a'), ('B', 'c', 'b'), ('B', 'c', 'c'), ('C', 'a', 'a'), ('C', 'a', 'b'), ('C', 'a', 'c'), ('C', 'b', 'a'), ('C', 'b', 'b'), ('C', 'b', 'c'), ('C', 'c', 'a'), ('C', 'c', 'b'), ('C', 'c', 'c')] zip()函数: 1234In [30]: list2=[(u,l,n) for u,l,n in zip(uppers,lowers,nums)]In [31]: list2Out[31]: [('A', 'a', 1), ('B', 'b', 2), ('C', 'c', 3)] 2. 集合解析式（set comprehensions）类似于列表解析，只是中括号[]变成大括号{}.集合有个区别是，是集合里面不重复元素: 1234In [18]: set1=&#123;2,2,2&#125;In [19]: &#123;x+1 for x in set1&#125;Out[19]: &#123;3&#125; 3. 字典解析式（dict comprehensions）{_keyexp : _valueexp for expresion in iterable} 通常是keys和values一起解析： 12345678In [1]: letters=['A','B','C','D']In [2]: nums=[1,2,3,4]In [3]: dict1=&#123; k : v for k,v in zip(letters,nums)&#125;In [4]: dict1Out[4]: &#123;'A': 1, 'B': 2, 'C': 3, 'D': 4&#125; 还有一种，values是keys的调用后的值，这种情况只解析keys就可以了： 123456In [7]: name = 'yulongjun'In [8]: counts=&#123;letter: name.count(letter) for letter in set(name)&#125;In [9]: countsOut[9]: &#123;'g': 1, 'j': 1, 'l': 1, 'n': 2, 'o': 1, 'u': 2, 'y': 1&#125; 当然，由于name字符串里面有重复的字母，name.count计算了多遍重复的字母，可以有更pythonic的用法，如下，set(name)就把name中重复的字母去掉了： 1counts=&#123;letter: name.count(letter) for letter in set(name)&#125; 4. 生成器解析式（generator comprehensions）生成器解析式和列表解析式用法差不多，只是把中括号[]变成小括号()，很多人会认为()生成的是元组（tuple），但其实是生成器（generator）: 1234567In [17]: gen1=(i for i in range(6))In [18]: gen1Out[18]: &lt;generator object &lt;genexpr&gt; at 0x0000028944C86570&gt;In [19]: type(gen1)Out[19]: generator 生成的生成器只能迭代完一次，再次从头迭代就不可用了: 123456789In [20]: list2=list(gen1)In [21]: list2Out[21]: [0, 1, 2, 3, 4, 5]In [22]: list1=list(gen1)In [23]: list1Out[23]: []]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>comprehension</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[byte和bytearray]]></title>
    <url>%2Fpython%2F20170510-04-bytes-bytearrays%2F</url>
    <content type="text"><![CDATA[bytesPython 3 特有的，Python2是不区分bytes和str。 bytes是不可变的。 Python 2 中： 12345type(b'xxxx')&lt;type 'str'&gt;&gt;&gt;&gt; type('xxxx')&lt;type 'str'&gt; Python 3 中： 12345In [1]: type(b'xxxx')Out[1]: bytesIn [3]: type('xxxx')Out[3]: str bytes 和 str 的区别在于，bytes是byte的序列，而str是unicode的序列。 str可使用encode的方法转化为bytes。 bytes使用decode的方法转化为str。 (默认的encode和decode的编码方式为utf-8）。 1234567891011121314151617181920212223242526272829In [4]: s = '于龙君'In [5]: b = s.encode()In [6]: bOut[6]: b'\xe4\xba\x8e\xe9\xbe\x99\xe5\x90\x9b'In [7]: b.decode()Out[7]: '于龙君'In [8]: for x in s: ...: print(x) ...: 于龙君In [9]: for x in b: ...: print(x) ...: 228186142233190153229144155 bytearraybytearray和bytes不一样的地方在于，bytearray是可变的。 1234567891011121314151617In [10]: s = '于龙君'In [11]: ba = bytearray(s.encode())In [12]: baOut[12]: bytearray(b'\xe4\xba\x8e\xe9\xbe\x99\xe5\x90\x9b')In [13]: ba.decode()Out[13]: '于龙君'In [14]: ba[:3] = bytearray('王'.encode())In [15]: baOut[16]: bytearray(b'\xe7\x8e\x8b\xe9\xbe\x99\xe5\x90\x9b')In [17]: ba.decode()Out[17]: '王龙君']]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>byte</tag>
        <tag>bytearray</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字典（Dictionarys）]]></title>
    <url>%2Fpython%2F20170510-03-dicts%2F</url>
    <content type="text"><![CDATA[字典是一种key-value结构。字典的key必须是可hash的值，并且是唯一的，value可以是任意值。 字典初始化12345678In [1]: d = dict()In [2]: d = &#123;&#125;In [3]: d = &#123;'a':1, 'b':2&#125;In [4]: dOut[4]: &#123;'a': 1, 'b': 2&#125; 字典的下标操作12345678910111213141516171819202122In [3]: d=&#123;'a':1, 'b':2&#125;In [4]: d['a']Out[4]: 1In [5]: d['a'] = 123In [6]: dOut[6]: &#123;'a': 123, 'b': 2&#125;In [7]: d['c']---------------------------------------------------------------------------KeyError Traceback (most recent call last)&lt;ipython-input-7-3e4d85f12902&gt; in &lt;module&gt;()----&gt; 1 d['c']KeyError: 'c'In [8]: d['c'] = 345In [9]: dOut[9]: &#123;'a': 123, 'b': 2, 'c': 345&#125; 增updateupdate的参数可以是以下几种情况： 字典 由二元组构成的可迭代对象 关键字参数 12345678910111213141516171819202122232425262728293031323334353637383940In [9]: dOut[9]: &#123;'a': 123, 'b': 2, 'c': 345&#125;In [10]: d.update(&#123;'f':111, 'e':222&#125;)In [11]: dOut[11]: &#123;'a': 123, 'b': 2, 'c': 345, 'e': 222, 'f': 111&#125;In [12]: d.update([('g',333),('h',444)])In [13]: dOut[13]: &#123;'a': 123, 'b': 2, 'c': 345, 'e': 222, 'f': 111, 'g': 333, 'h': 444&#125;In [14]: d.update([1, 2, 3])---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-14-8f8690e9b692&gt; in &lt;module&gt;()----&gt; 1 d.update([1, 2, 3])TypeError: cannot convert dictionary update sequence element #0 to a sequenceIn [15]: d.update((1, 2, 3))---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-15-78ef53cee86d&gt; in &lt;module&gt;()----&gt; 1 d.update((1, 2, 3))TypeError: cannot convert dictionary update sequence element #0 to a sequenceIn [16]: d.update(&#123;1, 2, 3&#125;)---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-16-0c35aaa27a8f&gt; in &lt;module&gt;()----&gt; 1 d.update(&#123;1, 2, 3&#125;)TypeError: cannot convert dictionary update sequence element #0 to a sequenceIn [17]: d.update(i=555)In [18]: dOut[18]: &#123;'a': 123, 'b': 2, 'c': 345, 'e': 222, 'f': 111, 'g': 333, 'h': 444, 'i': 555&#125; update如果key相同的话，第二个的value会覆盖第一个的value，算是一个改的功能了。 1234In [19]: d.update(a=456)In [20]: dOut[20]: &#123;'a': 456, 'b': 2, 'c': 345, 'e': 222, 'f': 111, 'g': 333, 'h': 444, 'i': 555&#125; setdefaultdefaultdict设置字典的默认值。参数为一个函数。 删pop pop(…) method of builtins.dict instanceD.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.If key is not found, d is returned if given, otherwise KeyError is raised popitem随机删除并返回一个k-v对。（用得不多） 123456789101112131415161718192021In [26]: dOut[26]: &#123;'a': 456, 'b': 2, 'c': 345, 'e': 222, 'f': 111, 'g': 333, 'h': 444, 'i': 555&#125;In [27]: d.pop('i')Out[27]: 555In [28]: d.pop('g',333)Out[28]: 333In [29]: d.popitem()Out[29]: ('c', 345)In [30]: del d['f']In [31]: dOut[31]: &#123;'a': 456, 'b': 2, 'e': 222, 'h': 444&#125;In [32]: d.clear()In [33]: dOut[33]: &#123;&#125; 访问（查）keys12345678910111213141516171819202122232425262728In [45]: dOut[45]: &#123;'a': 1, 'b': 2, 'c': 123&#125;In [46]: d.keys()Out[46]: dict_keys(['b', 'c', 'a'])In [47]: for k in d.keys(): ....: print('&#123;&#125; =&gt; &#123;&#125;'.format(k,d[k]) ....: ....: ) ....: b =&gt; 2c =&gt; 123a =&gt; 1In [53]: dk = d.keys()In [54]: dk -&#123;'a', 'b'&#125;Out[54]: &#123;'c'&#125;In [55]: dk &amp; &#123;'a','b'&#125;Out[55]: &#123;'a', 'b'&#125;In [56]: dk | &#123;'a', 'b'&#125;Out[56]: &#123;'a', 'b', 'c'&#125;In [57]: dk ^ &#123;'b', 'c', 'd'&#125;Out[57]: &#123;'a', 'd'&#125; values1234567In [58]: d.values()Out[58]: dict_values([2, 123, 1])In [59]: dv = d.values()In [60]: len(dv)Out[60]: 3 items1234567891011121314151617181920In [61]: d.items()Out[61]: dict_items([('b', 2), ('c', 123), ('a', 1)])In [62]: for k, v in d.items(): ....: print('&#123;&#125; =&gt;&#123;&#125;'.format(k, v)) ....: b =&gt;2c =&gt;123a =&gt;1In [63]: di = d.items()In [64]: di &amp; &#123;'d','e'&#125;Out[64]: set()In [65]: di | &#123;'d','e'&#125;Out[65]: &#123;'d', ('a', 1), ('c', 123), ('b', 2), 'e'&#125;In [68]: di^&#123;'d','e'&#125;Out[68]: &#123;'d', ('a', 1), ('c', 123), ('b', 2), 'e'&#125; 附加：OrderedDictOrderedDict，有序的字典 12345678910111213141516171819202122In [71]: from collections import OrderedDictIn [72]: od = OrderedDict()In [73]: od['a'] = 1In [74]: od['b'] = 2In [75]: od['c'] = 3In [76]: odOut[76]: OrderedDict([('a', 1), ('b', 2), ('c', 3)])In [77]: od.keys()Out[77]: odict_keys(['a', 'b', 'c'])In [78]: for k,v in od.items(): ....: print(k,v) ....: a 1b 2c 3 其他几个函数copyfromkeys1234d.fromkeys([1, 2, 3])d.fromkeys([1, 2, 3], False)hosts = [1, 2, 3]d.fromkeys(hosts, False) 3.9 字典（Dictionarys）@(Learning-Python3(2016下半年))[dict] 字典是一种key-value结构。字典的key必须是可hash的值，并且是唯一的，value可以是任意值。 字典初始化12345678In [1]: d = dict()In [2]: d = &#123;&#125;In [3]: d = &#123;'a':1, 'b':2&#125;In [4]: dOut[4]: &#123;'a': 1, 'b': 2&#125; 字典的下标操作12345678910111213141516171819202122In [3]: d=&#123;'a':1, 'b':2&#125;In [4]: d['a']Out[4]: 1In [5]: d['a'] = 123In [6]: dOut[6]: &#123;'a': 123, 'b': 2&#125;In [7]: d['c']---------------------------------------------------------------------------KeyError Traceback (most recent call last)&lt;ipython-input-7-3e4d85f12902&gt; in &lt;module&gt;()----&gt; 1 d['c']KeyError: 'c'In [8]: d['c'] = 345In [9]: dOut[9]: &#123;'a': 123, 'b': 2, 'c': 345&#125; 增updateupdate的参数可以是以下几种情况： 字典 由二元组构成的可迭代对象 关键字参数 12345678910111213141516171819202122232425262728293031323334353637383940In [9]: dOut[9]: &#123;'a': 123, 'b': 2, 'c': 345&#125;In [10]: d.update(&#123;'f':111, 'e':222&#125;)In [11]: dOut[11]: &#123;'a': 123, 'b': 2, 'c': 345, 'e': 222, 'f': 111&#125;In [12]: d.update([('g',333),('h',444)])In [13]: dOut[13]: &#123;'a': 123, 'b': 2, 'c': 345, 'e': 222, 'f': 111, 'g': 333, 'h': 444&#125;In [14]: d.update([1, 2, 3])---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-14-8f8690e9b692&gt; in &lt;module&gt;()----&gt; 1 d.update([1, 2, 3])TypeError: cannot convert dictionary update sequence element #0 to a sequenceIn [15]: d.update((1, 2, 3))---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-15-78ef53cee86d&gt; in &lt;module&gt;()----&gt; 1 d.update((1, 2, 3))TypeError: cannot convert dictionary update sequence element #0 to a sequenceIn [16]: d.update(&#123;1, 2, 3&#125;)---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-16-0c35aaa27a8f&gt; in &lt;module&gt;()----&gt; 1 d.update(&#123;1, 2, 3&#125;)TypeError: cannot convert dictionary update sequence element #0 to a sequenceIn [17]: d.update(i=555)In [18]: dOut[18]: &#123;'a': 123, 'b': 2, 'c': 345, 'e': 222, 'f': 111, 'g': 333, 'h': 444, 'i': 555&#125; update如果key相同的话，第二个的value会覆盖第一个的value，算是一个改的功能了。 1234In [19]: d.update(a=456)In [20]: dOut[20]: &#123;'a': 456, 'b': 2, 'c': 345, 'e': 222, 'f': 111, 'g': 333, 'h': 444, 'i': 555&#125; setdefaultdefaultdict设置字典的默认值。参数为一个函数。 删pop pop(…) method of builtins.dict instanceD.pop(k[,d]) -&gt; v, remove specified key and return the corresponding value.If key is not found, d is returned if given, otherwise KeyError is raised popitem随机删除并返回一个k-v对。（用得不多） 123456789101112131415161718192021In [26]: dOut[26]: &#123;'a': 456, 'b': 2, 'c': 345, 'e': 222, 'f': 111, 'g': 333, 'h': 444, 'i': 555&#125;In [27]: d.pop('i')Out[27]: 555In [28]: d.pop('g',333)Out[28]: 333In [29]: d.popitem()Out[29]: ('c', 345)In [30]: del d['f']In [31]: dOut[31]: &#123;'a': 456, 'b': 2, 'e': 222, 'h': 444&#125;In [32]: d.clear()In [33]: dOut[33]: &#123;&#125; 访问（查） 遍历keys12345678910111213141516171819202122232425262728In [45]: dOut[45]: &#123;'a': 1, 'b': 2, 'c': 123&#125;In [46]: d.keys()Out[46]: dict_keys(['b', 'c', 'a'])In [47]: for k in d.keys(): ....: print('&#123;&#125; =&gt; &#123;&#125;'.format(k,d[k]) ....: ....: ) ....: b =&gt; 2c =&gt; 123a =&gt; 1In [53]: dk = d.keys()In [54]: dk -&#123;'a', 'b'&#125;Out[54]: &#123;'c'&#125;In [55]: dk &amp; &#123;'a','b'&#125;Out[55]: &#123;'a', 'b'&#125;In [56]: dk | &#123;'a', 'b'&#125;Out[56]: &#123;'a', 'b', 'c'&#125;In [57]: dk ^ &#123;'b', 'c', 'd'&#125;Out[57]: &#123;'a', 'd'&#125; values1234567In [58]: d.values()Out[58]: dict_values([2, 123, 1])In [59]: dv = d.values()In [60]: len(dv)Out[60]: 3 items1234567891011121314151617181920In [61]: d.items()Out[61]: dict_items([('b', 2), ('c', 123), ('a', 1)])In [62]: for k, v in d.items(): ....: print('&#123;&#125; =&gt;&#123;&#125;'.format(k, v)) ....: b =&gt;2c =&gt;123a =&gt;1In [63]: di = d.items()In [64]: di &amp; &#123;'d','e'&#125;Out[64]: set()In [65]: di | &#123;'d','e'&#125;Out[65]: &#123;'d', ('a', 1), ('c', 123), ('b', 2), 'e'&#125;In [68]: di^&#123;'d','e'&#125;Out[68]: &#123;'d', ('a', 1), ('c', 123), ('b', 2), 'e'&#125; 附加：OrderedDictOrderedDict，有序的字典 12345678910111213141516171819202122In [71]: from collections import OrderedDictIn [72]: od = OrderedDict()In [73]: od['a'] = 1In [74]: od['b'] = 2In [75]: od['c'] = 3In [76]: odOut[76]: OrderedDict([('a', 1), ('b', 2), ('c', 3)])In [77]: od.keys()Out[77]: odict_keys(['a', 'b', 'c'])In [78]: for k,v in od.items(): ....: print(k,v) ....: a 1b 2c 3 其他几个函数copyfromkeys1234d.fromkeys([1, 2, 3])d.fromkeys([1, 2, 3], False)hosts = [1, 2, 3]d.fromkeys(hosts, False)]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>dict</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集合（Sets）]]></title>
    <url>%2Fpython%2F20170510-02-sets%2F</url>
    <content type="text"><![CDATA[增add增加一个元素到集合，如果元素已经存在，则没有任何操作。 add(…)Add an element to a set.This has no effect if the element is already present. update增加一个集合 update(…)Update a set with the union of itself and others. 删remove删除不存在的会抛出异常 remove(…)Remove an element from a set; it must be a member. If the element is not a member, raise a KeyError. discard删除不存在的不会抛出异常。 discard(…)Remove an element from a set if it is a member.If the element is not a member, do nothing. pop 删除并返回一个任意元素。 pop(…)Remove and return an arbitrary set element.Raises KeyError if the set is empty. clear删除所有元素 clear(…)Remove all elements from this set. 改、查没有一个方法可以直接的修改集合中的某个具体元素，因为没有一个方法，可以查找定位其中的某个具体元素。 集合的集合运算并集（union==|） union(…) Return the union of sets as a new set.(i.e. all elements that are in either set.) 交集（intersection==&amp;、intersection_update） intersection(…)Return the intersection of two sets as a new set.(i.e. all elements that are in both sets.) intersection_update(…)Update a set with the intersection of itself and another. 差集（difference==-、difference_update） difference(…)Return the difference of two or more sets as a new set. (i.e. all elements that are in this set but not the others.) difference_update(…)Remove all elements of another set from this set. 对称差集（symmetric_difference==^、symmetric_difference_update）对称差集，把a.difference(b)和b.difference(a)的值union起来 symmetric_difference(…)Return the symmetric difference of two sets as a new set.(i.e. all elements that are in exactly one of the sets.) symmetric_difference_update(…)Update a set with the symmetric difference of itself and another. 是否超集（isuperset） issuperset(…)Report whether this set contains another set. 是否子集(issubset) issubset(…)Report whether another set contains this set. 是否相交（isdisjoint） isdisjoint(…)Return True if two sets have a null intersection. 复制（copy） copy(…) Return a shallow copy of a set. 集合的应用 元素需要唯一而对顺序没有要求添加主机，要求不重复 1hosts = set(user_input.splitlines()) 我们需要集合运算时求未完成添加的主机（主机-已完成添加的主机） 1hosts - complete_hosts 求多个地方输入的一个并集hosts = input_a | input_b | input_c]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>set</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[格式化（Formatting）]]></title>
    <url>%2Fpython%2F20170510-01-formatting%2F</url>
    <content type="text"><![CDATA[printf style 老式的类似c语言的printf风格的格式化输出。 用百分号%来作为占位符。 12345678910111213In [16]: 'I am %s'%('yulongjun',) # 可以采用元组的方式Out[16]: 'I am yulongjun'In [18]: 'I am %(name)s'%&#123;'name':'yulongjun'&#125; # 可以采用字典的方式Out[18]: 'I am yulongjun'In [19]: 'I am %(name)s, my name is %(name)s'%&#123;'name':'yulongjun'&#125; # 多次出现，可以采用字典的方式Out[19]: 'I am yulongjun, my name is yulongjun'In [21]: name = "yulongjun"In [22]: 'I am %s'%name #只有一个元素可以直接跟在百分号后面，这里解释器自动做了一个封包，把name封包为元组（语法糖）Out[22]: 'I am yulongjun' 前面字符串中占位符后面的type，以下为常用的几个： type 说明 d、i、u 十进制整数 o、 八进制整数 x、X 十六进制整数（x的话数字里的字母为小写，X的话为大写） e、E 科学计数法（e的话科学计数的e为小写，E的话为大写） f、F 浮点数（默认保留6位小数) g、G 自动选择最优表示法（整数、浮点数或科学计数法） c 单个字符或整数转化为字符 s 用str()转化为字符串 r 用repr()转化为字符串 a 用ascii()转化为字符串 其中的str()和repr()调用的是__str__和__repr__的魔术方法，有个了解就可以，后续会讲到。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647In [48]: 'The number is %d'%56789Out[48]: 'The number is 56789'In [49]: 'The number is %i'%56789Out[49]: 'The number is 56789'In [50]: 'The number is %u'%56789Out[50]: 'The number is 56789'In [51]: 'The number is %o'%56789Out[51]: 'The number is 156725'In [52]: 'The number is %x'%56789Out[52]: 'The number is ddd5'In [53]: 'The number is %X'%56789Out[53]: 'The number is DDD5'In [54]: 'The number is %e'%56789Out[54]: 'The number is 5.678900e+04'In [55]: 'The number is %E'%56789Out[55]: 'The number is 5.678900E+04'In [56]: 'The number is %f'%0.123456789Out[56]: 'The number is 0.123457'In [57]: 'The number is %F'%0.123456789Out[57]: 'The number is 0.123457'In [58]: 'The number is %g'%0.123456789Out[58]: 'The number is 0.123457'In [60]: 'The number is %g'%0.0000000123456789Out[60]: 'The number is 1.23457e-08'In [61]: 'The number is %G'%123456789Out[61]: 'The number is 1.23457E+08'In [78]: 'The string is %s'%'yulongjun'Out[78]: 'The string is yulongjun'In [79]: 'The string is %r'%'yulongjun'Out[79]: "The string is 'yulongjun'"In [80]: 'The string is %a'%'于龙君'Out[80]: "The string is '\\u4e8e\\u9f99\\u541b'" 数字的位数和对齐的操作 -左对齐 +正负号 0补零 123456789101112131415161718In [86]: '%0.3f'%1.23456789 # 小数位为3位Out[86]: '1.235'In [87]: '%10.3f'%1.23456789 # 小数位为3位，总位数为10位，不够的左边补空格Out[87]: ' 1.235'In [88]: '%-10.3f'%1.23456789 # 加上-号，左对齐，右边补空格Out[88]: '1.235 'In [89]: '%+10.3f'%1.23456789 # 加上正负号Out[89]: ' +1.235'In [90]: '%+10.3f'%-1.23456789Out[90]: ' -1.235'In [91]: '%010.3f'%1.23456789 # 前面加0，算补零Out[91]: '000001.235' format Python3推荐format格式的格式化输出，功能更强大。 str.format(_args,*_kwargs) 字符串中的占位符用{}或者带索引的{INDEX}，或者更详细的格式（索引后面加冒号），如{0:10.3f}等。 format括号里可以跟位置参数，也可以跟关键字参数。 1234567891011121314151617In [97]: 'I am &#123;&#125;'.format('yulongjun')Out[97]: 'I am yulongjun'In [98]: 'I am &#123;&#125;, my age is &#123;&#125;'.format('yulongjun',18)Out[98]: 'I am yulongjun, my age is 18'In [99]: 'I am &#123;1&#125;, my age is &#123;0&#125;'.format(18,'yulongjun')Out[99]: 'I am yulongjun, my age is 18'In [100]: 'I am &#123;name&#125;, my age is &#123;age&#125;'.format(name='yulongjun', age=18)Out[100]: 'I am yulongjun, my age is 18'In [101]: 'I am &#123;name&#125;, my name is &#123;name&#125;'.format(name='yulongjun')Out[101]: 'I am yulongjun, my name is yulongjun'In [102]: 'I am &#123;0&#125;, my name is &#123;0&#125;'.format('yulongjun')Out[102]: 'I am yulongjun, my name is yulongjun' 可以在{}占位符中用字典和列表的下标操作，还可以用模块的子模块或子属性。 1234567891011121314151617181920In [105]: 'My &#123;config[spam]&#125; runs &#123;sys.platform&#125;'.format(sys=sys, config = &#123;'spam': 'laptop'&#125;)Out[105]: 'My laptop runs darwin'In [106]: lst = list('yulongjun')In [107]: 'first=&#123;0[0]&#125;, third=&#123;0[3]&#125;'.format(lst)Out[107]: 'first=y, third=o'In [108]: lst = list('yulongjun')In [109]: 'first=&#123;0[0]&#125;, third=&#123;0[2]&#125;'.format(lst)Out[109]: 'first=y, third=l'In [110]: 'first=&#123;0&#125;, last=&#123;1&#125;'.format(lst[0],lst[-1])Out[110]: 'first=y, last=n'In [113]: parts = lst[0], lst[-1], lst[1:-1]In [115]: 'first=&#123;0&#125;, last=&#123;1&#125;, middle=&#123;2&#125;'.format(*parts)Out[115]: "first=y, last=n, middle=['u', 'l', 'o', 'n', 'g', 'j', 'u']" sys.platform调用的sys的platform config[spam]是用的字典的下标操作 {0[1]使用的是列表的下标操作 *part调用的是位置参数 字符串也有具体的格式化，如字段宽度、对齐方式、补零、小数点精度，填充字符等。 字段宽度：整数部分大小 对齐方式：&gt;右对齐，&lt;左对齐，^居中对齐（和printf style有所不同） 补零：资源宽度的数字前面加0，默认为右对齐 小数点精度：宽度后面加.数字f，f处也可以用g，e 填充字符需要和对齐方式一起使用。 12345678910111213141516171819202122232425In [2]: 'the Number is &#123;:10f&#125;'.format(123.4567)Out[2]: 'the Number is 123.456700'In [3]: 'the Number is &#123;:10&#125;'.format(123.4567)Out[3]: 'the Number is 123.4567'In [4]: 'the Number is &#123;:&lt;10&#125;'.format(123.4567)Out[4]: 'the Number is 123.4567 'In [5]: 'the Number is &#123;:^10&#125;'.format(123.4567)Out[5]: 'the Number is 123.4567 'In [7]: 'the Number is &#123;:010&#125;'.format(123.4567)Out[7]: 'the Number is 00123.4567'In [13]: 'the Number is &#123;:.2f&#125;'.format(123.4567)Out[13]: 'the Number is 123.46'In [15]: 'the Number is &#123;:#^20.2f&#125;'.format(123.4567) ....: Out[15]: 'the Number is #######123.46#######'In [16]: 'the Number is &#123;:#&gt;20.2f&#125;'.format(123.4567) ....: Out[16]: 'the Number is ##############123.46']]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>formatting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解包（Unpacking）]]></title>
    <url>%2Fpython%2F20170509-06-unpacking%2F</url>
    <content type="text"><![CDATA[以下内容部分内容摘选自《Python Cookbook》第一章 第一节，部分内容为自己书写和总结。 任何序列（或者可迭代对象（iterable））都可以通过一个简单的赋值操作来分解为单独的变量。 12345678910111213141516171819202122232425262728293031In [1]: x,y = (1, 3)In [2]: xOut[2]: 1In [3]: yOut[3]: 3In [4]: data = ['ACME', 50, 90.1, (2012,12,21)]In [5]: name, share, price, date = dataIn [6]: nameOut[6]: 'ACME'In [7]: dateOut[7]: (2012, 12, 21)In [8]: name, shares, price, (year, month, day) = dataIn [9]: nameOut[9]: 'ACME'In [10]: yearOut[10]: 2012In [11]: monthOut[11]: 12In [12]: dayOut[12]: 21 如果元素个数不匹配，将会抛出一个ValueError： 123456789101112131415In [13]: x, y, z = (4,5) #变量多于要解包的元素个数---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-13-6ff7f0410c3e&gt; in &lt;module&gt;()----&gt; 1 x, y, z = (4,5)ValueError: not enough values to unpack (expected 3, got 2)In [14]: x, y = (4, 5, 6) # 变量少于要解包的元素个数---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-14-096cd5364eca&gt; in &lt;module&gt;()----&gt; 1 x, y = (4, 5, 6)ValueError: too many values to unpack (expected 2) 解包操作可以作用在各种内置结构上，只要对枪恰好是可迭代的：列表、元组、字符串、迭代器、生成器等。 有时候我们分解操作时候，有可能想丢弃某些值，我们可以选用一个用不到的变量名，比如_，来作为要丢弃的值的名称。 123456789In [5]: data = ['ACME', 50, 90.1, (2012,12,21)]In [6]: _, shares, price, _ = dataIn [7]: sharesOut[7]: 50In [8]: priceOut[8]: 90.1 *expressions来分解包，可以对任意长度的可迭代对象（iterable）中分解元素。 12345678910111213141516171819202122232425262728293031323334In [9]: head,*tail = (1,2,3,4)In [10]: headOut[10]: 1In [11]: tailOut[11]: [2, 3, 4]In [12]: first, *middle, last = [1,2,3,4,5,6,7] In [13]: firstOut[13]: 1In [14]: middleOut[14]: [2, 3, 4, 5, 6]In [15]: lastOut[15]: 7In [16]: lst = [1, [2, 3, 6, 7, 8, 10], 4]In [17]: a, (b, *_, c), d = lstIn [18]: aOut[18]: 1In [19]: bOut[19]: 2In [20]: cOut[20]: 10In [21]: dOut[21]: 4]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>unpacking</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[切片（Slice）]]></title>
    <url>%2Fpython%2F20170509-05-slice%2F</url>
    <content type="text"><![CDATA[切片基本操作取一个list、tuple、strings的部分元素我们经常遇到的操作，我们经常用一种切片（slice）的方法。 用法： seq[start:stop:step]：start为起始索引（包含），stop为结束索引(不包含)，step为步长，省略step的话表示默认值为1，步长可为负数，表示倒序取元素。 start和stop可以省略不写，start不写，表示从头（step为正数）或尾巴（step为负数）开始，stop不写表示从尾（step为正数）或头（step为负数）开始，而且省略的那个位置也包含在内。 如果start和stop同时省略，步长为正数，就是从头到尾，步长为负数，就是从尾到头。 正数超过索引长度的话，表示包含末尾，超过的就不算了；负数超过索引长度的话，表示包含开头，超过的就不算了。 长话短说，实践最重要，我们以list为例（tuple和string和此类似）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758In [1]: lst = list(range(100)) # 创建一个从0到99的列表In [2]: lstOut[2]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]In [3]: lst[0:8] # 包含0，不包含8，切片Out[3]: [0, 1, 2, 3, 4, 5, 6, 7]In [4]: lst[:8] # 省略开头，切片效果同上Out[4]: [0, 1, 2, 3, 4, 5, 6, 7]In [5]: lst[90:99] # 包含90，不包含99，切片Out[5]: [90, 91, 92, 93, 94, 95, 96, 97, 98]In [6]: lst[90:-1] # -1表示最后一个元素，切片效果同上Out[6]: [90, 91, 92, 93, 94, 95, 96, 97, 98]In [7]: lst[-10:-1] # -10表示倒数第10个元素，切片效果同上Out[7]: [90, 91, 92, 93, 94, 95, 96, 97, 98]In [8]: lst[90:10000] # 10000超出最大索引99，直接到最后一位（包含）Out[8]: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]In [9]: lst[90:] # 省略stop，直接到末尾（包含末尾）Out[9]: [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]In [12]: lst[-10000:10] # 负索引超出范围，从头开始(包含头)Out[12]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]In [13]: lst[:10] # 省略start，就从头开始Out[13]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]In [18]: lst[0:20:2] # 步长为2，表示每两个取元素Out[18]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]In [19]: lst[:20:2] # 同样可以省略开头Out[19]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]In [20]: lst[-1:-20:-2] # start比stop大的时候，step为负数才能取到值，否则取不到，为空Out[20]: [99, 97, 95, 93, 91, 89, 87, 85, 83, 81]In [21]: lst[-1:80:-2] # 正数负数可以结合Out[21]: [99, 97, 95, 93, 91, 89, 87, 85, 83, 81]In [22]: lst[::10] # 可以同时省略start和stopOut[22]: [0, 10, 20, 30, 40, 50, 60, 70, 80, 90]In [23]: lst[::-10] # 同上，步长变为负数，表示倒序切片Out[23]: [99, 89, 79, 69, 59, 49, 39, 29, 19, 9]In [24]: lst[:] # 原样复制Out[24]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]In [25]: lst[::-1] # 反转Out[25]: [99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0] 以上切片操作同样适用于元组和字符串（字符串在后面讲，字符串就不讲切片了，和列表的切片用法类似）。 对切片赋值 对连续切片赋值，如果所赋值的内容是可迭代的（iterable），会替换切片原来的元素，元素个数可以不相同。 对非连续的切片赋值，赋值的元素个数要和切片切出来的元素的个数相同。 1234567891011121314151617181920212223242526272829303132333435363738394041In [11]: lst = list(range(10))In [12]: lstOut[12]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]In [13]: lst[3:5] = ['x', 'y'] #个数相同的情况In [14]: lstOut[14]: [0, 1, 2, 'x', 'y', 5, 6, 7, 8, 9]In [15]: lst = list(range(10))In [16]: lst[3:5] = ['x', 'y','z'] #赋值个数比切片多的情况In [17]: lstOut[17]: [0, 1, 2, 'x', 'y', 'z', 5, 6, 7, 8, 9]In [18]: lst = list(range(10)) In [19]: lst[3:5] = ['x'] # 赋值个数比切片少的情况In [20]: lstOut[20]: [0, 1, 2, 'x', 5, 6, 7, 8, 9]In [21]: lst = list(range(10))In [22]: lst[3:8:2] # 不连续的切片Out[22]: [3, 5, 7]In [23]: lst[3:8:2]=['x'] # 不连续的切片，个数不同就抛出异常---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-23-1242dfd11688&gt; in &lt;module&gt;()----&gt; 1 lst[3:8:2]=['x']ValueError: attempt to assign sequence of size 1 to extended slice of size 3In [24]: lst[3:5] = ['x', 'y', 'z'] # 不连续的切片，个数相同就能赋值上。In [25]: lstOut[25]: [0, 1, 2, 'x', 'y', 'z', 5, 6, 7, 8, 9] 附加：成员操作符in 和not in可以用在列表、元组、字符串、字典、元组等。12345678910111213In [1]: lst = list(range(10))In [2]: lstOut[2]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]In [3]: 3 in lstOut[3]: TrueIn [4]: 10 in lstOut[4]: FalseIn [5]: 10 not in lstOut[5]: True]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>slice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[字符串（Strings）]]></title>
    <url>%2Fpython%2F20170509-04-strings%2F</url>
    <content type="text"><![CDATA[在Python3中，字符串是一个unicode的序列，在Python2中一个byte的序列。字符串的很多特性和元组类似，字符串也是不可变的。 定义字符串 &#39; &quot; &#39;&#39;&#39; &quot;&quot;&quot; Python中单引号和双引号的效果是一样的。 三个单引号或双引号表通常用在字符串为多行的情况下。 123456789In [1]: s = 'yulongjun'In [2]: s = "yulongjun"In [3]: s = ''' My name is YuLongjun. ...: My age is 18.'''In [4]: s = """ My name is YuLongjun. ...: My age is 18.""" 连接字符串12345678910In [1]: list1=['I','love','Python']In [2]: ' '.join(list1) #用空格连接Out[2]: 'I love Python'In [3]: ','.join(list1) #用逗号连接Out[3]: 'I,love,Python'In [5]: 'Yu' + 'Longjun'Out[5]: 'YuLongjun' 字符串分割split()函数，从左边开始分割 1234567891011121314151617181920212223242526272829In [9]: s='I love Python'In [10]: s.split() #默认是以空格为分隔符分割Out[10]: ['I', 'love', 'Python']In [11]: s.split('o') # 指定分隔符为oOut[11]: ['I l', 've Pyth', 'n']In [12]: s.split(' ',1) # 以空格为分隔符，最大分割次数为1Out[12]: ['I', 'love Python']In [13]: s='root:x:0:0:root:/root:/bin/bash'In [14]: s.split(':',1) # 以冒号为分隔符，最大分割次数为1Out[14]: ['root', 'x:0:0:root:/root:/bin/bash']In [16]: username,_=s.split(':',1)In [17]: usernameOut[17]: 'root'In [18]: line ='url:http://www.google.com'In [19]: key,value=line.split(':',1)In [20]: keyOut[20]: 'url'In [21]: valueOut[21]: 'http://www.google.com' rsplit()：和split()相反，是从右往左分割的 123In [18]: line='url:http://www.google.com'In [22]: line.rsplit(':',1)Out[22]: ['url:http', '//www.google.com'] splitlines：分割段 12345678910111213In [24]: s =''' ....: I love Python ....: I also love Linux ....: '''In [25]: s.splitlines()Out[25]: ['', 'I love Python', 'I also love Linux']In [26]: s.splitlines(True) #默认是False，即去掉换行符\n,True的话即保留换行符Out[26]: ['\n', 'I love Python\n', 'I also love Linux\n']In [27]: s.splitlines(False)Out[27]: ['', 'I love Python', 'I also love Linux'] partition分割 有点像split之后接了一个参数1，但是还会保留分隔符。 123456789101112131415161718192021222324252627In [29]: help(s.partition)# Help on built-in function partition:#partition(...) method of builtins.str instance #S.partition(sep) -&gt; (head, sep, tail) # Search for the separator sep in S, and return the part before it,the separator itself, and the part after it. If the separator is not found, return S and two empty strings.In [30]: s='root:x:0:0:root:/root:/bin/bash'In [31]: s.partition(':')Out[31]: ('root', ':', 'x:0:0:root:/root:/bin/bash')In [32]: h,_,t=s.partition(':')In [33]: tOut[33]: 'x:0:0:root:/root:/bin/bash'In [34]: h,_,t=t.partition(':')In [35]: tOut[35]: '0:0:root:/root:/bin/bash'In [36]: h,_,t=t.partition(':')In [37]: tOut[37]: '0:root:/root:/bin/bash' rpartition：和partition相反，是从右向左分割 字符串修改-大小写12345678910111213141516In [40]: s='I love Python'In [41]: s.capitalize() #首字母大写Out[41]: 'I love python'In [42]: s.title() #所有单词首字母大写Out[42]: 'I Love Python'In [43]: s.lower() #所有字母小写Out[43]: 'i love python'In [44]: s.upper() #所有字母大写Out[44]: 'I LOVE PYTHON'In [46]: s.swapcase() #所有字母大小写反转Out[46]: 'i LOVE pYTHON' 字符串修改-填充（对齐）、清除（修剪）1234567891011121314151617181920212223242526272829303132333435363738In [2]: s = 'Python'In [3]: s.center(30) #中间填充Out[3]: ' Python 'In [4]: s.center(30,'*') #中间填充，用*填充Out[4]: '************Python************'In [5]: s.ljust(30) #字符串在左边，右边填充Out[5]: 'Python 'In [6]: s.ljust(30,'*') #字符串在左边，右边用*填充Out[6]: 'Python************************'In [7]: s.rjust(30,'*') #字符串在左边，左边用*填充Out[7]: '************************Python'In [16]: s='root:x::0:0:root:/root:/bin/bash\n'In [17]: s.strip() #去掉两边的空白符（space、制表符tab、换行符\n)Out[17]: 'root:x::0:0:root:/root:/bin/bash'In [18]: s.lstrip() #去掉左边的空白符Out[18]: 'root:x::0:0:root:/root:/bin/bash\n'In [19]: s.rstrip() #去掉右边的空白符 Out[19]: 'root:x::0:0:root:/root:/bin/bash'In [20]: s = '##test##'In [21]: s.strip('#')Out[22]: 'test'In [22]: s.lstrip('#')Out[22]: 'test##'In [23]: s.rstrip('#')Out[23]: '##test' 字符串判断12345678910111213141516171819In [74]: poem='''There was a Young Lady of Norway,who casually sat in a doorway;when the door squeezed her flat,She exclaimed, "What of that?"this courageous Young Lady of Norway.'''In [76]: poem[:13]Out[76]: 'There was a Y'In [77]: len(poem) #字符串长度Out[77]: 166In [78]: poem.startswith('There') #以'There'开头Out[78]: TrueIn [79]: poem.endswith('Norway.') #以'Norway.'结尾Out[79]: TrueIn [80]: poem.isalnum() #所有字符都是字母或数字吗？is*还有很多用法，可以自己查看Out[80]: False 字符串查找12345678910111213141516In [81]: poem.count('a') #字符串'a'出现了多少次Out[81]: 16In [82]: poem.index('a') #字符串'a'出现的第一次的位置Out[82]: 7In [82]: poem.rindex('a') #字符串'a'从右边开始出现的第一次的位置Out[82]: 7In [83]: poem.find('a') #查找第一次出现'a'的位置Out[83]: 7In [83]: poem.find('a'，50,100) #查找从50到100，第一次出现'a'的位置Out[83]: 54In [84]: poem.rfind('a',50,100) #查找从50到100，从右开始找，第一次出现'a'的位置Out[84]: 94 replace()替换s.replace(old,new[,count]) 12345678910111213141516In [27]: s='root:x::0:0:root:/root:/bin/bash\n'In [28]: s.replace('root','yulongjun') #默认替换所有Out[28]: 'yulongjun:x::0:0:yulongjun:/yulongjun:/bin/bash\n'In [29]: s.replace('root','yulongjun',1) #只替换第一个Out[29]: 'yulongjun:x::0:0:root:/root:/bin/bash\n'In [30]: s.replace('root','yulongjun',-1) #负数还是替换所有Out[30]: 'yulongjun:x::0:0:yulongjun:/yulongjun:/bin/bash\n'In [31]: s.replace('root','yulongjun',-2) #负数还是替换所有Out[31]: 'yulongjun:x::0:0:yulongjun:/yulongjun:/bin/bash\n'In [32]: s.replace('root','yulongjun',1000) #超过最大个数也是替换所有Out[32]: 'yulongjun:x::0:0:yulongjun:/yulongjun:/bin/bash\n' Python3使用str类型，底层实现是用unicode编码str –&gt; encode –&gt; bytes bytes –&gt; decode –&gt; str 123456789101112In [12]: s='于龙君'In [13]: sOut[13]: '于龙君'In [14]: s.encode() #编码（不填参数默认utf-8）Out[14]: b'\xe4\xba\x8e\xe9\xbe\x99\xe5\x90\x9b'In [15]: d=s.encode()In [16]: d.decode() #解码（不填参数默认utf-8）Out[16]: '于龙君']]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[元组]]></title>
    <url>%2Fpython%2F20170509-03-tuples%2F</url>
    <content type="text"><![CDATA[元组和列表很像，包括索引查询、查找统计元素操作都很类似。但是元组是不可变的，无法改改元素，所以没有增删改操作。 初始化元组元组是不可变的，所以初始化后元组内的元素不可改变。1234t = tuple() # 初始化一个空元组t = () 也是初始化一个空元组t = (1,) 初始化一个元素的元组t = (1, 2, 3) # 初始化三个元素的元组 下标/索引查询 元组中的索引，也是从前往后，索引也是从0开始，从后往前，索引是从-1开始。 如果索引超出范围，将引发IndexError异常。 元组中，是无法更改元素的值的，更改会抛出TypeError的异常。 示例： 12345678910111213141516171819202122232425262728293031In [1]: t = (1, 2, 3)In [2]: t[0] # 索引0的值Out[2]: 1In [3]: t[1] # 索引1的值 Out[3]: 2In [4]: t[-1] # 索引-1，即最后一个的值Out[4]: 3In [5]: t[-2] # 索引-2，即倒数第二个值Out[5]: 2In [6]: t[3] # 索引3，超出范围，报错---------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-6-7d5cf04057c5&gt; in &lt;module&gt;()----&gt; 1 t[3]IndexError: tuple index out of rangeIn [7]: t[3] = 5 # 元组无法更改索引的元素，抛出TypeError---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-8-241a76bcd075&gt; in &lt;module&gt;()----&gt; 1 t[3] = 5TypeError: 'tuple' object does not support item assignment 查找/统计元素index查找某值的第一个索引，可以指定索引的开始和结束位置。（包含start，不包含stop） 如果值没有找到，则抛出ValueError。 index(…)T.index(value, [start, [stop]]) -&gt; integer – return first index of value.Raises ValueError if the value is not present. 1234567891011121314151617181920212223242526In [9]: t = tuple('abcdb')In [10]: tOut[10]: ('a', 'b', 'c', 'd', 'b')In [11]: t.index('b') # 查找'b'第一次出现的索引位置Out[11]: 1In [12]: t.index('b', 2) # 从索引2开始查找，找到第一次出现'b'的位置Out[12]: 4In [13]: t.index('x') # 没找到'x'---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-13-616584d1b884&gt; in &lt;module&gt;()----&gt; 1 tuple.index('x')ValueError: 'x' is not in tupleIn [14]: t.index('b', 2, 4) # 'b'没在2-4索引范围内出现---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-14-fa3eb33afd79&gt; in &lt;module&gt;()----&gt; 1 tuple.index('b', 2, 4)ValueError: 'b' is not in tuple count计数，返回值出现的次数。 count(...)T.count(value) -&gt; integer – return number of occurrences of value 12345In [21]: tOut[21]: ('a', 'b', 'c', 'd', 'b')In [22]: t.count('b')Out[22]: 2 len函数len函数可以用在列表、元组、字符串、字典、集合等上面，输出容器中元素的个数。 len(obj, /)Return the number of items in a container. 12345In [24]: tOut[24]: ('a', 'b', 'c', 'd', 'b')In [25]: len(t)Out[25]: 5]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>tuple</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[列表（Lists）]]></title>
    <url>%2Fpython%2F20170509-02-lists%2F</url>
    <content type="text"><![CDATA[初始化列表：123lst = list() # 初始化一个空列表lst = [] # 也是初始化一个空列表lst = [1, 2, 3] # 初始化三个元素 下标/索引操作 Python中的索引，从前往后，索引是从0开始，从后往前，索引是从-1开始。 如果索引超出范围，将引发IndexError异常。 更改列表中元素的值，采用lst[index] = new_value，索引可用正数，也可用负数，但是不能超出范围，否则也会抛出IndexError异常。 示例： 123456789101112131415161718192021222324252627282930313233343536373839In [2]: lst = [1, 2, 3]In [3]: lst[0] #索引0的值Out[3]: 1In [4]: lst[1] # 索引1的值Out[4]: 2In [5]: lst[-1] # 索引-1，即最后一个的值Out[5]: 3In [6]: lst[-2] # 索引-2，即倒数第二个值Out[6]: 2In [7]: lst[3] # 索引3，超出范围，报错--------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-7-298ffefac8cf&gt; in &lt;module&gt;()----&gt; 1 lst[3]IndexError: list index out of rangeIn [8]: lst[0] = 5 # 把索引为0的元素更改为5In [9]: lstOut[9]: [5, 2, 3]In [10]: lst[-1] = 10 # 索引为负数也可更改In [11]: lstOut[11]: [5, 2, 10]In [12]: lst[-4] = 12 # 超出索引范围，报错---------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-12-8f011d505efa&gt; in &lt;module&gt;()----&gt; 1 lst[-4] = 12IndexError: list assignment index out of range 给list增加元素append在列表最后增加一个对象，返回值为None 。 append(…)L.append(object) -&gt; None – append object to end 示例： 1234567In [14]: lstOut[14]: [5, 2, 10]In [15]: lst.append(12)In [16]: lstOut[16]: [5, 2, 10, 12] insert在索引之前插入一个对象。 insert操作索引超出范围是：如果是正索引，等效append，如果是负索引，等效于insert(0,object)。 insert(…) L.insert(index, object) – insert object before index 示例： 12345678910111213141516171819202122232425262728In [22]: lstOut[22]: [5, 2, 10, 12]In [23]: lst.insert(0, 7) # 在索引0之前插入7In [24]: lstOut[24]: [7, 5, 2, 10, 12]In [25]: lst.insert(3, 0) # 在索引3之前插入0In [26]: lstOut[26]: [7, 5, 2, 0, 10, 12]In [27]: lst.insert(7,13) # 超出范围，索引为正，就在最后面添加In [28]: lstOut[28]: [7, 5, 2, 0, 10, 12, 13]In [29]: lst.insert(100,76) # 超出范围，索引为正，就在最后面添加In [30]: lstOut[30]: [7, 5, 2, 0, 10, 12, 13, 76]In [31]: lst.insert(-100, 8) # 超出范围，索引为负，就在最前面In [32]: lstOut[32]: [8, 7, 5, 2, 0, 10, 12, 13, 76] extend通过添加一个可迭代对象（iterable）中的元素来扩展列表。 extend(…)L.extend(iterable) -&gt; None – extend list by appending elements from the iterable 示例： 123456789101112131415In [38]: lst1 = [1, 2, 3]In [39]: lst2 = [4, 5, 6]In [40]: lst1.extend(lst2) # lst2是一个可迭代对象（iterable），可给lst1做扩展In [41]: lst1Out[41]: [1, 2, 3, 4, 5, 6]In [43]: tuple1 = ('a', 'b', 'c')In [44]: lst2.extend(tuple1) # tuple1(元组后面会讲)是一个可迭代对象（iterable，会面会讲），可给lst2做扩展In [45]: lst2Out[45]: [4, 5, 6, 'a', 'b', 'c'] 删除元素pop删除并返回索引所在的元素（默认不填索引是指-1，最后一个）。 如果index超出索引范围，会抛出IndexError异常。 pop(…)L.pop([index]) -&gt; item – remove and return item at index (default last).Raises IndexError if list is empty or index is out of range. 1234567891011121314151617181920212223242526272829303132333435In [47]: lst = [1, 2, 3, 4, 5, 6]In [48]: lst.pop() # 不填索引，默认删除最后一个Out[48]: 6In [49]: lstOut[49]: [1, 2, 3, 4, 5]In [50]: lst.pop(3) # 删除索引为3的元素Out[50]: 4In [51]: lstOut[51]: [1, 2, 3, 5]In [52]: lst.pop(100) # 超出索引范围，抛出IndexError异常---------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-52-795b88347eea&gt; in &lt;module&gt;()----&gt; 1 lst.pop(100)IndexError: pop index out of rangeIn [53]: lst.pop(-1) # 删除索引为负数的元素Out[53]: 5In [54]: lstOut[54]: [1, 2, 3]In [55]: lst.pop(-100) # 超过索引范围，抛出IndexError异常---------------------------------------------------------------------------IndexError Traceback (most recent call last)&lt;ipython-input-54-d93c61a62a8e&gt; in &lt;module&gt;()----&gt; 1 lst.pop(-100)IndexError: pop index out of range remove删除第一次发现的值。 如果值不存在则抛出ValueError。 remove(…)L.remove(value) -&gt; None – remove first occurrence of value.Raises ValueError if the value is not present. 1234567891011121314In [57]: lst = [1, 2, 3, 1, 2, 3]In [58]: lst.remove(1) # 删除第一次查找到的1，第二个1没有删除In [59]: lstOut[59]: [2, 3, 1, 2, 3]In [60]: lst.remove(5) # 没有找到，抛出ValueError---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-60-29e61bbce0d4&gt; in &lt;module&gt;()----&gt; 1 lst.remove(5)ValueError: list.remove(x): x not in list clear清空列表。 clear(…)L.clear() -&gt; None – remove all items from L 123456In [1]: lst = [1, 2, 3]In [2]: lst.clear()In [3]: lstOut[3]: [] 查找/统计元素index查找某值的第一个索引，可以指定索引的开始和结束位置。（包含start，不包含stop） 如果值没有找到，则抛出ValueError。 index(…)L.index(value, [start, [stop]]) -&gt; integer – return first index of value.Raises ValueError if the value is not present. 1234567891011121314151617181920212223242526In [9]: lst = list('abcdb')In [10]: lstOut[10]: ['a', 'b', 'c', 'd', 'b']In [11]: lst.index('b') # 查找'b'第一次出现的索引位置Out[11]: 1In [12]: lst.index('b', 2) # 从索引2开始查找，找到第一次出现'b'的位置Out[12]: 4In [13]: lst.index('x') # 没找到'x'---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-13-616584d1b884&gt; in &lt;module&gt;()----&gt; 1 lst.index('x')ValueError: 'x' is not in listIn [14]: lst.index('b', 2, 4) # 'b'没在2-4索引范围内出现---------------------------------------------------------------------------ValueError Traceback (most recent call last)&lt;ipython-input-14-fa3eb33afd79&gt; in &lt;module&gt;()----&gt; 1 lst.index('b', 2, 4)ValueError: 'b' is not in list count计数，返回值出现的次数。 count(...)L.count(value) -&gt; integer – return number of occurrences of value 12345In [21]: lstOut[21]: ['a', 'b', 'c', 'd', 'b']In [22]: lst.count('b')Out[22]: 2 len函数len函数可以用在列表、元组、字符串、字典、集合等上面，输出容器中元素的个数。 len(obj, /)Return the number of items in a container. 12345In [24]: lstOut[24]: ['a', 'b', 'c', 'd', 'b']In [25]: len(lst)Out[25]: 5 修改列表sort原地排序，有两个参数，一个是key，表示排序用的函数，默认为None没有。一个是reverse（反向，降序排列），默认是False。 sort(…)L.sort(key=None, reverse=False) -&gt; None – stable sort “IN PLACE” 1234567891011121314In [30]: lst = list("yulongjun")In [31]: lstOut[31]: ['y', 'u', 'l', 'o', 'n', 'g', 'j', 'u', 'n']In [32]: lst.sort()In [33]: lstOut[33]: ['g', 'j', 'l', 'n', 'n', 'o', 'u', 'u', 'y']In [34]: lst.sort(reverse=True)In [35]: lstOut[35]: ['y', 'u', 'u', 'o', 'n', 'n', 'l', 'j', 'g'] reverse原地反转。 reverse(…)L.reverse() – reverse “IN PLACE” 123456In [37]: lst = [3, 1, 2, 5]In [38]: lst.reverse()In [39]: lstOut[39]: [5, 2, 1, 3] 其他copy采用赋值方法创建新列表，是指向原来的列表的那块内存，当进行更改时候，新老列表一起改变。 12345678910111213141516171819In [44]: lst = [1, 2, 3, 4, 5]In [45]: lst2 = lstIn [46]: lst2.remove(2)In [47]: lst2Out[47]: [1, 3, 4, 5]In [48]: lst1---------------------------------------------------------------------------NameError Traceback (most recent call last)&lt;ipython-input-48-f3e10dd48749&gt; in &lt;module&gt;()----&gt; 1 lst1NameError: name &apos;lst1&apos; is not definedIn [49]: lstOut[49]: [1, 3, 4, 5] 采用copy方法，就是复制出一块相同的内存(影子复制)。新旧列表互不影响。 copy(…)L.copy() -&gt; list – a shallow copy of L 1234567891011121314Out[49]: [1, 3, 4, 5]In [50]: lstOut[50]: [1, 3, 4, 5]In [51]: lst2 = lst.copy()In [52]: lst2.remove(3)In [53]: lst2Out[53]: [1, 4, 5]In [54]: lstOut[54]: [1, 3, 4, 5]]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数字（Numbers）]]></title>
    <url>%2Fpython%2F20170509-01-numbers%2F</url>
    <content type="text"><![CDATA[整理出Python3的数字类型，数字分为多种： 整数（int） 浮点数（float） 二进制数（binary） 八进制数（octal） 十六进制数（hex） 复数（complex number） 小数（decimal） 分数（fraction） 数字类型整数（integer)1234， −24， +42， 0 1234567891011121314&gt;&gt;&gt; int(9.1) #取整数9&gt;&gt;&gt; int('1111',2) #二进制转化十进制15&gt;&gt;&gt; int('0b1111',2) #二进制转化十进制15&gt;&gt;&gt; int('177',8) #八进制转化十进制127&gt;&gt;&gt; int('0o177',8) #八进制转化十进制127&gt;&gt;&gt; int('9ff',16) #十六进制转化十进制2559&gt;&gt;&gt; int('0x9ff',16) #十六进制转化十进制2559 浮点数（float）1.23，-1.23， 1.， .1， -1.， -.1， 3.14e-10， 4E210， 4.0e+210 float(9)， float(43210)，float(‘4e210’) 二进制整数（binary）0b1111 bin(15) 八进制整数（octal）0o177 oct(127) 十六进制（hex）0x9ff hex(2559) 复数（complex number）3+4j， 3.0+4.0j， 3j complex(3,4) 小数（decimal）精度高，比float更准确。 123&gt;&gt;&gt; from decimal import Decimal&gt;&gt;&gt; Decimal('1.1')-Decimal('.1')Decimal('1.0') 分数（fraction）123&gt;&gt;&gt; from fractions import Fraction&gt;&gt;&gt; Fraction(1,3)+Fraction(7,6)Fraction(3,2) 运算符+：加法：5+8=13 -：减法：90-10=80 *：乘法：4*7=28 /：浮点数除法：7/2=3.5 //：整数除法（地板除）：7//2=3 %：模（求余数）7%3=1 **：幂：3**4=81]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>number</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序控制结构]]></title>
    <url>%2Fpython%2F20170508-03-control%2F</url>
    <content type="text"><![CDATA[2.3.1 顺序结构从上到下顺序执行： 123a = 0a += 1print(a) 2.3.2 分支结构(1) 单分支结构[ 123x = 6if x &gt; 5: print("x is larger than 5") (2) 双分支结构[ 12345x = 6if x &gt; 5: print("x is larger than 5")else: print("x ls not larger than 5") (3) 多分支结构[ 1234567 x = 6if x &gt; 5: print("x is larger than 5")elif x &lt; 5: print("x ls smaller than 5")else: print("x is equal to 5") 2.3.3 循环结构（1） while循环结构[ 1234567891011In [1]: x = 0In [2]: while x &lt; 5: ...: print("&#123;&#125; is smaller than 5".format(x)) ...: x += 1 ...: 0 is smaller than 51 is smaller than 52 is smaller than 53 is smaller than 54 is smaller than 5 (2) for循环结构[ 12345678910111213In [3]: for i in range(10): ...: print(i) ...: 0123456789 (3) 循环体重不要修改可迭代对象不要出现下面的用法： 123 lst = range(10)for i in lst: lst.append(i) (4) 控制结构可以互相嵌套if elif else 、for、 while三种控制结构可以相互嵌套。 123for i in range(10): if i% 2 == 0: print(i) (5) break提前结束整个循环12345678In [8]: for i in range(10): ...: if i == 3: ...: break ...: print(i) ...: 012 (6) continue跳到下次循环开始1234567891011121314In [7]: for i in range(10): ...: if i == 3: ...: continue ...: print(i) ...: 012456789 (7) 循环外使用else123456789In [10]: a = 7In [11]: for i in range(2, a): ....: if a % i ==0: ....: break ....: else: ....: print('yes') ....: yes 循环结构中else子句是用来判断循环有没有提前退出，如果提前退出了，else子句不执行，如果没有提前退出，执行else子句。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>control</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运算符]]></title>
    <url>%2Fpython%2F20170508-02-operator%2F</url>
    <content type="text"><![CDATA[运算符分为以下几种类型： 算术运算符 逻辑运算符 比较运算符 位运算符 其他运算符 赋值运算符 成员运算符 身份运算符 2.2.1 算术运算符 算术运算符 说明 示例 + 加法 5+8=13 - 减法 90-10=80 * 乘法 4*7=28 / 浮点数除法 7/2=3.5 // 整除，或叫地板除 7//2=3 % 模，即求余数 7%3=1 ** 幂 3**4=81 2.2.2 比较运算符 比较运算符 说明 示例 &gt; 大于 3 &gt; 2 为True &lt; 小于 3 &lt; 2 为False == 等于 3 == 3为True != 不等于 3 != 3为False &gt;= 大于等于 5&gt;=4为True &lt;= 小于等于 6&lt;=3为False 2.2.3 逻辑运算符 比较运算符 说明 示例 and 与。两边都是真值，返回后面那个值,否则返回False 3 and 5 返回5，5 and 0返回False，True and False 返回False or 或 。两边有一个真值，返回那个真值，否则返回False 5 and 0返回5，True and False返回False not 非。非真为假，非假为真 not 5为True，not 0为False，not True为False，Not False为True 2.2.4 位运算符(用到很少，可以暂时理解下就好) [ [ 2.2.5 其他运算符赋值运算符 =：把右边赋值给左边，例如a = 5 a += b、a -= b、a *= b、a /=b、a //=b、a %= b、a **= b：相当于a = a + b、a = a - b… 成员运算符 in 和not in：一个元素是否存在于一组元素中。后续会讲到，略 身份运算符 is和not is：两者是否是指向同一个内存对象。 123456In [8]: x = 20In [9]: y = 20In [10]: x is yOut[10]: True]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>operator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[变量与命名规则(PEP8)]]></title>
    <url>%2Fpython%2F20170508-01-variable%2F</url>
    <content type="text"><![CDATA[2.1.1 写一个Hello world一个语言刚开始一般写一个hello world： 1print(&quot;Hello, world!&quot;) 2.1.2 变量 Python没有严格意义上的常量，都是可变的，可以随意改变数据的类型。 但是对与一个变量内的元素来说，分为可改变和不可改变的，可改变内部元素的如列表、字典、集合，不可改变内部元素的如元组、字符串。（本章后面小节会讲到。） 举个例子： 12345678910111213141516171819202122In [1]: var = 5In [2]: var = &quot;Python3&quot;In [3]: var = [1,2,3,4]In [4]: lst = [1,2,3,4]In [5]: lst[2] = &quot;c&quot;In [6]: lstOut[6]: [1, 2, &apos;c&apos;, 4]In [7]: strings = &quot;yulongjun&quot;In [8]: strings[0] = &quot;Y&quot;---------------------------------------------------------------------------TypeError Traceback (most recent call last)&lt;ipython-input-8-824d19b519c0&gt; in &lt;module&gt;()----&gt; 1 strings[0] = &quot;Y&quot;TypeError: &apos;str&apos; object does not support item assignment | 2.1.3 命名规则变量名只能包含以下字符： 小写字母（a-z） 大写字母（A-Z） 数字（0-9） 下划线（_） Python中以下划线开头的名字有特殊含义： -：单独的下划线。临时的名称使用，后面不会再次用到它。 _private：单下划线接一个名称。表明该名称的属性为私有。 __attr__：两组双下划线中间加一个名称。Python中系统属性名称，如__init__，__doc__。 更详细的命名约定可参照pep8规范，里面约定了Python中的命名的常用方法：PEP8规范中文版]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>variables</tag>
        <tag>PEP8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开发环境（vim/Jupyter Notebook/Pycharm）]]></title>
    <url>%2Fpython%2F20170507-03-ide%2F</url>
    <content type="text"><![CDATA[vimvim是一款非常好用的编辑器，学习曲线比较陡峭，但是当你学会并熟练应用的时候，绝对非常好用。这里不详细介绍vim的用法，感兴趣的可以看这本书《vim实用技巧》： 京东：http://item.jd.com/11445638.html 网上也有电子版的下载，请自行搜索。 jupyter notebookjupyter notebook，是一个强大的工具，可以一边测试代码，一边用markdown写标题和文字，算的上是一个在线编辑器+交互式IDE，使用pip可以安装 ：pip install jupyter如果访问pypi安装不是特别快可以更改pip的设置linux下是修改~/.pip/pip.conf： 123[global]index-url = http://mirrors.aliyun.com/pypi/simple/trusted-host = mirrors.aliyun.com MacOS下如果是用brew来安装的话，需要手动创建~/.pip目录，然后创建pip.conf文件，加入下面内容： 123[global]index-url = http://mirrors.aliyun.com/pypi/simple/trusted-host = mirrors.aliyun.com jupyter notebook使用指南 如果仅在所在的那台主机上使用 ：jupyter notebook这样会在所在的那台主机开启一个服务并打开主机的默认浏览器。我们在服务器里输入http://localhost:888或者http://127.0.0.1:8888就可以登录 如果用其他机器的浏览器访问的话：jupyter notebook --ip 0.0.0.0 --port 8888 --no-browser0.0.0.0表示全网可以访问；port后面跟的是端口，可以自定义；no-browser是指不打开本机的浏览器。 jupyter notebook经常用到的快捷方式 可参考http://blog.csdn.net/lawme/article/details/51034543 PyCharmPyCharm简介Pycharm 是 Jetbrains 公司出品的一款商业化的针对Python开发者开发的一款IDE产品，功能十分强大。分为两个版本， Community 和 Professional 版本,下面是两个版本的功能和区别： Features PyCharm Community Edition PyCharm Professional Edition 智能 Python 编辑器 ✔ ✔ 图形化的调试程序和测试运行程序 ✔ ✔ 导航和重构 ✔ ✔ 代码检查 ✔ ✔ VCS 支持 ✔ ✔ 科学化的工具 ✔ ✔ Web 开发 ✔ Python web 框架 ✔ Python 分析器 ✔ 远程开发功能 ✔ 数据库 &amp; SQL 支持 ✔ 你要想用做Web开发（ html，jss，JavaScript 等），或者用到Python的web框架如 Tornado，Django， Flask 等等高级功能，还是选择 Professional 版本吧。如果只是想了解 Python 的基本语法，Community 版足够你用了。（当然，如果只是了解基本语法，可以用 ipython ，还有从ipython中独立出来的 jupyter notebook）。 下载地址为：https://www.jetbrains.com/pycharm/download/ 安装后初始化 双击打开。第一次安装，点ok就可以了；如果之前装过PyCharm，或者有自定义的配置，可以选择上面一条。 这里我们点试用30天（Evaluate for free 30 days） 长期用可以选择付费，支持支付宝的哦，购买地址：https://www.jetbrains.com/pycharm/buy/ 。 IDE主题和编辑器的代码风格： 这里我一般都选Darcula，暗色风格。这个后续都是可以自行更改的。 重启，让主题生效。 创建项目 双击打开，界面如图： Create New Project：创建新项目。 Open： 打开项目或文件。 Check out from Version Control：从版本控制如GitHub，Git等， checkout 项目。 Register：是注册用，关于注册这里不涉及。 Configure：是配置PyCharm，这里面有Pycharm所有的配置项。 Get Help：获取帮助。 我们点击Create New Project，能看到很多项目模板 有纯粹的Python模板，Django，Flaskweb框架的模板等等，我们先选择Pure Python进行初步了解，纯粹的Python模板。右边栏位，Location是项目存放的位置，Interpreter是解释器版本选择，我们可以选择项目要使用的Python版本（新的 Pycharm 2016 已经支持pyenv了，笔者这里用的pyenv来控制Python版本)然后点击create创建。 第一次启动项目时候回进行索引，这时候可能你电脑的cpu和内存占用会很高，甚至风扇狂装，都是正常现象，一会儿就结束，如果使用macbook的话，由于是ssd的，速度会很快，半分钟差不多，索引期间最好不要操作Pycharm，等索引完了再说： 在左下角会提示你接入Tools Windows，包括Mac自带的终端，Python自带的解释器，还有一个TODO，这样，你可以完全沉浸在一个界面就可以了，基本上不用切换桌面或切换程序了，编程变得更专注： 如果想了解更多Pycharm的使用方法，可以看这个：IntelliJ IDEA 使用教程，IDEA是IntelliJ公司的旗舰产品，包含了PyCharm的所有功能。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pyenv</tag>
        <tag>pyenv-virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS下Python多版本控制软件的安装：pyenv、pyenv-virtualenv]]></title>
    <url>%2Fpython%2F20170507-02-pyenv-centos%2F</url>
    <content type="text"><![CDATA[pyenv安装1. 安装依赖包1yum -y install git gcc make patch zlib-devel gdbm-devel openssl-devel sqlite-devel bzip2-devel readline-devel 2. 安装pyenv1curl -L https://raw.githubusercontent.com/pyenv/pyenv-installer/master/bin/pyenv-installer | bash 3. 设置环境变量1234567cat &gt;&gt; .bash_profile &lt;&lt; EOF# pyenv settingsexport PATH="~/.pyenv/bin:\$PATH"eval "\$(pyenv init -)"eval "\$(pyenv virtualenv-init -)"EOF 4. 使之生效. .bash_profile或者source .bash_profile 这时候pyenv就可以使用了 pyenv使用指南1、 pyenv versions查看系统的上安装的Python版本。 其中前面的*表示当前工作目录正在使用的版本,其中 的 system表示系统自带的 Python 版本： 12$ pyenv versions* system (set by /Users/yulongjun/.pyenv/version) 2、 pyenv install &lt;version&gt;安装其他版本的Python。例如安装3.6.2和2.7.13版本： 1234567891011121314151617181920212223$ pyenv install 3.6.2 # 安装3.6.2版本的Python$ pyenv install 2.7.13 # 安装2.7.13版本的Python$ pyenv versions # 可以看到3个版本，system为系统自带的版本* system (set by /root/.pyenv/version) 2.7.13 3.6.2$ cd # 到家目录$ mkdir Python36 # 创建Python3.6的工作目录$ cd Python36$ pyenv local 3.6.2 # 使当前工作目录使用Python3.6.2版本$ python -V # 查看一下当前目录用Python的版本，确实是3.6.2Python3.6.2$ pip -V # 查看一下pip版本,是3.6的pippip 9.0.1 from /root/.pyenv/versions/3.6.2/lib/python3.6/site-packages (python 3.6) $ cd # 回到家目录$ mkdir Python27 # 创建python2.7的工作目录$ cd Python27$ pyenv local 2.7.13 # 使当前工作目录使用Python2.7.13版本$ python -V # 查看一下当前目录用Python的版本，确实是2.7.13Python 2.7.13$ pip -V # 查看一下pip版本，是2.7的pippip 9.0.1 from /root/.pyenv/versions/2.7.13/lib/python2.7/site-packages (python 2.7) pyenv-virtualenv的使用方法pyenv-virtualenv是用来创建一个干净的虚拟Python环境的命令，通常在创建干净的新项目时候使用。使用方法如下: 1、创建虚拟环境–pyenv virtualenv 版本号 虚拟环境名。 1$ pyenv virtualenv 3.6.2 venv-3.6.2 2、创建项目，让项目使用干净的Python3.6.2的虚拟环境： 123456[yulongjun@yulongjun ~]$ mkdir Learning-Python3[yulongjun@yulongjun ~]$ cd Learning-Python3/[yulongjun@yulongjun Learning-Python3]$ pyenv local venv-3.6.2(venv-3.6.2) [yulongjun@yulongjun Learning-Python3]$ cd ..[yulongjun@yulongjun ~]$ cd Learning-Python3/(venv-3.6.2) [yulongjun@yulongjun Learning-Python3]$ 我们会发现：只要我们进入Learning-Python3目录，就会自动激活virtualenv，退出Learning-Python3目录，就会关闭virtualenv。 如果要关闭自动激活，可以运行命令pyenv deactivate，要重新启用的话，运行pyenv activate 虚拟环境名。 国内网络环境如果下载不下来的话，可以把显示的下载地址的文件用自己电脑的迅雷下载下来，然后传到~/.pyenv/cache里(cache文件夹没有的话手动创建一个），然后执行pyenv install x.x.x就可以了。也可以使用阿里云来加速下载： vim ~/.pip/pip.conf 12345[global]index-url=http://mirrors.aliyun.com/pypi/simple/[install]trusted-host=mirrors.aliyun.com]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
        <tag>pyenv</tag>
        <tag>pyenv-virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS下Python多版本控制软件的安装：pyenv、pyenv-virtualenv]]></title>
    <url>%2Fpython%2F20170507-01-pyenv-macos%2F</url>
    <content type="text"><![CDATA[软件简介： pyenv，是一款特别好用的Python版本管理器，程序员可以建立不同的目录，在不同的目录里分别运行不同版本的Python， 并且互不影响，安装的包也互不影响。github项目地址：https://github.com/yyuu/pyenv pyenv-virtualenv， 是pyenv的一个plugin（插件），可以用来创建基于不同Python版本的干净的虚拟环境。github项目地址：https://github.com/yyuu/pyenv-virtualenv 安装思路：先安装macOS的软件包管理器brew，然后用brew安装pyenv和pyenv-virtualenv 1. 安装brew软件包管理器brew全名Homebrew，是macOS下的一款软件包管理器(macOS没有自己的软件包管理器)，类似于CentOS下面的yum，Ubuntu下的apt-get命令。 1/usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" 2. 用brew命令安装pyenv、pyenv-virtualenv12brew install pyenvbrew install pyenv-virtualenv 会先安装autoconf, pkg-config, openssl, readline 安装过程中我们可以看到一些Caveats（警告），需要我们手动处理一下。首先link readline到系统lib：1brew link readline --force 然后根据Caveats的提示修改环境变量，vim ~/.bash_profile添加下面内容： 123456#pyenv settingsexport PYENV_ROOT=/usr/local/var/pyenvif which pyenv &gt; /dev/null; then eval &quot;$(pyenv init -)&quot;; fi#pyenv-virtualenv settingsif which pyenv-virtualenv-init &gt; /dev/null; then eval &quot;$(pyenv virtualenv-init -)&quot;; fi 上面的两处设置让pyenv和pyenv-virtualenv更好用，用命令的时候可以补全。 设置完关闭终端，然后重启终端，即可生效。 3. pyenv的使用方法 **警告：pyenv安装Python是编译安装的，在使用之前要先安装zlib和SQLite3，要不然安装会报错。 安装zlib和SQLite3并链接： 1234brew install zlibbrew install sqlite3brew link zlib --forcebrew link sqlite3 --force 然后根据Caveats的提示修改环境变量，vim ~/.bash_profile添加下面内容： 12#sqlite3 settingsexport PATH="/usr/local/opt/sqlite/bin:$PATH" 用pyenv --help可以查看pyenv的使用帮助： 常用的几个pyenv命令： pyenv install x.y.z：安装Python，x.y.z是Python的版本，如pyenv install 3.6.2。 pyenv local x.y.z：设置当前目录的Python版本为x.y.z, 如pyenv local 3.6.2。 pyenv versions：查看安装的版本，前面带*号的表示当前目录下正在使用的版本。系统自带的Python是System，后安装的版本的都是版本号。 下面给出使用的例子：示例： 12345678910111213141516171819202122232425$ pyenv install 3.6.2 # 安装3.6.2版本的Python$ pyenv install 2.7.13 # 安装2.7.13版本的Python$ pyenv versions # 可以看到3个版本，system为系统自带的版本* system (set by /usr/local/var/pyenv/version) 2.7.13 3.6.2$ cd # 到家目录$ mkdir Python36 # 创建Python3.6的工作目录$ cd Python36$ pyenv local 3.6.2 # 使当前工作目录使用Python3.6.2版本$ python -V # 查看一下当前目录用Python的版本，确实是3.6.2Python3.6.2$ pip -V # 查看一下pip版本,是3.6的pippip 9.0.1 from /usr/local/var/pyenv/versions/3.6.2/lib/python3.6/site-packages $ cd # 回到家目录$ mkdir Python27 # 创建python2.7的工作目录$ cd Python27$ pyenv local 2.7.13 # 使当前工作目录使用Python2.7.13版本$ python -V # 查看一下当前目录用Python的版本，确实是2.7.13Python 2.7.13$ pip -V # 查看一下pip版本，是2.7的pippip 9.0.1 from /usr/local/var/pyenv/versions/2.7.13/lib/python2.7/site-packages (python 2.7) 4. pyenv-virtualenv的使用方法pyenv-virtualenv是用来创建一个干净的虚拟Python环境的命令，通常在创建干净的新项目时候使用。使用方法如下: 1.创建虚拟环境–pyenv virtualenv 版本号 虚拟环境名。 1$ pyenv virtualenv 3.6.2 venv-3.6.2 创建项目，让项目使用干净的Python3.6.2的虚拟环境： 123456[yulongjun@yulongjun ~]$ mkdir Learning-Python3[yulongjun@yulongjun ~]$ cd Learning-Python3/[yulongjun@yulongjun Learning-Python3]$ pyenv local venv-3.6.2(venv-3.6.2) [yulongjun@yulongjun Learning-Python3]$ cd ..[yulongjun@yulongjun ~]$ cd Learning-Python3/(venv-3.6.2) [yulongjun@yulongjun Learning-Python3]$ 我们会发现：只要我们进入Learning-Python3目录，就会自动激活virtualenv，退出Learning-Python3目录，就会关闭virtualenv。 如果要关闭自动激活，可以运行命令pyenv deactivate，要重新启用的话，运行pyenv activate 虚拟环境名。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>pyenv</tag>
        <tag>pyenv-virtualenv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Service Pack for ProLiant (SPP) Version 2016.10.0 -- HP服务器软件、驱动、固件汇总ISO（包含几乎所有操作系统和Gen6到9的机型）]]></title>
    <url>%2Fserver%2F20170408-01-hp-spp%2F</url>
    <content type="text"><![CDATA[简介：英文介绍页面： http://h17007.www1.hpe.com/us/en/enterprise/servers/products/service_pack/spp/index.aspx Service Pack for ProLiant (SPP)是一个全面的系统软件、驱动和固件更新解决方案，作为单个ISO映像提供。此解决方案使用HP Smart Update Manager（HP SUM）作为部署工具，并可以在所有HPE ProLiant Gen9、Gen8、Gen7、Gen6服务器上使用。 2016.10版本，增加了对下面几个系统的支持： Microsoft Windows Server 2016 Red Hat Enterprise 6.8 VMware vSphere 6.0 U2 使用方法：1. 安装Chrome浏览器并设置为默认浏览器。这里装Chrome浏览器的原因是因为SSP对低IE版本不支持，有些低版本的IE在启动web安装界面时候会有问题。Linux使用的Firefox没有这个问题，可以省略这步。 2. 解压iso并运行hpsumWindows下运行lauch_hpsum.bat，Linux下运行launch_hpsum.sh。 3. 进入浏览器web界面系统会自动弹出IE浏览器窗口，进入SPP界面，刷新固件、安装驱动和相应软件程序请点击Localhost引导式更新(Localhost Guided Update) 4. 选择部署模式 交互式—交互式安装 自动—自动更新全部的驱动、固件等 本次就交互式为例，单击正常（若选自动更新，选择之后则无需继续手动操作，直至自动更新完毕后重启服务器即可） 5. 清点步骤1：释放镜像中的文件并且清点，清点过程中Next按键为灰色不可点击，清点完成后点击下一步。 6. 安装清单步骤2：可升级的固件、驱动和应用软件清单列表已选定，表示现在的SPP里面的版本与已经安装的版本更高强制，表示现在的SPP里面的版本与已经安装的版本相同或者更低就ILO为例可以看到，当前的固件版本为3.10，SPP中最新的版本为3.30，建议更新 注意：可以根据需求强制选中强制或者已选定按钮标识，滑动条会滑动到另一侧.点击部署进入步骤3 7. 开始安装步骤3：开始安装。 8. 重启 安装完成后，方便停机的情况下选择重启重启服务器，固件以及部分驱动更新升效 下载地址这里提供百度网盘下载地址： Service Pack for ProLiant (SPP) Version 2016.10.0 (大小：6.52 GB) MD5 Checksum: 3e93873632c5758666c5b1c305d557ea 链接： http://pan.baidu.com/s/1mhTMZva 密码: bthq]]></content>
      <categories>
        <category>server</category>
      </categories>
      <tags>
        <tag>HP SSP</tag>
        <tag>Service Pack for ProLiant</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[目前所用操作系统版本汇总（持续更新）]]></title>
    <url>%2Fserver%2F20170404-server-operation-system%2F</url>
    <content type="text"><![CDATA[截止目前为止，运维主要用到下面几大类的操作系统。操作系统全用64位版本。Linux一般用小版本号为偶数版的，比较稳定，奇数版据统计不是特别稳定。 在传统企业当中，大部分系统还是用的红帽，但是最近两年，用CentOS和OEL的逐渐多了起来。CentOS方便安装和更新，OEL在Oracle数据库方面配套使用比较好用。 RHEL（Red Hat Enterprise Linux)传统企业中用的最多的版本 RHEL 5.8 （5中用的最多，历史遗留） RHEL 5.10 （新搭建的5，用此版本） RHEL 6.6 （6中用的比较多的版本） RHEL 6.8 （推荐后续安装的6用此版本，在5时代，8是时间最长，用的最多的版本，在6中也会延续） RHEL 7.2 （偶数较为稳定） RHEL 7.3 OEL（Oracle Enterprise Linux）通常与Oracle数据库一起配套安装，针对Oracle做了一些优化。 OEL 6.8 OEL 7.2 CentOS（等同于RHEL，可联网yum更新）CentOS现在传统企业开始用起来，互联网企业很久之前就已经开始用了。 CentOS 6.8 CentOS 7.2 CentOS 7.3 Windows ServerWindows Server 2003 版不再使用，目前主流用的 2008 R2 比较多，没有历史包袱的项目也开始使用 2012 R2 。 Windows Server 2008 R2 Enterprise SP1 VL版 Windows Server 2012 R2 Enterprise with Update VL版 Windows Desktop目前桌面版主用的两大系统，xp、8、8.1 已被摒弃。 Windows 7 Pro SP1 VL版（文件后缀为677816的版本可用，此版之后的版本不易激活） Windows 10 Enterprise 2016 LTSB版（目前最新的企业长期支持版） Windows 激活工具以前一直用的 HEU KMS 激活工具，用了三四年，去年发现一个 microKMS，能激活的系统更全。因此把这两个都贡献出来。 百度网盘下载地址（持续更新）链接: https://pan.baidu.com/s/1c12ZFtI 密码: mgcb]]></content>
      <categories>
        <category>server</category>
      </categories>
      <tags>
        <tag>Windows</tag>
        <tag>CentOS</tag>
        <tag>RHEL</tag>
        <tag>OEL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[新的开始]]></title>
    <url>%2Fcloud%2F20170404-03-start-cloud%2F</url>
    <content type="text"><![CDATA[全新的开始，敬请期待!]]></content>
      <categories>
        <category>cloud</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[新的开始]]></title>
    <url>%2Fsoftware%2F20170404-04-start-software%2F</url>
    <content type="text"><![CDATA[全新的开始，敬请期待!]]></content>
      <categories>
        <category>software</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[新的开始]]></title>
    <url>%2Flinux%2F20170404-01-start%2F</url>
    <content type="text"><![CDATA[全新的开始，敬请期待!]]></content>
  </entry>
</search>
